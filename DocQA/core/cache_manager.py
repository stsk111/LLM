"""
DocQA Pro - ç¼“å­˜ç®¡ç†æ¨¡å—
å®ç°ç´¢å¼•å’Œæ–‡æ¡£çš„æŒä¹…åŒ–ç¼“å­˜
"""

import hashlib
import pickle
import json
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
from langchain_core.documents import Document
from langchain_community.vectorstores import FAISS

from config import CACHE_DIR


class CacheManager:
    """ç´¢å¼•å’Œæ–‡æ¡£ç¼“å­˜ç®¡ç†å™¨"""
    
    def __init__(self, cache_dir: Path = CACHE_DIR):
        """
        åˆå§‹åŒ–ç¼“å­˜ç®¡ç†å™¨
        
        Args:
            cache_dir: ç¼“å­˜ç›®å½•è·¯å¾„
        """
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        print(f"ğŸ“¦ ç¼“å­˜ç®¡ç†å™¨åˆå§‹åŒ–ï¼Œç¼“å­˜ç›®å½•: {self.cache_dir}")
    
    def calculate_file_hash(self, file_path: str) -> str:
        """
        è®¡ç®—æ–‡ä»¶çš„MD5å“ˆå¸Œå€¼
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            
        Returns:
            æ–‡ä»¶çš„MD5å“ˆå¸Œå€¼
        """
        md5_hash = hashlib.md5()
        
        try:
            with open(file_path, "rb") as f:
                # åˆ†å—è¯»å–æ–‡ä»¶ï¼Œé¿å…å¤§æ–‡ä»¶å ç”¨è¿‡å¤šå†…å­˜
                for chunk in iter(lambda: f.read(4096), b""):
                    md5_hash.update(chunk)
            
            return md5_hash.hexdigest()
        
        except Exception as e:
            print(f"âŒ è®¡ç®—æ–‡ä»¶å“ˆå¸Œå¤±è´¥: {e}")
            raise
    
    def get_cache_path(self, file_hash: str) -> Path:
        """
        è·å–ç¼“å­˜æ–‡ä»¶è·¯å¾„
        
        Args:
            file_hash: æ–‡ä»¶å“ˆå¸Œå€¼
            
        Returns:
            ç¼“å­˜ç›®å½•è·¯å¾„
        """
        return self.cache_dir / file_hash
    
    def cache_exists(self, file_path: str) -> bool:
        """
        æ£€æŸ¥ç¼“å­˜æ˜¯å¦å­˜åœ¨
        
        Args:
            file_path: åŸå§‹æ–‡ä»¶è·¯å¾„
            
        Returns:
            ç¼“å­˜æ˜¯å¦å­˜åœ¨
        """
        try:
            file_hash = self.calculate_file_hash(file_path)
            cache_path = self.get_cache_path(file_hash)
            
            # æ£€æŸ¥æ‰€æœ‰å¿…éœ€çš„ç¼“å­˜æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            required_files = [
                cache_path / "index.faiss",
                cache_path / "index.pkl",
                cache_path / "chunks.pkl",
                cache_path / "metadata.json"
            ]
            
            return all(f.exists() for f in required_files)
        
        except Exception as e:
            print(f"âš ï¸  æ£€æŸ¥ç¼“å­˜å¤±è´¥: {e}")
            return False
    
    def save_cache(
        self,
        file_path: str,
        faiss_index: FAISS,
        chunks: List[Document],
        metadata: Dict[str, Any]
    ) -> bool:
        """
        ä¿å­˜ç´¢å¼•å’Œæ–‡æ¡£åˆ°ç¼“å­˜
        
        Args:
            file_path: åŸå§‹æ–‡ä»¶è·¯å¾„
            faiss_index: FAISSç´¢å¼•
            chunks: æ–‡æ¡£ç‰‡æ®µåˆ—è¡¨
            metadata: å…ƒæ•°æ®ï¼ˆé¡µæ•°ã€å—æ•°ç­‰ç»Ÿè®¡ä¿¡æ¯ï¼‰
            
        Returns:
            æ˜¯å¦ä¿å­˜æˆåŠŸ
        """
        try:
            file_hash = self.calculate_file_hash(file_path)
            cache_path = self.get_cache_path(file_hash)
            cache_path.mkdir(parents=True, exist_ok=True)
            
            print(f"ğŸ’¾ ä¿å­˜ç¼“å­˜åˆ°: {cache_path}")
            
            # 1. ä¿å­˜FAISSç´¢å¼•
            faiss_index.save_local(str(cache_path))
            print(f"  âœ“ FAISSç´¢å¼•å·²ä¿å­˜")
            
            # 2. ä¿å­˜chunksï¼ˆç”¨äºé‡å»ºBM25ç´¢å¼•ï¼‰
            chunks_file = cache_path / "chunks.pkl"
            with open(chunks_file, 'wb') as f:
                pickle.dump(chunks, f)
            print(f"  âœ“ æ–‡æ¡£ç‰‡æ®µå·²ä¿å­˜ ({len(chunks)} ä¸ª)")
            
            # 3. ä¿å­˜å…ƒæ•°æ®
            metadata_file = cache_path / "metadata.json"
            metadata_to_save = {
                **metadata,
                'file_path': str(file_path),
                'file_hash': file_hash,
                'cache_version': '1.0'
            }
            with open(metadata_file, 'w', encoding='utf-8') as f:
                json.dump(metadata_to_save, f, ensure_ascii=False, indent=2)
            print(f"  âœ“ å…ƒæ•°æ®å·²ä¿å­˜")
            
            print(f"âœ… ç¼“å­˜ä¿å­˜æˆåŠŸï¼")
            return True
        
        except Exception as e:
            print(f"âŒ ä¿å­˜ç¼“å­˜å¤±è´¥: {e}")
            return False
    
    def load_cache(
        self,
        file_path: str,
        embeddings
    ) -> Optional[Tuple[FAISS, List[Document], Dict[str, Any]]]:
        """
        ä»ç¼“å­˜åŠ è½½ç´¢å¼•å’Œæ–‡æ¡£
        
        Args:
            file_path: åŸå§‹æ–‡ä»¶è·¯å¾„
            embeddings: Embeddingæ¨¡å‹ï¼ˆç”¨äºåŠ è½½FAISSç´¢å¼•ï¼‰
            
        Returns:
            (FAISSç´¢å¼•, æ–‡æ¡£ç‰‡æ®µåˆ—è¡¨, å…ƒæ•°æ®) æˆ– None
        """
        try:
            if not self.cache_exists(file_path):
                return None
            
            file_hash = self.calculate_file_hash(file_path)
            cache_path = self.get_cache_path(file_hash)
            
            print(f"ğŸ“‚ ä»ç¼“å­˜åŠ è½½: {cache_path}")
            
            # 1. åŠ è½½FAISSç´¢å¼•
            faiss_index = FAISS.load_local(
                str(cache_path),
                embeddings,
                allow_dangerous_deserialization=True  # å…è®¸ååºåˆ—åŒ–æœ¬åœ°æ–‡ä»¶
            )
            print(f"  âœ“ FAISSç´¢å¼•å·²åŠ è½½")
            
            # 2. åŠ è½½chunks
            chunks_file = cache_path / "chunks.pkl"
            with open(chunks_file, 'rb') as f:
                chunks = pickle.load(f)
            print(f"  âœ“ æ–‡æ¡£ç‰‡æ®µå·²åŠ è½½ ({len(chunks)} ä¸ª)")
            
            # 3. åŠ è½½å…ƒæ•°æ®
            metadata_file = cache_path / "metadata.json"
            with open(metadata_file, 'r', encoding='utf-8') as f:
                metadata = json.load(f)
            print(f"  âœ“ å…ƒæ•°æ®å·²åŠ è½½")
            
            print(f"âœ… ç¼“å­˜åŠ è½½æˆåŠŸï¼")
            return faiss_index, chunks, metadata
        
        except Exception as e:
            print(f"âŒ åŠ è½½ç¼“å­˜å¤±è´¥: {e}")
            return None
    
    def clear_cache(self, file_path: Optional[str] = None) -> bool:
        """
        æ¸…é™¤ç¼“å­˜
        
        Args:
            file_path: è¦æ¸…é™¤çš„æ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™æ¸…é™¤æ‰€æœ‰ç¼“å­˜
            
        Returns:
            æ˜¯å¦æ¸…é™¤æˆåŠŸ
        """
        try:
            if file_path:
                # æ¸…é™¤æŒ‡å®šæ–‡ä»¶çš„ç¼“å­˜
                file_hash = self.calculate_file_hash(file_path)
                cache_path = self.get_cache_path(file_hash)
                
                if cache_path.exists():
                    import shutil
                    shutil.rmtree(cache_path)
                    print(f"ğŸ—‘ï¸  å·²æ¸…é™¤ç¼“å­˜: {cache_path}")
            else:
                # æ¸…é™¤æ‰€æœ‰ç¼“å­˜
                if self.cache_dir.exists():
                    import shutil
                    for item in self.cache_dir.iterdir():
                        if item.is_dir():
                            shutil.rmtree(item)
                    print(f"ğŸ—‘ï¸  å·²æ¸…é™¤æ‰€æœ‰ç¼“å­˜")
            
            return True
        
        except Exception as e:
            print(f"âŒ æ¸…é™¤ç¼“å­˜å¤±è´¥: {e}")
            return False
    
    def list_caches(self) -> List[Dict[str, Any]]:
        """
        åˆ—å‡ºæ‰€æœ‰ç¼“å­˜
        
        Returns:
            ç¼“å­˜ä¿¡æ¯åˆ—è¡¨
        """
        caches = []
        
        try:
            if not self.cache_dir.exists():
                return caches
            
            for cache_dir in self.cache_dir.iterdir():
                if cache_dir.is_dir():
                    metadata_file = cache_dir / "metadata.json"
                    if metadata_file.exists():
                        with open(metadata_file, 'r', encoding='utf-8') as f:
                            metadata = json.load(f)
                            caches.append({
                                'hash': cache_dir.name,
                                'file_path': metadata.get('file_path', 'Unknown'),
                                'total_pages': metadata.get('total_pages', 0),
                                'total_chunks': metadata.get('total_chunks', 0),
                                'cache_dir': str(cache_dir)
                            })
        
        except Exception as e:
            print(f"âš ï¸  åˆ—å‡ºç¼“å­˜å¤±è´¥: {e}")
        
        return caches
    
    def get_cache_size(self) -> int:
        """
        è·å–ç¼“å­˜æ€»å¤§å°ï¼ˆå­—èŠ‚ï¼‰
        
        Returns:
            ç¼“å­˜æ€»å¤§å°
        """
        total_size = 0
        
        try:
            if not self.cache_dir.exists():
                return 0
            
            for item in self.cache_dir.rglob('*'):
                if item.is_file():
                    total_size += item.stat().st_size
        
        except Exception as e:
            print(f"âš ï¸  è®¡ç®—ç¼“å­˜å¤§å°å¤±è´¥: {e}")
        
        return total_size
    
    def format_cache_size(self, size_bytes: int) -> str:
        """
        æ ¼å¼åŒ–ç¼“å­˜å¤§å°
        
        Args:
            size_bytes: å­—èŠ‚æ•°
            
        Returns:
            æ ¼å¼åŒ–çš„å¤§å°å­—ç¬¦ä¸²
        """
        for unit in ['B', 'KB', 'MB', 'GB']:
            if size_bytes < 1024.0:
                return f"{size_bytes:.2f} {unit}"
            size_bytes /= 1024.0
        return f"{size_bytes:.2f} TB"


def create_cache_manager(cache_dir: Path = CACHE_DIR) -> CacheManager:
    """
    åˆ›å»ºç¼“å­˜ç®¡ç†å™¨çš„ä¾¿æ·å‡½æ•°
    
    Args:
        cache_dir: ç¼“å­˜ç›®å½•è·¯å¾„
        
    Returns:
        ç¼“å­˜ç®¡ç†å™¨å®ä¾‹
    """
    return CacheManager(cache_dir)
