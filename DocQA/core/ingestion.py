"""
DocQA Pro - æ–‡æ¡£æ‘„å–å¤„ç†æ¨¡å—
PDFè§£æžä¸Žæ–‡æœ¬åˆ‡åˆ†åŠŸèƒ½ (æ”¯æŒç¼“å­˜)
"""

import os
import json
import hashlib
from pathlib import Path
from typing import List, Dict, Any, Optional, Callable
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document

from config import CHUNK_SIZE, CHUNK_OVERLAP, MAX_FILE_SIZE_MB

# ç¼“å­˜ç›®å½•é…ç½®
CACHE_DIR = ".cache/ingestion_chunks"

class PDFIngestionPipeline:
    """PDFæ–‡æ¡£å¤„ç†ç®¡é“"""
    
    def __init__(
        self,
        chunk_size: int = CHUNK_SIZE,
        chunk_overlap: int = CHUNK_OVERLAP,
        progress_callback: Optional[Callable[[str, int, int], None]] = None,
        use_cache: bool = True  # æ–°å¢žç¼“å­˜å¼€å…³
    ):
        """
        åˆå§‹åŒ–PDFå¤„ç†ç®¡é“
        """
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.progress_callback = progress_callback
        self.use_cache = use_cache
        
        # ç¡®ä¿ç¼“å­˜ç›®å½•å­˜åœ¨
        if self.use_cache:
            os.makedirs(CACHE_DIR, exist_ok=True)
        
        # åˆå§‹åŒ–æ–‡æœ¬åˆ‡åˆ†å™¨
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            length_function=len,
            separators=["\n\n", "\n", " ", ""]
        )
    
    def _update_progress(self, message: str, current: int = 0, total: int = 100):
        """æ›´æ–°è¿›åº¦"""
        if self.progress_callback:
            self.progress_callback(message, current, total)

    def _calculate_file_hash(self, file_path: str) -> str:
        """è®¡ç®—æ–‡ä»¶çš„MD5å“ˆå¸Œå€¼ï¼Œç”¨äºŽç¼“å­˜é”®"""
        hash_md5 = hashlib.md5()
        with open(file_path, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hash_md5.update(chunk)
        # å°†åˆ†å—å‚æ•°ä¹ŸåŠ å…¥å“ˆå¸Œï¼Œå¦‚æžœå‚æ•°å˜äº†ï¼Œç¼“å­˜ä¹Ÿåº”è¯¥å¤±æ•ˆ
        params = f"{self.chunk_size}_{self.chunk_overlap}"
        hash_md5.update(params.encode('utf-8'))
        return hash_md5.hexdigest()

    def _save_to_cache(self, file_hash: str, result: Dict[str, Any]):
        """å°†å¤„ç†ç»“æžœä¿å­˜åˆ°ç£ç›˜ JSON"""
        cache_path = os.path.join(CACHE_DIR, f"{file_hash}.json")
        
        # Document å¯¹è±¡ä¸èƒ½ç›´æŽ¥ JSON åºåˆ—åŒ–ï¼Œéœ€è¦è½¬ dict
        serializable_result = result.copy()
        if "chunks" in serializable_result:
            serializable_result["chunks"] = [
                {
                    "page_content": doc.page_content,
                    "metadata": doc.metadata,
                    "type": "Document"
                } 
                for doc in serializable_result["chunks"]
            ]
            
        with open(cache_path, 'w', encoding='utf-8') as f:
            json.dump(serializable_result, f, ensure_ascii=False, indent=2)

    def _load_from_cache(self, file_hash: str) -> Optional[Dict[str, Any]]:
        """ä»Žç£ç›˜åŠ è½½ç¼“å­˜"""
        cache_path = os.path.join(CACHE_DIR, f"{file_hash}.json")
        if not os.path.exists(cache_path):
            return None
            
        try:
            with open(cache_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # å°† dict é‡æ–°è½¬å›ž LangChain Document å¯¹è±¡
            if "chunks" in data:
                data["chunks"] = [
                    Document(page_content=item["page_content"], metadata=item["metadata"])
                    for item in data["chunks"]
                ]
            return data
        except Exception as e:
            print(f"âš ï¸ ç¼“å­˜è¯»å–å¤±è´¥ (å°†é‡æ–°å¤„ç†): {e}")
            return None

    def validate_pdf(self, file_path: str) -> Dict[str, Any]:
        """éªŒè¯PDFæ–‡ä»¶ (é€»è¾‘ä¿æŒä¸å˜)"""
        path = Path(file_path)
        if not path.exists():
            return {"valid": False, "error": f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}"}
        if path.suffix.lower() != '.pdf':
            return {"valid": False, "error": "åªæ”¯æŒPDFæ–‡ä»¶æ ¼å¼"}
        try:
            file_size_mb = path.stat().st_size / (1024 * 1024)
            if file_size_mb > MAX_FILE_SIZE_MB:
                return {"valid": False, "error": f"æ–‡ä»¶è¿‡å¤§: {file_size_mb:.1f}MB"}
        except OSError:
            return {"valid": False, "error": "æ— æ³•è¯»å–æ–‡ä»¶ä¿¡æ¯"}
        return {"valid": True, "size_mb": file_size_mb, "name": path.name}
    
    def load_pdf(self, file_path: str) -> List[Document]:
        """åŠ è½½PDFå¹¶æå–æ–‡æœ¬ (é€»è¾‘ä¿æŒä¸å˜)"""
        # ... (æ­¤å¤„çœç•¥æœªæ”¹åŠ¨çš„ä»£ç ï¼Œä¸Žä½ åŽŸç‰ˆä¸€è‡´ï¼Œç›´æŽ¥å¤ç”¨å³å¯) ...
        # ä¸ºäº†ä»£ç ç®€æ´ï¼Œè¯·å°†åŽŸæ¥çš„ load_pdf å®Œæ•´ä»£ç ä¿ç•™åœ¨è¿™é‡Œ
        validation = self.validate_pdf(file_path)
        if not validation["valid"]:
            raise ValueError(validation["error"])
        
        self._update_progress("å¼€å§‹åŠ è½½PDFæ–‡ä»¶...", 0, 100)
        try:
            loader = PyPDFLoader(file_path)
            pages = loader.load()
            self._update_progress("PDFåŠ è½½å®Œæˆï¼Œå¼€å§‹å¤„ç†é¡µé¢...", 30, 100)
            processed_docs = []
            for i, doc in enumerate(pages):
                doc.metadata.update({
                    "page": i + 1, "source": file_path, "total_pages": len(pages)
                })
                processed_docs.append(doc)
            self._update_progress("æ–‡æ¡£åŠ è½½å®Œæˆ", 100, 100)
            return processed_docs
        except Exception as e:
            self._update_progress(f"PDFåŠ è½½å¤±è´¥: {str(e)}", 0, 100)
            raise RuntimeError(str(e))

    def chunk_documents(self, documents: List[Document]) -> List[Document]:
        """åˆ‡åˆ†æ–‡æ¡£ (é€»è¾‘ä¿æŒä¸å˜)"""
        # ... (æ­¤å¤„çœç•¥æœªæ”¹åŠ¨çš„ä»£ç ï¼Œä¸Žä½ åŽŸç‰ˆä¸€è‡´ï¼Œç›´æŽ¥å¤ç”¨å³å¯) ...
        # è¯·å°†åŽŸæ¥çš„ chunk_documents å®Œæ•´ä»£ç ä¿ç•™åœ¨è¿™é‡Œ
        if not documents: return []
        self._update_progress("å¼€å§‹æ–‡æœ¬åˆ‡åˆ†...", 0, 100)
        try:
            chunked_docs = []
            total_docs = len(documents)
            for i, doc in enumerate(documents):
                chunks = self.text_splitter.split_documents([doc])
                for j, chunk in enumerate(chunks):
                    chunk.metadata.update({
                        "chunk_id": f"page_{doc.metadata['page']}_chunk_{j}",
                        "chunk_index": j,
                        "total_chunks_in_page": len(chunks)
                    })
                chunked_docs.extend(chunks)
                self._update_progress(f"æ­£åœ¨åˆ‡åˆ†ç¬¬ {i+1}/{total_docs} é¡µ...", int((i+1)*100/total_docs), 100)
            self._update_progress(f"æ–‡æœ¬åˆ‡åˆ†å®Œæˆ", 100, 100)
            return chunked_docs
        except Exception as e:
            self._update_progress(f"æ–‡æœ¬åˆ‡åˆ†å¤±è´¥: {e}", 0, 100)
            raise RuntimeError(str(e))

    def process_pdf(self, file_path: str) -> Dict[str, Any]:
        """
        å®Œæ•´å¤„ç†PDFæ–‡æ¡£ (å·²é›†æˆç¼“å­˜é€»è¾‘)
        """
        try:
            # 1. è®¡ç®—å“ˆå¸Œï¼Œå°è¯•è¯»å–ç¼“å­˜
            if self.use_cache:
                file_hash = self._calculate_file_hash(file_path)
                cached_result = self._load_from_cache(file_hash)
                
                if cached_result:
                    self._update_progress("ðŸš€ å‘½ä¸­ç¼“å­˜ï¼Œç›´æŽ¥åŠ è½½å¤„ç†ç»“æžœ...", 100, 100)
                    return cached_result

            # 2. ç¼“å­˜æœªå‘½ä¸­ï¼Œæ‰§è¡Œå¸¸è§„æµç¨‹
            documents = self.load_pdf(file_path)
            chunks = self.chunk_documents(documents)
            
            stats = {
                "total_pages": len(documents),
                "total_chunks": len(chunks),
                "avg_chunks_per_page": len(chunks) / len(documents) if documents else 0,
                "chunk_size": self.chunk_size,
                "chunk_overlap": self.chunk_overlap
            }
            
            result = {
                "success": True,
                "chunks": chunks,
                "stats": stats,
                "message": f"PDFå¤„ç†å®Œæˆï¼š{stats['total_pages']} é¡µ -> {stats['total_chunks']} ä¸ªæ–‡æœ¬å—"
            }

            # 3. ä¿å­˜åˆ°ç¼“å­˜
            if self.use_cache:
                self._save_to_cache(file_hash, result)

            return result
            
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "chunks": [],
                "stats": {}
            }

def create_pdf_pipeline(progress_callback: Optional[Callable] = None, use_cache: bool = True) -> PDFIngestionPipeline:
    return PDFIngestionPipeline(progress_callback=progress_callback, use_cache=use_cache)