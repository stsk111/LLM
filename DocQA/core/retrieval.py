"""
DocQA Pro - æ£€ç´¢æ¨¡å—
Embeddingã€å‘é‡ç´¢å¼•ã€æ··åˆæ£€ç´¢åŠŸèƒ½
"""

import os
import pickle
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
import numpy as np

from langchain_community.vectorstores import FAISS
from langchain_community.retrievers import BM25Retriever
from langchain_core.documents import Document
from langchain_core.embeddings import Embeddings

from config import (
    EMBEDDING_MODEL_PATH, EMBEDDING_DEVICE, EMBEDDING_BATCH_SIZE,
    FAISS_INDEX_TYPE, FAISS_USE_GPU, DENSE_WEIGHT, SPARSE_WEIGHT,
    RETRIEVAL_TOP_K, CACHE_DIR
)


class LangChainEmbeddingsWrapper(Embeddings):
    """LangChain Embeddingsæ¥å£åŒ…è£…å™¨"""
    
    def __init__(self, model):
        """
        åˆå§‹åŒ–åŒ…è£…å™¨
        
        Args:
            model: SentenceTransformeræ¨¡å‹å®ä¾‹
        """
        self.model = model
    
    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """å¯¹æ–‡æœ¬åˆ—è¡¨è¿›è¡Œå‘é‡åŒ–"""
        embeddings = self.model.encode(
            texts,
            batch_size=EMBEDDING_BATCH_SIZE,
            normalize_embeddings=True,
            show_progress_bar=False
        )
        return embeddings.tolist()
    
    def embed_query(self, text: str) -> List[float]:
        """å¯¹å•ä¸ªæŸ¥è¯¢è¿›è¡Œå‘é‡åŒ–"""
        embedding = self.model.encode(
            text,
            normalize_embeddings=True,
            show_progress_bar=False
        )
        return embedding.tolist()


class EmbeddingEngine:
    """Embeddingæ¨¡å‹å¼•æ“"""
    
    def __init__(self, model_path: str = str(EMBEDDING_MODEL_PATH)):
        """
        åˆå§‹åŒ–Embeddingæ¨¡å‹
        
        Args:
            model_path: æ¨¡å‹è·¯å¾„
        """
        self.model_path = model_path
        self.embeddings = None
        self._load_model()
    
    def _load_model(self):
        """åŠ è½½æœ¬åœ°Embeddingæ¨¡å‹"""
        try:
            print(f"ğŸ”„ åŠ è½½Embeddingæ¨¡å‹: {self.model_path}")
            
            # åœ¨åŠ è½½å‰å†æ¬¡ç¡®ä¿ç¯å¢ƒå˜é‡ç”Ÿæ•ˆ
            import os
            os.environ['TORCH_LOAD_WEIGHTS_ONLY'] = '0'
            os.environ['TORCH_ALLOW_VULNERABLE_LOAD'] = '1'
            
            # ä½¿ç”¨sentence-transformersç›´æ¥åŠ è½½
            from sentence_transformers import SentenceTransformer
            
            # åŠ è½½æ¨¡å‹ï¼Œæ˜¾å¼ä¼ é€’æƒé‡åŠ è½½å‚æ•°ï¼ˆå¦‚æœåº“æ”¯æŒï¼‰
            self.model = SentenceTransformer(
                self.model_path,
                device=EMBEDDING_DEVICE,
                trust_remote_code=True,
                model_kwargs={"weights_only": False}
            )
            
            # åˆ›å»ºLangChainå…¼å®¹çš„embeddingsåŒ…è£…å™¨
            self.embeddings = LangChainEmbeddingsWrapper(self.model)
            
            print("âœ… Embeddingæ¨¡å‹åŠ è½½æˆåŠŸ")
            
        except Exception as e:
            print(f"âŒ Embeddingæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise
    
    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """å¯¹æ–‡æœ¬åˆ—è¡¨è¿›è¡Œå‘é‡åŒ–ï¼ˆç›´æ¥è°ƒç”¨æ¥å£ï¼‰"""
        return self.embeddings.embed_documents(texts)
    
    def embed_query(self, text: str) -> List[float]:
        """å¯¹å•ä¸ªæŸ¥è¯¢è¿›è¡Œå‘é‡åŒ–ï¼ˆç›´æ¥è°ƒç”¨æ¥å£ï¼‰"""
        return self.embeddings.embed_query(text)
    


class FAISSIndexBuilder:
    """FAISSå‘é‡ç´¢å¼•æ„å»ºå™¨"""
    
    def __init__(self, embedding_engine: EmbeddingEngine):
        """
        åˆå§‹åŒ–ç´¢å¼•æ„å»ºå™¨
        
        Args:
            embedding_engine: Embeddingå¼•æ“
        """
        self.embedding_engine = embedding_engine
        self.vector_store = None
    
    def create_index(self, chunks: List[Document]) -> FAISS:
        """
        æ„å»ºFAISSå‘é‡ç´¢å¼•
        
        Args:
            chunks: æ–‡æ¡£ç‰‡æ®µåˆ—è¡¨
            
        Returns:
            FAISSå‘é‡å­˜å‚¨
        """
        if not chunks:
            raise ValueError("æ–‡æ¡£ç‰‡æ®µåˆ—è¡¨ä¸èƒ½ä¸ºç©º")
        
        try:
            print(f"ğŸ”„ æ„å»ºFAISSç´¢å¼•ï¼Œå…± {len(chunks)} ä¸ªæ–‡æ¡£ç‰‡æ®µ...")
            
            # ä½¿ç”¨FAISSæ„å»ºå‘é‡å­˜å‚¨
            self.vector_store = FAISS.from_documents(
                chunks, 
                self.embedding_engine.embeddings
            )
            
            print("âœ… FAISSç´¢å¼•æ„å»ºæˆåŠŸ")
            return self.vector_store
            
        except Exception as e:
            print(f"âŒ FAISSç´¢å¼•æ„å»ºå¤±è´¥: {e}")
            raise
    
    def save_index(self, index: FAISS, save_path: str):
        """
        ä¿å­˜ç´¢å¼•åˆ°æœ¬åœ°
        
        Args:
            index: FAISSç´¢å¼•
            save_path: ä¿å­˜è·¯å¾„
        """
        try:
            Path(save_path).parent.mkdir(parents=True, exist_ok=True)
            index.save_local(save_path)
            print(f"âœ… ç´¢å¼•å·²ä¿å­˜åˆ°: {save_path}")
        except Exception as e:
            print(f"âŒ ç´¢å¼•ä¿å­˜å¤±è´¥: {e}")
            raise
    
    def load_index(self, load_path: str) -> FAISS:
        """
        ä»æœ¬åœ°åŠ è½½ç´¢å¼•
        
        Args:
            load_path: åŠ è½½è·¯å¾„
            
        Returns:
            FAISSç´¢å¼•
        """
        try:
            if not Path(load_path).exists():
                raise FileNotFoundError(f"ç´¢å¼•æ–‡ä»¶ä¸å­˜åœ¨: {load_path}")
            
            self.vector_store = FAISS.load_local(
                load_path, 
                self.embedding_engine.embeddings
            )
            print(f"âœ… ç´¢å¼•å·²åŠ è½½: {load_path}")
            return self.vector_store
            
        except Exception as e:
            print(f"âŒ ç´¢å¼•åŠ è½½å¤±è´¥: {e}")
            raise


class HybridRetriever:
    """æ··åˆæ£€ç´¢å™¨ (Dense + Sparse)"""
    
    def __init__(self, faiss_index: FAISS, documents: List[Document]):
        """
        åˆå§‹åŒ–æ··åˆæ£€ç´¢å™¨
        
        Args:
            faiss_index: FAISSå‘é‡ç´¢å¼•
            documents: åŸå§‹æ–‡æ¡£åˆ—è¡¨ï¼ˆç”¨äºBM25ï¼‰
        """
        self.faiss_index = faiss_index
        self.documents = documents
        
        # åˆå§‹åŒ–ç¨ å¯†æ£€ç´¢å™¨ï¼ˆFAISSï¼‰
        self.dense_retriever = faiss_index.as_retriever(
            search_kwargs={"k": RETRIEVAL_TOP_K}
        )
        self.faiss_index = faiss_index  # ä¿å­˜åŸå§‹ç´¢å¼•ç”¨äºç›´æ¥æŸ¥è¯¢
        
        # åˆå§‹åŒ–ç¨€ç–æ£€ç´¢å™¨ï¼ˆBM25ï¼‰
        self.sparse_retriever = BM25Retriever.from_documents(documents)
        self.sparse_retriever.k = RETRIEVAL_TOP_K
        
        print("âœ… æ··åˆæ£€ç´¢å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def retrieve(self, query: str, top_k: int = RETRIEVAL_TOP_K) -> List[Document]:
        """
        æ‰§è¡Œæ··åˆæ£€ç´¢
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›ç»“æœæ•°é‡
            
        Returns:
            æ£€ç´¢ç»“æœæ–‡æ¡£åˆ—è¡¨
        """
        try:
            # ç¨ å¯†æ£€ç´¢ï¼ˆå‘é‡ç›¸ä¼¼åº¦ï¼‰- ä½¿ç”¨invokeä»£æ›¿get_relevant_documents
            dense_results = self.dense_retriever.invoke(query)
            
            # ç¨€ç–æ£€ç´¢ï¼ˆBM25å…³é”®è¯åŒ¹é…ï¼‰
            sparse_results = self.sparse_retriever.invoke(query)
            
            # ä½¿ç”¨RRFï¼ˆReciprocal Rank Fusionï¼‰èåˆç»“æœ
            fused_results = self._fuse_results(
                dense_results, sparse_results, 
                DENSE_WEIGHT, SPARSE_WEIGHT
            )
            
            # è¿”å›Top-Kç»“æœ
            return fused_results[:top_k]
            
        except Exception as e:
            print(f"âŒ æ··åˆæ£€ç´¢å¤±è´¥: {e}")
            raise
    
    def _fuse_results(
        self, 
        dense_results: List[Document], 
        sparse_results: List[Document],
        dense_weight: float = 0.5,
        sparse_weight: float = 0.5
    ) -> List[Document]:
        """
        èåˆç¨ å¯†å’Œç¨€ç–æ£€ç´¢ç»“æœ
        
        Args:
            dense_results: ç¨ å¯†æ£€ç´¢ç»“æœ
            sparse_results: ç¨€ç–æ£€ç´¢ç»“æœ
            dense_weight: ç¨ å¯†æ£€ç´¢æƒé‡
            sparse_weight: ç¨€ç–æ£€ç´¢æƒé‡
            
        Returns:
            èåˆåçš„ç»“æœ
        """
        # ä½¿ç”¨RRFç®—æ³•èåˆç»“æœ
        doc_scores = {}
        
        # è®¡ç®—ç¨ å¯†æ£€ç´¢åˆ†æ•°
        for i, doc in enumerate(dense_results):
            doc_id = self._get_doc_id(doc)
            rrf_score = dense_weight / (60 + i + 1)  # RRFå…¬å¼ï¼Œk=60
            doc_scores[doc_id] = doc_scores.get(doc_id, 0) + rrf_score
        
        # è®¡ç®—ç¨€ç–æ£€ç´¢åˆ†æ•°
        for i, doc in enumerate(sparse_results):
            doc_id = self._get_doc_id(doc)
            rrf_score = sparse_weight / (60 + i + 1)
            doc_scores[doc_id] = doc_scores.get(doc_id, 0) + rrf_score
        
        # æŒ‰åˆ†æ•°æ’åº
        sorted_docs = sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)
        
        # æ ¹æ®doc_idæ‰¾å›åŸæ–‡æ¡£
        id_to_doc = {}
        for doc in dense_results + sparse_results:
            doc_id = self._get_doc_id(doc)
            if doc_id not in id_to_doc:
                id_to_doc[doc_id] = doc
        
        # è¿”å›æ’åºåçš„æ–‡æ¡£
        fused_results = []
        for doc_id, score in sorted_docs:
            if doc_id in id_to_doc:
                doc = id_to_doc[doc_id]
                # æ·»åŠ èåˆåˆ†æ•°åˆ°metadata
                doc.metadata['fusion_score'] = score
                fused_results.append(doc)
        
        return fused_results
    
    def _get_doc_id(self, doc: Document) -> str:
        """è·å–æ–‡æ¡£å”¯ä¸€æ ‡è¯†"""
        # ä½¿ç”¨chunk_idæˆ–è€…é¡µç +å†…å®¹hashä½œä¸ºå”¯ä¸€æ ‡è¯†
        if 'chunk_id' in doc.metadata:
            return doc.metadata['chunk_id']
        else:
            # å¤‡ç”¨æ–¹æ¡ˆï¼šä½¿ç”¨é¡µç å’Œå†…å®¹hash
            page = doc.metadata.get('page', 0)
            content_hash = hash(doc.page_content[:100])  # ä½¿ç”¨å‰100å­—ç¬¦çš„hash
            return f"page_{page}_hash_{content_hash}"


def create_retrieval_system(chunks: List[Document]) -> Tuple[EmbeddingEngine, FAISSIndexBuilder, HybridRetriever]:
    """
    åˆ›å»ºå®Œæ•´çš„æ£€ç´¢ç³»ç»Ÿ
    
    Args:
        chunks: æ–‡æ¡£ç‰‡æ®µåˆ—è¡¨
        
    Returns:
        (Embeddingå¼•æ“, ç´¢å¼•æ„å»ºå™¨, æ··åˆæ£€ç´¢å™¨)
    """
    # åˆ›å»ºEmbeddingå¼•æ“
    embedding_engine = EmbeddingEngine()
    
    # åˆ›å»ºFAISSç´¢å¼•
    index_builder = FAISSIndexBuilder(embedding_engine)
    faiss_index = index_builder.create_index(chunks)
    
    # åˆ›å»ºæ··åˆæ£€ç´¢å™¨
    hybrid_retriever = HybridRetriever(faiss_index, chunks)
    
    return embedding_engine, index_builder, hybrid_retriever