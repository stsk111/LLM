[
  {
    "id": "0000",
    "question": "SFT 模型通常需要多少块 GPU 来完成训练？",
    "ground_truth": "通常需要数十块GPU，花费数天时间完成训练。",
    "contexts": [
      "根据模型的大小和训练数据量，通常需要数十块GPU，花费数天时间完成训练。"
    ],
    "question_type": "simple",
    "page_num": 21,
    "source_file": "doc00_11_33.pdf"
  },
  {
    "id": "0001",
    "question": "生成过程是如何进行的？",
    "ground_truth": "w1w2 · · · wm 的生成过程可以看作单词逐个生成的过程。首先生成 w1，之后根据 w1 生成 w2，然后根据 w1 和 w2 生成 w3，依此类推，根据前 m − 1 个单词生成最后一个单词 wm。",
    "contexts": [
      "w1w2 · · · wm 的生成过程可以看作单词逐个生成的过程。首先生成 w1，之后根据 w1 生成 w2，然后根据 w1 和 w2 生成 w3，依此类推，根据前 m − 1 个单词生成最后一个单词 wm。"
    ],
    "question_type": "simple",
    "page_num": 12,
    "source_file": "doc00_11_33.pdf"
  },
  {
    "id": "0002",
    "question": "Baichuan2的发布月份是什么？",
    "ground_truth": "2023 年 9 月",
    "contexts": [
      "Baichuan2 2023 年 9 月 百川 530 亿 对话模型"
    ],
    "question_type": "simple",
    "page_num": 19,
    "source_file": "doc00_11_33.pdf"
  },
  {
    "id": "0003",
    "question": "What function is used to apply softmax to the scores?",
    "ground_truth": "scores = F.softmax(scores, dim =-1)",
    "contexts": [
      "scores = F.softmax(scores, dim =-1)"
    ],
    "question_type": "simple",
    "page_num": 32,
    "source_file": "doc00_11_33.pdf"
  },
  {
    "id": "0004",
    "question": "GPT-3 完成一次训练的总计算量是多少？",
    "ground_truth": "3640PFLOPS",
    "contexts": [
      "根据文献 [39] 中的介绍，GPT-3 完成一次训练的总计算量是 3640PFLOPS， 按照NVIDIA A100 80GB GPU 和平均利用率达到 50% 计算， 需要花费近一个月的时间使用1000 块 GPU 完成。"
    ],
    "question_type": "simple",
    "page_num": 20,
    "source_file": "doc00_11_33.pdf"
  },
  {
    "id": "0005",
    "question": "有监督微调模型具备哪些能力？",
    "ground_truth": "经过训练的SFT模型具备初步的指令理解能力和上下文理解能力，能够完成开放领域问答、阅读理解、翻译、生成代码等任务，也具备了一定的对未知任务的泛化能力。",
    "contexts": [
      "经过训练的SFT模型具备初步的指令理解能力和上下文理解能力，能够完成开放领域问答、阅读理解、翻译、生成代码等任务，也具备了一定的对未知任务的泛化能力。"
    ],
    "question_type": "simple",
    "page_num": 21,
    "source_file": "doc00_11_33.pdf"
  },
  {
    "id": "0006",
    "question": "大语言模型的构建流程主要包含几个阶段？",
    "ground_truth": "主要包含四个阶段：预训练、 有监督微调、 奖励建模和强化学习。",
    "contexts": [
      "OpenAI 使用的大语言模型构建流程主要包含四个阶段：预训练、 有监督微调、 奖励建模和强化学习。"
    ],
    "question_type": "simple",
    "page_num": 20,
    "source_file": "doc00_11_33.pdf"
  },
  {
    "id": "0007",
    "question": "第 5 章重点介绍什么技术？",
    "ground_truth": "模型微调技术，有监督微调数据的构造策略以及高效微调方法：LoRA、Delta Tuning 等方法。",
    "contexts": [
      "第 5 章重点介绍模型微调技术，有监督微调数据的构造策略以及高效微调方法：LoRA、Delta Tuning 等方法；"
    ],
    "question_type": "simple",
    "page_num": 24,
    "source_file": "doc00_11_33.pdf"
  },
  {
    "id": "0008",
    "question": "残差连接的主要作用是什么？",
    "ground_truth": "避免在优化过程中因网络过深而产生潜在的梯度消失问题。",
    "contexts": [
      "残差连接主要是指使用一条直连通道直接将对应子层的输入连接到输出，避免在优化过程中因网络过深而产生潜在的梯度消失问题。"
    ],
    "question_type": "simple",
    "page_num": 33,
    "source_file": "doc00_11_33.pdf"
  },
  {
    "id": "0009",
    "question": "在大语言模型的指令理解阶段，如何通过有监督微调和强化学习的方法提升模型的输出质量？",
    "ground_truth": "通过有监督微调和强化学习方法，可以使模型理解指令并生成类人回答，从而提升输出质量。",
    "contexts": [
      "第 5 章和第 6 章聚焦于大语言模型指令理解阶段的核心研究内容，探讨如何通过有监督微调和强化学习方法，使模型能够理解指令并生成类人回答。",
      "如果奖励模型的目标是针对系统所有的输出都能够高质量地进行判决，那么该问题的难度在某种程度上与文本生成等价。"
    ],
    "question_type": "multi_context",
    "page_num": 24,
    "source_file": "doc00_11_33.pdf"
  },
  {
    "id": "0010",
    "question": "在大语言模型中，如何通过 n 元语法模型和平滑技术来解决零概率问题？",
    "ground_truth": "n 元语法模型通过假设任意单词的出现概率只与过去 n-1 个词相关，从而减少了模型参数空间的复杂性。然而，尽管 n 元语言模型能缓解句子概率为零的问题，语言的多样性使得无法覆盖所有 n-gram。这时候，需要使用平滑技术来为所有可能出现的字符串分配一个非零的概率值，以避免零概率问题。平滑处理的基本思想是调整最大似然估计，提高低概率事件，降低高概率事件，使整体的概率分布趋于均匀。",
    "contexts": [
      "n 元语法或 n 元文法（n-gram）模型，其中，n-gram 表示由 n 个连续单词构成的单元。",
      "平滑是指为了产生更合理的概率，对最大似然估计进行调整的一类方法，也称为数据平滑（Data Smoothing）。"
    ],
    "question_type": "multi_context",
    "page_num": 12,
    "source_file": "doc00_11_33.pdf"
  },
  {
    "id": "0011",
    "question": "在自然语言处理领域，如何克服 n 元语言模型的局限性以实现更好的性能？",
    "ground_truth": "通过应用预训练语言模型（PLM）的方法，例如使用基于 Transformer 结构的 GPT 和 BERT 模型，可以有效克服 n 元语言模型的局限性。预训练模型能够通过海量数据学习有效的特征表示，并且在下游任务中只需进行微调，避免了 n 元模型对上下文长度的限制和对人工规则的依赖。",
    "contexts": [
      "以GPT和BERT为代表的基于 Transformer 结构的大规模预训练语言模型的出现，使自然语言处理全面进入预训练微调范式新时代。",
      "n 元语言模型仍然有以下三个较为明显的缺点。（1）无法对长度超过 n 的上下文建模。"
    ],
    "question_type": "multi_context",
    "page_num": 13,
    "source_file": "doc00_11_33.pdf"
  },
  {
    "id": "0012",
    "question": "模型的参数维度是什么？",
    "ground_truth": "512",
    "contexts": [
      "d_model = 512"
    ],
    "question_type": "simple",
    "page_num": 39,
    "source_file": "doc01_34_57.pdf"
  },
  {
    "id": "0013",
    "question": "什么函数用于将输入 x 进行旋转处理？",
    "ground_truth": "rotate_half",
    "contexts": [
      "def rotate_half(x):"
    ],
    "question_type": "simple",
    "page_num": 55,
    "source_file": "doc01_34_57.pdf"
  },
  {
    "id": "0014",
    "question": "通过上述代码可以得到什么输出？",
    "ground_truth": "通过上述代码可以得到如下输出：",
    "contexts": [
      "通过上述代码可以得到如下输出："
    ],
    "question_type": "simple",
    "page_num": 49,
    "source_file": "doc01_34_57.pdf"
  },
  {
    "id": "0015",
    "question": "GPT-2 相较于 GPT 引入了哪种前置层归一化方法？",
    "ground_truth": "前置层归一化方法",
    "contexts": [
      "为了使模型训练过程更加稳定，GPT-2 相较于 GPT 引入了前置层归一化方法，将第一个层归一化移动到多头自注意力层之前，将第二个层归一化移动到全连接层之前。"
    ],
    "question_type": "simple",
    "page_num": 51,
    "source_file": "doc01_34_57.pdf"
  },
  {
    "id": "0016",
    "question": "在启动整个模型训练之前，需要对预训练数据进行什么处理？",
    "ground_truth": "将预训练数据根据训练好的词元分析器进行处理。",
    "contexts": [
      "在启动整个模型训练之前，还需要将预训练数据根据训练好的词元分析器进行处理。"
    ],
    "question_type": "simple",
    "page_num": 45,
    "source_file": "doc01_34_57.pdf"
  },
  {
    "id": "0017",
    "question": "DecoderLayer 的构造函数中定义了多少个 Norm 实例？",
    "ground_truth": "3",
    "contexts": [
      "self.norm_1 = Norm(d_model)\nself.norm_2 = Norm(d_model)\nself.norm_3 = Norm(d_model)"
    ],
    "question_type": "simple",
    "page_num": 37,
    "source_file": "doc01_34_57.pdf"
  },
  {
    "id": "0018",
    "question": "What does the Encoder class use to embed the source input?",
    "ground_truth": "self.embed = Embedder(vocab_size, d_model)",
    "contexts": [
      "self.embed = Embedder(vocab_size, d_model)"
    ],
    "question_type": "simple",
    "page_num": 36,
    "source_file": "doc01_34_57.pdf"
  },
  {
    "id": "0019",
    "question": "如何在使用BertTokenizerFast时保存模型配置，并利用填充掩码功能进行预测？",
    "ground_truth": "可以通过创建一个模型路径并检查其是否存在，若不存在则创建该目录，随后保存tokenizer的模型配置。同时，可以使用pipeline来进行填充掩码的预测。",
    "contexts": [
      "model_path = 'pretrained-bert'",
      "tokenizer = BertTokenizerFast.from_pretrained(model_path)",
      "fill_mask = pipeline('fill-mask', model=model, tokenizer=tokenizer)",
      "tokenizer.save_model(model_path)",
      "with open(os.path.join(model_path, 'config.json'), 'w') as f: json.dump(tokenizer_cfg, f)"
    ],
    "question_type": "multi_context",
    "page_num": 49,
    "source_file": "doc01_34_57.pdf"
  },
  {
    "id": "0020",
    "question": "在 GPT-2 模型中，层归一化的实施与大规模语言模型 LLaMA 的超参数设置有什么关系？",
    "ground_truth": "GPT-2 通过引入前置层归一化方法来提高模型训练的稳定性，而 LLaMA 模型则使用不同规模的超参数配置，包括层数和自注意力头数，这影响了模型的复杂性和计算需求。",
    "contexts": [
      "为了使模型训练过程更加稳定，GPT-2 相较于 GPT 引入了前置层归一化方法，将第一个层归一化移动到多头自注意力层之前。",
      "表 2.1 不同规模的 LLaMA 模型使用的超参数包括层数和自注意力头数，这影响了模型的复杂性和计算需求。"
    ],
    "question_type": "multi_context",
    "page_num": 51,
    "source_file": "doc01_34_57.pdf"
  },
  {
    "id": "0021",
    "question": "在训练大规模语言模型时，如何使用预训练的tokenizer和模型进行填充掩码操作？",
    "ground_truth": "在训练大规模语言模型时，首先需要定义模型参数并初始化模型，然后可以使用预训练的tokenizer（如BertTokenizerFast）和填充掩码pipeline来对输入进行处理。通过pipeline，可以使用模型和tokenizer对包含掩码的句子进行预测。",
    "contexts": [
      "model = Transformer(src_vocab, trg_vocab, d_model, N, heads)",
      "tokenizer = BertTokenizerFast.from_pretrained(model_path)"
    ],
    "question_type": "multi_context",
    "page_num": 39,
    "source_file": "doc01_34_57.pdf"
  },
  {
    "id": "0022",
    "question": "代码的主要来源是什么？",
    "ground_truth": "编程问答社区（如 Stack Exchange）和公共软件仓库（如 GitHub）。",
    "contexts": [
      "代码的主要来源是编程问答社区（如 Stack Exchange）和公共软件仓库（如 GitHub）。"
    ],
    "question_type": "simple",
    "page_num": 76,
    "source_file": "doc02_58_80.pdf"
  },
  {
    "id": "0023",
    "question": "后缀数组 A 的元素是如何排列的？",
    "ground_truth": "A 中的元素按照后缀的字典顺序排列。",
    "contexts": [
      "A 中的元素按照后缀的字典顺序排列。"
    ],
    "question_type": "simple",
    "page_num": 79,
    "source_file": "doc02_58_80.pdf"
  },
  {
    "id": "0024",
    "question": "稀疏混合专家模型中采用什么策略时，分配给不同专家的词元可能需要一些共有知识？",
    "ground_truth": "常规的门控策略",
    "contexts": [
      "稀疏混合专家模型中采用常规的门控策略时，分配给不同专家的词元可能需要一些共有知识"
    ],
    "question_type": "simple",
    "page_num": 69,
    "source_file": "doc02_58_80.pdf"
  },
  {
    "id": "0025",
    "question": "FlashAttention 算法是如何减少全局内存消耗的？",
    "ground_truth": "FlashAttention 就提出了不使用中间注意力矩阵，通过存储归一化因子来减少全局内存消耗的方法。",
    "contexts": [
      "FlashAttention 就提出了不使用中间注意力矩阵，通过存储归一化因子来减少全局内存消耗的方法。"
    ],
    "question_type": "simple",
    "page_num": 63,
    "source_file": "doc02_58_80.pdf"
  },
  {
    "id": "0026",
    "question": "如何删除隐私数据？",
    "ground_truth": "删除隐私数据最直接的方法是采用基于规则的算法，BigScience ROOTS Corpus 在构建过程中就采用了基于命名实体识别的方法，利用命名实体识别算法检测姓名、地址、电话号码等个人信息内容并进行删除或者替换。",
    "contexts": [
      "删除隐私数据最直接的方法是采用基于规则的算法，BigScience ROOTS Corpus 在构建过程中就采用了基于命名实体识别的方法，利用命名实体识别算法检测姓名、地址、电话号码等个人信息内容并进行删除或者替换。"
    ],
    "question_type": "simple",
    "page_num": 80,
    "source_file": "doc02_58_80.pdf"
  },
  {
    "id": "0027",
    "question": "什么模型被称为软混合专家模型？",
    "ground_truth": "软混合专家模型（SoftMoE）",
    "contexts": [
      "（c）软混合专家模型（SoftMoE）"
    ],
    "question_type": "simple",
    "page_num": 68,
    "source_file": "doc02_58_80.pdf"
  },
  {
    "id": "0028",
    "question": "哪些大语言模型采用了稀疏混合专家架构？",
    "ground_truth": "Switch Transformer、DeepSeekMoE、AdaMoE、Yuan 2.0-M32、OpenMoE、Qwen1.5-MoE-A2.7B",
    "contexts": [
      "此外，众多大语言模型也都采用了稀疏混合专家架构，包括Switch Transformer、DeepSeekMoE、AdaMoE、Yuan 2.0-M32、OpenMoE、Qwen1.5-MoE-A2.7B等。"
    ],
    "question_type": "simple",
    "page_num": 69,
    "source_file": "doc02_58_80.pdf"
  },
  {
    "id": "0029",
    "question": "NVIDIA H100 中全局内存的容量是多少？",
    "ground_truth": "80GB",
    "contexts": [
      "NVIDIA H100 中全局内存有 80GB 空间"
    ],
    "question_type": "simple",
    "page_num": 61,
    "source_file": "doc02_58_80.pdf"
  },
  {
    "id": "0030",
    "question": "稀疏混合专家模型的p值是多少？",
    "ground_truth": "p=0.16",
    "contexts": [
      "p=0.16"
    ],
    "question_type": "simple",
    "page_num": 68,
    "source_file": "doc02_58_80.pdf"
  },
  {
    "id": "0031",
    "question": "在自然语言处理的模型中，如何处理未登录词的表示，并且这个处理对模型性能有什么影响？",
    "ground_truth": "在传统的自然语言处理模型中，未登录词（OOV）会被给予一个默认的通用表示，即使用一个标识为'[UNK]'的向量。词表示模型在训练过程中将'[UNK]'的向量作为词表示矩阵的一部分进行更新。当词表过小时，未登录词的比例较高，可能会影响模型的性能；而当词表过大时，低频词的词向量难以有效训练，亦会对模型性能产生负面影响。",
    "contexts": [
      "在深度学习模型中，词表示模型会预先在词表中加入一个默认的 '[UNK]' （unknown）标识，表示未知词，并在训练的过程中将[UNK] 的向量作为词表示矩阵的一部分一起训练。",
      "当词表过小时，未登录词的比例较高，影响模型性能；当词表过大时，大量低频词出现在词表中，这些词的词向量很难。"
    ],
    "question_type": "multi_context",
    "page_num": 80,
    "source_file": "doc02_58_80.pdf"
  },
  {
    "id": "0032",
    "question": "Mixtral-8x7B模型在性能上如何与Huatuo-26M数据集相关的医疗问答对进行比较？",
    "ground_truth": "Mixtral-8x7B模型在很多基准测试中展现出了优于或等同于包含700亿参数的Llama-2-70B的性能，而Huatuo-26M数据集是规模最大的中文医疗问答数据集之一，包含逾2600万条高质量的医疗问答对，提供了疾病、症状、治疗方法以及药物信息等方面的知识。",
    "contexts": [
      "Mixtral-8x7B模型在很多基准测试中，展现出了优于或等同于包含了700亿参数的Llama-2-70B的性能。",
      "Huatuo-26M是目前规模最大的中文医疗问答数据集之一，包含逾2600万条高质量的医疗问答对，涵盖疾病、症状、治疗方法以及药物信息等诸多方面。"
    ],
    "question_type": "multi_context",
    "page_num": 69,
    "source_file": "doc02_58_80.pdf"
  },
  {
    "id": "0033",
    "question": "大语言模型训练中通用数据的重要性是什么，以及目前业界对预训练数据配比有何共识？",
    "ground_truth": "大语言模型训练中通用数据占比非常高，包括网页、对话文本、书籍等多种类型，提供了大规模且多样的训练数据。然而，业界截至2025年2月对预训练数据的配比尚未达成广泛的共识。",
    "contexts": [
      "通用数据在大语言模型训练数据中占比非常高，主要包括网页、对话文本、书籍、代码、百科等不同类型的数据，为大语言模型提供了大规模且多样的训练数据。",
      "截至2025年2月，业界关于预训练数据的配比还没达成广泛的共识。"
    ],
    "question_type": "multi_context",
    "page_num": 74,
    "source_file": "doc02_58_80.pdf"
  },
  {
    "id": "0034",
    "question": "团队使用了哪两种方法对数据集进行去重？",
    "ground_truth": "MinHash 和 URL",
    "contexts": [
      "团队利用 MinHash 和 URL 对数据集进行全面去重，并按语言独立进行。"
    ],
    "question_type": "simple",
    "page_num": 102,
    "source_file": "doc03_81_104.pdf"
  },
  {
    "id": "0035",
    "question": "数据质量对大语言模型训练效果的影响是什么？",
    "ground_truth": "数据质量通常被认为是影响大语言模型训练效果的关键因素之一。",
    "contexts": [
      "数据质量通常被认为是影响大语言模型训练效果的关键因素之一。"
    ],
    "question_type": "simple",
    "page_num": 89,
    "source_file": "doc03_81_104.pdf"
  },
  {
    "id": "0036",
    "question": "LLaMA 模型训练使用了哪些类型的数据来源？",
    "ground_truth": "包括网页、代码、论文、图书等。",
    "contexts": [
      "可以看到，LLaMA 模型训练混合了大量不同来源的数据，包括网页、代码、论文、图书等。"
    ],
    "question_type": "simple",
    "page_num": 92,
    "source_file": "doc03_81_104.pdf"
  },
  {
    "id": "0037",
    "question": "Chinchilla 语言模型具有多少个参数？",
    "ground_truth": "700 亿个参数",
    "contexts": [
      "最终确定 Chinchilla 语言模型具有 700 亿个参数，使用了 1.4 万亿个词元进行训练。"
    ],
    "question_type": "simple",
    "page_num": 87,
    "source_file": "doc03_81_104.pdf"
  },
  {
    "id": "0038",
    "question": "arXiv 上的论文是用什么编写的？",
    "ground_truth": "LaTeX",
    "contexts": [
      "arXiv 上的论文是用 LaTeX 编写的，其中公式、符号、表格等内容的表示非常适合语言模型学习。"
    ],
    "question_type": "simple",
    "page_num": 95,
    "source_file": "doc03_81_104.pdf"
  },
  {
    "id": "0039",
    "question": "arXiv 是什么？",
    "ground_truth": "arXiv 是一个自 1991 年开始运营的论文预印版本发布服务平台。",
    "contexts": [
      "arXiv 是一个自 1991 年开始运营的论文预印版本发布服务平台。发布在 arXiv 上的论文主要集中在数学、计算机科学和物理领域。arXiv 上的论文是用 LaTeX 编写的，其中公式、符号、表格等内容的表示非常适合语言模型学习。"
    ],
    "question_type": "simple",
    "page_num": 95,
    "source_file": "doc03_81_104.pdf"
  },
  {
    "id": "0040",
    "question": "CulturaX 研究团队使用了哪两种语言识别工具？",
    "ground_truth": "cld3 和 FastText",
    "contexts": [
      "在处理mC4 和 OSCAR 数据集时， 一个较为突出的问题是二者分别使用了cld3 和 FastText 这两种不同的语言识别工具。"
    ],
    "question_type": "simple",
    "page_num": 102,
    "source_file": "doc03_81_104.pdf"
  },
  {
    "id": "0041",
    "question": "ROOTS数据集包含多少种语言？",
    "ground_truth": "59种语言",
    "contexts": [
      "ROOTS（Responsible Open-science Open-collaboration Text Sources ）数据集是 BigScience 项目在训练具有 1760 亿个参数的 BLOOM 大语言模型时使用的数据集。该数据集包含 46 种自然语言和 13 种编程语言，总计 59 种语言，整个数据集的大小约 1.6TB。"
    ],
    "question_type": "simple",
    "page_num": 96,
    "source_file": "doc03_81_104.pdf"
  },
  {
    "id": "0042",
    "question": "在训练大语言模型时，为什么多语言场景下的网络爬虫数据集比精选数据集更具优势？",
    "ground_truth": "多语言场景下，网络爬虫数据集可以高效收集多种语言的数据，尽管其原始数据质量参差不齐，但经过清洗后可以很好应用于大语言模型训练。而精选数据集通常有限，可能无法覆盖多种语言的需求。",
    "contexts": [
      "在多语言场景下，网络爬虫数据集更具优势，它有助于高效收集多语言数据。",
      "在训练大语言模型之前，构建一个准备充分的预训练语料库尤为重要。"
    ],
    "question_type": "multi_context",
    "page_num": 86,
    "source_file": "doc03_81_104.pdf"
  },
  {
    "id": "0043",
    "question": "在多语言数据集的构建过程中，如何确保数据的质量和去重，特别是在处理来自网页的数据时？",
    "ground_truth": "CulturaX研究团队使用一系列全面的指标，包括单词数量、字符和单词重复比率等来删除噪声文档，同时利用不同语言的停用词和标记词列表计算比率以删除文档。此外，通过FastText获取语言识别信度辅助过滤。对于网页数据，尽管原始数据质量参差不齐，但经过清洗后，这些数据可以很好地应用于大语言模型训练。冗余去除是保证训练数据质量的关键，CulturaX研究团队利用MinHash和URL对数据集进行全面去重，并按语言独立进行。",
    "contexts": [
      "CulturaX研究团队利用不同语言的停用词和标记词列表计算比率以删除文档，还通过FastText获取语言识别信度辅助过滤。",
      "在多语言场景下，网络爬虫数据集更具优势，它有助于高效收集多语言数据。"
    ],
    "question_type": "multi_context",
    "page_num": 102,
    "source_file": "doc03_81_104.pdf"
  },
  {
    "id": "0044",
    "question": "如何通过加强数学问题数据集的方式提升大语言模型在数学任务上的能力？",
    "ground_truth": "通过在Pile数据集中专门增加数学问题数据集，期望增强通过Pile数据集训练的语言模型的数学能力。",
    "contexts": [
      "大语言模型在数学任务上的表现较差， 这可能是由于训练集中缺乏数学问题。因此，Pile 数据集中专门增加了数学问题数据集，期望增强通过 Pile 数据集训练的语言模型的数学能力。",
      "pair_freqs = compute_pair_freqs(splits)"
    ],
    "question_type": "multi_context",
    "page_num": 85,
    "source_file": "doc03_81_104.pdf"
  },
  {
    "id": "0045",
    "question": "混合并行将哪些并行策略混合使用？",
    "ground_truth": "混合并行将多种并行策略如数据并行、流水线并行和张量并行等混合使用。",
    "contexts": [
      "混合并行将多种并行策略如数据并行、流水线并行和张量并行等混合使用。"
    ],
    "question_type": "simple",
    "page_num": 124,
    "source_file": "doc04_105_127.pdf"
  },
  {
    "id": "0046",
    "question": "大语言模型是基于什么结构的？",
    "ground_truth": "Transformer 结构",
    "contexts": [
      "大语言模型都是以Transformer 结构为基础"
    ],
    "question_type": "simple",
    "page_num": 118,
    "source_file": "doc04_105_127.pdf"
  },
  {
    "id": "0047",
    "question": "1F1B 交错式调度模式要求微批次的数量是流水线阶段的什么倍数？",
    "ground_truth": "整数倍",
    "contexts": [
      "1F1B 交错式调度模式要求微批次的数量是流水线阶段的整数倍。"
    ],
    "question_type": "simple",
    "page_num": 117,
    "source_file": "doc04_105_127.pdf"
  },
  {
    "id": "0048",
    "question": "What is the default value for 'local_rank' in the argument parser?",
    "ground_truth": "0",
    "contexts": [
      "parser.add_argument('--local_rank', default =0, type=int, help='node rank for distributed training' )"
    ],
    "question_type": "simple",
    "page_num": 114,
    "source_file": "doc04_105_127.pdf"
  },
  {
    "id": "0049",
    "question": "单个计算设备在推理阶段采用 FP32 格式进行存储时，需要多少GB的计算设备内存空间来存储GPT-3的参数？",
    "ground_truth": "700GB",
    "contexts": [
      "如果在推理阶段采用 FP32 格式进行存储，则需要 700GB 的计算设备内存空间"
    ],
    "question_type": "simple",
    "page_num": 109,
    "source_file": "doc04_105_127.pdf"
  },
  {
    "id": "0050",
    "question": "What does the DistributedSampler class require to be available?",
    "ground_truth": "Requires distributed package to be available",
    "contexts": [
      "if not dist.is_available(): raise RuntimeError(\"Requires distributed package to be available\" )"
    ],
    "question_type": "simple",
    "page_num": 112,
    "source_file": "doc04_105_127.pdf"
  },
  {
    "id": "0051",
    "question": "1F1B 流水线并行策略引入了什么机制？",
    "ground_truth": "任务调度机制",
    "contexts": [
      "1F1B 流水线并行策略引入了任务调度机制，使得下游设备能够在等待上游计算的同时执行其他可并行的任务，从而提高设备的利用率。"
    ],
    "question_type": "simple",
    "page_num": 116,
    "source_file": "doc04_105_127.pdf"
  },
  {
    "id": "0052",
    "question": "数据并行和GPipe策略在并行计算中有什么主要区别？",
    "ground_truth": "数据并行系统中，每个计算设备都有整个神经网络模型的模型副本，进行前向计算和梯度同步，而GPipe策略在前向计算完成后才能执行后向计算，且可能产生并行气泡。虽然GPipe减少了并行气泡，但在执行后向计算时仍然依赖于前向计算的完成。",
    "contexts": [
      "在数据并行系统中，每个计算设备都有整个神经网络模型的模型副本，进行迭代时，每个计算设备只分配一个批次数据样本的子集。",
      "GPipe策略可以减少一定的并行气泡，但是只有当一个小批次中所有的前向计算都完成时，才能执行后向计算。"
    ],
    "question_type": "multi_context",
    "page_num": 111,
    "source_file": "doc04_105_127.pdf"
  },
  {
    "id": "0053",
    "question": "在张量并行与1F1B交错式调度模式中，如何解决大规模语言模型中设备间的计算和存储问题？",
    "ground_truth": "张量并行需要根据模型的具体结构和算子类型，将参数切分到不同设备，并保证数学一致性。对于大语言模型，嵌入式表示算子的参数存储需求很高，因此可以按词维度切分，以减少每个设备的显存需求。同时，1F1B交错式调度模式通过让每个设备处理多个层的子集，从而更高效地利用计算资源，提升了内存消耗和计算效率。",
    "contexts": [
      "张量并行需要根据模型的具体结构和算子类型，解决如何将参数切分到不同设备，以及如何保证切分后的数学一致性这两个问题。",
      "在新的模式下，设备1 可以处理层 1、2、9、10，设备 2 处理层 3、4、11、12，依此类推。"
    ],
    "question_type": "multi_context",
    "page_num": 118,
    "source_file": "doc04_105_127.pdf"
  },
  {
    "id": "0054",
    "question": "在分布式系统中，如何处理多个计算设备的输出以确保最终结果与单个计算设备一致？",
    "ground_truth": "在分布式系统中，多个计算设备的输出需要进行合并，最终得到与单个计算设备等价的计算结果。这是因为多个计算设备并行执行各自的子任务，从而加速整体计算。",
    "contexts": [
      "一个模型训练任务往往会有大量的训练样本作为输入，可以利用一个计算设备完成，也可以将整个模型的训练任务拆分成多个子任务，分发给不同的计算设备，实现并行计算。此后，还需要对每个计算设备的输出进行合并，最终得到与单个计算设备等价的计算结果。",
      "由于同一个服务器内部的多个计算设备之间可能并不共享内存，因此无论这些计算设备是处于一个服务器还是多个服务器中，其系统架构都属于分布式系统范畴。"
    ],
    "question_type": "multi_context",
    "page_num": 107,
    "source_file": "doc04_105_127.pdf"
  },
  {
    "id": "0055",
    "question": "什么是训练的批量大小？",
    "ground_truth": "GLOBAL_BATCH_SIZE",
    "contexts": [
      "\"train_batch_size\": GLOBAL_BATCH_SIZE"
    ],
    "question_type": "simple",
    "page_num": 151,
    "source_file": "doc05_128_151.pdf"
  },
  {
    "id": "0056",
    "question": "GLOBAL_BATCH_SIZE定义了什么？",
    "ground_truth": "GLOBAL_BATCH_SIZE定义了全局的批次大小。",
    "contexts": [
      "GLOBAL_BATCH_SIZE定义了全局的批次大小。这通常是所有 GPU 加起来的总批次大小。"
    ],
    "question_type": "simple",
    "page_num": 149,
    "source_file": "doc05_128_151.pdf"
  },
  {
    "id": "0057",
    "question": "ZeRO-3 配置参数的 stage 是多少？",
    "ground_truth": "3",
    "contexts": [
      "以下是 DeepSpeed 使用 ZeRO-3 配置参数的样例：{\"zero_optimization\": {\"stage\": 3,},"
    ],
    "question_type": "simple",
    "page_num": 143,
    "source_file": "doc05_128_151.pdf"
  },
  {
    "id": "0058",
    "question": "如何初始化进程组？",
    "ground_truth": "dist.init_process_group(\"gloo\", rank =rank, world_size =world_size)",
    "contexts": [
      "dist.init_process_group(\"gloo\", rank =rank, world_size =world_size)"
    ],
    "question_type": "simple",
    "page_num": 129,
    "source_file": "doc05_128_151.pdf"
  },
  {
    "id": "0059",
    "question": "DeepSpeed 支持哪些类型的 ZeRO 分片机制？",
    "ground_truth": "ZeRO-0、ZeRO-1、ZeRO-2、ZeRO-3 以及 ZeRO-Infinity。",
    "contexts": [
      "DeepSpeed 中也支持多种类型 ZeRO 的分片机制，包括ZeRO-0、ZeRO-1、ZeRO-2、ZeRO-3 以及 ZeRO-Infinity。"
    ],
    "question_type": "simple",
    "page_num": 142,
    "source_file": "doc05_128_151.pdf"
  },
  {
    "id": "0060",
    "question": "What optimizer class is used in the document?",
    "ground_truth": "torch.optim.Adam",
    "contexts": [
      "optimizer_class=torch.optim.Adam, # ЇልਔAdam"
    ],
    "question_type": "simple",
    "page_num": 129,
    "source_file": "doc05_128_151.pdf"
  },
  {
    "id": "0061",
    "question": "如何在使用 ZeRO-3 的基础上配置优化器以便将状态转移到 CPU 中，并且选择适当的优化器类型？",
    "ground_truth": "在使用 ZeRO-3 的基础上，可以通过在配置文件中设置 'offload_optimizer' 为 'cpu' 来将优化器状态转移到 CPU 中。同时，可以根据是否选择了 offload 选项来决定使用 DeepSpeedCPUAdam 还是 FusedAdam 作为优化器。",
    "contexts": [
      "\"zero_optimization\": { \"stage\": 3, \"offload_optimizer\": { \"device\": \"cpu\" } }",
      "AdamOptimizer = DeepSpeedCPUAdam if args.offload else FusedAdam"
    ],
    "question_type": "multi_context",
    "page_num": 143,
    "source_file": "doc05_128_151.pdf"
  },
  {
    "id": "0062",
    "question": "在使用分布式训练时，如何选择合适的数据采样器？",
    "ground_truth": "在分布式训练环境中，需要使用 DistributedSampler 确保每个进程或节点获得数据的一个不重复的子集，而在单机环境中则可以使用普通的随机或顺序采样器。",
    "contexts": [
      "使用 PyTorch 和 transformers 库来设置预训练模型的数据加载器， 以实现在单机或多机分布式训练环境中对数据的加载和采样。",
      "通过检查 args.local_rank 是否为 −1，代码会选择使用普通的采样器（单机）还是分布式采样器 （多机） 。DistributedSampler 确保在分布式训练环境中， 每个进程或节点都能获得数据的一个不重复的子集。"
    ],
    "question_type": "multi_context",
    "page_num": 129,
    "source_file": "doc05_128_151.pdf"
  },
  {
    "id": "0063",
    "question": "在使用参数服务器架构进行模型训练时，如何确保模型能够正确处理文本的长度和填充设置？",
    "ground_truth": "在参数服务器架构中，训练服务器需要提供大量计算资源，同时在加载和配置模型时，需要确保词元分析器能够处理各种文本的长度，并进行适当的填充设置。",
    "contexts": [
      "参数服务器需要提供充足的内存资源和通信资源，训练服务器需要提供大量的计算资源。",
      "使用 transformers 库加载和配置 LLaMA 模型及其相关的词元分析器，从 transformers 库中导入 LLaMA 模型、相应的词元分析器和模型配置后，使用from_pretrained 方法加载预训练的 LLaMA 模型、词元分析器和配置。"
    ],
    "question_type": "multi_context",
    "page_num": 132,
    "source_file": "doc05_128_151.pdf"
  },
  {
    "id": "0064",
    "question": "如何计算训练中的 TFLOPs?",
    "ground_truth": "train_tflops = train_flops_per_iteration / (e2e_time * gpus_per_model * (10**12))",
    "contexts": [
      "train_tflops = train_flops_per_iteration / (e2e_time * gpus_per_model * (10**12))"
    ],
    "question_type": "simple",
    "page_num": 155,
    "source_file": "doc06_152_174.pdf"
  },
  {
    "id": "0065",
    "question": "在生成任务输入和输出的步骤中，如何处理非分类任务？",
    "ground_truth": "对于非分类任务，使用输入优先的方法，先根据任务产生输入，再根据任务指令和输入生成输出。",
    "contexts": [
      "对于非分类任务，使用输入优先的方法，先根据任务产生输入，再根据任务指令和输入生成输出。"
    ],
    "question_type": "simple",
    "page_num": 164,
    "source_file": "doc06_152_174.pdf"
  },
  {
    "id": "0066",
    "question": "指令微调在模型获取各类关键能力的进程中发挥着什么作用？",
    "ground_truth": "不可或缺的作用",
    "contexts": [
      "然而，指令微调在模型获取各类关键能力的进程中却发挥着不可或缺的作用。"
    ],
    "question_type": "simple",
    "page_num": 172,
    "source_file": "doc06_152_174.pdf"
  },
  {
    "id": "0067",
    "question": "Flan 2022 在构建过程中使用了哪些技术？",
    "ground_truth": "任务混合和输入反转等技术。",
    "contexts": [
      "Flan 2022 构建过程中还使用了任务混合和输入反转等技术。"
    ],
    "question_type": "simple",
    "page_num": 161,
    "source_file": "doc06_152_174.pdf"
  },
  {
    "id": "0068",
    "question": "What happens if args.global_rank is 0?",
    "ground_truth": "save_hf_format(model, tokenizer, args)",
    "contexts": [
      "if args.global_rank == 0:\nsave_hf_format(model, tokenizer, args)"
    ],
    "question_type": "simple",
    "page_num": 155,
    "source_file": "doc06_152_174.pdf"
  },
  {
    "id": "0069",
    "question": "在进行指令微调时，如何确保模型的输出符合预期的要求？",
    "ground_truth": "在进行指令微调时，确保模型的输出符合预期的要求，可以通过构造包含精准、清晰的指令输入的数据对来实现。这些指令输入详细阐释了任务的目标，并明确规定了输出需要满足的各项要求。同时，在模型训练的过程中，可以通过保存模型的不同阶段，并利用打印功能来监控训练过程中的各种指标，例如通过打印吞吐量来评估训练效率。",
    "contexts": [
      "指令微调数据通常由文本对构成，包含“指令输入”与“答案输出”两个关键部分。",
      "在进行指令微调时，确保模型的输出符合预期的要求，可以通过构造包含精准、清晰的指令输入的数据对来实现。",
      "save_hf_format(model, tokenizer, args) 和 save_zero_three_model(model, args.global_rank, args.output_dir, zero_stage=args.zero_stage) 这类函数可以帮助保存模型的不同阶段，从而监控训练过程中的各种指标。"
    ],
    "question_type": "multi_context",
    "page_num": 155,
    "source_file": "doc06_152_174.pdf"
  },
  {
    "id": "0070",
    "question": "在训练模型中，如何结合选择子集的硬掩码或软权重与损失函数的优化来提高模型的性能？",
    "ground_truth": "在训练模型时，可以通过选择子集的硬掩码或软权重来优化模型参数，并结合带有自监督语言建模损失的双层优化问题，从而提高模型的性能。在片段A中，模型在训练过程中计算损失并进行反向传播，而在片段B中，优化问题的形式化表示表明可以通过负对数似然损失来优化所选子集的模型参数。通过这种方式，可以更有效地训练模型。",
    "contexts": [
      "loss = outputs.loss",
      "从 S 中选择子集的硬掩码或软权重；内循环用于优化在 Sb 上的模型参数 θ。",
      "model.backward(loss)",
      "可以将带有自监督语言建模损失的双层优化问题，按照如下方法形式化表示："
    ],
    "question_type": "multi_context",
    "page_num": 155,
    "source_file": "doc06_152_174.pdf"
  },
  {
    "id": "0071",
    "question": "如何在命名实体识别和情感分类任务中确保数据的多样性和有效性？",
    "ground_truth": "在命名实体识别中，通过均匀选择每种实体类型的样本并强调语义多样性，确保数据的多样性。而在情感分类任务中，针对每个类标签生成相应的输入，确保对情感的准确分类。",
    "contexts": [
      "均匀选择每种实体类型的样本，同时强调语义多样性，通过选择文本相似度较低的样本来确保数据的多样性。",
      "Given the classification task definition and the class labels, generate an input that corresponds to each of the class labels."
    ],
    "question_type": "multi_context",
    "page_num": 161,
    "source_file": "doc06_152_174.pdf"
  },
  {
    "id": "0072",
    "question": "find_and_replace 会根据什么参数从基础模型中找出模块？",
    "ground_truth": "config 中的参数",
    "contexts": [
      "find_and_replace 会根据 config 中的参数从基础模型的 named_parameters 中找出包含指定名称的模块（默认为“q” “v” ，即注意力模块的Q 和 V 矩阵）"
    ],
    "question_type": "simple",
    "page_num": 181,
    "source_file": "doc07_175_198.pdf"
  },
  {
    "id": "0073",
    "question": "线性插值法的优点是什么？",
    "ground_truth": "线性插值法具有良好的数值稳定性，并且不需要修改模型架构，只需要少量微调即可将 LLaMA.",
    "contexts": [
      "线性插值法具有良好的数值稳定性（具体推导请参考文献 [242]），并且不需要修改模型架构，只需要少量微调（例如，在 pile 数据集上进行 1000 步的微调）即可将 LLaMA"
    ],
    "question_type": "simple",
    "page_num": 187,
    "source_file": "doc07_175_198.pdf"
  },
  {
    "id": "0074",
    "question": "如何获取评估数据？",
    "ground_truth": "return self.raw_datasets[\"eval\"]",
    "contexts": [
      "return self .raw_datasets[\"eval\"]"
    ],
    "question_type": "simple",
    "page_num": 193,
    "source_file": "doc07_175_198.pdf"
  },
  {
    "id": "0075",
    "question": "在第一轮训练中，Rank 4 的损失是多少？",
    "ground_truth": "1.6396484375",
    "contexts": [
      "running - Rank: 4, Epoch 1/2, Step 1/341, trained samples: 128/341, Loss 1.6396484375"
    ],
    "question_type": "simple",
    "page_num": 198,
    "source_file": "doc07_175_198.pdf"
  },
  {
    "id": "0076",
    "question": "ChatDoctor指令数据集的规模是多少？",
    "ground_truth": "11.5 万",
    "contexts": [
      "ChatDoctor 德克萨斯大学西南医学中心 11.5 万 医疗 公开"
    ],
    "question_type": "simple",
    "page_num": 177,
    "source_file": "doc07_175_198.pdf"
  },
  {
    "id": "0077",
    "question": "如何设置ds_accelerator?",
    "ground_truth": "Setting ds_accelerator to cuda (auto detect)",
    "contexts": [
      "Setting ds_accelerator to cuda (auto detect)"
    ],
    "question_type": "simple",
    "page_num": 191,
    "source_file": "doc07_175_198.pdf"
  },
  {
    "id": "0078",
    "question": "文献 [239] 中提出了什么方案来平滑化敏感性？",
    "ground_truth": "文献 [239] 中提出了一种新的方案来平滑化敏感性，以及量化其不确定性。",
    "contexts": [
      "然而，根据文献 [239] 中的实验结果，该敏感性度量受限于小批量采样带来的高方差和不确定性，因此并不完全可靠。相应地，文献 [239] 中提出了一种新的方案来平滑化敏感性，以及量化其不确定性。"
    ],
    "question_type": "simple",
    "page_num": 183,
    "source_file": "doc07_175_198.pdf"
  },
  {
    "id": "0079",
    "question": "get_peft_model 函数的功能是什么？",
    "ground_truth": "该函数封装了基础模型并得到一个 PeftModel 类的模型。",
    "contexts": [
      "接下来介绍 peft 库对 LoRA 的实现， 也就是上述代码中get_peft_model 函数的功能。 该函数封装了基础模型并得到一个 PeftModel 类的模型。"
    ],
    "question_type": "simple",
    "page_num": 179,
    "source_file": "doc07_175_198.pdf"
  },
  {
    "id": "0080",
    "question": "Alpaca数据集的规模是多少条?",
    "ground_truth": "5.2 万条",
    "contexts": [
      "Alpaca Data Standford Alpaca 5.2 万条 英文 公开"
    ],
    "question_type": "simple",
    "page_num": 176,
    "source_file": "doc07_175_198.pdf"
  },
  {
    "id": "0081",
    "question": "在训练过程中，如何使用不同的位置编码方法来增强模型的外推能力，并且在训练中如何对OPT模型进行微调和奖励模型的训练？",
    "ground_truth": "位置编码的外推能力来源于相对位置信息，而在训练OPT模型时，通过train.py脚本对OPT-1.3b模型进行监督微调和对OPT-350m模型进行奖励模型的训练。",
    "contexts": [
      "位置编码的长度外推能力来源于位置编码中表征相对位置信息的部分，相对位置信息不同于绝对位置信息， 对于训练时的依赖较少。",
      "表示通过 train.py 脚本进行步骤一和步骤二的训练，分别对OPT-1.3b 模型进行监督微调和对 OPT-350m 模型进行奖励模型的训练。"
    ],
    "question_type": "multi_context",
    "page_num": 184,
    "source_file": "doc07_175_198.pdf"
  },
  {
    "id": "0082",
    "question": "在使用 DeepSpeed 进行模型训练时，如何设置训练步骤和模型参数，以便同时满足片段A和片段B中的要求？",
    "ground_truth": "可以通过命令行参数设置训练步骤和模型参数，例如在片段A中提到的 'deepspeed main.py --data_path <my_data>/my_dataset' 和在片段B中提到的 '--step 训练步骤参数' 来实现。",
    "contexts": [
      "--data_path <my_data>/my_dataset",
      "--step 训练步骤参数， 表示运行哪个步骤， 可选参数为1、2、3。"
    ],
    "question_type": "multi_context",
    "page_num": 196,
    "source_file": "doc07_175_198.pdf"
  },
  {
    "id": "0083",
    "question": "Kimi k1.5 的多模态数据包括哪五类?",
    "ground_truth": "字幕、图像-文本交错数据、OCR、知识和一般问题回答五类。",
    "contexts": [
      "作为多模态模型，Kimi k1.5 的多模态数据包括字幕、图像-文本交错数据、OCR、知识和一般问题回答五类。"
    ],
    "question_type": "simple",
    "page_num": 220,
    "source_file": "doc08_199_221.pdf"
  },
  {
    "id": "0084",
    "question": "RLOO算法是基于哪个算法发展而来的？",
    "ground_truth": "REINFORCE算法",
    "contexts": [
      "REINFORCE Leave-One-Out（RLOO）算法是在 REINFORCE 算法基础上发展而来的一种改进算法"
    ],
    "question_type": "simple",
    "page_num": 212,
    "source_file": "doc08_199_221.pdf"
  },
  {
    "id": "0085",
    "question": "随着样本数量 k 的增加，计算量会如何变化？",
    "ground_truth": "计算量会相应增大。",
    "contexts": [
      "随着样本数量 k 的增加，计算量会相应增大。"
    ],
    "question_type": "simple",
    "page_num": 214,
    "source_file": "doc08_199_221.pdf"
  },
  {
    "id": "0086",
    "question": "强化学习算法对模型性能优化的有效性是谁证明的？",
    "ground_truth": "Diamond 等",
    "contexts": [
      "Diamond 等，也展现出强大的推理能力，证明了强化学习算法对模型性能优化的有效性。"
    ],
    "question_type": "simple",
    "page_num": 217,
    "source_file": "doc08_199_221.pdf"
  },
  {
    "id": "0087",
    "question": "DeepSeek-R1 在 AIME 2024 上的 Pass@1 得分是多少？",
    "ground_truth": "79.8%",
    "contexts": [
      "DeepSeek-R1 在多个推理任务中表现出色，在 AIME 2024 上 Pass@1 得分达到 79.8%，略超 OpenAI-o1-1217。"
    ],
    "question_type": "simple",
    "page_num": 219,
    "source_file": "doc08_199_221.pdf"
  },
  {
    "id": "0088",
    "question": "环境会如何根据智能体的行为给予反馈？",
    "ground_truth": "通常以奖励的形式。",
    "contexts": [
      "环境会根据智能体的行为给予反馈，通常以奖励的形式。"
    ],
    "question_type": "simple",
    "page_num": 201,
    "source_file": "doc08_199_221.pdf"
  },
  {
    "id": "0089",
    "question": "面向推理的强化学习采用了什么样的大规模强化学习训练过程？",
    "ground_truth": "采用与 DeepSeek-R1-Zero 相同的大规模强化学习训练过程。",
    "contexts": [
      "在冷启动微调后，采用与 DeepSeek-R1-Zero 相同的大规模强化学习训练过程，聚焦于编码、数学、科学和逻辑推理等推理密集型任务。"
    ],
    "question_type": "simple",
    "page_num": 218,
    "source_file": "doc08_199_221.pdf"
  },
  {
    "id": "0090",
    "question": "GRPO算法如何在平衡策略的探索与利用的同时优化目标函数？",
    "ground_truth": "GRPO通过对目标函数的优化，能够利用组内奖励信息，同时平衡策略的探索与利用，从而实现高效稳定的训练。",
    "contexts": [
      "优化目标是最大化轨迹的期望回报 J(θ)，即：J(θ) = Eτ ∼P (τ ;θ) [R(τ )]",
      "通过对这个目标函数的优化，GRPO 能够在利用组内奖励信息的同时，平衡策略的探索与利用，实现高效稳定的训练。"
    ],
    "question_type": "multi_context",
    "page_num": 205,
    "source_file": "doc08_199_221.pdf"
  },
  {
    "id": "0091",
    "question": "在推理密集型任务的强化学习训练中，RLOO算法相比REINFORCE算法在方差降低和样本利用效率方面表现如何？",
    "ground_truth": "RLOO算法通过构建动态基线，能够更有效地降低梯度估计的方差，从而在优化过程中更加稳定，收敛更快。而在样本利用效率方面，RLOO充分利用多个样本之间的关系，使得每个样本不仅用于自身的梯度计算，还参与构建其他样本的基线，这大大提高了样本的利用效率。相比之下，REINFORCE算法的样本利用效率有限，主要用于自身的梯度计算。",
    "contexts": [
      "面向推理的强化学习...聚焦于编码、数学、科学和逻辑推理等推理密集型任务。",
      "RLOO通过利用多个样本构建动态基线，能更有效地降低梯度估计的方差...大大提高了样本的利用效率。"
    ],
    "question_type": "multi_context",
    "page_num": 218,
    "source_file": "doc08_199_221.pdf"
  },
  {
    "id": "0092",
    "question": "在策略梯度算法中，如何通过剪切机制解决高方差问题以提高优化的稳定性？",
    "ground_truth": "PPO算法通过引入剪切机制，限制重要性权重在特定范围内，以避免优化不稳定，从而解决高方差问题。",
    "contexts": [
      "基于蒙特卡洛采样的 REINFORCE 方法作为经典的策略梯度算法，存在以下显著缺陷：首先，其依赖完整轨迹采样的蒙特卡洛特性导致梯度估计方差过高，这不仅会显著延缓收敛速度，还容易引发策略更新方向的剧烈波动，造成训练过程的不稳定性。",
      "PPO算法引入了剪切机制，通过将权重限制在特定范围内来避免优化不稳定，即：JPPO(θ) = E(s,a)∼πθ′ (a|s) clip(πθ(a|s)/πθ′ (a|s), 1 − ε, 1 + ε) A(s, a)。"
    ],
    "question_type": "multi_context",
    "page_num": 208,
    "source_file": "doc08_199_221.pdf"
  },
  {
    "id": "0093",
    "question": "ppo_mini_batch_size 设置为多少？",
    "ground_truth": "64",
    "contexts": [
      "这个参数设置为 64，意味着每次从训练集中选取 64 个样本组成一个小批次，用于计算梯度和更新演员模型的参数。"
    ],
    "question_type": "simple",
    "page_num": 229,
    "source_file": "doc09_222_245.pdf"
  },
  {
    "id": "0094",
    "question": "在模仿学习中，训练数据包含了什么？",
    "ground_truth": "输入和相应的期望输出，即专家生成的正确答案。",
    "contexts": [
      "在模仿学习中，训练数据包含了输入和相应的期望输出，即专家生成的正确答案。"
    ],
    "question_type": "simple",
    "page_num": 227,
    "source_file": "doc09_222_245.pdf"
  },
  {
    "id": "0095",
    "question": "Drm 表示什么？",
    "ground_truth": "训练数据集的经验分布。",
    "contexts": [
      "其中 Drm 表示训练数据集的经验分布。"
    ],
    "question_type": "simple",
    "page_num": 227,
    "source_file": "doc09_222_245.pdf"
  },
  {
    "id": "0096",
    "question": "Kimi k1.5 在 AIME 上达到了多少分？",
    "ground_truth": "77.5 分",
    "contexts": [
      "出色的推理性能：在长思维链模式下，Kimi k1.5 在多个基准测试和模态中达到了 SOTA 模型 OpenAI o1 正式版的水平，如在 AIME 上达到 77.5 分，MA TH 500上达到 96.2 分，在 Codeforces"
    ],
    "question_type": "simple",
    "page_num": 222,
    "source_file": "doc09_222_245.pdf"
  },
  {
    "id": "0097",
    "question": "ImageBind可以支持哪些数据的编码？",
    "ground_truth": "图像、文本、音频、深度、热成像和惯性测量单元（Inertial Measurement Unit，IMU）等多种数据的编码。",
    "contexts": [
      "ImageBind[256] 则可以支持图像、 文本、 音频、 深度、 热成像和惯性测量单元 （Inertial Measurement Unit，IMU） 等多种数据的编码。"
    ],
    "question_type": "simple",
    "page_num": 242,
    "source_file": "doc09_222_245.pdf"
  },
  {
    "id": "0098",
    "question": "整个系统通过什么实现多角色的协同训练？",
    "ground_truth": "资源池管理",
    "contexts": [
      "整个系统通过资源池管理实现多角色 （Actor/Critic/Ref Policy 等） 的协同训练， 具有分布式训练、弹性资源调度和可扩展的架构设计等特点。"
    ],
    "question_type": "simple",
    "page_num": 231,
    "source_file": "doc09_222_245.pdf"
  },
  {
    "id": "0099",
    "question": "如何初始化训练器的工作者？",
    "ground_truth": "trainer.init_workers()",
    "contexts": [
      "trainer.init_workers()"
    ],
    "question_type": "simple",
    "page_num": 232,
    "source_file": "doc09_222_245.pdf"
  },
  {
    "id": "0100",
    "question": "强化学习的核心在于实现什么?",
    "ground_truth": "智能体策略与价值函数的优化，从而提升性能与稳定性。",
    "contexts": [
      "强化学习的核心在于通过一系列精心设计的计算和控制机制，实现智能体策略与价值函数的优化，从而提升性能与稳定性。"
    ],
    "question_type": "simple",
    "page_num": 235,
    "source_file": "doc09_222_245.pdf"
  },
  {
    "id": "0101",
    "question": "在多模态学习中，KOSMOS-1如何结合语言理解与视觉感知能力，而ImageBind-LLM的训练方法又是如何支持多模态指令跟随能力的？",
    "ground_truth": "KOSMOS-1通过原生支持多模态数据，实现语言理解和视觉感知的结合，能够处理文本、图像和语音输入，从而胜任语言任务、感知-语言任务和视觉任务。而ImageBind-LLM则通过图像-文本对齐训练，利用一个可学习的绑定网络将LLaMA与ImageBind图像编码器的嵌入空间对齐，实现多模态的指令跟随能力。",
    "contexts": [
      "KOSMOS 是微软开发的一系列多模态大语言模型，将语言模型原生支持多模态数据作为目标。",
      "KOSMOS-1 从预训练阶段开始之初，便引入多模态数据，支持文本、图像和语音输入。",
      "ImageBind-LLM 通过仅进行图像-文本对齐训练，实现了多模态的指令跟随能力。",
      "在训练过程中，ImageBind-LLM 采用一个可学习的绑定网络，将LLaMA 与 ImageBind 图像编码器的嵌入空间对齐。"
    ],
    "question_type": "multi_context",
    "page_num": 240,
    "source_file": "doc09_222_245.pdf"
  },
  {
    "id": "0102",
    "question": "在评估模型回答的质量时，如何影响答案提取和人类偏好数据集的设计？",
    "ground_truth": "在评估模型回答的质量时，答案提取方法（如'严格'或'灵活'）会影响最终得分，而人类偏好数据集的设计则通过收集对话的选择与拒绝，确保数据集中的每个配对比较具有相同的权重。使用更有害的回应可能导致对话朝着更有害的方向发展。",
    "contexts": [
      "answer = extract_solution(solution_str=solution_str, method=method)",
      "收集数据的时候，... 只把数据集中的每个配对比较都当作二选一，并且权重相同"
    ],
    "question_type": "multi_context",
    "page_num": 234,
    "source_file": "doc09_222_245.pdf"
  },
  {
    "id": "0103",
    "question": "在深度学习中，如何结合强化学习和掩码预测方法来优化模型的性能？",
    "ground_truth": "强化学习中的PPO（Proximal Policy Optimization）方法可以通过特征转换和损失函数优化来提升模型性能，而掩码预测则通过自编码器的方式在视觉语言模型中增强对图像和文本的理解。这两者结合可以提升模型对缺失信息的恢复能力。",
    "contexts": [
      "run_ppo(config)",
      "掩码（Masking）预测方法扮演着重要角色， 它本质上属于自编码器的一种特殊变体。"
    ],
    "question_type": "multi_context",
    "page_num": 232,
    "source_file": "doc09_222_245.pdf"
  },
  {
    "id": "0104",
    "question": "X-VLM 的训练依赖于哪些大规模标注数据集？",
    "ground_truth": "COCO、Visual Genome、SBU 和 Conceptual Captions",
    "contexts": [
      "X-VLM 的训练依赖于多个大规模标注数据集，包括 COCO、Visual Genome、SBU 和 Conceptual Captions。"
    ],
    "question_type": "simple",
    "page_num": 258,
    "source_file": "doc10_246_268.pdf"
  },
  {
    "id": "0105",
    "question": "AnyGPT 能够处理哪些模态的输入与输出？",
    "ground_truth": "文本、语音、图像和音乐四种模态",
    "contexts": [
      "AnyGPT 统一了文本、语音、图像和音乐四种模态，并实现了任意模态组合的相互转换，为多模态交互提供了一个"
    ],
    "question_type": "simple",
    "page_num": 251,
    "source_file": "doc10_246_268.pdf"
  },
  {
    "id": "0106",
    "question": "MiniGPT-4 如何减少训练开销？",
    "ground_truth": "MiniGPT-4 将预训练的大语言模型和视觉编码器同时冻结，只需要单独训练线性投影层，使视觉特征和语言模型对齐。",
    "contexts": [
      "为了减少训练开销、避免全参数微调带来的潜在威胁， MiniGPT-4 将预训练的大语言模型和视觉编码器同时冻结，只需要单独训练线性投影层，使视觉特征和语言模型对齐。"
    ],
    "question_type": "simple",
    "page_num": 264,
    "source_file": "doc10_246_268.pdf"
  },
  {
    "id": "0107",
    "question": "语音到文本模态的转换是目前广泛采用的方法之一吗？",
    "ground_truth": "是的。",
    "contexts": [
      "语音到文本模态的转换是目前广泛采用的方法之一。"
    ],
    "question_type": "simple",
    "page_num": 249,
    "source_file": "doc10_246_268.pdf"
  },
  {
    "id": "0108",
    "question": "这张图片呈现了什么？",
    "ground_truth": "这是一张站着的鸟的照片。",
    "contexts": [
      "This is a photo of a standing bird."
    ],
    "question_type": "simple",
    "page_num": 256,
    "source_file": "doc10_246_268.pdf"
  },
  {
    "id": "0109",
    "question": "DataComp 框架的目标是什么？",
    "ground_truth": "旨在构建能够在 38 项下游任务中表现卓越的图像-文本数据组合。",
    "contexts": [
      "该框架基于标准化的CLIP 架构与预训练参数， 旨在构建能够在 38 项下游任务中表现卓越的图像-文本数据组合。"
    ],
    "question_type": "simple",
    "page_num": 256,
    "source_file": "doc10_246_268.pdf"
  },
  {
    "id": "0110",
    "question": "什么方法将RLHF从文本领域适配到视觉语言任务？",
    "ground_truth": "事实增强 RLHF（Factually Augmented RLHF）",
    "contexts": [
      "创新方法——事实增强 RLHF（Factually Augmented RLHF ） 。该方法将RLHF 从文本领域适配到视觉语言任务，通过在奖励模型中加入图像标题和真实多选题的额外事实信息，减少奖励滥用问题。"
    ],
    "question_type": "simple",
    "page_num": 259,
    "source_file": "doc10_246_268.pdf"
  },
  {
    "id": "0111",
    "question": "Qformer使用的基础模型是什么？",
    "ground_truth": "BertLMHeadModel",
    "contexts": [
      "Qformer = BertLMHeadModel(config=encoder_config)"
    ],
    "question_type": "simple",
    "page_num": 263,
    "source_file": "doc10_246_268.pdf"
  },
  {
    "id": "0112",
    "question": "在代码中，如何计算损失（loss）?",
    "ground_truth": "loss = outputs.loss",
    "contexts": [
      "loss = outputs.loss"
    ],
    "question_type": "simple",
    "page_num": 267,
    "source_file": "doc10_246_268.pdf"
  },
  {
    "id": "0113",
    "question": "MouSi模型如何结合不同视觉专家的信息，以更好地识别图像中的文本，并且如何通过ChatGPT提高文本描述的质量？",
    "ground_truth": "MouSi模型通过引入多个视觉专家（如CLIP、SAM和Layout Mv3专家）编码处理图像信息，并利用多视觉融合网络整合这些输出序列，以完成视觉问答和光学字符识别任务。同时，ChatGPT被用作自动化的文本质量评估者，检查生成的文本描述，修正其中的语义和语法错误，从而提高描述的质量。",
    "contexts": [
      "基于 MouSi 模型，当用户上传一张描绘风媒花授粉过程的图片并询问“哪些球果产生花粉？”时，该图片依次经过 CLIP 专家、SAM 专家、Layout Mv3 专家及其他专家的编码处理，产生多组不同的视觉标记。",
      "研究人员利用ChatGPT强大的语言理解和生成能力，让其作为一个自动化的文本质量评估者，对生成的5000个图像-文本对进行检查。"
    ],
    "question_type": "multi_context",
    "page_num": 254,
    "source_file": "doc10_246_268.pdf"
  },
  {
    "id": "0114",
    "question": "如何通过多补丁-单标记投影方法提高多模态大语言模型在视觉和语音处理中的效率？",
    "ground_truth": "多补丁-单标记投影方法能够有效降低视觉信号传输的冗余，并减少视觉大语言模型后续处理的计算成本，从而显著提高推理效率。与此同时，语音语言模型通过端到端架构直接学习音频特征与语言语义的映射关系，增强了模型在开放世界场景中的泛化能力。这两者结合可以实现视觉与语音模态的高效融合与处理。",
    "contexts": [
      "MouSi 模型提出了多补丁-单标记投影方法， 以按比例减少每个专家的输出标记数量。",
      "语音语言模型通过端到端架构直接学习音频特征与语言语义的映射关系，从而增强了模型在开放世界场景中的泛化能力。"
    ],
    "question_type": "multi_context",
    "page_num": 254,
    "source_file": "doc10_246_268.pdf"
  },
  {
    "id": "0115",
    "question": "在直接投射方法中，如何将语音特征与文本模态结合以生成输出，并与S2T模式有何不同？",
    "ground_truth": "直接投射方法通过连接器将语音特征映射到大语言模型的文本模态嵌入空间，生成的语音嵌入向量与输入文本的嵌入向量拼接，形成一个融合语音和文本信息的新嵌入向量。而S2T模式则以语音作为输入，通常用于自动语音识别任务，不需要文本编码器，而是通过音频编码器提取特征并生成文本输出。",
    "contexts": [
      "直接投射方法通过连接器将语音特征映射到大语言模型的文本模态嵌入空间，生成的语音嵌入向量与输入文本的嵌入向量拼接，形成一个融合语音和文本信息的新嵌入向量。",
      "S2T 是最基础的模式，模型以语音作为输入，并生成对应的文本输出。这种模式通常用于自动语音识别（Automatic Speech Recognition，ASR）任务。"
    ],
    "question_type": "multi_context",
    "page_num": 249,
    "source_file": "doc10_246_268.pdf"
  },
  {
    "id": "0116",
    "question": "思维链提示方式的目的是什么？",
    "ground_truth": "提升模型的推理能力。",
    "contexts": [
      "思维链（Chain-of-Thought，CoT）提示方式...使得模型不仅输出最终结果，还输出中间步骤，从而提升模型的推理能力。"
    ],
    "question_type": "simple",
    "page_num": 286,
    "source_file": "doc11_269_292.pdf"
  },
  {
    "id": "0117",
    "question": "大模型智能体的核心能力包括哪些？",
    "ground_truth": "感知、规划、记忆以及工具使用",
    "contexts": [
      "大模型智能体的核心能力涵盖了感知、规划、记忆以及工具使用，这些能力使其能够弥补传统大模型无法与外部世界交互的局限性。"
    ],
    "question_type": "simple",
    "page_num": 281,
    "source_file": "doc11_269_292.pdf"
  },
  {
    "id": "0118",
    "question": "智能体的概念核心聚焦于什么？",
    "ground_truth": "个体的自主性",
    "contexts": [
      "尤为关键的是，智能体概念的核心聚焦于个体的自主性，即赋予其运用意志、抉择判断以及付诸行动的能力，使之摆脱了单纯被动回应外部刺激的模式。"
    ],
    "question_type": "simple",
    "page_num": 271,
    "source_file": "doc11_269_292.pdf"
  },
  {
    "id": "0119",
    "question": "ToolLLM 提出了什么方法来提升开源大模型的工具使用能力？",
    "ground_truth": "ToolLLM 提出了通过构建 ToolBench 数据集，为3000 余种工具（涵盖16000多个 API）自动生成任务指令，并利用深度优先搜索算法自动化构建解决方案路径，从而对开源大模型进行微调，显著提升其基于教程学习的工具使用能力。",
    "contexts": [
      "ToolLLM 提出了通过构建 ToolBench 数据集，为3000 余种工具（涵盖16000多个 API）自动生成任务指令，并利用深度优先搜索算法自动化构建解决方案路径，从而对开源大模型进行微调，显著提升其基于教程学习的工具使用能力。"
    ],
    "question_type": "simple",
    "page_num": 281,
    "source_file": "doc11_269_292.pdf"
  },
  {
    "id": "0120",
    "question": "感知模块负责从环境中获取哪些形式的信息？",
    "ground_truth": "文本、视觉、听觉等多种形式的信息",
    "contexts": [
      "感知模块负责从环境中获取文本、 视觉、 听觉等多种形式的信息, 并将其传递给其他模块进行处理。"
    ],
    "question_type": "simple",
    "page_num": 276,
    "source_file": "doc11_269_292.pdf"
  },
  {
    "id": "0121",
    "question": "无反馈规划指的是什么？",
    "ground_truth": "无反馈规划（Planning without feedback）指在规划阶段一次性生成完整的任务和子任务拆分计划，并严格按照该计划逐步执行，而不根据外界变化进行实时调整。",
    "contexts": [
      "无反馈规划（Planning without feedback）指在规划阶段一次性生成完整的任务和子任务拆分计划，并严格按照该计划逐步执行，而不根据外界变化进行实时调整。"
    ],
    "question_type": "simple",
    "page_num": 278,
    "source_file": "doc11_269_292.pdf"
  },
  {
    "id": "0122",
    "question": "大语言模型在视觉感知领域的表现如何？",
    "ground_truth": "尽管大语言模型在理解和处理多轮对话方面展现了卓越的性能，但仍然无法处理视觉模态信息。",
    "contexts": [
      "尽管大语言模型在理解和处理多轮对话方面展现了卓越的性能，但仍然无法处理视觉模态信息。"
    ],
    "question_type": "simple",
    "page_num": 277,
    "source_file": "doc11_269_292.pdf"
  },
  {
    "id": "0123",
    "question": "最终构建了多少条轨迹？",
    "ground_truth": "1,866 条轨迹。",
    "contexts": [
      "最终构建了 1,866 条轨迹。"
    ],
    "question_type": "simple",
    "page_num": 291,
    "source_file": "doc11_269_292.pdf"
  },
  {
    "id": "0124",
    "question": "MemoryBank 的核心组件之一是什么？",
    "ground_truth": "记忆存储（Memory Stroage）是 MemoryBank 的核心组件之一，存储了丰富的信息，包含日常对话记录、过去的事件总结和用户个性评估的演变， 从而构建了一个动态的多层次记忆全景图。",
    "contexts": [
      "记忆存储（Memory Stroage）是 MemoryBank 的核心组件之一，存储了丰富的信息，包含日常对话记录、 过去的事件总结和用户个性评估的演变， 从而构建了一个动态的多层次记忆全景图。"
    ],
    "question_type": "simple",
    "page_num": 292,
    "source_file": "doc11_269_292.pdf"
  },
  {
    "id": "0125",
    "question": "大模型智能体是如何解决早期基于强化学习的智能体面临的哪些问题的？",
    "ground_truth": "大模型智能体通过融合大模型的强大多模态理解与生成能力，突破了早期智能体在知识迁移、计算成本高和大量数据训练等方面的限制，同时也解决了大模型本身无法感知外部环境和调用外部工具的问题。",
    "contexts": [
      "大模型智能体具有感知、决策、行动和记忆的能力，...以支持持续学习和适应动态环境。",
      "随着大模型的发展，其在诸多领域展现出惊人的语义处理能力，能够快速生成文本、回答问题，甚至完成一些复杂的知识推理任务。"
    ],
    "question_type": "multi_context",
    "page_num": 272,
    "source_file": "doc11_269_292.pdf"
  },
  {
    "id": "0126",
    "question": "如何结合视觉信息和思维链提示来提升大语言模型在多模态感知中的表现？",
    "ground_truth": "结合视觉信息和思维链提示可以显著提升大语言模型在多模态感知中的表现。通过将视觉输入转化为文本描述，能够为智能体提供更全面的环境理解，而思维链提示则能提升推理过程的准确性和可解释性。具体来说，研究人员通过将视觉编码与语言理解对齐的方法增强了模型对视觉信息的感知能力，同时利用思维链提示降低推理难度，提高最终结果的准确率。",
    "contexts": [
      "将视觉信息与其他模态数据相结合，能够使智能体对外部环境的理解更加全面且精准。",
      "通过实验发现，使用由不同人员编写的符号推理示例在准确率上存在高达 28.2% 的差异，而改变范例的顺序在大多数任务中则只产生了不到 2% 的变化。"
    ],
    "question_type": "multi_context",
    "page_num": 277,
    "source_file": "doc11_269_292.pdf"
  },
  {
    "id": "0127",
    "question": "在智能体如何结合长期记忆和视觉感知能力来提升其任务执行效率与环境理解能力上，有哪些关键点？",
    "ground_truth": "智能体通过长期记忆构建记忆库实现知识的持久化存储与高效调用，并在执行任务中系统化地存储经验和知识。当需要处理陌生场景时，结合视觉信息能够提升智能体的文本感知能力，而将视觉输入转换为文本描述的技术可以帮助智能体理解环境信息。通过将长期记忆与视觉感知结合，智能体能够更全面地理解外部环境，从而提升任务的执行效率和准确性。",
    "contexts": [
      "长期记忆通过构建记忆库来实现管理和检索，支持知识的持久化存储与高效调用。",
      "在陌生场景下，提升智能体的文本感知能力显得尤为重要。"
    ],
    "question_type": "multi_context",
    "page_num": 279,
    "source_file": "doc11_269_292.pdf"
  },
  {
    "id": "0128",
    "question": "如何完成角色扮演的交互过程？",
    "ground_truth": "可以直接调用 step() 函数完成角色扮演的交互过程，获取两个智能体的新一轮输出。",
    "contexts": [
      "基于 RolePlaying 的结构，可以直接调用 step() 函数完成角色扮演的交互过程，获取两个智能体的新一轮输出："
    ],
    "question_type": "simple",
    "page_num": 299,
    "source_file": "doc12_293_315.pdf"
  },
  {
    "id": "0129",
    "question": "如何构建消息？",
    "ground_truth": "message = construct_message(agent_contexts_other, question, 2*round - 1 )",
    "contexts": [
      "message = construct_message(agent_contexts_other, question, 2*round - 1 )"
    ],
    "question_type": "simple",
    "page_num": 296,
    "source_file": "doc12_293_315.pdf"
  },
  {
    "id": "0130",
    "question": "LangChain 中的记忆模块提供了哪些基本操作？",
    "ground_truth": "记忆系统需要支持两个基本操作：读取和写入。",
    "contexts": [
      "记忆系统需要支持两个基本操作：读取和写入。"
    ],
    "question_type": "simple",
    "page_num": 313,
    "source_file": "doc12_293_315.pdf"
  },
  {
    "id": "0131",
    "question": "Jimmy has how much money?",
    "ground_truth": "$18",
    "contexts": [
      "Jimmy has $16 + $2 = $ boxed{18}$."
    ],
    "question_type": "simple",
    "page_num": 297,
    "source_file": "doc12_293_315.pdf"
  },
  {
    "id": "0132",
    "question": "LangChain 中的链有哪些类型？",
    "ground_truth": "除了上例中的 LLMChain，LangChain 中的链还包含 RouterChain、SimpleSequentialChain、SequentialChain、TransformChain 等。",
    "contexts": [
      "除了上例中的 LLMChain，LangChain 中的链还包含 RouterChain、SimpleSequentialChain、SequentialChain、TransformChain 等。"
    ],
    "question_type": "simple",
    "page_num": 312,
    "source_file": "doc12_293_315.pdf"
  },
  {
    "id": "0133",
    "question": "Jimmy has how much money?",
    "ground_truth": "$18",
    "contexts": [
      "Yes, based on the information provided and the solutions given by other agents, Jimmy has $18."
    ],
    "question_type": "simple",
    "page_num": 297,
    "source_file": "doc12_293_315.pdf"
  },
  {
    "id": "0134",
    "question": "What is the function of a retriever?",
    "ground_truth": "检索器是一个接口，其功能是基于非结构化查询返回相应的文档。",
    "contexts": [
      "Retrievers（检索器） 是一个接口， 其功能是基于非结构化查询返回相应的文档。"
    ],
    "question_type": "simple",
    "page_num": 309,
    "source_file": "doc12_293_315.pdf"
  },
  {
    "id": "0135",
    "question": "类公开了多少个方法？",
    "ground_truth": "两个",
    "contexts": [
      "类公开了两个方法：一个用于文档嵌入表示，另一个用于查询嵌入表示。"
    ],
    "question_type": "simple",
    "page_num": 308,
    "source_file": "doc12_293_315.pdf"
  },
  {
    "id": "0136",
    "question": "如何在使用 ChatOpenAI 进行对话时获取 assistant 的回复？",
    "ground_truth": "通过执行代码 chat_llm_chain.predict(human_input='Hi there my friend') 可以得到 assistant 的回复，然后可以继续通过 chat_llm_chain.predict(human_input='Not too bad - how are you?') 来获取进一步的对话。",
    "contexts": [
      "chat_llm_chain.predict(human_input='Hi there my friend') 可以得到如下输出结果：'Hello! How can I assist you today, my friend?'",
      "content = completion['choices'][0]['message']['content']"
    ],
    "question_type": "multi_context",
    "page_num": 315,
    "source_file": "doc12_293_315.pdf"
  },
  {
    "id": "0137",
    "question": "如何使用向量存储的检索器来获取与用户输入相关的文档，并同时使用大语言模型来处理用户的对话？",
    "ground_truth": "可以通过get_relevant_documents方法或异步调用aget_relevant_documents方法获得与查询文档最相关的文档，同时使用大语言模型结合ChatPromptTemplate来处理用户的对话。",
    "contexts": [
      "它的使用非常简单，可以通过get_relevant_documents方法或通过异步调用aget_relevant_documents方法获得与查询文档最相关的文档。",
      "prompt = ChatPromptTemplate.from_messages([SystemMessage(content=\"You are a chatbot having a conversation with a human.\"), MessagesPlaceholder(variable_name=\"chat_history\"), HumanMessagePromptTemplate.from_template(\"{human_input}\")])"
    ],
    "question_type": "multi_context",
    "page_num": 310,
    "source_file": "doc12_293_315.pdf"
  },
  {
    "id": "0138",
    "question": "如何使用基于向量存储的检索器来检索与查询文档最相关的文档，以及向量存储的主要功能是什么？",
    "ground_truth": "基于向量存储的检索器通过get_relevant_documents方法或异步调用aget_relevant_documents方法获得与查询文档最相关的文档。向量存储的主要功能是将数据转化为嵌入表示并存储生成的嵌入向量，利用这些嵌入向量在查询阶段检索与查询内容最相似的文档。",
    "contexts": [
      "它的使用非常简单， 可以通过get_relevant_documents 方法或通过异步调用 aget_relevant_documents 方法获得与查询文档最相关的文档。",
      "V ector Stores（向量存储）是存储和检索非结构化数据的主要方式之一。它首先将数据转化为嵌入表示，然后存储生成的嵌入向量。在查询阶段，系统会利用这些嵌入向量来检索与查询内容“最相似”的文档。"
    ],
    "question_type": "multi_context",
    "page_num": 310,
    "source_file": "doc12_293_315.pdf"
  },
  {
    "id": "0139",
    "question": "Coze是一个什么类型的平台？",
    "ground_truth": "Coze（扣子）是一个大模型智能体开发平台。",
    "contexts": [
      "Coze（扣子）是一个大模型智能体开发平台，整合了插件、长短期记忆、工作流、卡片等丰富功能，能够以低门槛、快速搭建个性化或具备商业价值的智能体，并发布到豆包、飞书、网页等多种平台，实现全场景覆盖。"
    ],
    "question_type": "simple",
    "page_num": 322,
    "source_file": "doc13_316_339.pdf"
  },
  {
    "id": "0140",
    "question": "索引模块在 RAG 系统中的核心任务是什么？",
    "ground_truth": "其核心任务是将文档划分为可管理的片段（Chunk），也成为“块”，为后续的检索和生成提供组织良好的内容基础。",
    "contexts": [
      "索引（Index）是 RAG 系统中至关重要的过程，其核心任务是将文档划分为可管理的片段（Chunk），也成为“块”，为后续的检索和生成提供组织良好的内容基础。"
    ],
    "question_type": "simple",
    "page_num": 334,
    "source_file": "doc13_316_339.pdf"
  },
  {
    "id": "0141",
    "question": "OpenAI 于哪一年推出了 SearchGPT？",
    "ground_truth": "2024",
    "contexts": [
      "OpenAI 于 2024 年推出了 SearchGPT，进一步推动了 AI 搜索技术的发展"
    ],
    "question_type": "simple",
    "page_num": 326,
    "source_file": "doc13_316_339.pdf"
  },
  {
    "id": "0142",
    "question": "大语言模型在本科低年级知识点记忆能力测试中的表现是多少？",
    "ground_truth": "73.6%",
    "contexts": [
      "根据 LLMEV AL-3[411] 评测结果，GPT-4 Turbo 在本科低年级知识点记忆能力测试中的表现仅为 73.6%。"
    ],
    "question_type": "simple",
    "page_num": 325,
    "source_file": "doc13_316_339.pdf"
  },
  {
    "id": "0143",
    "question": "检索增强生成的映射关系是基于什么建立的？",
    "ground_truth": "基于 D 建立从 Q 到 A 的映射关系。",
    "contexts": [
      "应用f 的任务是基于 D 建立从 Q 到 A 的映射关系。"
    ],
    "question_type": "simple",
    "page_num": 326,
    "source_file": "doc13_316_339.pdf"
  },
  {
    "id": "0144",
    "question": "Modular RAG 系统由多少个模块组成？",
    "ground_truth": "多个独立但紧密协作的模块",
    "contexts": [
      "Modular RAG 系统由多个独立但紧密协作的模块组成， 每个模块负责处理特定的功能或任务。"
    ],
    "question_type": "simple",
    "page_num": 333,
    "source_file": "doc13_316_339.pdf"
  },
  {
    "id": "0145",
    "question": "显性事实查询的答案通常存在于什么形式的文档中？",
    "ground_truth": "显性事实查询的答案通常直接存在于特定领域的文档或文档片段中，以明文形式呈现。",
    "contexts": [
      "显性事实查询的答案通常直接存在于特定领域的文档或文档片段中，以明文形式呈现。"
    ],
    "question_type": "simple",
    "page_num": 329,
    "source_file": "doc13_316_339.pdf"
  },
  {
    "id": "0146",
    "question": "如何通过查询改写和技能添加来提升大语言模型在复杂问题求解中的表现？",
    "ground_truth": "查询改写可以通过对用户的原始查询进行语义优化和结构调整，增强检索效率，并确保生成内容与用户需求的高度匹配。与此同时，为智能体添加技能，比如多模态插件，可以扩展模型的功能，使其能够处理多样化的信息，从而提升整体表现。",
    "contexts": [
      "查询转换（Query Transformation）又称查询改写（Query Rewrite），是指通过对用户的原始查询进行改写或重构，将其转换为更适合检索和生成的形式，从而提升系统的理解能力和检索效果。",
      "如果模型能力能覆盖智能体功能，则仅需编写提示词；否则需添加技能拓展能力。例如，文本类模型无法处理多模态内容，可绑定多模态插件理解PPT、图片等。"
    ],
    "question_type": "multi_context",
    "page_num": 339,
    "source_file": "doc13_316_339.pdf"
  },
  {
    "id": "0147",
    "question": "在处理复杂查询时，RAG系统如何提高检索效果，并同时应对多模态数据的挑战？",
    "ground_truth": "RAG系统通过检索前优化模块改善检索效果，具体方法包括查询扩展，以丰富用户查询的内容并减少语言歧义。此外，随着系统支持多模态数据，跨模态对齐和生成能力的提升成为新的挑战，要求系统能够有效处理不同类型的数据并确保输出连贯且具有上下文相关性。",
    "contexts": [
      "预检索模块通过对用户查询进行重构、扩展或语义优化，能够减少语言歧义和表述模糊，从而为下游检索任务提供更精准的输入。",
      "多样化的数据类型需要统一的检索框架，而目前的跨模态检索策略尚不足以同时有效处理文本、图像以及潜在的视频或音频数据。"
    ],
    "question_type": "multi_context",
    "page_num": 338,
    "source_file": "doc13_316_339.pdf"
  },
  {
    "id": "0148",
    "question": "如何在Coze平台上构建一个夸夸机器人，并确保其能够自动选择最佳的动作序列？",
    "ground_truth": "在Coze平台构建夸夸机器人可以通过简单的五个步骤完成。首先，创建一个智能体，输入名称和功能介绍，并编写提示词以定义人设与回复逻辑。这些提示词将影响智能体在会话中的回复效果。同时，智能体的核心思想是使用大语言模型作为推理引擎，以确定要采取的动作序列。通过将大语言模型与动作列表结合，智能体可以自动选择最佳的动作序列，从而实现自动化决策和行动。",
    "contexts": [
      "使用 Coze 平台可以通过以下简单的五个步骤就可以构造快速搭建一个“夸夸机器人”，并在多个平台提供对外服务。",
      "智能体的核心思想是使用大语言模型来选择要执行的一系列动作。在链中，操作序列是硬编码在代码中的。"
    ],
    "question_type": "multi_context",
    "page_num": 322,
    "source_file": "doc13_316_339.pdf"
  },
  {
    "id": "0149",
    "question": "RRR 方法在预检索阶段引入了什么模块？",
    "ground_truth": "查询重写模块",
    "contexts": [
      "文献 [425] 提出的“重写-检索-阅读” （Rewrite-Retrieve-Read，RRR）方法就是一个典型的线性 RAG 流模式。在预检索阶段，RRR 方法引入了查询重写模块，该模块是基于 T5-large 模型微调的小型可训练语言模型。"
    ],
    "question_type": "simple",
    "page_num": 349,
    "source_file": "doc14_340_362.pdf"
  },
  {
    "id": "0150",
    "question": "后检索分支模式的特点是什么？",
    "ground_truth": "后检索分支的特点在于单一查询驱动的检索过程，而并行生成则聚焦于对不同文档块的独立处理。",
    "contexts": [
      "后检索分支的特点在于单一查询驱动的检索过程，而并行生成则聚焦于对不同文档块的独立处理。"
    ],
    "question_type": "simple",
    "page_num": 351,
    "source_file": "doc14_340_362.pdf"
  },
  {
    "id": "0151",
    "question": "权重 λ(d, q) 是由什么确定的？",
    "ground_truth": "权重 λ(d, q) 由文档 d 和输入查询 q 之间的相似度得分确定。",
    "contexts": [
      "权重 λ(d, q) 由文档 d 和输入查询 q 之间的相似度得分确定。"
    ],
    "question_type": "simple",
    "page_num": 348,
    "source_file": "doc14_340_362.pdf"
  },
  {
    "id": "0152",
    "question": "文献[440] 采用了与哪些方法类似的方式来生成高质量的大规模数据？",
    "ground_truth": "与 Self-RAG[431] 和 SAIL[441] 类似的方法",
    "contexts": [
      "文献[440] 采用了与 Self-RAG[431] 和 SAIL[441] 类似的方法，设计了一套自动化的数据生成流程，以优化查询、检索信息并生成精确的响应，同时减少人工干预所需的资源和时间成本。"
    ],
    "question_type": "simple",
    "page_num": 357,
    "source_file": "doc14_340_362.pdf"
  },
  {
    "id": "0153",
    "question": "RAG 系统可以通过哪种方式直接进行判断？",
    "ground_truth": "RAG 系统还可以通过大语言模型直接进行判断（LLM Judge）。",
    "contexts": [
      "RAG 系统还可以通过大语言模型直接进行判断（LLM Judge） 。"
    ],
    "question_type": "simple",
    "page_num": 347,
    "source_file": "doc14_340_362.pdf"
  },
  {
    "id": "0154",
    "question": "RQ-RAG 方法提出了哪三种选择方式？",
    "ground_truth": "基于PPL的选择基于置信度的选择基于集成的选择集成",
    "contexts": [
      "基于PPL的选择基于置信度的选择集成"
    ],
    "question_type": "simple",
    "page_num": 359,
    "source_file": "doc14_340_362.pdf"
  },
  {
    "id": "0155",
    "question": "RQ-RAG 在推理过程中采用了什么策略？",
    "ground_truth": "RQ-RAG 在推理过程中采用了一种树形解码策略。",
    "contexts": [
      "RQ-RAG 在推理过程中采用了一种树形解码策略，其具体流程如图9.18所示。"
    ],
    "question_type": "simple",
    "page_num": 359,
    "source_file": "doc14_340_362.pdf"
  },
  {
    "id": "0156",
    "question": "GTE 模型在预训练阶段使用了多少对无标注的文本对？",
    "ground_truth": "约8 亿对无标注的文本对",
    "contexts": [
      "GTE 模型在预训练阶段， 使用了约8 亿对无标注的文本对， 数据来源多样， 包括网页数据"
    ],
    "question_type": "simple",
    "page_num": 355,
    "source_file": "doc14_340_362.pdf"
  },
  {
    "id": "0157",
    "question": "在医疗信息检索中，如何通过动态权重分配机制提高检索系统的性能，同时结合MMD和MPD数据集的特性进行优化？",
    "ground_truth": "通过倒数排名融合（RRF）技术，可以有效整合多个检索结果的排名，从而增强整体预测性能与排名精度。同时，MMD数据集提供了可靠的医学信息检索评估基准，而MPD数据集则确保了分析的准确性和可靠性，经过预处理和清洗的文献增强了数据的质量。因此，结合这些数据集的特性，利用RRF的动态权重分配机制，可以针对特定领域进行微调，以应对复杂的查询挑战。",
    "contexts": [
      "MMD 是一个综合且可靠的医学信息检索评估基准，专注于医疗领域的检索系统性能测试。",
      "倒数排名融合 （Reciprocal Rank Fusion，RRF） 是一种集成技术， 专门用于将多个检索结果的排名整合为统一的列表。"
    ],
    "question_type": "multi_context",
    "page_num": 357,
    "source_file": "doc14_340_362.pdf"
  },
  {
    "id": "0158",
    "question": "在使用检索增强生成（RAG）方法时，如何优化大语言模型以减少生成中的幻觉问题，同时又能确保生成结果的多样性和准确性？",
    "ground_truth": "通过实施 Hallucination Aware Tuning（RAG-HA T）方法来训练幻觉检测模型，从而识别和解释幻觉，并提供防御性建议。此外，利用预检索分支模式，通过生成多个子查询并并行检索，能够提升检索的全面性与生成结果的多样性，最终通过融合模块整合生成的答案，以确保结果的质量和准确性。",
    "contexts": [
      "大模型幻觉指的是大语言模型生成的内容中出现与事实不符、缺乏依据或与输入信息相矛盾的表述。在实际应用中，即使采用检索增强生成（RAG）方法，大语言模型仍然可能出现幻觉问题，例如对检索到的内容进行错误或扭曲的解释。",
      "预检索分支是一种通过生成多个子查询并并行检索的模式，用于提高检索的全面性和生成结果的多样性。最终，这些生成的答案通过融合模块进行整合，形成最终结果。"
    ],
    "question_type": "multi_context",
    "page_num": 360,
    "source_file": "doc14_340_362.pdf"
  },
  {
    "id": "0159",
    "question": "混合检索和递归型循环检索在信息检索中的作用是什么？",
    "ground_truth": "混合检索结合了稀疏检索和稠密检索的优势，能够提高检索系统的效率和效果，尤其在处理同义词和复杂语义关系时表现出色。而递归型循环检索则通过每一步依赖于前一步的输出，逐层深入挖掘信息，适合需要分步推理的任务场景。两者共同促进了现代信息检索的精确性和深度。",
    "contexts": [
      "混合检索（Hybrid Retrieval）是结合稀疏检索和稠密检索优势的检索方法，用于提升检索系统的效率和效果。",
      "递归型检索的显著特点在于每一步都依赖于前一步的输出，并通过不断加深检索过程，逐步挖掘更深层次的信息。"
    ],
    "question_type": "multi_context",
    "page_num": 341,
    "source_file": "doc14_340_362.pdf"
  },
  {
    "id": "0160",
    "question": "状态空间模型中，A 矩阵代表什么？",
    "ground_truth": "状态转移矩阵",
    "contexts": [
      "其中，A 是状态转移矩阵、B 表示控制量对状态量的影响、C 表示当前状态量对输出影响和 D 表示当前控制量对输出影响，"
    ],
    "question_type": "simple",
    "page_num": 385,
    "source_file": "doc15_363_386.pdf"
  },
  {
    "id": "0161",
    "question": "final_rag_chain 是什么？",
    "ground_truth": "final_rag_chain 是一个包含 retrieval_chain_rag_fusion 和 question 的组合。",
    "contexts": [
      "final_rag_chain = (\"context\": retrieval_chain_rag_fusion, \"question\": itemgetter( \"question\") | prompt | llm | StrOutputParser())"
    ],
    "question_type": "simple",
    "page_num": 379,
    "source_file": "doc15_363_386.pdf"
  },
  {
    "id": "0162",
    "question": "ROUGE 主要用于评估什么任务？",
    "ground_truth": "生成摘要任务",
    "contexts": [
      "ROUGE（Recall-Oriented Understudy for Gisting Evaluation ）ROUGE 主要用于评估生成摘要任务，衡量生成内容与参考答案的文本片段重叠程度。"
    ],
    "question_type": "simple",
    "page_num": 372,
    "source_file": "doc15_363_386.pdf"
  },
  {
    "id": "0163",
    "question": "使用的嵌入模型是什么？",
    "ground_truth": "HuggingFaceEmbedding(model_name= \"BAAI/bge-large-zh-v1.5\")",
    "contexts": [
      "embed_model = HuggingFaceEmbedding(model_name= \"BAAI/bge-large-zh-v1.5\")"
    ],
    "question_type": "simple",
    "page_num": 378,
    "source_file": "doc15_363_386.pdf"
  },
  {
    "id": "0164",
    "question": "OmniEval包含多少个自动生成的测试示例？",
    "ground_truth": "11400",
    "contexts": [
      "OmniEval包含 11400 个自动生成的测试示例和 1700 个人工标注的测试示例。"
    ],
    "question_type": "simple",
    "page_num": 370,
    "source_file": "doc15_363_386.pdf"
  },
  {
    "id": "0165",
    "question": "大语言模型推理遵循什么模式？",
    "ground_truth": "自回归模式",
    "contexts": [
      "图 10.1 大语言模型推理遵循自回归模式"
    ],
    "question_type": "simple",
    "page_num": 381,
    "source_file": "doc15_363_386.pdf"
  },
  {
    "id": "0166",
    "question": "文档评分的合成方法包括哪几种版本？",
    "ground_truth": "离散版本、连续版本和混合版本。",
    "contexts": [
      "合成方法旨在通过多种策略对文档进行重新排序，以提高检索结果的相关性。这些方法包括离散版本、连续版本和混合版本。"
    ],
    "question_type": "simple",
    "page_num": 364,
    "source_file": "doc15_363_386.pdf"
  },
  {
    "id": "0167",
    "question": "在使用OllamaLLM生成搜索查询时，如何确保检索结果既全面又多样化？",
    "ground_truth": "在使用OllamaLLM生成搜索查询时，可以通过确保检索组件能够全面覆盖用户的不同信息需求，避免信息冗余，从而实现检索结果的全面性和多样性。同时，检索组件需要具备动态适应性，以快速适应知识库的变化，确保在检索过程中能够获取到最新的相关信息。",
    "contexts": [
      "llm = OllamaLLM(model=\"qwen2.5\")",
      "检索组件需要确保其检索结果能够全面覆盖查询的不同维度，同时避免信息冗余。多样性评估旨在衡量检索结果是否包含多样化的视角或信息来源，尤其是在处理开放域问答或多轮对话时，这一点尤为重要。",
      "由于RAG 系统依赖动态更新的知识库（如互联网爬取的数据），检索组件需要能够快速适应知识库的变化。"
    ],
    "question_type": "multi_context",
    "page_num": 378,
    "source_file": "doc15_363_386.pdf"
  },
  {
    "id": "0168",
    "question": "大语言模型在提升效率方面有哪些研究方向，并且在自然语言处理任务中展现了哪些能力？",
    "ground_truth": "大语言模型在自然语言理解与生成等任务中展现了卓越的能力，并推动了人工智能技术的快速发展。为应对资源消耗的挑战，研究者们从模型、数据和计算框架等多个角度探索了提升大模型效率的优化方法，包括模型压缩、量化、数据选择和优化训练框架等技术。",
    "contexts": [
      "大语言模型在自然语言理解与生成等任务中展现了卓越的能力，不仅推动了人工智能技术的快速发展，也为社会各领域的应用带来了深远的影响。",
      "因此，如何在保持模型性能的同时提高其效率，已成为当前大模型研究中的重要议题。为应对这一问题，研究者们从模型、数据和计算框架等多个角度探索了提升大模型效率的优化方法。"
    ],
    "question_type": "multi_context",
    "page_num": 378,
    "source_file": "doc15_363_386.pdf"
  },
  {
    "id": "0169",
    "question": "如何评估生成内容的创造性和真实性？",
    "ground_truth": "评估生成内容的创造性可以通过语义嵌入的多样性指标和冗余度分析来实现。高语义相似性分值表明生成的内容更具创造性和多样性，而低冗余度意味着信息表达新颖且丰富。在真实性方面，真实性检测通过如FEVER评分等指标，评估生成内容与事实的匹配程度，以避免生成“幻觉”内容。",
    "contexts": [
      "该指标通过评估生成内容中各部分的语义相似性来计算其多样性，分值越高，表明生成内容在语义表达上越具有差异性，从而更具创造性和多样性。",
      "真实性检测在避免生成“幻觉”内容方面发挥关键作用，FEVER评分是一种常用的方法，用于测试生成内容与事实的匹配程度。"
    ],
    "question_type": "multi_context",
    "page_num": 374,
    "source_file": "doc15_363_386.pdf"
  },
  {
    "id": "0170",
    "question": "降低训练精度被广泛认为是减少训练成本的什么方向之一？",
    "ground_truth": "降低训练精度被广泛认为是减少训练成本最具潜力的方向之一。",
    "contexts": [
      "降低训练精度被广泛认为是减少训练成本最具潜力的方向之一， 它可以提供更高的速度、 更小的内存占用以及更低的通信开销。"
    ],
    "question_type": "simple",
    "page_num": 397,
    "source_file": "doc16_387_409.pdf"
  },
  {
    "id": "0171",
    "question": "ExpertSparsity 是什么？",
    "ground_truth": "ExpertSparsity 是一种专家稀疏化方法，用于 MoE 中的前馈神经网络专家的稀疏化。",
    "contexts": [
      "ExpertSparsity 是一种专家稀疏化方法，用于 MoE 中的前馈神经网络专家的稀疏化。"
    ],
    "question_type": "simple",
    "page_num": 393,
    "source_file": "doc16_387_409.pdf"
  },
  {
    "id": "0172",
    "question": "什么是知识蒸馏？",
    "ground_truth": "知识蒸馏（Knowledge Distillation, KD）是一种广泛应用的模型压缩技术，其核心思想是将大型模型 （称为教师模型，Teacher Model） 的知识迁移到较小的模型 （称为学生模型，Student Model） 中。",
    "contexts": [
      "知识蒸馏（Knowledge Distillation, KD）是一种广泛应用的模型压缩技术，其核心思想是将大型模型 （称为教师模型，Teacher Model） 的知识迁移到较小的模型 （称为学生模型，Student Model） 中。"
    ],
    "question_type": "simple",
    "page_num": 393,
    "source_file": "doc16_387_409.pdf"
  },
  {
    "id": "0173",
    "question": "SpecInfer 采用了什么方法来构建词元树？",
    "ground_truth": "SpecInfer 采用了一种静态扩展策略，以预设的扩展配置表示为向量 < k 1, k2, ..., km >。",
    "contexts": [
      "SpecInfer 采用了一种静态扩展策略，以预设的扩展配置表示为向量 < k 1, k2, ..., km >。"
    ],
    "question_type": "simple",
    "page_num": 406,
    "source_file": "doc16_387_409.pdf"
  },
  {
    "id": "0174",
    "question": "整个流程从什么开始？",
    "ground_truth": "整个流程从初始化一个预训练的学生模型开始。",
    "contexts": [
      "整个流程从初始化一个预训练的学生模型开始，依次通过以下步骤进行："
    ],
    "question_type": "simple",
    "page_num": 396,
    "source_file": "doc16_387_409.pdf"
  },
  {
    "id": "0175",
    "question": "FP8 的无穷大表示是什么？",
    "ground_truth": "N/A",
    "contexts": [
      "无穷大（Infinity） N/A S.11111.002"
    ],
    "question_type": "simple",
    "page_num": 398,
    "source_file": "doc16_387_409.pdf"
  },
  {
    "id": "0176",
    "question": "稀疏化的目标是什么？",
    "ground_truth": "其目标是通过增加模型参数或激活中零值元素的比例，降低计算复杂度和内存使用。",
    "contexts": [
      "稀疏化 （Sparsification） 是一种模型压缩技术， 其目标是通过增加模型参数或激活中零值元素的比例，降低计算复杂度和内存使用。"
    ],
    "question_type": "simple",
    "page_num": 391,
    "source_file": "doc16_387_409.pdf"
  },
  {
    "id": "0177",
    "question": "MSS算法是什么的验证方法？",
    "ground_truth": "MSS算法用于验证推测词元树。",
    "contexts": [
      "为了使用随机解码验证推测词元树，SpecInfer 引入了一种多步推测采样（Multi-step Speculative Sampling，MSS）算法来进行验证。"
    ],
    "question_type": "simple",
    "page_num": 406,
    "source_file": "doc16_387_409.pdf"
  },
  {
    "id": "0178",
    "question": "FP8-LM 框架是由哪些组织的研究人员开源的？",
    "ground_truth": "Microsoft Azure 和 Microsoft Research",
    "contexts": [
      "为了解决这一问题，Microsoft Azure 和 Microsoft Research 的研究人员开源了 FP8-LM 框架。"
    ],
    "question_type": "simple",
    "page_num": 398,
    "source_file": "doc16_387_409.pdf"
  },
  {
    "id": "0179",
    "question": "在使用 FP8 进行大语言模型训练时，如何解决低精度全局归约操作引发的数值不稳定性问题？",
    "ground_truth": "为了解决使用 FP8 进行大语言模型训练时出现的数值不稳定性问题，可以采用后缩放方法。在全局归约过程中，先对梯度进行求和，然后在梯度收集的过程中进行除法缩放。这种方法使梯度值接近 FP8 数据类型的最大值，有效缓解了下溢问题，尽管可能引发上溢问题。",
    "contexts": [
      "使用 FP8 进行大语言模型的训练并非易事，主要面临数据下溢或上溢问题，以及因 FP8 数据格式动态范围较窄和精度较低而引发的量化误差，这些问题可能导致数值不稳定性，甚至在训练过程中出现不可逆的发散现象。",
      "具体而言，在全局归约过程中，跨 GPU 聚合梯度通常有两种标准方法：预缩放和后缩放。后缩放方法使梯度值接近 FP8 数据类型的最大值，有效缓解了下溢问题。"
    ],
    "question_type": "multi_context",
    "page_num": 398,
    "source_file": "doc16_387_409.pdf"
  },
  {
    "id": "0180",
    "question": "在大语言模型中，如何通过稀疏化技术和推测采样技术来优化模型的性能和生成多样化的输出？",
    "ground_truth": "稀疏化技术通过增加模型参数或激活中零值元素的比例，降低计算复杂度和内存使用，从而优化模型性能。推测采样技术则在保持输出分布等效性的同时，与核采样的概率特性一致，生成多样化的词元序列，适配解码框架。",
    "contexts": [
      "稀疏化利用计算过程中对零元素的高效忽略，实现了资源的节约和性能的优化。",
      "推测采样在保持输出分布等效性的同时，与核采样的概率特性一致，从而能够生成多样化的词元序列。"
    ],
    "question_type": "multi_context",
    "page_num": 391,
    "source_file": "doc16_387_409.pdf"
  },
  {
    "id": "0181",
    "question": "在训练大型语言模型时，如何通过知识蒸馏和并行计算来提高学生模型的生成准确性和训练效率？",
    "ground_truth": "通过采用反向 KLD 的知识蒸馏方法，MiniLLM 引导学生模型更加关注教师分布的高概率区域，从而提升生成内容的准确性。同时，使用 FP8 格式进行前向计算和反向梯度通信，以及序列并行和张量并行技术，可以有效节省内存和提高训练效率。",
    "contexts": [
      "θ 对 pT 的空白区域（void region）赋予不合理的高概率， 在自由运行的生成过程中， 这种现象可能会导致学生模型生成在教师分布 pT 下几乎不可能出现的样本。",
      "序列并行通过将输入序列拆分为多个子序列，并将这些子序列分配到不同的设备上， 从而有效节省激活内存。"
    ],
    "question_type": "multi_context",
    "page_num": 394,
    "source_file": "doc16_387_409.pdf"
  },
  {
    "id": "0182",
    "question": "MedQA是什么？",
    "ground_truth": "MedQA是美国医学执业考试中的医学知识。",
    "contexts": [
      "MedQA（USMLE）问题 + 答案（4 ∼ 5 个选项） 11450/1273 美国医学执业考试中的医学知识"
    ],
    "question_type": "simple",
    "page_num": 430,
    "source_file": "doc17_410_433.pdf"
  },
  {
    "id": "0183",
    "question": "vLLM 可以支持哪些常用模型？",
    "ground_truth": "vLLM 可以支持 Aquila、Baichuan、BLOOM、Falcon、GPT-2、InternLM、LLaMA、LLaMA-2 等常用模型。",
    "contexts": [
      "vLLM 可以支持 Aquila、Baichuan、BLOOM、Falcon、GPT-2、InternLM、LLaMA、LLaMA-2等常用模型，使用方式也非常简单，不用对原始模型进行任何修改。"
    ],
    "question_type": "simple",
    "page_num": 412,
    "source_file": "doc17_410_433.pdf"
  },
  {
    "id": "0184",
    "question": "大语言模型评估分为哪三个大的方面？",
    "ground_truth": "知识与能力、伦理与安全，以及垂直领域评估。",
    "contexts": [
      "从整体上可以将大语言模型评估分为三个大的方面：知识与能力、伦理与安全，以及垂直领域评估。"
    ],
    "question_type": "simple",
    "page_num": 417,
    "source_file": "doc17_410_433.pdf"
  },
  {
    "id": "0185",
    "question": "每年参与高考的人数是多少？",
    "ground_truth": "1200 万",
    "contexts": [
      "Gaokao（高考） 1200 万 中文"
    ],
    "question_type": "simple",
    "page_num": 420,
    "source_file": "doc17_410_433.pdf"
  },
  {
    "id": "0186",
    "question": "评估数据集和训练数据集应该是什么关系？",
    "ground_truth": "相互独立的，以避免数据泄露的问题。",
    "contexts": [
      "评估数据集和训练数据集应该是相互独立的，以避免数据泄露的问题。"
    ],
    "question_type": "simple",
    "page_num": 416,
    "source_file": "doc17_410_433.pdf"
  },
  {
    "id": "0187",
    "question": "CUAD 数据集中包含多少份合同？",
    "ground_truth": "500 多份合同",
    "contexts": [
      "CUAD 数据集中包括 500 多份合同， 每份合同都经过The Atticus Project 法律专家的精心标记， 以识别41 种不同类型的重要条款， 总共有超过13000 个标注。"
    ],
    "question_type": "simple",
    "page_num": 429,
    "source_file": "doc17_410_433.pdf"
  },
  {
    "id": "0188",
    "question": "在评估文本生成类任务时，如何选择合适的数据集以确保评估的有效性和代表性？",
    "ground_truth": "评估数据集和训练数据集应该是相互独立的，以避免数据泄露的问题。此外，数据集选择还需要具有代表性，应该能够很好地代表模型在实际应用中可能遇到的情况，这意味着它应该涵盖各种情况和样本，以便模型在各种情况下都能表现良好。评估数据集的规模也应该足够大，以充分评估模型的性能，并且应该包含一些特殊情况的样本，以确保模型在处理异常或边缘情况时仍具有良好的性能。",
    "contexts": [
      "评估数据集和训练数据集应该是相互独立的，以避免数据泄露的问题。此外，数据集选择还需要具有代表性，应该能够很好地代表模型在实际应用中可能遇到的情况。",
      "大语言模型评估同样涉及数据集选择问题，但是大语言模型可以在单一模型中完成自然语言理解、逻辑推理、自然语言生成、多语言处理等任务。"
    ],
    "question_type": "multi_context",
    "page_num": 416,
    "source_file": "doc17_410_433.pdf"
  },
  {
    "id": "0189",
    "question": "在评估大语言模型的能力时，如何结合工具使用和特定任务场景的选择来提升模型的表现？",
    "ground_truth": "评估大语言模型的能力时，需要结合工具使用的能力和特定任务场景的选择。片段A提到，Web浏览器插件使ChatGPT能够访问最新的信息，并且通过复杂的推理任务评估模型的工具使用能力，如数学问题求解。而片段B则强调了评估任务的多样性，具体包括问答和摘要，并通过选择覆盖率高、与用户任务对应的场景来进行高效评估。因此，通过有效利用工具和选择恰当的评估任务场景，可以增强大语言模型在特定技能上的表现。",
    "contexts": [
      "Web 浏览器插件使 ChatGPT 能够访问最新的信息。",
      "评估的功能包括任务（如问答、摘要）和领域（如维基百科 2018 年的数据集）。"
    ],
    "question_type": "multi_context",
    "page_num": 428,
    "source_file": "doc17_410_433.pdf"
  },
  {
    "id": "0190",
    "question": "如何利用大语言模型生成的自然语言指令在多模态环境中执行家庭任务？",
    "ground_truth": "可以通过大语言模型生成详细的自然语言指令，然后在虚拟环境中执行这些指令。比如在VirtualHome模拟器中，研究人员通过众包收集了大量家庭任务的描述，并将其转换为简单的代码，从而实现智能体执行这些任务。同时，vLLM的优化支持了多模态环境，为复杂推理场景提供了高效的解决方案，能够与多种常用模型兼容。",
    "contexts": [
      "大语言模型还具有从外部环境接收反馈并根据行为指令执行操作的能力，例如生成用自然语言描述的详细且高度逼真的行动计划，并用来操作智能体。",
      "vLLM V1 的优化涵盖了执行效率、缓存管理、推理架构和多模态支持，为复杂推理场景提供了更加高效、灵活和可扩展的解决方案。"
    ],
    "question_type": "multi_context",
    "page_num": 427,
    "source_file": "doc17_410_433.pdf"
  },
  {
    "id": "0191",
    "question": "如何避免大语言模型训练中的数据泄露问题？",
    "ground_truth": "OpenAI 的研究人员对于每个基准测试， 会生成一个 “干净” 版本， 该版本会移除所有可能泄露的样本。",
    "contexts": [
      "为了避免这个因素的干扰，OpenAI 的研究人员对于每个基准测试， 会生成一个 “干净” 版本， 该版本会移除所有可能泄露的样本。"
    ],
    "question_type": "simple",
    "page_num": 444,
    "source_file": "doc18_434_456.pdf"
  },
  {
    "id": "0192",
    "question": "当 B 和 C 的值大于多少时，麦克尼马尔检验可以相对准确地近似计算 p 值？",
    "ground_truth": "50",
    "contexts": [
      "当 B 和 C 的值大于 50 时，麦克尼马尔检验可以相对准确地近似计算p 值"
    ],
    "question_type": "simple",
    "page_num": 441,
    "source_file": "doc18_434_456.pdf"
  },
  {
    "id": "0193",
    "question": "LLMEV AL第二期的目标是什么？",
    "ground_truth": "LLMEV AL第二期（LLMEV AL-2）的目标是以用户日常使用为主线，重点考查大语言模型解决不同专业本科生和研究生在日常学习中所遇到的问题的能力。",
    "contexts": [
      "LLMEV AL第二期（LLMEV AL-2）的目标是以用户日常使用为主线， 重点考查大语言模型解决不同专业本科生和研究生在日常学习中所遇到的问题的能力。"
    ],
    "question_type": "simple",
    "page_num": 454,
    "source_file": "doc18_434_456.pdf"
  },
  {
    "id": "0194",
    "question": "SWE-bench Verified 是什么的改进版本？",
    "ground_truth": "它是原版SWE-bench 的改进版本",
    "contexts": [
      "它是原版SWE-bench 的改进版本，旨在解决原版在实际评估中暴露的多个问题， 例如单元测试过于严格、 问题描述不明确以及环境配置难度较高等。"
    ],
    "question_type": "simple",
    "page_num": 448,
    "source_file": "doc18_434_456.pdf"
  },
  {
    "id": "0195",
    "question": "使用大语言模型进行评估的过程比较简单吗？",
    "ground_truth": "是的，使用大语言模型进行评估的过程比较简单。",
    "contexts": [
      "使用大语言模型进行评估的过程比较简单， 例如针对文本质量判断问题， 要构造任务说明、 待评估样本及对大语言模型的指令，将上述内容输入大语言模型，对给定的待评估样本质量进行评估，图 8.11 给出的指令要求大语言模型采用 5 级李克特量表法。"
    ],
    "question_type": "simple",
    "page_num": 438,
    "source_file": "doc18_434_456.pdf"
  },
  {
    "id": "0196",
    "question": "LLMEval-3 考虑的重要因素是什么？",
    "ground_truth": "防止作弊是 LLMEval-3 考虑的重要因素。",
    "contexts": [
      "防止作弊是 LLMEval-3 考虑的重要因素。"
    ],
    "question_type": "simple",
    "page_num": 454,
    "source_file": "doc18_434_456.pdf"
  },
  {
    "id": "0197",
    "question": "评估生成文本中知识准确性涉及哪些方面？",
    "ground_truth": "这涉及事实陈述、概念解释、历史事件描述等方面。",
    "contexts": [
      "知识准确性：评估生成文本中所呈现的知识是否准确无误。 这涉及事实陈述、概念解释、历史事件描述等方面。"
    ],
    "question_type": "simple",
    "page_num": 435,
    "source_file": "doc18_434_456.pdf"
  },
  {
    "id": "0198",
    "question": "司南平台支持多少种开源模型的评测？",
    "ground_truth": "超过100种",
    "contexts": [
      "此外，平台支持超过100种开源模型的评测，并预留接口供开发者接入自定义模型或 API 模型，如 OpenAI 接口。"
    ],
    "question_type": "simple",
    "page_num": 449,
    "source_file": "doc18_434_456.pdf"
  },
  {
    "id": "0199",
    "question": "p 值的意义是什么？",
    "ground_truth": "p 值是观察这个经验 （或更大的） 卡方值的概率。",
    "contexts": [
      "p 值是观察这个经验 （或更大的） 卡方值的概率。"
    ],
    "question_type": "simple",
    "page_num": 440,
    "source_file": "doc18_434_456.pdf"
  },
  {
    "id": "0200",
    "question": "在LLMEval基准测试中，如何确保评估模型在知识问答任务上的表现既准确又具有挑战性？",
    "ground_truth": "LLMEval-3通过采用新颖的评测模式，即“题库考试”模式，旨在评估模型在中文知识问答任务上的表现，并提供一个公平的比较平台。同时，评估涉及知识准确性、丰富性和一致性，以确保生成文本的可信度和逻辑关系的正确性。",
    "contexts": [
      "LLMEval-3基准测试提供了更加全面且更具挑战性的问题。其目标是评估模型在中文知识问答任务上的表现，并提供一个公平的比较平台，以便研究人员可以评估不同模型的知识问答效果。",
      "知识层面的评估主要关注知识准确性、知识丰富性和知识一致性。要求生成文本所涉及的知识准确无误、丰富全面，确保文本的可信度。"
    ],
    "question_type": "multi_context",
    "page_num": 454,
    "source_file": "doc18_434_456.pdf"
  },
  {
    "id": "0201",
    "question": "在评测编程模型时，如何考虑统计检验的准确性？",
    "ground_truth": "在评测编程模型时，使用'PASS@K'指标评估模型表现，同时在进行统计检验时，如果B和C的值大于50，可以使用麦克尼马尔检验来近似计算p值；而当B和C的值较小（B + C < 25）时，则建议使用二项式检验公式来计算p值。",
    "contexts": [
      "评测方式是将问题提示词输入模型，让模型生成代码并通过测试用例验证其正确性。评估采用“PASS@K”指标，核心在于模拟真实编程场景，考察模型在理解上下文、逻辑推理以及多步操作中的表现。",
      "当B和C的值大于50时，麦克尼马尔检验可以相对准确地近似计算p值，如果B和C的值相对较小（B + C < 25），则建议使用以下二项式检验公式计算p值。"
    ],
    "question_type": "multi_context",
    "page_num": 447,
    "source_file": "doc18_434_456.pdf"
  },
  {
    "id": "0202",
    "question": "LLMEV AL-Medicine专题医学领域大模型评测和OpenCompass司南平台在评测体系上有何异同？",
    "ground_truth": "LLMEV AL-Medicine专题医学领域大模型评测主要聚焦于医疗领域的评估，提出了医疗增强评测体系框架，并将评估体系分为医生职业资格考试、综合性医疗评估和专项能力评测三大类，强调了评估的复杂性和对智能模型能力的考量。而OpenCompass司南平台则是一个开源开放评测体系，提供公平、客观、可复现的标准化评测，涵盖多领域、多任务的客观评测手段，侧重于基础能力和综合能力的评估，包括语言、知识和代码等维度。两者在核心目标上存在差异，前者更专注于医疗领域，而后者则提供更广泛的评测平台。",
    "contexts": [
      "LLMEV AL团队联合复旦大学医学院，复旦大学附属华山医院，复旦大学附属肿瘤医院，共同推出 LLMEV AL-Medicine专题医学领域大模型评测，选择医疗领域作为核心评测领域，提出医疗增强评测体系框架。",
      "OpenCompass 司南平台是由上海人工智能实验室研发的大模型开源开放评测体系，其核心目标是为大语言模型的性能评估提供一个公平、客观、可复现的标准化平台。"
    ],
    "question_type": "multi_context",
    "page_num": 455,
    "source_file": "doc18_434_456.pdf"
  },
  {
    "id": "0203",
    "question": "大语言模型自何时以来在多个领域的应用开发取得了显著进展？",
    "ground_truth": "自2023年以来",
    "contexts": [
      "自2023年以来，大语言模型在多个领域的应用开发取得了显著进展，包括智能客服、内容生成、教育辅助、医疗咨询、代码生成等场景。"
    ],
    "question_type": "simple",
    "page_num": 458,
    "source_file": "doc19_457_480.pdf"
  },
  {
    "id": "0204",
    "question": "llama.cpp 支持哪些类型的推理？",
    "ground_truth": "它不仅支持在 CPU 和 GPU 之间的混合推理。",
    "contexts": [
      "llama.cpp 的主要优势在于其跨平台兼容性和灵活性。它不仅支持在 CPU 和 GPU 之间的混合推理，使得即使在显存不足的情况下也能运行大型模型，还提供了广泛的后端支持（如 Vulkan、SYCL 和 Metal） 。"
    ],
    "question_type": "simple",
    "page_num": 473,
    "source_file": "doc19_457_480.pdf"
  },
  {
    "id": "0205",
    "question": "国内有哪些应用利用AI技术实现智能答疑？",
    "ground_truth": "作业帮和学而思网校",
    "contexts": [
      "在国内，类似的应用也很普遍，像作业帮和学而思网校利用AI 技术实现了智能答疑，学生只需拍照或输入问题，系统便能快速分析并生成详细解答，极大地方便了学习过程。"
    ],
    "question_type": "simple",
    "page_num": 463,
    "source_file": "doc19_457_480.pdf"
  },
  {
    "id": "0206",
    "question": "根据文档，生命的意义是什么？",
    "ground_truth": "我相信生命的意义是找到自己的真理并按照它生活。",
    "contexts": [
      "我相信生命的意义是找到自己的真理并按照它生活。"
    ],
    "question_type": "simple",
    "page_num": 475,
    "source_file": "doc19_457_480.pdf"
  },
  {
    "id": "0207",
    "question": "大语言模型在企业管理中有哪些应用？",
    "ground_truth": "大语言模型在企业管理和决策支持中表现出了很大的使用前景，能够从大量非结构化文本数据中快速提取关键信息，帮助企业科学制定决策。会议记录与摘要是大语言模型在企业管理中的另一重要应用场景。",
    "contexts": [
      "大语言模型在企业管理和决策支持中表现出了很大的使用前景，能够从大量非结构化文本数据中快速提取关键信息，帮助企业科学制定决策。",
      "会议记录与摘要是大语言模型在企业管理中的另一重要应用场景。"
    ],
    "question_type": "simple",
    "page_num": 464,
    "source_file": "doc19_457_480.pdf"
  },
  {
    "id": "0208",
    "question": "大语言模型与搜索的结合在什么领域展现了巨大潜力？",
    "ground_truth": "电子商务、知识管理、在线教育、医疗健康等领域",
    "contexts": [
      "这种结合在电子商务、知识管理、在线教育、医疗健康等领域展现了巨大潜力"
    ],
    "question_type": "simple",
    "page_num": 462,
    "source_file": "doc19_457_480.pdf"
  },
  {
    "id": "0209",
    "question": "如何获取用户选择的文本？",
    "ground_truth": "const selectedText = window.getSelection().toString().trim();",
    "contexts": [
      "const selectedText = window.getSelection().toString().trim();"
    ],
    "question_type": "simple",
    "page_num": 470,
    "source_file": "doc19_457_480.pdf"
  },
  {
    "id": "0210",
    "question": "Otter.AI 的主要功能是什么？",
    "ground_truth": "能够实时记录会议内容并生成简洁的摘要，方便参会者快速回顾会议要点，或者让未参会人员轻松了解关键内容。",
    "contexts": [
      "例如，Otter.AI 结合语音识别和自然语言处理技术，能够实时记录会议内容并生成简洁的摘要，方便参会者快速回顾会议要点，或者让未参会人员轻松了解关键内容。"
    ],
    "question_type": "simple",
    "page_num": 464,
    "source_file": "doc19_457_480.pdf"
  },
  {
    "id": "0211",
    "question": "大语言模型如何在创作和信息处理领域同时发挥作用？",
    "ground_truth": "大语言模型在创作领域通过生成多种形式的文本内容，如故事片段、对话场景和歌词，帮助写作者和音乐人提升创作效率。而在信息处理领域，它们与浏览器智能插件结合，提供自动摘要和翻译功能，帮助用户高效获取信息和理解多语言内容。",
    "contexts": [
      "例如，Sudowrite 能够根据用户的提示词和需求生成多种形式的文本内容， 并提供包括润色、 摘要、大纲生成等各类能力。",
      "自动摘要可以帮助我们快速提取网页的核心内容， 避免浪费时间在冗长的信息中； 网页翻译能够打破语言障碍， 让我们轻松访问不同语言的内容资源。"
    ],
    "question_type": "multi_context",
    "page_num": 459,
    "source_file": "doc19_457_480.pdf"
  },
  {
    "id": "0212",
    "question": "Replika 如何利用大语言模型提供情感支持，同时 Ollama 在本地启动大语言模型的步骤是什么？",
    "ground_truth": "Replika 利用大语言模型学习用户的语言风格和情感反应，提供个性化的对话和情感支持，同时整合情感管理工具，帮助用户缓解压力和焦虑。Ollama 则可以通过命令 'ollama run llama3.2' 在本地启动 Llama 3.2，或使用 'ollama serve' 命令将大语言模型作为后端服务进行使用。",
    "contexts": [
      "Replika 是由美国 Luka 公司开发的一款人工智能聊天机器人应用，致力于为用户提供个性化的对话和情感支持体验。",
      "Ollama 使用非常简单，在安装完成后，如果想在本地启动Llama 3.2，可以直接使用如下命令：ollama run llama3.2。"
    ],
    "question_type": "multi_context",
    "page_num": 460,
    "source_file": "doc19_457_480.pdf"
  },
  {
    "id": "0213",
    "question": "如何在使用Ollama启动Llama 3.2后，通过REST API实现大语言模型在内容创作中的应用？",
    "ground_truth": "在安装并启动Ollama的Llama 3.2后，可以通过REST API调用模型来实现内容创作。例如，可以使用ollama serve命令启动服务，然后利用生成的API接口，输入主题或关键词，快速生成高质量的文章初稿、新闻报道或创意故事情节。",
    "contexts": [
      "Ollama 使用非常简单， 在安装完成后， 如果想在本地启动Llama 3.2， 可以直接使用如下命令： ollama run llama3.2",
      "大语言模型在内容创作与生成领域展现出了强大的能力，能够显著提高内容创作的效率与质量。"
    ],
    "question_type": "multi_context",
    "page_num": 477,
    "source_file": "doc19_457_480.pdf"
  }
]