"""
DocQA è¯„ä¼°æ¨¡å—
å®ç°æ£€ç´¢è´¨é‡å’Œå›ç­”è´¨é‡çš„è¯„ä¼°æŒ‡æ ‡
"""

import json
import logging
import time
from typing import List, Dict, Any, Tuple, Optional
from pathlib import Path

# ç¬¬ä¸‰æ–¹åº“ä¾èµ–
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from rouge_score import rouge_scorer
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
import jieba
from sentence_transformers import SentenceTransformer

# æœ¬åœ°æ¨¡å—
import config
from core.qa_chain import DocQAChain
from core.retrieval import HybridRetriever
from core.reranker import BGEReranker
from llm_engine.chat_llm import ChatLLM
from core.ingestion import create_pdf_pipeline
from core.cache_manager import create_cache_manager
from core.retrieval import EmbeddingEngine, FAISSIndexBuilder

# é…ç½®æ—¥å¿—ï¼ˆå°½é‡ç®€æ´ï¼Œé¿å…è¯„ä¼°æ—¶åˆ·å±ï¼‰
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s %(levelname)s %(name)s: %(message)s",
)


logger = logging.getLogger(__name__)


class DocQAEvaluator:
    """DocQAç³»ç»Ÿè¯„ä¼°å™¨"""
    
    def __init__(self, docqa_chain: DocQAChain):
        """
        åˆå§‹åŒ–è¯„ä¼°å™¨
        
        Args:
            docqa_chain: DocQAé—®ç­”é“¾å®ä¾‹
        """
        self.docqa_chain = docqa_chain
        self._rouge_scorer = rouge_scorer.RougeScorer(
            ['rouge1', 'rouge2', 'rougeL'], 
            use_stemmer=False
        )
        self._smoothing_function = SmoothingFunction().method4
        
        # åˆå§‹åŒ–è¯­ä¹‰æ¨¡å‹ï¼ˆç”¨äºè®¡ç®—ç›¸ä¼¼åº¦ï¼‰
        from config import EMBEDDING_MODEL_PATH

        self._sentence_model = None
        model_path = str(EMBEDDING_MODEL_PATH)

        try:
            from config import EMBEDDING_DEVICE
            device = str(EMBEDDING_DEVICE) if EMBEDDING_DEVICE else "cpu"

            logger.info(f"å¼€å§‹åŠ è½½è¯­ä¹‰æ¨¡å‹: {model_path} (device={device})")
            self._sentence_model = SentenceTransformer(
                model_path,
                device=device,
                trust_remote_code=True,
                model_kwargs={"weights_only": False}
            )
            logger.info(f"è¯­ä¹‰æ¨¡å‹åŠ è½½å®Œæˆ: {model_path} (device={device})")
        except OSError as e:
            logger.error(f"è¯­ä¹‰æ¨¡å‹åŠ è½½å¤±è´¥ï¼ˆç–‘ä¼¼æœ¬åœ°æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨æˆ–ä¸å®Œæ•´ï¼‰: {e}")
        except ValueError as e:
            logger.error(f"è¯­ä¹‰æ¨¡å‹åŠ è½½å¤±è´¥ï¼ˆé…ç½®æˆ–å‚æ•°ä¸æ­£ç¡®ï¼‰: {e}")
    
    def _tokenize_chinese(self, text: str) -> List[str]:
        """
        ä¸­æ–‡åˆ†è¯
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            
        Returns:
            åˆ†è¯ç»“æœ
        """
        text = (text or "").strip()
        if not text:
            return []
        return [t for t in jieba.cut(text) if t and t.strip()]

    def _tokenize_for_rouge(self, text: str) -> str:
        tokens = self._tokenize_chinese(text)
        return " ".join(tokens)
    
    def _calculate_recall_at_k(
        self, 
        retrieved_contexts: List[str], 
        ground_truth_contexts: List[str], 
        k: int,
        similarity_threshold: float = 0.8
    ) -> float:
        """
        è®¡ç®—Recall@K
        
        Args:
            retrieved_contexts: æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡åˆ—è¡¨
            ground_truth_context: çœŸå®ä¸Šä¸‹æ–‡
            k: å‰Kä¸ªç»“æœ
            
        Returns:
            Recall@Kåˆ†æ•°
        """
        if not retrieved_contexts or not ground_truth_contexts:
            return 0.0

        # å–å‰kä¸ªæ£€ç´¢ç»“æœ
        top_k_contexts = retrieved_contexts[:k]

        # å‘½ä¸­ä»»æ„ gt å³ç®—å‘½ä¸­ï¼›ä»…ä½¿ç”¨ gt in ctx + è¯­ä¹‰ç›¸ä¼¼åº¦é˜ˆå€¼
        for ctx in top_k_contexts:
            if not isinstance(ctx, str):
                continue
            ctx = ctx.strip()
            if not ctx:
                continue

            for gt in ground_truth_contexts:
                if not isinstance(gt, str):
                    continue
                gt = gt.strip()
                if not gt:
                    continue

                if gt in ctx:
                    return 1.0

                similarity = self._calculate_text_similarity(gt, ctx)
                if similarity > similarity_threshold:
                    return 1.0

        return 0.0
    
    def _calculate_mrr(
        self, 
        retrieved_contexts: List[str], 
        ground_truth_contexts: List[str],
        similarity_threshold: float = 0.8,
        max_depth: int = 20
    ) -> float:
        """
        è®¡ç®—MRR (Mean Reciprocal Rank)
        
        Args:
            retrieved_contexts: æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡åˆ—è¡¨
            ground_truth_context: çœŸå®ä¸Šä¸‹æ–‡
            
        Returns:
            MRRåˆ†æ•°
        """
        if not retrieved_contexts or not ground_truth_contexts:
            return 0.0

        # é™åˆ¶æœ€å¤§æ£€ç´¢æ·±åº¦ï¼Œé¿å…æ— æ„ä¹‰éå†å¯¼è‡´è¯„ä¼°è¿‡æ…¢
        limited_contexts = retrieved_contexts[:max_depth] if max_depth and max_depth > 0 else retrieved_contexts

        for i, ctx in enumerate(limited_contexts, 1):
            if not isinstance(ctx, str):
                continue
            ctx = ctx.strip()
            if not ctx:
                continue

            for gt in ground_truth_contexts:
                if not isinstance(gt, str):
                    continue
                gt = gt.strip()
                if not gt:
                    continue

                if gt in ctx:
                    return 1.0 / i

                similarity = self._calculate_text_similarity(gt, ctx)
                if similarity > similarity_threshold:
                    return 1.0 / i

        return 0.0
    
    def _calculate_text_similarity(self, text1: str, text2: str) -> float:
        """
        è®¡ç®—æ–‡æœ¬ç›¸ä¼¼åº¦
        
        Args:
            text1: æ–‡æœ¬1
            text2: æ–‡æœ¬2
            
        Returns:
            ç›¸ä¼¼åº¦åˆ†æ•°
        """
        if not text1 or not text2:
            return 0.0
        
        # ä½¿ç”¨è¯­ä¹‰æ¨¡å‹è®¡ç®—ç›¸ä¼¼åº¦
        if self._sentence_model:
            try:
                embeddings = self._sentence_model.encode([text1, text2])
                similarity = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]
                return float(similarity)
            except (OSError, ValueError, RuntimeError) as e:
                logger.warning(f"è¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {e}")
        
        # é€€å›åˆ°ç®€å•çš„è¯æ±‡é‡å 
        words1 = set(self._tokenize_chinese(text1))
        words2 = set(self._tokenize_chinese(text2))
        
        intersection = words1.intersection(words2)
        union = words1.union(words2)
        
        return len(intersection) / len(union) if union else 0.0
    
    def _calculate_rouge_scores(self, prediction: str, reference: str) -> Dict[str, float]:
        """
        è®¡ç®—ROUGEåˆ†æ•°
        
        Args:
            prediction: é¢„æµ‹ç­”æ¡ˆ
            reference: å‚è€ƒç­”æ¡ˆ
            
        Returns:
            ROUGEåˆ†æ•°å­—å…¸
        """
        try:
            # rouge-score é»˜è®¤æŒ‰ç©ºæ ¼åˆ‡åˆ† tokenï¼›ä¸­æ–‡è‹¥ä¸åˆ†è¯ä¼šå¯¼è‡´åˆ†æ•°å¼‚å¸¸åä½
            reference_tok = self._tokenize_for_rouge(reference)
            prediction_tok = self._tokenize_for_rouge(prediction)

            scores = self._rouge_scorer.score(reference_tok, prediction_tok)
            return {
                'rouge1': scores['rouge1'].fmeasure,
                'rouge2': scores['rouge2'].fmeasure,
                'rougeL': scores['rougeL'].fmeasure
            }
        except (ValueError, TypeError) as e:
            logger.error(f"ROUGEè®¡ç®—å¤±è´¥: {e}")
            return {'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0}
    
    def _calculate_bleu_score(self, prediction: str, reference: str) -> float:
        """
        è®¡ç®—BLEUåˆ†æ•°
        
        Args:
            prediction: é¢„æµ‹ç­”æ¡ˆ
            reference: å‚è€ƒç­”æ¡ˆ
            
        Returns:
            BLEUåˆ†æ•°
        """
        try:
            prediction = (prediction or "").strip()
            reference = (reference or "").strip()
            if not prediction or not reference:
                return 0.0

            reference_tokens = [self._tokenize_chinese(reference)]
            prediction_tokens = self._tokenize_chinese(prediction)

            if not reference_tokens[0] or not prediction_tokens:
                return 0.0
            
            bleu_score = sentence_bleu(
                reference_tokens,
                prediction_tokens,
                smoothing_function=self._smoothing_function
            )
            return float(bleu_score)
        except (ValueError, IndexError, ZeroDivisionError) as e:
            logger.error(f"BLEUè®¡ç®—å¤±è´¥: {e}")
            return 0.0
    
    def _calculate_semantic_similarity(self, text1: str, text2: str) -> float:
        """
        è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦
        
        Args:
            text1: æ–‡æœ¬1
            text2: æ–‡æœ¬2
            
        Returns:
            è¯­ä¹‰ç›¸ä¼¼åº¦åˆ†æ•°
        """
        return self._calculate_text_similarity(text1, text2)
    
    def evaluate_single_sample(self, sample: Dict[str, Any]) -> Dict[str, Any]:
        """
        è¯„ä¼°å•ä¸ªæ ·æœ¬
        
        Args:
            sample: æµ‹è¯•æ ·æœ¬
            
        Returns:
            è¯„ä¼°ç»“æœ
        """
        question = sample['question']
        ground_truth = sample['ground_truth']
        ground_contexts = sample['contexts']
        
        logger.info(f"è¯„ä¼°é—®é¢˜: {question}")
        
        start_time = time.time()
        
        # è°ƒç”¨DocQAç³»ç»Ÿï¼ˆä¸æ•è·å¼‚å¸¸ï¼Œä¾¿äºæŸ¥çœ‹å®Œæ•´å †æ ˆï¼‰
        result = self.docqa_chain.ask(question, stream=False)
        
        end_time = time.time()
        response_time = end_time - start_time
        
        # æå–æ£€ç´¢ç»“æœå’Œç­”æ¡ˆ
        # sources å½“å‰ç»“æ„ä¸º List[dict]ï¼ˆç”± core/qa_chain.py::_format_sources ç”Ÿæˆï¼‰
        sources = result.get('sources', [])

        retrieved_contexts = [
            (s.get('content') or s.get('content_preview') or '').strip()
            for s in sources
            if isinstance(s, dict) and (s.get('content') or s.get('content_preview'))
        ]
        predicted_answer = result.get('answer', '')
        
        # è®¡ç®—æ£€ç´¢æŒ‡æ ‡
        
        similarity_threshold = float(getattr(config, 'EVAL_SIMILARITY_THRESHOLD', 0.8))
        max_mrr_depth = int(getattr(config, 'EVAL_MRR_MAX_DEPTH', 20))

        recall_1 = self._calculate_recall_at_k(retrieved_contexts, ground_contexts, 1, similarity_threshold=similarity_threshold)
        recall_3 = self._calculate_recall_at_k(retrieved_contexts, ground_contexts, 3, similarity_threshold=similarity_threshold)
        recall_5 = self._calculate_recall_at_k(retrieved_contexts, ground_contexts, 5, similarity_threshold=similarity_threshold)
        mrr = self._calculate_mrr(retrieved_contexts, ground_contexts, similarity_threshold=similarity_threshold, max_depth=max_mrr_depth)
        
        # è®¡ç®—å›ç­”è´¨é‡æŒ‡æ ‡
        rouge_scores = self._calculate_rouge_scores(predicted_answer, ground_truth)
        bleu_score = self._calculate_bleu_score(predicted_answer, ground_truth)
        semantic_sim = self._calculate_semantic_similarity(predicted_answer, ground_truth)
        
        evaluation_result = {
            'sample_id': sample.get('id', ''),
            'question': question,
            'ground_truth': ground_truth,
            'predicted_answer': predicted_answer,
            'ground_context': ground_contexts,
            'retrieved_contexts': retrieved_contexts,
            'response_time': response_time,
            'retrieval_metrics': {
                'recall@1': recall_1,
                'recall@3': recall_3,
                'recall@5': recall_5,
                'mrr': mrr
            },
            'answer_quality_metrics': {
                'rouge1': rouge_scores['rouge1'],
                'rouge2': rouge_scores['rouge2'],
                'rougeL': rouge_scores['rougeL'],
                'bleu': bleu_score,
                'semantic_similarity': semantic_sim
            }
        }

        logger.info(json.dumps(evaluation_result, ensure_ascii=False, indent=2))
        logger.info(f"æ ·æœ¬ {sample.get('id', '')} è¯„ä¼°å®Œæˆ")
        return evaluation_result
    
    def evaluate_dataset(
        self, 
        testset_path: str, 
        output_path: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        è¯„ä¼°æ•´ä¸ªæµ‹è¯•é›†
        
        Args:
            testset_path: æµ‹è¯•é›†è·¯å¾„
            output_path: è¾“å‡ºè·¯å¾„ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            è¯„ä¼°ç»“æœæ±‡æ€»
        """
        logger.info(f"å¼€å§‹è¯„ä¼°æµ‹è¯•é›†: {testset_path}")
        
        # åŠ è½½æµ‹è¯•é›†
        try:
            with open(testset_path, 'r', encoding='utf-8') as f:
                testset = json.load(f)
            logger.info(f"åŠ è½½æµ‹è¯•é›†æˆåŠŸï¼Œå…± {len(testset)} ä¸ªæ ·æœ¬")
        except Exception as e:
            logger.error(f"åŠ è½½æµ‹è¯•é›†å¤±è´¥: {e}")
            raise
        
        # é€ä¸ªè¯„ä¼°
        all_results = []
        successful_evaluations = 0
        
        for i, sample in enumerate(testset):
            logger.info(f"è¯„ä¼°è¿›åº¦: {i+1}/{len(testset)}")
            result = self.evaluate_single_sample(sample)
            all_results.append(result)
            
            if 'error' not in result:
                successful_evaluations += 1
        
        # è®¡ç®—å¹³å‡æŒ‡æ ‡
        avg_metrics = self._calculate_average_metrics(all_results)
        
        # æ„å»ºæœ€ç»ˆæŠ¥å‘Š
        final_report = {
            'evaluation_summary': {
                'total_samples': len(testset),
                'successful_evaluations': successful_evaluations,
                'success_rate': successful_evaluations / len(testset) if testset else 0,
                'evaluation_time': time.strftime('%Y-%m-%d %H:%M:%S')
            },
            'average_metrics': avg_metrics,
            'detailed_results': all_results
        }
        
        # ä¿å­˜ç»“æœ
        if output_path:
            self._save_results(final_report, output_path)
        
        # æ‰“å°æ‘˜è¦
        self._print_summary(final_report)
        
        return final_report
    
    def _calculate_average_metrics(self, all_results: List[Dict[str, Any]]) -> Dict[str, float]:
        """
        è®¡ç®—å¹³å‡æŒ‡æ ‡
        
        Args:
            all_results: æ‰€æœ‰è¯„ä¼°ç»“æœ
            
        Returns:
            å¹³å‡æŒ‡æ ‡å­—å…¸
        """
        successful_results = [r for r in all_results if 'error' not in r]
        
        if not successful_results:
            return {}
        
        metrics = {
            'avg_recall@1': 0.0,
            'avg_recall@3': 0.0,
            'avg_recall@5': 0.0,
            'avg_mrr': 0.0,
            'avg_rouge_1': 0.0,
            'avg_rouge_2': 0.0,
            'avg_rouge_l': 0.0,
            'avg_bleu': 0.0,
            'avg_semantic_similarity': 0.0,
            'avg_response_time': 0.0
        }
        
        for result in successful_results:
            retrieval_metrics = result.get('retrieval_metrics', {})
            answer_metrics = result.get('answer_quality_metrics', {})
            
            metrics['avg_recall@1'] += retrieval_metrics.get('recall@1', 0)
            metrics['avg_recall@3'] += retrieval_metrics.get('recall@3', 0)
            metrics['avg_recall@5'] += retrieval_metrics.get('recall@5', 0)
            metrics['avg_mrr'] += retrieval_metrics.get('mrr', 0)
            metrics['avg_rouge_1'] += answer_metrics.get('rouge1', 0)
            metrics['avg_rouge_2'] += answer_metrics.get('rouge2', 0)
            metrics['avg_rouge_l'] += answer_metrics.get('rougeL', 0)
            metrics['avg_bleu'] += answer_metrics.get('bleu', 0)
            metrics['avg_semantic_similarity'] += answer_metrics.get('semantic_similarity', 0)
            metrics['avg_response_time'] += result.get('response_time', 0)
        
        # è®¡ç®—å¹³å‡å€¼
        num_successful = len(successful_results)
        for key in metrics:
            metrics[key] /= num_successful
        
        return metrics
    
    def _save_results(self, results: Dict[str, Any], output_path: str) -> None:
        """
        ä¿å­˜è¯„ä¼°ç»“æœ
        
        Args:
            results: è¯„ä¼°ç»“æœ
            output_path: è¾“å‡ºè·¯å¾„
        """
        try:
            output_file = Path(output_path)
            output_file.parent.mkdir(parents=True, exist_ok=True)
            
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            
            logger.info(f"è¯„ä¼°ç»“æœå·²ä¿å­˜è‡³: {output_path}")
        except (OSError, TypeError, ValueError) as e:
            logger.error(f"ä¿å­˜ç»“æœå¤±è´¥: {e}")
    
    def _print_summary(self, results: Dict[str, Any]) -> None:
        """
        æ‰“å°è¯„ä¼°æ‘˜è¦
        
        Args:
            results: è¯„ä¼°ç»“æœ
        """
        summary = results['evaluation_summary']
        metrics = results['average_metrics']
        
        logger.info(
            json.dumps(
                {
                    "event": "evaluation_summary",
                    "evaluation_summary": summary,
                    "average_metrics": metrics,
                },
                ensure_ascii=False,
            )
        )


class NoReranker:
    def rerank(
        self,
        query: str,
        documents: List[Any],
        top_n: int = 5,
        score_threshold: float = 0.0,
    ):
        if not documents:
            return []
        return [(doc, 0.0) for doc in documents[:top_n]]


def create_docqa_chain(pdf_path: str, enable_rerank: bool = True) -> DocQAChain:
    """
    åˆ›å»ºDocQAé“¾å®ä¾‹

    Args:
        pdf_path: PDFæ–‡æ¡£è·¯å¾„

    Returns:
        DocQAé“¾å®ä¾‹

    Raises:
        FileNotFoundError: PDFè·¯å¾„ä¸å­˜åœ¨
        ValueError: æ–‡æ¡£å¤„ç†å¤±è´¥
    """
    logger.info(f"åˆå§‹åŒ–DocQAç³»ç»Ÿï¼ŒPDFè·¯å¾„: {pdf_path}")

    pdf_file = Path(pdf_path)
    if not pdf_file.exists():
        raise FileNotFoundError(f"PDFæ–‡ä»¶ä¸å­˜åœ¨: {pdf_path}")

    try:
        # åˆå§‹åŒ–ç¼“å­˜ç®¡ç†å™¨
        from config import ENABLE_CACHE
        cache_manager = create_cache_manager() if ENABLE_CACHE else None
        
        # åˆå§‹åŒ–æ ¸å¿ƒç»„ä»¶æ¨¡å‹ï¼ˆå•ä¾‹æ¨¡å¼æ€æƒ³ï¼‰
        llm = ChatLLM()
        embedding_engine = EmbeddingEngine()
        reranker = BGEReranker() if enable_rerank else NoReranker()

        documents = None
        faiss_index = None
        stats = None
        from_cache = False

        # å°è¯•ä»ç¼“å­˜åŠ è½½
        if cache_manager and cache_manager.cache_exists(pdf_path):
            logger.info("ğŸ¯ æ£€æµ‹åˆ°æ–‡æ¡£ç¼“å­˜ï¼Œå°è¯•å¿«é€ŸåŠ è½½...")
            cache_result = cache_manager.load_cache(pdf_path, embedding_engine.embeddings)
            if cache_result:
                faiss_index, documents, stats = cache_result
                from_cache = True
                logger.info("âœ… æˆåŠŸä»ç¼“å­˜åŠ è½½ç´¢å¼•å’Œæ–‡æ¡£ç‰‡æ®µ")

        if not from_cache:
            logger.info("ğŸ“„ ç¼“å­˜ä¸å­˜åœ¨æˆ–åŠ è½½å¤±è´¥ï¼Œå¼€å§‹å¤„ç†PDF...")
            # å¤„ç†PDFæ–‡æ¡£
            pipeline = create_pdf_pipeline()
            result = pipeline.process_pdf(str(pdf_file))
            
            if not result["success"]:
                raise ValueError(f"æ–‡æ¡£å¤„ç†å¤±è´¥: {result.get('error')}")
            
            documents = result["chunks"]
            stats = result["stats"]
            logger.info(f"æ–‡æ¡£å¤„ç†å®Œæˆï¼Œå…± {len(documents)} ä¸ªç‰‡æ®µ")
            
            # åˆ›å»ºå‘é‡ç´¢å¼•
            index_builder = FAISSIndexBuilder(embedding_engine)
            faiss_index = index_builder.create_index(documents)
            
            # ä¿å­˜åˆ°ç¼“å­˜
            if cache_manager:
                logger.info("ğŸ’¾ ä¿å­˜å¤„ç†ç»“æœåˆ°ç¼“å­˜...")
                cache_manager.save_cache(
                    pdf_path,
                    index_builder.vector_store,
                    documents,
                    stats
                )
        
        # åˆ›å»ºæ£€ç´¢å™¨
        retriever = HybridRetriever(faiss_index, documents)
        
        # åˆ›å»ºé—®ç­”é“¾
        docqa_chain = DocQAChain(llm, retriever, reranker)
        
        logger.info("DocQAç³»ç»Ÿåˆå§‹åŒ–æˆåŠŸ")
        return docqa_chain
        
    except Exception as e:
        logger.error(f"DocQAç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}")
        raise


if __name__ == "__main__":
    # å›ºå®šå‚æ•°é…ç½®
    pdf_path = 'dataset/raw/doc.pdf'
    testset_path = 'dataset/testset/testset_final.json'
    output_path = 'output/evaluation_report_no_reranker.json'

    try:
        # åˆ›å»ºDocQAé“¾
        enable_rerank = bool(getattr(config, "EVAL_ENABLE_RERANK", True))
        logger.info(f"è¯„ä¼°å¼€å…³: enable_rerank={enable_rerank}")
        docqa_chain = create_docqa_chain(pdf_path, enable_rerank=enable_rerank)

        # åˆ›å»ºè¯„ä¼°å™¨
        evaluator = DocQAEvaluator(docqa_chain)

        # æ‰§è¡Œè¯„ä¼°
        results = evaluator.evaluate_dataset(testset_path, output_path)

        logger.info("è¯„ä¼°å®Œæˆ!")

    except Exception as e:
        logger.error(f"è¯„ä¼°å¤±è´¥: {e}")
        raise