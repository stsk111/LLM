import os
import json
import random
import time
import re
import glob
from typing import List, Dict
import dotenv
from openai import OpenAI
from tqdm import tqdm

# å¤ç”¨ä½ çš„æ ¸å¿ƒæ¨¡å—
from core.ingestion import create_pdf_pipeline
from langchain_core.documents import Document

# åŠ è½½ç¯å¢ƒå˜é‡
dotenv.load_dotenv()

# é…ç½®
API_KEY = os.getenv("API_KEY")
BASE_URL = os.getenv("BASE_URL")

PDF_DIR = "dataset/split_docs" 
QUESTIONS_PER_DOC = 15 # è¿™é‡Œçš„é…ç½®ä¸»è¦ç”¨äºè®¡ç®—æ€»ç›®æ ‡ï¼Œæˆ–è€…ä½ å¯ä»¥æ ¹æ®æ–‡ä»¶æ•°é‡è‡ªåŠ¨è®¡ç®—
OUTPUT_FILE = "dataset/testset/testset_final.jsonl"

# é—®é¢˜åˆ†å¸ƒé…ç½®
DISTRIBUTION = {
    "simple": 9,        # äº‹å®ç±»
    "reasoning": 3,     # æ¨ç†ç±»
    "multi_context": 3  # å¤šè·³ç±»
}

# åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯
client = OpenAI(api_key=API_KEY, base_url=BASE_URL)

def get_page_offset(filename: str) -> int:
    """
    è§£ææ–‡ä»¶åè·å–èµ·å§‹é¡µç 
    æœŸæœ›æ ¼å¼: docXX_start_end.pdf (ä¾‹å¦‚: doc00_1_20.pdf)
    å¦‚æœè§£æå¤±è´¥ï¼Œé»˜è®¤è¿”å› 1
    """
    # æ­£åˆ™åŒ¹é…ï¼šåŒ¹é…æ–‡ä»¶åä¸­çš„ä¸¤ä¸ªæ•°å­—éƒ¨åˆ† _(d+)_(d+)
    # group(1) æ˜¯èµ·å§‹é¡µï¼Œgroup(2) æ˜¯ç»“æŸé¡µ
    match = re.search(r'_(\d+)_(\d+)\.pdf$', filename)

    return int(match.group(1))
    

def generate_questions_by_llm(chunks: List[Document], doc_name: str, page_offset: int) -> List[Dict]:
    """
    æ ¸å¿ƒç”Ÿæˆé€»è¾‘ï¼šæ ¹æ®chunksç”ŸæˆæŒ‡å®šåˆ†å¸ƒçš„é—®é¢˜
    Args:
        page_offset: å½“å‰æ–‡æ¡£åœ¨åŸä¹¦ä¸­çš„èµ·å§‹é¡µç 
    """
    generated_data = []
    
    # å®šä¹‰ä¸€ä¸ªå†…éƒ¨å‡½æ•°æ¥è®¡ç®—ç»å¯¹é¡µç 
    def get_abs_page(chunk):
        # ç›¸å¯¹é¡µç  (ä»1å¼€å§‹)
        relative_page = chunk.metadata.get('page', 1) 
        # ç»å¯¹é¡µç  = èµ·å§‹é¡µ + ç›¸å¯¹é¡µ - 1
        return page_offset + relative_page - 1

    # --- 1. ç”Ÿæˆ Simple (äº‹å®ç±») ---
    target_chunks = random.sample(chunks, k=min(len(chunks), DISTRIBUTION["simple"]))
    for chunk in target_chunks:
        abs_page = get_abs_page(chunk)
        prompt = f"""
        åŸºäºä»¥ä¸‹æ–‡æ¡£ç‰‡æ®µï¼Œç”Ÿæˆ 1 ä¸ªã€ç®€å•äº‹å®ç±»(simple)ã€‘é—®ç­”å¯¹ã€‚
        è¦æ±‚ï¼š
        1. ç­”æ¡ˆå¿…é¡»èƒ½ç›´æ¥ä»ç‰‡æ®µä¸­æ‰¾åˆ°ã€‚
        2. "contexts" å­—æ®µå¿…é¡»ä¸¥æ ¼æ‘˜å½•åŸæ–‡å¥å­ã€‚
        3. è¾“å‡ºçº¯ JSON æ ¼å¼ã€‚

        æ–‡æ¡£ç‰‡æ®µ (åŸä¹¦ç¬¬ {abs_page} é¡µ):
        {chunk.page_content[:1500]}
        """
        data = _call_llm(prompt, abs_page, "simple", doc_name) # ä¼ å…¥ç»å¯¹é¡µç 
        if data: generated_data.append(data)

    # --- 2. ç”Ÿæˆ Reasoning (æ¨ç†ç±») ---
    target_chunks = random.sample(chunks, k=min(len(chunks), DISTRIBUTION["reasoning"]))
    for chunk in target_chunks:
        abs_page = get_abs_page(chunk)
        prompt = f"""
        åŸºäºä»¥ä¸‹æ–‡æ¡£ç‰‡æ®µï¼Œç”Ÿæˆ 1 ä¸ªã€æ·±åº¦æ¨ç†ç±»(reasoning)ã€‘é—®ç­”å¯¹ã€‚
        è¦æ±‚ï¼š
        1. é—®é¢˜åŒ…å«"ä¸ºä»€ä¹ˆ"ã€"å¦‚ä½•å½±å“"æˆ–"å¯¹æ¯”"ã€‚
        2. éœ€è¦ç»“åˆç‰‡æ®µä¸­çš„é€»è¾‘è¿›è¡Œæ¨æ–­æ‰èƒ½å›ç­”ã€‚
        3. è¾“å‡ºçº¯ JSON æ ¼å¼ã€‚

        æ–‡æ¡£ç‰‡æ®µ (åŸä¹¦ç¬¬ {abs_page} é¡µ):
        {chunk.page_content[:1500]}
        """
        data = _call_llm(prompt, abs_page, "reasoning", doc_name)
        if data: generated_data.append(data)

    # --- 3. ç”Ÿæˆ Multi_context (å¤šè·³ç±») ---
    for _ in range(DISTRIBUTION["multi_context"]):
        if len(chunks) < 2: break
        c1, c2 = random.sample(chunks, 2)
        
        abs_page_1 = get_abs_page(c1)
        abs_page_2 = get_abs_page(c2)

        prompt = f"""
        åŸºäºä»¥ä¸‹ä¸¤ä¸ªä¸åŒçš„æ–‡æ¡£ç‰‡æ®µï¼Œç”Ÿæˆ 1 ä¸ªã€å¤šæ–‡æ¡£ç»¼åˆç±»(multi_context)ã€‘é—®ç­”å¯¹ã€‚
        è¦æ±‚ï¼š
        1. é—®é¢˜å¿…é¡»éœ€è¦åŒæ—¶ç»“åˆç‰‡æ®µAå’Œç‰‡æ®µBçš„ä¿¡æ¯æ‰èƒ½å›ç­”ã€‚
        2. "contexts" å­—æ®µéœ€åŒ…å«ä¸¤ä¸ªç‰‡æ®µä¸­çš„å…³é”®å¥ã€‚
        3. è¾“å‡ºçº¯ JSON æ ¼å¼ã€‚

        ç‰‡æ®µA (åŸä¹¦ç¬¬ {abs_page_1} é¡µ):
        {c1.page_content[:800]}
        
        ç‰‡æ®µB (åŸä¹¦ç¬¬ {abs_page_2} é¡µ):
        {c2.page_content[:800]}
        """
        # å¤šè·³é—®é¢˜é¡µç é€šå¸¸è®°å½•ä¸»è¦æ¥æºçš„é¡µç ï¼Œæˆ–è€…è®°å½•ä¸ºåˆ—è¡¨ã€‚
        # è¿™é‡Œä¸ºäº†ä¿æŒ schema ä¸€è‡´ï¼Œæˆ‘ä»¬è®°å½•ç‰‡æ®µAçš„é¡µç 
        data = _call_llm(prompt, abs_page_1, "multi_context", doc_name, extra_chunk=c2)
        if data: generated_data.append(data)

    return generated_data

def _call_llm(prompt: str, abs_page_num: int, q_type: str, doc_name: str, extra_chunk=None) -> Dict:
    """LLM è°ƒç”¨ä¸ JSON æ¸…æ´—é€šç”¨å‡½æ•°"""
    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini", 
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„æ•°æ®é›†ç”Ÿæˆä¸“å®¶ã€‚è¯·åªè¿”å› JSONï¼Œä¸è¦åŒ…å« Markdown æ ¼å¼æ ‡è®°ã€‚"},
                {"role": "user", "content": prompt}
            ],
            temperature=0.7
        )
        content = response.choices[0].message.content.strip()
        
        if content.startswith("```"):
            content = content.replace("```json", "").replace("```", "")
            
        parsed = json.loads(content)
        if isinstance(parsed, list): parsed = parsed[0]
        
        return {
            "question": parsed.get("question", ""),
            "ground_truth": parsed.get("ground_truth") or parsed.get("answer", ""),
            "contexts": parsed.get("contexts", []),
            "question_type": q_type,
            "page_num": abs_page_num,  # âœ… è¿™é‡Œç°åœ¨æ˜¯ç»å¯¹é¡µç 
            "source_file": doc_name    # è¿™é‡Œå¯ä»¥è®°å½•åŸå§‹æ‹†åˆ†æ–‡ä»¶åï¼Œæˆ–è€…ä½ å¯ä»¥æ”¹ä¸ºè®°å½•åŸä¹¦å
        }
    except Exception as e:
        print(f"  [Error] ç”Ÿæˆå¤±è´¥: {e}")
        return None

def clean_and_convert_jsonl(input_path: str):
    """
    è¯»å– .jsonl æ–‡ä»¶ï¼Œæ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š
    1. æ•°æ®æ¸…æ´—ï¼šå‰”é™¤ä»»ä½•åŒ…å«ç©ºå€¼ï¼ˆNone, ç©ºå­—ç¬¦ä¸², ç©ºåˆ—è¡¨ï¼‰çš„è®°å½•ã€‚
    2. ID é‡ç½®ï¼šå¯¹ä¿ç•™ä¸‹æ¥çš„æœ‰æ•ˆæ•°æ®ï¼Œé‡æ–°ç”Ÿæˆè¿ç»­çš„ ID (000, 001...)ã€‚
    3. æ ¼å¼è½¬æ¢ï¼šä¿å­˜ä¸ºæ ‡å‡† .json æ ¼å¼ã€‚

    é¢å¤–æ¸…æ´—ï¼š
    - å°† "contexts" å­—æ®µç»Ÿä¸€ä¸º list[str]ã€‚

    Args:
        input_path (str): è¾“å…¥çš„ .jsonl æ–‡ä»¶è·¯å¾„
    """
    if not os.path.exists(input_path):
        print(f"âŒ é”™è¯¯ï¼šæ–‡ä»¶ä¸å­˜åœ¨ - {input_path}")
        return

    output_path = input_path.rsplit('.', 1)[0] + ".json"
    final_data = []

    total_lines = 0
    dropped_lines = 0

    print(f"ğŸ”„ æ­£åœ¨æ¸…æ´—å¹¶è½¬æ¢: {input_path} ...")

    def _clean_contexts(value):
        if value is None:
            return []
        if isinstance(value, str):
            v = value.strip()
            return [v] if v else []
        if isinstance(value, list):
            cleaned = []
            for x in value:
                if x is None:
                    continue
                s = str(x).strip()
                if s:
                    cleaned.append(s)
            return cleaned
        s = str(value).strip()
        return [s] if s else []

    try:
        with open(input_path, "r", encoding="utf-8") as f:
            for line_num, line in enumerate(f):
                line = line.strip()
                if not line:
                    continue

                total_lines += 1

                try:
                    item = json.loads(line)

                    # å…ˆæ¸…æ´— contextsï¼Œä¿è¯ç±»å‹ç»Ÿä¸€
                    item["contexts"] = _clean_contexts(item.get("contexts"))

                    # å†æ‰§è¡Œç©ºå€¼è¿‡æ»¤
                    if any(not v for _, v in item.items()):
                        dropped_lines += 1
                        continue

                    # --- 2. æ’å…¥è¿ç»­ ID ---
                    new_item = {
                        "id": f"{len(final_data):04d}",
                        **item
                    }
                    final_data.append(new_item)

                except json.JSONDecodeError:
                    print(f"âŒ ç¬¬ {line_num+1} è¡Œ JSON æ ¼å¼é”™è¯¯ï¼Œè·³è¿‡ã€‚")
                    dropped_lines += 1

        # --- 3. ä¿å­˜ç»“æœ ---
        if final_data:
            with open(output_path, "w", encoding="utf-8") as f:
                json.dump(final_data, f, ensure_ascii=False, indent=2)

            print(f"ğŸ‰ å¤„ç†å®Œæˆï¼")
            print(f"ğŸ“Š ç»Ÿè®¡ï¼šåŸæ•°æ® {total_lines} æ¡ -> æœ‰æ•ˆæ•°æ® {len(final_data)} æ¡ (å‰”é™¤ {dropped_lines} æ¡)")
            print(f"ğŸ’¾ è¾“å‡ºæ–‡ä»¶: {output_path}")
        else:
            print("âš ï¸ è­¦å‘Šï¼šæ²¡æœ‰æå–åˆ°ä»»ä½•æœ‰æ•ˆæ•°æ®ï¼Œæœªç”Ÿæˆè¾“å‡ºæ–‡ä»¶ã€‚")

    except Exception as e:
        print(f"âŒ å¤„ç†è¿‡ç¨‹ä¸­å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")

def main():
    pipeline = create_pdf_pipeline()

    os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)

    # âœ… ä¿®æ”¹ 1: æ‰«æç›®å½•ä¸‹æ‰€æœ‰çš„ PDF æ–‡ä»¶ï¼Œè€Œä¸æ˜¯ç”¨ range çŒœæµ‹
    # å‡è®¾æ–‡ä»¶åæ ¼å¼: doc00_1_20.pdf, doc01_21_40.pdf
    pdf_files = sorted(glob.glob(os.path.join(PDF_DIR, "*.pdf")))

    if not pdf_files:
        print(f"âŒ é”™è¯¯: åœ¨ {PDF_DIR} ç›®å½•ä¸‹æ²¡æœ‰æ‰¾åˆ° PDF æ–‡ä»¶")
        return

    print(f"ğŸš€ å¼€å§‹æ‰¹é‡ç”Ÿæˆä»»åŠ¡ï¼šæ‰«æåˆ° {len(pdf_files)} ä¸ªæ–‡ä»¶")

    for file_path in tqdm(pdf_files, desc="å¤„ç†æ–‡æ¡£"):
        filename = os.path.basename(file_path)

        # âœ… ä¿®æ”¹ 2: è·å–é¡µç åç§»é‡
        page_offset = get_page_offset(filename)

        try:
            # 1. ä½¿ç”¨ ingestion.py å¤„ç† PDF
            result = pipeline.process_pdf(file_path)
            if not result['success']:
                print(f"âŒ è§£æå¤±è´¥: {filename} - {result.get('error')}")
                continue

            chunks = result['chunks']
            if not chunks:
                print(f"âš ï¸ è­¦å‘Š: {filename} æœªæå–åˆ°æ–‡æœ¬")
                continue

            # 2. è°ƒç”¨ LLM ç”Ÿæˆæ•°æ®
            # âœ… ä¿®æ”¹ 3: ä¼ å…¥ page_offset
            print(f"  æ­£åœ¨ç”Ÿæˆ {filename} (èµ·å§‹é¡µ: {page_offset})...")
            doc_questions = generate_questions_by_llm(chunks, filename, page_offset)

            with open(OUTPUT_FILE, "a", encoding="utf-8") as f:
                for item in doc_questions:
                    f.write(json.dumps(item, ensure_ascii=False) + "\n")

            print(f"  âœ… {filename} å®Œæˆï¼Œæ•°æ®å·²è¿½åŠ ")
            time.sleep(1)

        except Exception as e:
            print(f"âŒ å¤„ç† {filename} æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")

    print(f"\nğŸ’¾ LLMç”Ÿæˆçš„æ•°æ®é›†å·²ä¿å­˜è‡³: {OUTPUT_FILE}")

    clean_and_convert_jsonl(OUTPUT_FILE)

if __name__ == "__main__":
    # main()
    clean_and_convert_jsonl(OUTPUT_FILE)
