{
  "evaluation_summary": {
    "total_samples": 214,
    "successful_evaluations": 214,
    "success_rate": 1.0,
    "evaluation_time": "2026-02-10 08:20:27"
  },
  "average_metrics": {
    "avg_recall@1": 0.40654205607476634,
    "avg_recall@3": 0.4532710280373832,
    "avg_recall@5": 0.46261682242990654,
    "avg_mrr": 0.43123052959501557,
    "avg_rouge_1": 0.25715056023984456,
    "avg_rouge_2": 0.11846728743828498,
    "avg_rouge_l": 0.25050799482944835,
    "avg_bleu": 0.1514712466815282,
    "avg_semantic_similarity": 0.735492196317031,
    "avg_response_time": 4.489876418470223
  },
  "detailed_results": [
    {
      "sample_id": "0000",
      "question": "SFT 模型通常需要多少块 GPU 来完成训练？",
      "ground_truth": "通常需要数十块GPU，花费数天时间完成训练。",
      "predicted_answer": "根据文档中的描述，SFT 模型通常需要数十块 GPU 来完成训练。具体来说，文档提到“根据模型的大小和训练数据量，通常需要数十块GPU”。[1][2]",
      "ground_context": [
        "根据模型的大小和训练数据量，通常需要数十块GPU，花费数天时间完成训练。"
      ],
      "retrieved_contexts": [
        "大规模语言模型：从理论到实践 11\n花费 266.4 万 H800 GPU 小时。由于训练过程需要消耗大量的计算资源，并很容易受到超参数影\n响，因此，如何提升分布式计算效率并使模型训练稳定收敛是本阶段的研究重点。\n有监督微调（Supervised Fine Tuning，SFT） ， 也称为指令微调， 利用少量高质量数据集， 通过\n有监督训练使模型具备问题回答、翻译、写作等能力。有监督微调的数据包含用户输入的提示词\n和对应的理想输出结果。用户输入包括问题、闲聊对话、任务指令等多种形式和任务。\n例如：཮౵Ĥ\n֊ն࿐གྷႵ 4ࡾ\nਟ཮౵\nྍ౵b\n利用这些有监督数据，使用与预训练阶段相同的语言模型训练算法，在基础模型的基础上进行训\n练， 得到有监督微调模型 （SFT 模型） 。 经过训练的SFT 模型具备初步的指令理解能力和上下文理\n解能力，能够完成开放领域问答、阅读理解、翻译、生成代码等任务，也具备了一定的对未知任\n务的泛化能力。由于有监督微调阶段所需的训练数据量较少，SFT 模型的训练过程并不需要消耗\n大量的计算资源。 根据模型的大小和训练数据量， 通常需要数十块GPU， 花费数天时间完成...",
        "大量的计算资源。 根据模型的大小和训练数据量， 通常需要数十块GPU， 花费数天时间完成训练。\nSFT 模型具备了初步的任务完成能力，可以开放给用户使用，很多类 ChatGPT 的模型都属于该类\n型， 包括Alpaca[35]、Vicuna[41]、MOSS、ChatGLM-6B 等。 很多这类模型的效果非常好， 甚至在一些\n评测中达到了 ChatGPT 的 90% 的效果[35, 41]。当前的一些研究表明，有监督微调阶段的数据选择\n对 SFT 模型效果有非常大的影响[42]，因此构造少量并且高质量的训练数据是本阶段的研究重点。",
        "性知识 （Factual Knowledge） 和常识性知识 （Commonsense） 在内的世界知识 （World Knowledge） 。 根\n据文献 [39] 中的介绍，GPT-3 完成一次训练的总计算量是 3640PFLOPS， 按照NVIDIA A100 80GB\nGPU 和平均利用率达到 50% 计算， 需要花费近一个月的时间使用1000 块 GPU 完成。 由于GPT-3\n的训练采用 NVIDIA V100 32GB GPU ，其实际计算成本远高于上述计算。文献[29] 介绍了参数量\n同样是 1750 亿个的 OPT 模型，该模型训练使用 992 块 NVIDIA A100 80GB GPU ，整体训练时间\n将近 2 个月。BLOOM[31] 模型的参数量也是 1750 亿个，该模型训练一共花费 3.5 个月，使用包含\n384 块 NVIDIA A100 80GB GPU 集群完成。可以看到，大语言模型的训练需要花费大量的计算资\n源和时间。LLaMA、Falcon、 百川 （Baichuan） 等模型都属于基础语言模型。 即便是DeepSeek-V3[40]\n经过了大量..."
      ],
      "response_time": 3.589877128601074,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.33333333333333337,
        "rouge2": 0.0,
        "rougeL": 0.33333333333333337,
        "bleu": 0.08382475612465994,
        "semantic_similarity": 0.8095095753669739
      }
    },
    {
      "sample_id": "0001",
      "question": "生成过程是如何进行的？",
      "ground_truth": "w1w2 · · · wm 的生成过程可以看作单词逐个生成的过程。首先生成 w1，之后根据 w1 生成 w2，然后根据 w1 和 w2 生成 w3，依此类推，根据前 m − 1 个单词生成最后一个单词 wm。",
      "predicted_answer": "生成过程可以看作单词逐个生成的过程。首先生成第一个单词 \\(w_1\\)，接着根据已生成的第一个单词 \\(w_1\\) 来生成第二个单词 \\(w_2\\)，然后再根据前两个单词 \\(w_1\\) 和 \\(w_2\\) 来生成第三个单词 \\(w_3\\)，依此类推，直到生成第 \\(m\\) 个单词 \\(w_m\\)。这个过程可以通过公式(1.1)来表达：\n\n\\[P(w_1w_2 \\cdots w_m) = P(w_1)P(w_2|w_1)P(w_3|w_1w_2) \\cdots P(w_m|w_1w_2 \\cdots w_{m-1})\\]\n\n或者用连乘的形式表示：\n\n\\[P(w_1w_2 \\cdots w_m) = \\prod_{i=1}^{m} P(w_i | w_1w_2 \\cdots w_{i-1})\\]\n\n例如，对于句子“把 努力 变成 一种 习惯”，其生成过程如下：\n\n\\[P(把 努力 变成 一种 习惯) = P(把) \\times P(努力|把) \\times P(变成|把 努力) \\times P(一种|把 努力 变成) \\times P(习惯|把 努力 变成 一种)\\]\n\n[1]",
      "ground_context": [
        "w1w2 · · · wm 的生成过程可以看作单词逐个生成的过程。首先生成 w1，之后根据 w1 生成 w2，然后根据 w1 和 w2 生成 w3，依此类推，根据前 m − 1 个单词生成最后一个单词 wm。"
      ],
      "retrieved_contexts": [
        "2 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\nP (w1w2 · · · wm) = P (w1)P (w2|w1)P (w3|w1w2) · · · P (wm|w1w2 · · · wm−1)\n=\nmY\ni=1\nP (wi|w1w2 · · · wi−1)\n(1.1)\n由此，w1w2 · · · wm 的生成过程可以看作单词逐个生成的过程。首先生成 w1，之后根据 w1 生成\nw2，然后根据 w1 和 w2 生成 w3，依此类推，根据前 m − 1 个单词生成最后一个单词 wm。例如，\n对于句子“ܸ的概率计算，使用式 (1.1) 可以转化为\nP (把 努力 变成 一种 习惯 ) = P (把) × P (努力|把) × P (变成|把 努力)×\nP (一种|把 努力 变成 ) × P (习惯|把 努力 变成 一种 )\n(1.2)\n通过上述过程，将联合概率 P (w1w2 · · · wm) 转换为多个条件概率的乘积。但是，仅通过上述过程\n模型的参数空间依然没有减小，P (wm|w1w2 · · · wm−1) 的参数空间依然是天文数字。为了解决上\n述问题，可以进一步...",
        "大语言模型：从理论到实践 317\n点， 首先通过检索模块 （Retriever） 根据查询内容定位并查找相关数据源， 然后筛选出与查询高度\n相关的信息作为检索结果。这些检索结果随后与生成模块（Generator）协作，以增强生成过程的\n质量和效果。\n用户输入各种模态数据\n检索模块生成模块结果\n文本代码图像音频视频知识科学数据……3D\n数据\n编码索引\n稀疏检索稠密检索其他\nTr ansfor merLSTMDiffusionGAN……\n文本\n代码\n图片\n视频\n知识\n科学数据\n3D\n音频\n图 9.1 典型大模型检索增强生成过程 [413]\n检索模块负责从外部知识库或数据源中定位与用户查询相关的信息。检索器通常基于向量检\n索技术或其他高效的检索算法，将输入的自然语言查询转换为向量表示，并与外部数据源中的内\n容进行匹配。外部数据源可以是文档数据库、知识图谱、 API 接口或实时搜索引擎等。检索模块\n不仅需要快速准确地定位相关内容，还需对检索结果进行筛选和排序，以确保返回的内容与用户\n查询具有高度相关性。这一模块的性能直接影响生成器后续处理的质量和效率。\n生成模块则是基于检索器提供的相关内容生成..."
      ],
      "response_time": 9.967590570449829,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.04347826086956521,
        "rouge2": 0.022222222222222227,
        "rougeL": 0.04347826086956521,
        "bleu": 0.048015233669236566,
        "semantic_similarity": 0.8735629320144653
      }
    },
    {
      "sample_id": "0002",
      "question": "Baichuan2的发布月份是什么？",
      "ground_truth": "2023 年 9 月",
      "predicted_answer": "Baichuan2的发布月份是2023年9月。[1]",
      "ground_context": [
        "Baichuan2 2023 年 9 月 百川 530 亿 对话模型"
      ],
      "retrieved_contexts": [
        "星火 2023 年 5 月 科大讯飞 - 对话模型\n浦语书生 2023 年 6 月 浦江实验室 - 对话模型\nClaude 2 2023 年 7 月 Anthropic - 对话模型\nBaichuan2 2023 年 9 月 百川 530 亿 对话模型\nKimi 2023 年 10 月 月之暗面 - 对话模型\nGemini 2023 年 12 月 Google - 对话模型\nGLM-4 2024 年 1 月 智谱华章 - 对话模型\nClaude 3 2024 年 1 月 Anthropic - 对话模型\nGPT-4o 2024 年 5 月 OpenAI 2000 亿 1 对话模型\n豆包 2024 年 5 月 字节跳动 - 对话模型\n星火 2.0 2024 年 6 月 科大讯飞 - 对话模型\nStep-2 2024 年 7 月 阶跃星辰 10000 亿 对话模型\nGPT-o1 2024 年 9 月 OpenAI 3000 亿 1 对话模型\nClaude 3.5 2024 年 10 月 Anthropic - 对话模型\nGPT-o3 2024 年 12 月 OpenAI - 推理模型\n豆包..."
      ],
      "response_time": 3.101078987121582,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.6666666666666666,
        "rouge2": 0.5,
        "rougeL": 0.6666666666666666,
        "bleu": 0.1933853138176172,
        "semantic_similarity": 0.6069027185440063
      }
    },
    {
      "sample_id": "0003",
      "question": "What function is used to apply softmax to the scores?",
      "ground_truth": "scores = F.softmax(scores, dim =-1)",
      "predicted_answer": "根据提供的文档，我无法回答这个问题。文档中并没有提到具体使用哪个函数来应用softmax到分数上。[1][2]",
      "ground_context": [
        "scores = F.softmax(scores, dim =-1)"
      ],
      "retrieved_contexts": [
        "来加快计算速度，避免从全局内存中读取和写入注意力矩阵。达成该目标需要做到在不访问整个\n输入的情况下计算 Softmax 函数，并且后向传播中不能存储中间注意力矩阵。在标准 Attention 算\n法中，Softmax 计算按行进行，即在与 V 做矩阵乘法之前，需要完成 Q、K 每个分块中的一整行\n的计算。 在得到Softmax 的结果后， 再与矩阵V 分块做矩阵乘。 而在FlashAttention 中， 将输入分\n割成块，并在输入块上进行多次传递，以增量的方式执行 Softmax 计算。",
        "全面丰富，又能准确反映问题的多维特性。融合模块的引入，使系统在应对复杂查询时能够提供\n更加综合且连贯的回答，大幅提升了整体的适应能力与输出质量。融合模块主要包含大语言模型\n融合、加权继承以及倒数排名融合等方法。\n大语言模型融合是多分支信息整合的直接方法之一， 利用大语言模型强大的分析与整合能力，\n将不同分支的信息进行统一处理。然而，这种方法面临一些挑战，特别是在处理超出大语言模型\n上下文窗口限制的长答案时。为了缓解这一问题，通常会先对每个分支的答案进行摘要提取，提\n炼关键内容后再输入 LLM， 从而在长度限制内保留最重要的信息。 这种方法确保了答案的完整性\n与精确性，即使在处理复杂的多分支生成时也能提供高质量的整合结果。\n加权集成是一种基于多分支生成结果的加权选择方法，通过不同分支生成的词元（token）的\n加权值来综合选择最终输出。具体而言，权重是通过文档与输入查询的相似度得分计算的，使用\nSoftmax 函数对权重进行归一化，确保所有权重之和为 1。该方法可按如下公式计算：\np(y|q, Dq) =\nX\nd∈Dq\np(y|d, q) · λ(d, q) (9.8)\n权重 λ(d, ..."
      ],
      "response_time": 3.2245466709136963,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.4444444444444444,
        "rouge2": 0.0,
        "rougeL": 0.4444444444444444,
        "bleu": 0.010479097231107725,
        "semantic_similarity": 0.6750924587249756
      }
    },
    {
      "sample_id": "0004",
      "question": "GPT-3 完成一次训练的总计算量是多少？",
      "ground_truth": "3640PFLOPS",
      "predicted_answer": "GPT-3 完成一次训练的总计算量是 3640PFLOPS [1]。",
      "ground_context": [
        "根据文献 [39] 中的介绍，GPT-3 完成一次训练的总计算量是 3640PFLOPS， 按照NVIDIA A100 80GB GPU 和平均利用率达到 50% 计算， 需要花费近一个月的时间使用1000 块 GPU 完成。"
      ],
      "retrieved_contexts": [
        "性知识 （Factual Knowledge） 和常识性知识 （Commonsense） 在内的世界知识 （World Knowledge） 。 根\n据文献 [39] 中的介绍，GPT-3 完成一次训练的总计算量是 3640PFLOPS， 按照NVIDIA A100 80GB\nGPU 和平均利用率达到 50% 计算， 需要花费近一个月的时间使用1000 块 GPU 完成。 由于GPT-3\n的训练采用 NVIDIA V100 32GB GPU ，其实际计算成本远高于上述计算。文献[29] 介绍了参数量\n同样是 1750 亿个的 OPT 模型，该模型训练使用 992 块 NVIDIA A100 80GB GPU ，整体训练时间\n将近 2 个月。BLOOM[31] 模型的参数量也是 1750 亿个，该模型训练一共花费 3.5 个月，使用包含\n384 块 NVIDIA A100 80GB GPU 集群完成。可以看到，大语言模型的训练需要花费大量的计算资\n源和时间。LLaMA、Falcon、 百川 （Baichuan） 等模型都属于基础语言模型。 即便是DeepSeek-V3[40]\n经过了大量...",
        "用数千个计算设备花费几十天。分布式训练系统需要克服计算墙、显存墙、通信墙等挑战，以确\n保集群内的所有资源得到充分利用，从而加速训练过程并缩短训练周期。\n• 计算墙：单个计算设备所能提供的计算能力与大语言模型所需的总计算量之间存在巨大差\n异。2022 年 3 月发布的 NVIDIA H100 SXM 的单卡 FP16 算力只有 2000 TFLOPS（Floating\nPoint Operations Per Second ） ， 而GPT-3 需要 314 ZFLOPS 的总计算量， 两者相差了8 个数量\n级。\n• 显存墙： 单个计算设备无法完整存储一个大语言模型的参数。GPT-3 包含 1750 亿个参数， 如\n果在推理阶段采用 FP32 格式进行存储，则需要 700GB 的计算设备内存空间，而 NVIDIA\nH100 GPU 只有 80GB 显存。\n• 通信墙： 分布式训练系统中各计算设备之间需要频繁地进行参数传输和同步。 由于通信的延\n迟和带宽限制，这可能成为训练的瓶颈。在 GPT-3 的训练过程中，如果分布式系统中存在\n128 个模型副本，那么在每次迭代过程中至少需要传输 89..."
      ],
      "response_time": 3.0515902042388916,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.4,
        "rouge2": 0.0,
        "rougeL": 0.4,
        "bleu": 0.01571523700545522,
        "semantic_similarity": 0.49076345562934875
      }
    },
    {
      "sample_id": "0005",
      "question": "有监督微调模型具备哪些能力？",
      "ground_truth": "经过训练的SFT模型具备初步的指令理解能力和上下文理解能力，能够完成开放领域问答、阅读理解、翻译、生成代码等任务，也具备了一定的对未知任务的泛化能力。",
      "predicted_answer": "有监督微调模型（SFT模型）具备初步的指令理解和上下文理解能力，能够完成开放领域问答、阅读理解、翻译、生成代码等任务，并且具有一定的泛化能力来应对未知任务[1][3]。",
      "ground_context": [
        "经过训练的SFT模型具备初步的指令理解能力和上下文理解能力，能够完成开放领域问答、阅读理解、翻译、生成代码等任务，也具备了一定的对未知任务的泛化能力。"
      ],
      "retrieved_contexts": [
        "大规模语言模型：从理论到实践 11\n花费 266.4 万 H800 GPU 小时。由于训练过程需要消耗大量的计算资源，并很容易受到超参数影\n响，因此，如何提升分布式计算效率并使模型训练稳定收敛是本阶段的研究重点。\n有监督微调（Supervised Fine Tuning，SFT） ， 也称为指令微调， 利用少量高质量数据集， 通过\n有监督训练使模型具备问题回答、翻译、写作等能力。有监督微调的数据包含用户输入的提示词\n和对应的理想输出结果。用户输入包括问题、闲聊对话、任务指令等多种形式和任务。\n例如：཮౵Ĥ\n֊ն࿐གྷႵ 4ࡾ\nਟ཮౵\nྍ౵b\n利用这些有监督数据，使用与预训练阶段相同的语言模型训练算法，在基础模型的基础上进行训\n练， 得到有监督微调模型 （SFT 模型） 。 经过训练的SFT 模型具备初步的指令理解能力和上下文理\n解能力，能够完成开放领域问答、阅读理解、翻译、生成代码等任务，也具备了一定的对未知任\n务的泛化能力。由于有监督微调阶段所需的训练数据量较少，SFT 模型的训练过程并不需要消耗\n大量的计算资源。 根据模型的大小和训练数据量， 通常需要数十块GPU， 花费数天时间完成...",
        "6. 强化学习\n通过有监督微调，大语言模型已初步具备遵循人类指令并完成多类型任务的能力。然而该方\n法存在显著局限：首先需要构建海量指令 -答案对数据集，高质量回复标注需耗费高昂人力成本；\n其次交叉熵损失函数要求模型输出与标准答案逐字匹配，既无法适应自然语言的表达多样性，也\n难以解决输出对输入微小变动的敏感性，这在需要深度推理的复杂任务中尤为突出。\n当前大语言模型中的强化学习技术主要沿着两个方向演进：其一是基于人类反馈的强化学习\n（Reinforcement Learning from Human Feedback，RLHF） ， 通过奖励模型对生成文本进行整体质量评估，\n使模型能自主探索更优的回复策略， 并使得模型回复与人类偏好和价值观对齐。 典型如ChatGPT 等\n对话系统， 通过人类偏好数据训练奖励模型， 结合近端策略优化 （Proximal Policy Optimization，PPO）\n算法实现对齐优化。其二是面向深度推理的强化学习框架，以 OpenAI 的 O 系列模型和 DeepSeek\n的 R 系列为代表，通过答案校验引导模型进行多步推理。这类方法将复杂问题分解为长思...",
        "5. 指令微调\n指令微调又称有监督微调，是指在预训练大语言模型的基础上，通过使用有标注的自然语言\n形式的数据， 对模型参数进行微调， 使模型具备指令遵循（Instruction Following） 能力， 能够完成\n各类预先设计的任务，并可以在零样本情况下处理诸多下游任务。经过海量数据预训练后的语言\n模型虽然具备了大量的 “知识” ， 但是由于其训练时的目标仅是进行下一个词的预测， 因此不能够\n理解并遵循人类自然语言形式的指令。为了使模型具有理解并响应人类指令的能力，还需要使用\n指令数据对其进行调整。如何构造指令数据，如何高效低成本地进行指令微调训练，以及如何在\n语言模型基础上进一步扩大上下文等问题，是大语言模型在指令微调阶段的核心。\n本章先介绍大语言模型指令微调训练方法，在此基础上介绍高效模型微调及模型上下文窗口\n扩展方法，最后介绍指令微调的代码实践。\n5.1 指令微调训练\n指令微调具体训练过程并不复杂，主要分为如下三个步骤： （ 1）针对每一项任务去明确地\n定义相应的自然语言形式的指令或者提示，这些指令或提示对任务目标以及输出要求进行清晰描\n述； （2）把训练数据调整成包含指令以...",
        "下游任务进行微调。 这些模型参数量大都在10 亿个以上， 由于微调的计算量很大， 这类模型的影\n响力在当时相较 BERT 类模型有不小的差距。\n能力探索阶段集中于 2019 年至 2022 年，由于大语言模型很难针对特定任务进行微调，研究\n人员开始探索在不针对单一任务进行微调的情况下如何发挥大语言模型的能力。2019 年，Radford\n等人在文献 [11] 中使用 GPT-2 模型研究了大语言模型在零样本情况下的任务处理能力。 在此基础\n上，Brown 等人在 GPT-3[13] 模型上研究了通过语境学习进行少样本学习的方法，将不同任务的少\n量有标注的实例拼接到待分析的样本之前输入语言模型，语言模型根据实例理解任务并给出正确\n的结果。基于 GPT-3 的语境学习在 TriviaQA、WebQS、CoQA 等评测集合中都展示出了非常强的\n能力，在有些任务中甚至超过了此前的有监督方法。上述方法不需要修改语言模型的参数，模型\n在处理不同任务时无须花费大量计算资源进行模型微调。仅依赖语言模型本身，其性能在很多任\n务上仍然很难达到有监督学习 （Supervised Learning） 的效果，...",
        "大规模语言模型：从理论到实践 13\n强化学习（Reinforcement Learning，RL） 阶段根据数十万条提示词， 利用前一阶段训练的奖励\n模型， 给出SFT 模型对提示词回答结果的质量评估， 并与语言模型建模目标综合得到更好的效果。\n该阶段使用的提示词数量与有监督微调阶段类似，数量在十万个量级，并且不需要人工提前给出\n该提示词所对应的理想回复。使用强化学习，在 SFT 模型的基础上调整参数，使最终生成的文本\n可以获得更高的奖励 （Reward） 。 该阶段需要的计算量较预训练阶段也少很多， 通常仅需要数十块\nGPU，数天即可完成训练。文献 [24] 给出了强化学习和有监督微调的对比，在模型参数量相同的\n情况下，强化学习可以得到相较于有监督微调好得多的效果。关于为什么强化学习相比有监督微\n调可以得到更好结果的问题， 截至2025 年 2 月还没有完整或得到普遍共识的解释。 目前相对得到\n认可的观点是，强化学习使得模型具备更好的泛化能力[43]。同时，Andrej Karpathy 也指出，强化\n学习并不是没有问题的，它会使基础模型的熵降低，从而减少模型输出的多样性。经过强化学..."
      ],
      "response_time": 4.357249021530151,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.5,
        "rouge2": 0.0,
        "rougeL": 0.5,
        "bleu": 0.4968394599548699,
        "semantic_similarity": 0.8805713653564453
      }
    },
    {
      "sample_id": "0006",
      "question": "大语言模型的构建流程主要包含几个阶段？",
      "ground_truth": "主要包含四个阶段：预训练、 有监督微调、 奖励建模和强化学习。",
      "predicted_answer": "大语言模型的构建流程主要包含四个阶段：预训练、有监督微调、奖励建模和强化学习[1]。",
      "ground_context": [
        "OpenAI 使用的大语言模型构建流程主要包含四个阶段：预训练、 有监督微调、 奖励建模和强化学习。"
      ],
      "retrieved_contexts": [
        "10 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n1.3 大语言模型的构建流程\n根据 OpenAI 联合创始人 Andrej Karpathy 在微软 Build 2023 大会上公开的信息，OpenAI 使用\n的大语言模型构建流程如图1.3 所示， 主要包含四个阶段：预训练、 有监督微调、 奖励建模和强化\n学习。这四个阶段都需要不同规模的数据集及不同类型的算法，会产出不同类型的模型，所需要\n的资源也有非常大的差别。\n图 1.3 OpenAI 使用的大语言模型构建流程\n预训练（Pretraining） 阶段需要利用海量的训练数据 （数据来自互联网网页、 维基百科、 书籍、\nGitHub、 论文、 问答网站等） ， 构建包含数千亿甚至数万亿单词的具有多样性的内容。 利用由数千块\n高性能 GPU 和高速网络组成的超级计算机， 花费数十天完成深度神经网络参数训练， 构建基础模\n型 （Base Model） 。 基础模型对长文本进行建模， 使模型具有语言生成能力， 根据输入的提示词， 模\n型可以生成文本补全句子。有一部分研究人员认为，语言模型建模过程中隐含地构建了包括事实\n性知识...",
        "4 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\nѠ\n图 1.1 大语言模型的缩放法则 [17]\n1.2 大语言模型的发展历程\n大语言模型的发展历程虽然只有不到5 年，但是发展速度相当惊人，截至2025 年 2 月，国内\n外有超过百种大语言模型相继发布。特别是2024 年 12 月 DeepSeek V3 和 2025 年 1 月 DeepSeek\nR1 模型的开源， 不仅在训练效率和思考推理上取得了突破， 还赢得了国际社会对中国人工智能技术\n的高度认可。中国人民大学赵鑫教授团队在《大语言模型》书中按照时间线给出了2019 年至 2024\n年 6 月比较有影响力并且模型参数量超过100 亿个的大语言模型，我们在此基础上扩展到2025 年\n2 月， 如图1.2 所示。 大语言模型的发展可以粗略地分为如下三个阶段： 基础模型阶段、 能力探索阶\n段和突破发展阶段。\n图 1.2 大语言模型发展时间线 [18]\n基础模型阶段主要集中于 2018 年至 2021 年。2017 年，V aswani等人提出了 Transformer[12]\n架构，在机器翻译任务上取得了突破性进展。201...",
        "152 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n据集。该数据集精炼后包含约 5.2 万条数据，能够用于提升大语言模型在开放领域信息抽取任务\n中的表现，从而显著增强其能力。\n图 5.1 适用于大模型开放领域命名实体识别任务 B 2NERD 数据集构建过程[188]\n3. 自动构建指令\n手动构建指令数据代价高昂，需要大量的人力投入。因此，一些研究尝试寻找更高效的替代\n方法。具有代表性的工作如 Self-Instruct[189]，利用大语言模型的生成能力自动构建指令。\nSelf-Instruct 数据生成是一个迭代过程。如图5.2 所示，它包含以下 4 个步骤。",
        "1. 绪论\n大语言模型是一种由包含数百亿个及以上参数的深度神经网络构建的语言模型，通常使用自\n监督学习方法通过大量无标注文本进行训练。2018 年以来，Google、OpenAI、Meta、百度、华为\n等公司和研究机构相继发布了 BERT[1]、GPT[2] 等多种模型， 这些模型在几乎所有自然语言处理任\n务中都表现出色。2019 年，大语言模型呈现爆发式的增长，特别是 2022 年 11 月 ChatGPT（Chat\nGenerative Pre-trained Transformer）的发布，引起了全世界的广泛关注。用户可以使用自然语言与\n系统交互，实现问答、分类、摘要、翻译、聊天等从理解到生成的各种任务。大语言模型展现出\n了强大的对世界知识的掌握和对语言的理解能力。\n本章主要介绍大语言模型的基本概念、发展历程和构建流程。\n1.1 大语言模型的基本概念\n使用语言是人类与其他动物最重要的区别之一， 而人类的多种智能也与此密切相关， 逻辑思维\n以语言的形式表达， 大量的知识也以文字的形式记录和传播。 如今， 互联网上已经拥有数万亿个网页\n的资源， 其中大部分信息都是用自然语言描述的。 ...",
        "1.3 大语言模型的构建流程 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\n1.4 本书的内容安排 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\n2 大语言模型基础 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ...."
      ],
      "response_time": 3.762781858444214,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.6155748316594309,
        "semantic_similarity": 0.8412023782730103
      }
    },
    {
      "sample_id": "0007",
      "question": "第 5 章重点介绍什么技术？",
      "ground_truth": "模型微调技术，有监督微调数据的构造策略以及高效微调方法：LoRA、Delta Tuning 等方法。",
      "predicted_answer": "第 5 章重点介绍模型微调技术，有监督微调数据的构造策略以及高效微调方法：LoRA、Delta Tuning 等方法[1][2]。",
      "ground_context": [
        "第 5 章重点介绍模型微调技术，有监督微调数据的构造策略以及高效微调方法：LoRA、Delta Tuning 等方法；"
      ],
      "retrieved_contexts": [
        "和强化学习方法，使模型能够理解指令并生成类人回答。第 5 章重点介绍模型微调技术，有监督\n微调数据的构造策略以及高效微调方法：LoRA、Delta Tuning 等方法；第 6 章则围绕强化学习展\n开， 讲解其基础理论与近端策略优化（PPO）技术， 并结合实际案例， 以DeepSpeed-Chat 和 veRL\n框架为例，详细说明如何训练类 ChatGPT 系统。\n第 7 章、第 8 章和第 9 章围绕提升大语言模型能力展开详细探讨，内容涵盖多模态大语言模\n型、智能体实践及检索增强生成。第 7 章重点介绍多模态大语言模型的基础理论、架构设计与训\n练策略，并探讨其在实际场景中的应用实践；第 8 章聚焦智能体的发展历程与大语言模型智能体\n的架构设计，深入分析智能体的实现原理，并以 LangChain 为例详细阐述具体实践；第 9 章则围\n绕检索增强生成展开讨论，介绍其核心思想与实现方式，涵盖检索增强框架的设计、检索模块与",
        "14 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n第2章大语言模型基础\n第3章大语言模型预训练数据第4章分布式训练预训练\n第7章多模态大语言模型第8章大模型智能体大模型增强\n第5章有监督微调第6章强化学习指令理解\n理论基础\n大模型应用\n第9章检索增强生成\n第10章大语言模型效率优化第11章大语言模型评估第12章大语言模型应用开发\n图 1.5 本书章节安排\n第 2 章介绍大语言模型的基础理论知识，包括语言模型的定义、Transformer 结构、大语言模\n型框架等内容，并以 LLaMA 使用的模型结构为例介绍代码实例。\n第 3 章和第 4 章围绕大语言模型预训练阶段的主要研究内容开展介绍，包括模型分布式训练\n中需要掌握的数据并行、 流水线并行、 模型并行及ZeRO 系列优化方法。 除此之外， 还将介绍预训\n练需要使用的数据分布和数据预处理方法， 并以DeepSpeed 为例介绍如何进行大语言模型预训练。\n第 5 章和第 6 章聚焦于大语言模型指令理解阶段的核心研究内容，探讨如何通过有监督微调\n和强化学习方法，使模型能够理解指令并生成类人回答。第 5 章重点介绍模型微调技术，..."
      ],
      "response_time": 3.9436838626861572,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.6666666666666666,
        "rouge2": 0.5714285714285715,
        "rougeL": 0.6666666666666666,
        "bleu": 0.6372451658253607,
        "semantic_similarity": 0.8600019216537476
      }
    },
    {
      "sample_id": "0008",
      "question": "残差连接的主要作用是什么？",
      "ground_truth": "避免在优化过程中因网络过深而产生潜在的梯度消失问题。",
      "predicted_answer": "残差连接主要是指使用一条直连通道直接将对应子层的输入连接到输出，避免在优化过程中因网络过深而产生潜在的梯度消失问题[1][2]。",
      "ground_context": [
        "残差连接主要是指使用一条直连通道直接将对应子层的输入连接到输出，避免在优化过程中因网络过深而产生潜在的梯度消失问题。"
      ],
      "retrieved_contexts": [
        "def forward(self, x):\nx = self.dropout(F.relu(self.linear_1(x)))\nx = self.linear_2(x)\nreturn x\n2.1.4 残差连接与层归一化\n由 Transformer 结构组成的网络结构通常都非常庞大。 编码器和解码器均由很多层基本的Trans-\nformer 块组成，每一层中都包含复杂的非线性映射，这就导致模型的训练比较困难。因此，研究\n人员在 Transformer 块中进一步引入了残差连接与层归一化技术， 以进一步提升训练的稳定性。 具\n体来说，残差连接主要是指使用一条直连通道直接将对应子层的输入连接到输出，避免在优化过\n程中因网络过深而产生潜在的梯度消失问题：\nxl+1 = f (xl) + xl (2.10)\n其中 xl 表示第 l 层的输入，f (·) 表示一个映射函数。 此外， 为了使每一层的输入/输出稳定在一个\n合理的范围内，层归一化技术被进一步引入每个 Transformer 块中：\nLN(x) = α · x − µ\nσ + b (2.11)",
        "大规模语言模型：从理论到实践 17\n运行多个独立注意力机制， 进而从多维度捕捉输入序列信息。 它使得序列中任意两个单词之\n间的依赖关系可以直接被建模而不基于传统的循环结构，从而更好地解决文本的长程依赖\n问题。\n• 位置感知前馈网络层（Position-wise Feed-Forward Network） ：通过全连接层对输入文本序列\n中的每个单词表示进行更复杂的变换。\n• 残差连接： 对应图中的Add 部分。 它是一条分别作用在上述两个子层中的直连通路， 被用于\n连接两个子层的输入与输出，使信息流动更高效，有利于模型的优化。\n• 层归一化：对应图中的 Norm 部分。它作用于上述两个子层的输出表示序列，对表示序列进\n行层归一化操作，同样起到稳定优化的作用。\n词元嵌入表示\n多头注意力\nAdd & Norm\nAdd & Norm\nAdd & Norm\n掩码多头注意力\n输出（迭代右移)\n×N\n输出概率\nLinearSoftmax\nN×\n词元嵌入表示\n多头注意力\n位置感知前馈网络Add & Norm\nAdd & Norm\n输入\n位置编码 位置编码\n位置感知前馈网络\n图 2.1 基于 Trans..."
      ],
      "response_time": 4.106638431549072,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.32913544603363937,
        "semantic_similarity": 0.7164160013198853
      }
    },
    {
      "sample_id": "0009",
      "question": "在大语言模型的指令理解阶段，如何通过有监督微调和强化学习的方法提升模型的输出质量？",
      "ground_truth": "通过有监督微调和强化学习方法，可以使模型理解指令并生成类人回答，从而提升输出质量。",
      "predicted_answer": "在大语言模型的指令理解阶段，可以通过以下方式提升模型的输出质量：\n\n1. **有监督微调（Supervised Fine Tuning, SFT）**：这种方法利用少量高质量数据集，通过有监督训练使模型具备问题回答、翻译、写作等能力。具体来说，数据包含用户输入的提示词和对应的理想输出结果。训练过程中，使用与预训练阶段相同的语言模型训练算法，在基础模型的基础上进行训练，得到SFT模型。这样经过训练的SFT模型具备初步的指令理解和上下文理解能力，能够完成多种任务[2][4]。\n\n2. **强化学习（Reinforcement Learning, RL）**：在有监督微调阶段之后，可以利用强化学习来进一步提升模型的输出质量。具体做法是利用前一阶段训练的奖励模型，给出行回复的结果质量评估，并结合语言模型建模目标，得到更好的效果。这一阶段同样使用提示词进行训练，但不需要人工提前给出理想的回复。强化学习通过调整模型参数，使最终生成的文本获得更高的奖励分数[3]。\n\n这两种方法相结合，不仅提升了模型对特定任务的理解和执行能力，还增强了模型的泛化能力和多样性[1][3]。",
      "ground_context": [
        "第 5 章和第 6 章聚焦于大语言模型指令理解阶段的核心研究内容，探讨如何通过有监督微调和强化学习方法，使模型能够理解指令并生成类人回答。",
        "如果奖励模型的目标是针对系统所有的输出都能够高质量地进行判决，那么该问题的难度在某种程度上与文本生成等价。"
      ],
      "retrieved_contexts": [
        "6. 强化学习\n通过有监督微调，大语言模型已初步具备遵循人类指令并完成多类型任务的能力。然而该方\n法存在显著局限：首先需要构建海量指令 -答案对数据集，高质量回复标注需耗费高昂人力成本；\n其次交叉熵损失函数要求模型输出与标准答案逐字匹配，既无法适应自然语言的表达多样性，也\n难以解决输出对输入微小变动的敏感性，这在需要深度推理的复杂任务中尤为突出。\n当前大语言模型中的强化学习技术主要沿着两个方向演进：其一是基于人类反馈的强化学习\n（Reinforcement Learning from Human Feedback，RLHF） ， 通过奖励模型对生成文本进行整体质量评估，\n使模型能自主探索更优的回复策略， 并使得模型回复与人类偏好和价值观对齐。 典型如ChatGPT 等\n对话系统， 通过人类偏好数据训练奖励模型， 结合近端策略优化 （Proximal Policy Optimization，PPO）\n算法实现对齐优化。其二是面向深度推理的强化学习框架，以 OpenAI 的 O 系列模型和 DeepSeek\n的 R 系列为代表，通过答案校验引导模型进行多步推理。这类方法将复杂问题分解为长思...",
        "大规模语言模型：从理论到实践 11\n花费 266.4 万 H800 GPU 小时。由于训练过程需要消耗大量的计算资源，并很容易受到超参数影\n响，因此，如何提升分布式计算效率并使模型训练稳定收敛是本阶段的研究重点。\n有监督微调（Supervised Fine Tuning，SFT） ， 也称为指令微调， 利用少量高质量数据集， 通过\n有监督训练使模型具备问题回答、翻译、写作等能力。有监督微调的数据包含用户输入的提示词\n和对应的理想输出结果。用户输入包括问题、闲聊对话、任务指令等多种形式和任务。\n例如：཮౵Ĥ\n֊ն࿐གྷႵ 4ࡾ\nਟ཮౵\nྍ౵b\n利用这些有监督数据，使用与预训练阶段相同的语言模型训练算法，在基础模型的基础上进行训\n练， 得到有监督微调模型 （SFT 模型） 。 经过训练的SFT 模型具备初步的指令理解能力和上下文理\n解能力，能够完成开放领域问答、阅读理解、翻译、生成代码等任务，也具备了一定的对未知任\n务的泛化能力。由于有监督微调阶段所需的训练数据量较少，SFT 模型的训练过程并不需要消耗\n大量的计算资源。 根据模型的大小和训练数据量， 通常需要数十块GPU， 花费数天时间完成...",
        "大规模语言模型：从理论到实践 13\n强化学习（Reinforcement Learning，RL） 阶段根据数十万条提示词， 利用前一阶段训练的奖励\n模型， 给出SFT 模型对提示词回答结果的质量评估， 并与语言模型建模目标综合得到更好的效果。\n该阶段使用的提示词数量与有监督微调阶段类似，数量在十万个量级，并且不需要人工提前给出\n该提示词所对应的理想回复。使用强化学习，在 SFT 模型的基础上调整参数，使最终生成的文本\n可以获得更高的奖励 （Reward） 。 该阶段需要的计算量较预训练阶段也少很多， 通常仅需要数十块\nGPU，数天即可完成训练。文献 [24] 给出了强化学习和有监督微调的对比，在模型参数量相同的\n情况下，强化学习可以得到相较于有监督微调好得多的效果。关于为什么强化学习相比有监督微\n调可以得到更好结果的问题， 截至2025 年 2 月还没有完整或得到普遍共识的解释。 目前相对得到\n认可的观点是，强化学习使得模型具备更好的泛化能力[43]。同时，Andrej Karpathy 也指出，强化\n学习并不是没有问题的，它会使基础模型的熵降低，从而减少模型输出的多样性。经过强化学...",
        "5. 指令微调\n指令微调又称有监督微调，是指在预训练大语言模型的基础上，通过使用有标注的自然语言\n形式的数据， 对模型参数进行微调， 使模型具备指令遵循（Instruction Following） 能力， 能够完成\n各类预先设计的任务，并可以在零样本情况下处理诸多下游任务。经过海量数据预训练后的语言\n模型虽然具备了大量的 “知识” ， 但是由于其训练时的目标仅是进行下一个词的预测， 因此不能够\n理解并遵循人类自然语言形式的指令。为了使模型具有理解并响应人类指令的能力，还需要使用\n指令数据对其进行调整。如何构造指令数据，如何高效低成本地进行指令微调训练，以及如何在\n语言模型基础上进一步扩大上下文等问题，是大语言模型在指令微调阶段的核心。\n本章先介绍大语言模型指令微调训练方法，在此基础上介绍高效模型微调及模型上下文窗口\n扩展方法，最后介绍指令微调的代码实践。\n5.1 指令微调训练\n指令微调具体训练过程并不复杂，主要分为如下三个步骤： （ 1）针对每一项任务去明确地\n定义相应的自然语言形式的指令或者提示，这些指令或提示对任务目标以及输出要求进行清晰描\n述； （2）把训练数据调整成包含指令以...",
        "下游任务进行微调。 这些模型参数量大都在10 亿个以上， 由于微调的计算量很大， 这类模型的影\n响力在当时相较 BERT 类模型有不小的差距。\n能力探索阶段集中于 2019 年至 2022 年，由于大语言模型很难针对特定任务进行微调，研究\n人员开始探索在不针对单一任务进行微调的情况下如何发挥大语言模型的能力。2019 年，Radford\n等人在文献 [11] 中使用 GPT-2 模型研究了大语言模型在零样本情况下的任务处理能力。 在此基础\n上，Brown 等人在 GPT-3[13] 模型上研究了通过语境学习进行少样本学习的方法，将不同任务的少\n量有标注的实例拼接到待分析的样本之前输入语言模型，语言模型根据实例理解任务并给出正确\n的结果。基于 GPT-3 的语境学习在 TriviaQA、WebQS、CoQA 等评测集合中都展示出了非常强的\n能力，在有些任务中甚至超过了此前的有监督方法。上述方法不需要修改语言模型的参数，模型\n在处理不同任务时无须花费大量计算资源进行模型微调。仅依赖语言模型本身，其性能在很多任\n务上仍然很难达到有监督学习 （Supervised Learning） 的效果，..."
      ],
      "response_time": 9.613832473754883,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.015443095705045908,
        "semantic_similarity": 0.8310821056365967
      }
    },
    {
      "sample_id": "0010",
      "question": "在大语言模型中，如何通过 n 元语法模型和平滑技术来解决零概率问题？",
      "ground_truth": "n 元语法模型通过假设任意单词的出现概率只与过去 n-1 个词相关，从而减少了模型参数空间的复杂性。然而，尽管 n 元语言模型能缓解句子概率为零的问题，语言的多样性使得无法覆盖所有 n-gram。这时候，需要使用平滑技术来为所有可能出现的字符串分配一个非零的概率值，以避免零概率问题。平滑处理的基本思想是调整最大似然估计，提高低概率事件，降低高概率事件，使整体的概率分布趋于均匀。",
      "predicted_answer": "在大语言模型中，可以通过 n 元语法模型和平滑技术来解决零概率问题。具体来说，n 元语法模型通过仅考虑有限数量的前 n 个单词来近似计算下一个词的概率，从而缓解了句子概率为零的问题 [1]。然而，由于训练数据的限制，仍可能存在未见过的组合，这时就需要采用平滑技术来分配这些未见过组合的非零概率 [1][2]。平滑技术通过对最大似然估计进行调整，提高了低概率事件的可能性，降低了高概率事件的可能性，使得整体的概率分布更加合理 [1]。",
      "ground_context": [
        "n 元语法或 n 元文法（n-gram）模型，其中，n-gram 表示由 n 个连续单词构成的单元。",
        "平滑是指为了产生更合理的概率，对最大似然估计进行调整的一类方法，也称为数据平滑（Data Smoothing）。"
      ],
      "retrieved_contexts": [
        "P (wi|w1w2 · · · wi−1) = P (wi|wi−(n−1)wi−(n−2) · · · wi−1)\nP (wi|wi−1\n1 ) = P (wi|wi−1\ni−n+1)\n(1.3)\n满足上述条件的模型被称为 n 元语法或 n 元文法（n-gram）模型。其中，n-gram 表示由 n 个连续\n单词构成的单元，也被称为n 元语法单元。\n虽然 n 元语言模型能缓解句子概率为零的问题，但语言是由人和时代创造的，具备无尽的可\n能性，再庞大的训练数据也无法覆盖所有的 n-gram，而训练数据中的零频率并不代表零概率。因\n此，需要使用平滑技术（Smoothing）解决，为所有可能出现的字符串分配一个非零的概率值，从\n而避免零概率问题。平滑是指为了产生更合理的概率，对最大似然估计进行调整的一类方法，也\n称为数据平滑（Data Smoothing）。平滑处理的基本思想是提高低概率事件，降低高概率事件，使\n整体的概率分布趋于均匀。 这类方法通常被称为统计语言模型（Statistical Language Models，SLM）\n。相关平滑算法细节可以参考《自然语言处理导论》的第 6 ...",
        "n 元语言模型从整体上看与训练数据规模和模型的阶数（考虑上下文的数量）有较大的关系，\n不同的平滑算法在不同情况下的表现有较大的差距。虽然平滑算法较好地解决了零概率问题，但\n是基于稀疏表示的 n 元语言模型仍然有以下三个较为明显的缺点。\n（1）无法对长度超过 n 的上下文建模。\n（2）依赖人工设计规则的平滑技术。\n（3）当 n 增大时，数据的稀疏性随之增大，模型的参数量更是呈指数级增加，受数据稀疏问\n题的影响，其参数难以被准确学习。\n此外，n 元文法中单词的离散表示也忽略了单词之间的相似性。 因此， 基于分布式表示和神经"
      ],
      "response_time": 5.871921539306641,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.6153846153846153,
        "rouge2": 0.5454545454545454,
        "rougeL": 0.4615384615384615,
        "bleu": 0.15778461663752336,
        "semantic_similarity": 0.9272022247314453
      }
    },
    {
      "sample_id": "0011",
      "question": "在自然语言处理领域，如何克服 n 元语言模型的局限性以实现更好的性能？",
      "ground_truth": "通过应用预训练语言模型（PLM）的方法，例如使用基于 Transformer 结构的 GPT 和 BERT 模型，可以有效克服 n 元语言模型的局限性。预训练模型能够通过海量数据学习有效的特征表示，并且在下游任务中只需进行微调，避免了 n 元模型对上下文长度的限制和对人工规则的依赖。",
      "predicted_answer": "为了克服 n 元语言模型的局限性以实现更好的性能，可以从以下几个方面入手：\n\n1. 使用更大的上下文窗口来建模更长的依赖关系，从而减少对短距离依赖性的过度关注[1]。\n2. 引入预训练语言模型，如BERT、GPT等，利用大规模数据集进行预训练，然后再针对具体任务进行微调，这样可以在保持模型泛化能力的同时提升性能[2][3][4]。\n3. 利用Transformer架构及其优化的注意力机制，可以有效捕捉长距离依赖关系，同时减少数据稀疏性带来的负面影响[4]。",
      "ground_context": [
        "以GPT和BERT为代表的基于 Transformer 结构的大规模预训练语言模型的出现，使自然语言处理全面进入预训练微调范式新时代。",
        "n 元语言模型仍然有以下三个较为明显的缺点。（1）无法对长度超过 n 的上下文建模。"
      ],
      "retrieved_contexts": [
        "n 元语言模型从整体上看与训练数据规模和模型的阶数（考虑上下文的数量）有较大的关系，\n不同的平滑算法在不同情况下的表现有较大的差距。虽然平滑算法较好地解决了零概率问题，但\n是基于稀疏表示的 n 元语言模型仍然有以下三个较为明显的缺点。\n（1）无法对长度超过 n 的上下文建模。\n（2）依赖人工设计规则的平滑技术。\n（3）当 n 增大时，数据的稀疏性随之增大，模型的参数量更是呈指数级增加，受数据稀疏问\n题的影响，其参数难以被准确学习。\n此外，n 元文法中单词的离散表示也忽略了单词之间的相似性。 因此， 基于分布式表示和神经",
        "P (wi|w1w2 · · · wi−1) = P (wi|wi−(n−1)wi−(n−2) · · · wi−1)\nP (wi|wi−1\n1 ) = P (wi|wi−1\ni−n+1)\n(1.3)\n满足上述条件的模型被称为 n 元语法或 n 元文法（n-gram）模型。其中，n-gram 表示由 n 个连续\n单词构成的单元，也被称为n 元语法单元。\n虽然 n 元语言模型能缓解句子概率为零的问题，但语言是由人和时代创造的，具备无尽的可\n能性，再庞大的训练数据也无法覆盖所有的 n-gram，而训练数据中的零频率并不代表零概率。因\n此，需要使用平滑技术（Smoothing）解决，为所有可能出现的字符串分配一个非零的概率值，从\n而避免零概率问题。平滑是指为了产生更合理的概率，对最大似然估计进行调整的一类方法，也\n称为数据平滑（Data Smoothing）。平滑处理的基本思想是提高低概率事件，降低高概率事件，使\n整体的概率分布趋于均匀。 这类方法通常被称为统计语言模型（Statistical Language Models，SLM）\n。相关平滑算法细节可以参考《自然语言处理导论》的第 6 ...",
        "发展，使得大规模文本非常容易获取，因此训练超大规模的基于神经网络的语言模型成为可能。\n受计算机视觉领域采用 ImageNet[9] 对模型进行一次预训练， 使模型可以通过海量图像充分学\n习如何提取特征，再根据任务目标进行模型精调的预训练范式影响，自然语言处理领域基于预训\n练语言模型的方法逐渐成为主流。 以ELMo[10] 为代表的动态词向量模型开启了语言模型预训练的\n大门。此后， 以GPT[11] 和 BERT[1] 为代表的基于 Transformer 结构[12] 的大规模预训练语言模型的\n出现，使自然语言处理全面进入预训练微调范式新时代。将预训练模型应用于下游任务时，不需\n要了解太多的任务细节，不需要设计特定的神经网络结构，只需要“微调”预训练模型，使用具\n体任务的标注数据在预训练语言模型上进行监督训练，就可以取得显著的性能提升。这类方法通\n常被称为预训练语言模型（Pre-trained Language Models，PLM）。\n2020 年，OpenAI 发布了由包含 1750 亿个参数的神经网络构成的生成式大规模预训练语言模\n型 GPT-3（Generative Pre-t...",
        "下游任务进行微调。 这些模型参数量大都在10 亿个以上， 由于微调的计算量很大， 这类模型的影\n响力在当时相较 BERT 类模型有不小的差距。\n能力探索阶段集中于 2019 年至 2022 年，由于大语言模型很难针对特定任务进行微调，研究\n人员开始探索在不针对单一任务进行微调的情况下如何发挥大语言模型的能力。2019 年，Radford\n等人在文献 [11] 中使用 GPT-2 模型研究了大语言模型在零样本情况下的任务处理能力。 在此基础\n上，Brown 等人在 GPT-3[13] 模型上研究了通过语境学习进行少样本学习的方法，将不同任务的少\n量有标注的实例拼接到待分析的样本之前输入语言模型，语言模型根据实例理解任务并给出正确\n的结果。基于 GPT-3 的语境学习在 TriviaQA、WebQS、CoQA 等评测集合中都展示出了非常强的\n能力，在有些任务中甚至超过了此前的有监督方法。上述方法不需要修改语言模型的参数，模型\n在处理不同任务时无须花费大量计算资源进行模型微调。仅依赖语言模型本身，其性能在很多任\n务上仍然很难达到有监督学习 （Supervised Learning） 的效果，...",
        "2. 大语言模型基础\n语言模型的核心目标是对自然语言的概率分布进行建模，这一任务在自然语言处理研究中占\n据重要地位，是其基础性工作之一。大量研究围绕这一目标，从不同角度展开了探索，包括 n 元\n语言模型（n-gram Language Models ） 、神经语言模型和预训练语言模型等。这些研究在不同发展\n阶段对自然语言处理任务产生了深远影响。 随着基于Transformer 架构的语言模型不断发展， 以及\n预训练-微调范式在各类自然语言处理任务中取得突破性成果，自 2020 年 OpenAI 发布 GPT-3 以\n来，大语言模型的研究逐步深入。尽管大语言模型参数规模庞大，并且通过有监督微调和强化学\n习可以完成众多任务，其理论基础仍然离不开对语言建模的核心研究。\n本章首先介绍 Transformer 结构， 并在此基础上讲解生成式预训练语言模型GPT、 大语言模型\n的网络结构、注意力机制优化及相关实践。关于 n 元语言模型、神经语言模型及其他预训练语言\n模型的内容，可参考《自然语言处理导论》第 6 章[4]。\n2.1 Transformer 结构\nTransformer 结构[44] ..."
      ],
      "response_time": 5.864958763122559,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.4444444444444444,
        "rouge2": 0.0,
        "rougeL": 0.1111111111111111,
        "bleu": 0.09272757486390507,
        "semantic_similarity": 0.8154418468475342
      }
    },
    {
      "sample_id": "0012",
      "question": "模型的参数维度是什么？",
      "ground_truth": "512",
      "predicted_answer": "根据提供的文档，我无法回答这个问题。",
      "ground_context": [
        "d_model = 512"
      ],
      "retrieved_contexts": [],
      "response_time": 2.7515296936035156,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.0,
        "semantic_similarity": 0.44486963748931885
      }
    },
    {
      "sample_id": "0013",
      "question": "什么函数用于将输入 x 进行旋转处理？",
      "ground_truth": "rotate_half",
      "predicted_answer": "根据提供的文档，我无法回答这个问题。",
      "ground_context": [
        "def rotate_half(x):"
      ],
      "retrieved_contexts": [],
      "response_time": 2.2337870597839355,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.0,
        "semantic_similarity": 0.427607923746109
      }
    },
    {
      "sample_id": "0014",
      "question": "通过上述代码可以得到什么输出？",
      "ground_truth": "通过上述代码可以得到如下输出：",
      "predicted_answer": "第一段代码执行后可以得到一个元组 (5, 1536) 和一组浮点数列表 [1]。\n第二段代码执行后可以得到以下输出：\n```\nchi-squared-a: ...\np-value-a: ...\nchi-squared-b: ...\np-value-b: ...\n``` 具体数值取决于计算结果 [2]。\n第三段代码的执行将生成LLMChain实例并运行，但不会显示具体输出，除非设置 `verbose=True` 或指定回调函数 [3]。",
      "ground_context": [
        "通过上述代码可以得到如下输出："
      ],
      "retrieved_contexts": [
        "embedded_query[:5]\n执行上述代码可以得到如下输出：\n(5, 1536)\n[0.0053587136790156364,\n-0.0004999046213924885,\n0.038883671164512634,\n-0.003001077566295862,\n-0.00900818221271038]",
        "print('chi-squared-a:', chi2)\nprint('p-value-a:', p)\ntb_b = np.array([[9945, 25],\n[15, 15]])\nchi2, p = mcnemar(ary=tb_b, exact =True)\nprint('chi-squared-b:', chi2)\nprint('p-value-b:', p)\n可以得到如下输出：",
        "chain = LLMChain(llm=llm, prompt =prompt, callbacks =[handler])\nchain.run(number=2)\n#đ൐Ⴈverboseݔࢲ֥\nchain = LLMChain(llm=llm, prompt =prompt, verbose =True)\nchain.run(number=2)\n#֥callbacksݔࢲ֥\nchain = LLMChain(llm=llm, prompt =prompt)\nchain.run(number=2, callbacks =[handler])\n执行上述程序可以得到如下输出："
      ],
      "response_time": 5.149465560913086,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.010119473283306682,
        "semantic_similarity": 0.630526602268219
      }
    },
    {
      "sample_id": "0015",
      "question": "GPT-2 相较于 GPT 引入了哪种前置层归一化方法？",
      "ground_truth": "前置层归一化方法",
      "predicted_answer": "GPT-2 相较于 GPT 引入了前置层归一化方法，具体来说，将第一个层归一化移动到多头自注意力层之前，将第二个层归一化移动到全连接层之前 [1]。",
      "ground_context": [
        "为了使模型训练过程更加稳定，GPT-2 相较于 GPT 引入了前置层归一化方法，将第一个层归一化移动到多头自注意力层之前，将第二个层归一化移动到全连接层之前。"
      ],
      "retrieved_contexts": [
        "层归一化\n…\n文本预测文本分类\n12✕\n+\nAttenOutFC多头自注意力\nMatMul\nSoftmax\nQueryKeyValue\nQKVLinear\n图 2.4 GPT-2 的模型结构\n接下来，分别介绍 RMSNorm 归一化函数、SwiGLU 激活函数和 RoPE 的具体内容和实现。\n1. RMSNorm 归一化函数\n为了使模型训练过程更加稳定，GPT-2 相较于 GPT 引入了前置层归一化方法，将第一个层归\n一化移动到多头自注意力层之前，将第二个层归一化移动到全连接层之前。同时，残差连接的位",
        "大规模语言模型：从理论到实践 41\n后，OpenAI 就不再开源（也没有开源模型） ，因此并不清楚ChatGPT 和 GPT-4 采用的模型架构。\n本节将以 LLaMA 模型为例，介绍大语言模型架构在 Transformer 原始结构上的改进，并介绍\nTransformer 结构中空间和时间占比最大的注意力机制的优化方法。\n2.3.1 LLaMA 的模型结构\n文献 [34] 介绍了 LLaMA 采用的 Transformer 结构和细节， 与2.1 节介绍的 Transformer 结构的\n不同之处为采用了前置层归一化 （Pre-normalization） 方法并使用RMSNorm 归一化函数 （Root Mean\nSquare Normalizing Function） ，激活函数更换为SwiGLU，使用了旋转位置嵌入（Rotary Positional\nEmbeddings，RoPE） ，使用的Transformer 结构与 GPT-2 类似，如图2.4 所示。\n层归一化\n全连接层\n多头自注意力\n+\n文本和位置嵌入表示\n层归一化\nTr ansfor mer\n层归一化\n…\n文本预测..."
      ],
      "response_time": 3.8556532859802246,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.0560569929753793,
        "semantic_similarity": 0.7149588465690613
      }
    },
    {
      "sample_id": "0016",
      "question": "在启动整个模型训练之前，需要对预训练数据进行什么处理？",
      "ground_truth": "将预训练数据根据训练好的词元分析器进行处理。",
      "predicted_answer": "在启动整个模型训练之前，需要将预训练数据根据训练好的词元分析器进行处理。如果文档长度超过 512 个词元，就直接截断。[1]",
      "ground_context": [
        "在启动整个模型训练之前，还需要将预训练数据根据训练好的词元分析器进行处理。"
      ],
      "retrieved_contexts": [
        "大规模语言模型：从理论到实践 35\n3. 预处理数据集\n在启动整个模型训练之前，还需要将预训练数据根据训练好的词元分析器进行处理。如果文\n档长度超过 512 个词元，就直接截断。数据处理代码如下所示：",
        "182 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n此外，还可以只对模型进行指令微调。例如，通过路径 training/ step1_supervised_\nfinetuning/training_scripts/llama2/run_llama2_7b.sh启动对应的脚本可以微调LLaMA-2 7B\n模型，脚本通过运行 training/step1_supervised_finetuning/main.py 启动训练。\n5.4.2 数据预处理\n训练一个属于自己的大语言模型，数据是非常重要的。通常，使用相关任务的数据进行优化\n的模型会在目标任务上表现得更好。在 DeepSpeed-Chat 中使用新的数据，需要进行如下操作。\n（1）准备数据，并把数据处理成程序能够读取的格式，如 JSON、arrow。\n（2）在数据处理代码文件 training/ utils/ data/ raw_datasets.py 和 training/ utils/\ndata/data_utils.py 中增加对新增数据的处理。\n（3）在训练脚本中增加对新增数据的支持，并开始模型训练。\n在指令微调...",
        "--max_seq_len 512\n--learning_rate 9.65e-6\n--weight_decay 0.\n--num_train_epochs 2\n--gradient_accumulation_steps 1\n--lr_scheduler_type cosine\n--num_warmup_steps 0\n--seed 1234\n--gradient_checkpointing\n--zero_stage $ZERO_STAGE\n--deepspeed\n--output_dir $OUTPUT\n&> $OUTPUT/training.log\n5.4.4 模型训练\n数据预处理和自定义模型的修改都完成后，就可以正式进行训练了。进入步骤一指令微调的\n路径 training/step1_supervised_finetuning 下，把上述构造的训练脚本放置到 training/",
        "大规模语言模型：从理论到实践 135\npip install deepspeed\n1. 训练数据配置\n使用 PyTorch 和 transformers 库来设置预训练模型的数据加载器， 以实现在单机或多机分布式\n训练环境中对数据的加载和采样。需要导入的模块如下。\n• DataLoader 是 PyTorch 提供的工具，用于从数据集加载数据到模型进行训练或评估。\n• RandomSampler 和 SequentialSampler 是 PyTorch 提供的两种采样器。RandomSampler 随机采\n样数据，而 SequentialSampler 顺序采样数据。\n• DistributedSampler 是用于分布式训练的数据采样器。\n• default_data_collator 是 transformers 库提供的默认数据收集器，用于将多个样本整合为一个\n批量数据。\n• create_pretrain_dataset 是一个自定义函数，用于创建预训练数据集。\n通过检查 args.local_rank 是否为 −1，代码会选择使用普通的采样器（单机）还是分布式采样\n器 （多...",
        "144 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n6. 模型训练\n借助 DeepSpeed 框架实现对模型的训练，训练步骤大致分为以下几个阶段。\n（1） 训练前的准备： 使用print_rank_0 函数输出当前的训练状态。 该函数确保只有指定的进程\n（通常是主进程）会打印消息，避免了多进程环境下的重复输出。在开始训练之前，对模型进行一\n次评估，计算模型的困惑度。\n（2） 训练循环： 每个周期的开始， 都会打印当前周期和总周期数。 在每次迭代中， 数据批次先被\n移动到相应的 GPU 设备， 接着模型对这个批次进行前向传播计算损失。 使用model.backward(loss)\n计算梯度，并使用 model.step() 更新模型参数。对于主进程，还会使用 print_throughput 函数打印\n吞吐量，这有助于了解模型的训练速度和效率。\n（3）保存模型：如果指定了输出目录，则模型的状态和配置将被保存。模型可以在不同的格\n式中保存，例如 HuggingFace 的模型格式或 DeepSpeed 的 Zero Stage 3 特定格式。save_hf_format\n函数..."
      ],
      "response_time": 5.598755598068237,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.3319238584964926,
        "semantic_similarity": 0.7421612739562988
      }
    },
    {
      "sample_id": "0017",
      "question": "DecoderLayer 的构造函数中定义了多少个 Norm 实例？",
      "ground_truth": "3",
      "predicted_answer": "根据提供的文档，我无法回答这个问题。",
      "ground_context": [
        "self.norm_1 = Norm(d_model)\nself.norm_2 = Norm(d_model)\nself.norm_3 = Norm(d_model)"
      ],
      "retrieved_contexts": [],
      "response_time": 2.1216087341308594,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.0,
        "semantic_similarity": 0.43968504667282104
      }
    },
    {
      "sample_id": "0018",
      "question": "What does the Encoder class use to embed the source input?",
      "ground_truth": "self.embed = Embedder(vocab_size, d_model)",
      "predicted_answer": "根据提供的文档，我无法回答这个问题。",
      "ground_context": [
        "self.embed = Embedder(vocab_size, d_model)"
      ],
      "retrieved_contexts": [],
      "response_time": 2.2019829750061035,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.0,
        "semantic_similarity": 0.4201216697692871
      }
    },
    {
      "sample_id": "0019",
      "question": "如何在使用BertTokenizerFast时保存模型配置，并利用填充掩码功能进行预测？",
      "ground_truth": "可以通过创建一个模型路径并检查其是否存在，若不存在则创建该目录，随后保存tokenizer的模型配置。同时，可以使用pipeline来进行填充掩码的预测。",
      "predicted_answer": "要使用 `BertTokenizerFast` 保存模型配置并利用填充掩码功能进行预测，首先需要加载预训练模型路径下的分词器，然后创建一个填充掩码管道。具体步骤如下：\n\n```python\ntokenizer = BertTokenizerFast.from_pretrained(model_path)  # 加载分词器\nfill_mask = pipeline(\"fill-mask\", model=model, tokenizer=tokenizer)  # 创建填充掩码管道\n```\n\n接下来，定义一些示例文本，并对每个示例执行填充掩码预测：\n\n```python\nexamples = [\n    \"Today's most trending hashtags on [MASK] is Donald Trump\",\n    \"The [MASK] was cloudy yesterday, but today it's rainy.\",\n]\n\nfor example in examples:\n    for prediction in fill_mask(example):\n        print(f\"{prediction['sequence']}, confidence: {prediction['score']}\")  # 打印预测结果及其置信度\n    print(\"=\"*50)  # 分割线\n```\n\n这样就可以完成模型配置的保存以及填充掩码功能的预测操作。[1]",
      "ground_context": [
        "model_path = 'pretrained-bert'",
        "tokenizer = BertTokenizerFast.from_pretrained(model_path)",
        "fill_mask = pipeline('fill-mask', model=model, tokenizer=tokenizer)",
        "tokenizer.save_model(model_path)",
        "with open(os.path.join(model_path, 'config.json'), 'w') as f: json.dump(tokenizer_cfg, f)"
      ],
      "retrieved_contexts": [
        "#༅ఖ\ntokenizer = BertTokenizerFast.from_pretrained(model_path)\nfill_mask = pipeline(\"fill-mask\", model =model, tokenizer =tokenizer)\n#ྛყҩ\nexamples = [\n\"Today's most trending hashtags on [MASK] is Donald Trump\" ,\n\"The [MASK] was cloudy yesterday, but today it's rainy.\" ,\n]\nfor example in examples:\nfor prediction in fill_mask(example):\nprint(f\"{prediction['sequence']}, confidence: {prediction['score']}\")\nprint(\"=\"*50)\n通过上述代码可以得到如下输出："
      ],
      "response_time": 7.976315021514893,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.06557377049180328,
        "rouge2": 0.0,
        "rougeL": 0.06557377049180328,
        "bleu": 0.009354757489240374,
        "semantic_similarity": 0.755542516708374
      }
    },
    {
      "sample_id": "0020",
      "question": "在 GPT-2 模型中，层归一化的实施与大规模语言模型 LLaMA 的超参数设置有什么关系？",
      "ground_truth": "GPT-2 通过引入前置层归一化方法来提高模型训练的稳定性，而 LLaMA 模型则使用不同规模的超参数配置，包括层数和自注意力头数，这影响了模型的复杂性和计算需求。",
      "predicted_answer": "根据提供的文档，我无法回答这个问题。文档中提到了LLaMA模型的具体层归一化方法以及其超参数设置，但未提及这些与GPT-2模型的关系。[1][3]",
      "ground_context": [
        "为了使模型训练过程更加稳定，GPT-2 相较于 GPT 引入了前置层归一化方法，将第一个层归一化移动到多头自注意力层之前。",
        "表 2.1 不同规模的 LLaMA 模型使用的超参数包括层数和自注意力头数，这影响了模型的复杂性和计算需求。"
      ],
      "retrieved_contexts": [
        "大规模语言模型：从理论到实践 41\n后，OpenAI 就不再开源（也没有开源模型） ，因此并不清楚ChatGPT 和 GPT-4 采用的模型架构。\n本节将以 LLaMA 模型为例，介绍大语言模型架构在 Transformer 原始结构上的改进，并介绍\nTransformer 结构中空间和时间占比最大的注意力机制的优化方法。\n2.3.1 LLaMA 的模型结构\n文献 [34] 介绍了 LLaMA 采用的 Transformer 结构和细节， 与2.1 节介绍的 Transformer 结构的\n不同之处为采用了前置层归一化 （Pre-normalization） 方法并使用RMSNorm 归一化函数 （Root Mean\nSquare Normalizing Function） ，激活函数更换为SwiGLU，使用了旋转位置嵌入（Rotary Positional\nEmbeddings，RoPE） ，使用的Transformer 结构与 GPT-2 类似，如图2.4 所示。\n层归一化\n全连接层\n多头自注意力\n+\n文本和位置嵌入表示\n层归一化\nTr ansfor mer\n层归一化\n…\n文本预测...",
        "46 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n4. 模型整体框架\n基于上述模型和网络结构可以实现解码器层，根据自回归方式利用训练数据进行模型训练的\n过程与 2.2.3 节介绍的过程基本一致。不同规模的 LLaMA 模型使用的超参数如表2.1 所示。由于\n大语言模型的参数量非常大， 并且需要大量的数据进行训练， 因此仅利用单个GPU 很难完成训练，\n需要依赖分布式模型训练框架（第 4 章将详细介绍相关内容） 。",
        "大规模语言模型：从理论到实践 47\n表 2.1 不同规模的 LLaMA 模型使用的超参数 [34]\n参数规模 层数 自注意力头数 嵌入表示维度 学习率 全局批次大小 训练词元数量（个）\n6.7B¬ 32 32 4096 3.0e-4 400 万 1.0 万亿\n13.0B 40 40 5120 3.0e-4 400 万 1.0 万亿\n32.5B 60 52 6656 1.5e-4 400 万 1.4 万亿\n65.2B 80 64 8192 1.5e-4 400 万 1.4 万亿\nHuggingFace transformers 库中 LLaMA 解码器的整体代码实现如下所示：\n¬ B，即 Billion，表示十亿个。"
      ],
      "response_time": 3.678757429122925,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.7499999999999999,
        "rouge2": 0.3333333333333333,
        "rougeL": 0.5,
        "bleu": 0.06707594122770388,
        "semantic_similarity": 0.8393595814704895
      }
    },
    {
      "sample_id": "0021",
      "question": "在训练大规模语言模型时，如何使用预训练的tokenizer和模型进行填充掩码操作？",
      "ground_truth": "在训练大规模语言模型时，首先需要定义模型参数并初始化模型，然后可以使用预训练的tokenizer（如BertTokenizerFast）和填充掩码pipeline来对输入进行处理。通过pipeline，可以使用模型和tokenizer对包含掩码的句子进行预测。",
      "predicted_answer": "根据提供的文档，我无法回答这个问题。文档中没有具体提及关于使用预训练的tokenizer和模型进行填充掩码操作的内容。",
      "ground_context": [
        "model = Transformer(src_vocab, trg_vocab, d_model, N, heads)",
        "tokenizer = BertTokenizerFast.from_pretrained(model_path)"
      ],
      "retrieved_contexts": [
        "发展，使得大规模文本非常容易获取，因此训练超大规模的基于神经网络的语言模型成为可能。\n受计算机视觉领域采用 ImageNet[9] 对模型进行一次预训练， 使模型可以通过海量图像充分学\n习如何提取特征，再根据任务目标进行模型精调的预训练范式影响，自然语言处理领域基于预训\n练语言模型的方法逐渐成为主流。 以ELMo[10] 为代表的动态词向量模型开启了语言模型预训练的\n大门。此后， 以GPT[11] 和 BERT[1] 为代表的基于 Transformer 结构[12] 的大规模预训练语言模型的\n出现，使自然语言处理全面进入预训练微调范式新时代。将预训练模型应用于下游任务时，不需\n要了解太多的任务细节，不需要设计特定的神经网络结构，只需要“微调”预训练模型，使用具\n体任务的标注数据在预训练语言模型上进行监督训练，就可以取得显著的性能提升。这类方法通\n常被称为预训练语言模型（Pre-trained Language Models，PLM）。\n2020 年，OpenAI 发布了由包含 1750 亿个参数的神经网络构成的生成式大规模预训练语言模\n型 GPT-3（Generative Pre-t...",
        "大规模语言模型：从理论到实践 185\n# main.py\n#ࡱ\nmodeling_baichuan = import_module(\"models.Baichuan-7B.modeling_baichuan\")\ntokenization_baichuan = import_module(\"models.Baichuan-7B.tokenization_baichuan\")\n#౼Baichuanো\nBaiChuanForCausalLM = getattr(modeling_baichuan, \"BaiChuanForCausalLM\")\nBaiChuanTokenizer = getattr(tokenization_baichuan, \"BaiChuanTokenizer\")\n对模型代码文件路径做相应的修改，改为本地存储模型代码的路径。然后，同样在 main.py 中对\n对应的模型加载进行修改：\n# main.py\n# ჰ൓ս઒\ntokenizer = load_hf_tokenizer(args.model_name_or_path, fast_tokenizer =True)\nm..."
      ],
      "response_time": 4.126953840255737,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.33333333333333337,
        "rouge2": 0.0,
        "rougeL": 0.33333333333333337,
        "bleu": 0.07384871754683459,
        "semantic_similarity": 0.7283931970596313
      }
    },
    {
      "sample_id": "0022",
      "question": "代码的主要来源是什么？",
      "ground_truth": "编程问答社区（如 Stack Exchange）和公共软件仓库（如 GitHub）。",
      "predicted_answer": "代码的主要来源是编程问答社区（如 Stack Exchange[103, 104]）和公共软件仓库（如 GitHub[27, 100, 105]）。[1]",
      "ground_context": [
        "代码的主要来源是编程问答社区（如 Stack Exchange）和公共软件仓库（如 GitHub）。"
      ],
      "retrieved_contexts": [
        "66 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n式化语言， 它对应着长程依赖和准确的执行逻辑[102]。 代码的语法结构、 关键字和特定的编程范式\n都对其含义和功能起着重要的作用。代码的主要来源是编程问答社区（如 Stack Exchange[103, 104]）\n和公共软件仓库（如 GitHub[27, 100, 105]） 。编程问答社区中的数据包含了开发者提出的问题、其他\n开发者的回答及相关代码示例。这些数据提供了丰富的语境和真实世界中的代码使用场景。公共\n软件仓库中的数据包含了大量的开源代码，涵盖多种编程语言和不同领域。这些代码库中的很多\n代码经过了严格的代码评审和实际的使用测试，因此具有一定的可靠性。\n3.1.2 领域数据\n特定领域预训练语料库是为特定领域或主题量身定制的。这类语料库通常用于大语言模型的\n增量预训练阶段。在用通用预训练语料库训练出一个基础模型之后，如果该模型需要应用于某一\n特定领域的下游任务，就可以进一步利用特定领域预训练语料库对模型进行增量预训练。这一过\n程在基于初始通用预训练所获得的通用能力基础上，增强了模型在特定领域的能力。虽然领域数\n据..."
      ],
      "response_time": 4.01189661026001,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.5,
        "rouge2": 0.2,
        "rougeL": 0.5,
        "bleu": 0.34917266802171815,
        "semantic_similarity": 0.8681443929672241
      }
    },
    {
      "sample_id": "0023",
      "question": "后缀数组 A 的元素是如何排列的？",
      "ground_truth": "A 中的元素按照后缀的字典顺序排列。",
      "predicted_answer": "后缀数组 A 的元素是按照后缀的字典顺序排列的。例如，在序列 \"banana\" 的情况下，其后缀包括 \"banana\" \"anana\" \"nana\" \"ana\" \"na\" \"a\"，对应的后缀数组 A 为 [6, 4, 2, 1, 5, 3]。[1]",
      "ground_context": [
        "A 中的元素按照后缀的字典顺序排列。"
      ],
      "retrieved_contexts": [
        "缀数组 A 是一个整数数组， 其中每个元素表示S 中的一个后缀的起始位置。A 中的元素按照后缀\n的字典顺序排列。 例如， 序列 “banana” 的后缀包括 “banana” “anana” “nana” “ana” “na” “a” ， 对应的后\n缀数组 A 为 [6, 4, 2, 1, 5, 3]。 根据数组A， 可以很容易地找出相同的子串。 如果Si···i+|s| = Sj···j+|s|，\n那么 i 和 j 在数组 A 中一定在紧邻的位置上。文献 [124] 中设计了并行的后缀数组构造方法，针\n对 Wiki-40B 训练数据（约包含 4GB 文本内容） ，使用拥有96 核 CPU 以及 768GB 内存的服务器，\n可以在 140 秒内完成计算。 对于包含350GB 文本的 C4 数据集， 仅需要12 小时就可以完成后缀数\n组构造。\n在文档级别上，大部分大语言模型依靠文档之间的表面特征相似度（例如 n-gram 重叠比例）\n进行检测并删除重复文档[31, 34, 60, 124]。LLaMA[34] 采用 CCNet[125] 的处理模式，先将文档拆分为段\n落，并把所有字母转换...",
        "cional Autónoma de México (UNAM/Universidad Nacional Autónoma de México/Universidad Nacional\nAutónoma de México/Universidad Nacional Autónoma de México/Universidad Nacional Autónoma de\n· · · ” 。由于重复循环对语言模型生成的文本质量有非常大的影响，因此在预训练数据中需要删除\n这些包含大量重复单词或者短语的句子。\n在 RefinedWeb[60] 的构造过程中使用了文献 [124] 提出的过滤方法，进行了句子级别的过滤。\n该方法提取并过滤文档间超过一定长度的相同字符串。 给定两个文档xi 和 xj， 其中存在长度为k\n的公共子串 xa···a+k\ni = xb···b+k\nj 。 当k ⩾ 50 时， 就将其中一个子串过滤。 公共子串匹配的关键是如何\n高效地完成字符串匹配， 文献[60] 将整个文档 D 转换为一个超长的字符串序列 S， 之后构造序列S\n的后缀数组 （Suffix Array）A。 ..."
      ],
      "response_time": 4.264830112457275,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.11764705882352941,
        "rouge2": 0.0,
        "rougeL": 0.11764705882352941,
        "bleu": 0.06006158040232693,
        "semantic_similarity": 0.700559675693512
      }
    },
    {
      "sample_id": "0024",
      "question": "稀疏混合专家模型中采用什么策略时，分配给不同专家的词元可能需要一些共有知识？",
      "ground_truth": "常规的门控策略",
      "predicted_answer": "稀疏混合专家模型中采用常规的门控策略时，分配给不同专家的词元可能需要一些共有知识或信息才能处理[2][3]。",
      "ground_context": [
        "稀疏混合专家模型中采用常规的门控策略时，分配给不同专家的词元可能需要一些共有知识"
      ],
      "retrieved_contexts": [
        "x1\n<latexit sha1_base64=\"IfvsN6LQ0T10Hxv1EOheRmClmyE=\">AAAB+nicbVC7TgJBFL2LL8QXamkzkZhYkV1iwJJoY4lRHglsyOwwCxNmZjczs0ay8gm22tsZW3/G1i9xgC0UPMlNTs65N+fmBDFn2rjul5NbW9/Y3MpvF3Z29/YPiodHLR0litAmiXikOgHWlDNJm4YZTjuxolgEnLaD8fXMbz9QpVkk780kpr7AQ8lCRrCx0t1jv9IvltyyOwdaJV5GSpCh0S9+9wYRSQSVhnCsdddzY+OnWBlGOJ0WeommMSZjPKRdSyUWVPvp/NUpOrPKAIWRsiMNmqu/L1IstJ6IwG4KbEZ62ZuJ/3qBWEo24aWfMhknhkqyCA4TjkyEZj2gAVOUGD6xBBPF7O+IjLDCxNi2CrYUb7mCVdKqlL1quXp7UapfZfXk4QRO4Rw8qEEdbqABTSAwhGd4gVfnyXlz3p2PxWrOyW6O4Q+czx9SH...",
        "x2\n图 2.11 共享专家模型 [68]\n稀疏混合专家模型中采用常规的门控策略时，分配给不同专家的词元可能需要一些共有知识\n或信息才能处理。 因此， 多个专家可能会在各自的参数中获取同样的知识， 进而导致专家参数出现\n冗余。如果构建专门用于捕捉并整合不同情境下共有知识的共享专家，那么其他专家之间的参数\n冗余情况将可能得到缓解。这种冗余情况的缓解，有助于构建一个参数利用更高效且专家专业性\n更强的模型。因此，DeepSeekMoE[74] 提出了分离 Ks 个专家作为共享专家的思路。无论门控网络\n所给出的结果如何， 每个词元都将被确定性地分配给这些共享专家， 如图2.11 所示， 深色块Shared\nFFN 为共享专家，所有输入都会分配给共享专家。为保持计算成本恒定，其他经门控网络分配的\n专家中被激活专家的数量将减少 Ks 个。\n稀疏混合专家模型中的 MoE 层对于并行计算也十分友好，能更便捷地在单个 GPU 上实现高\n效计算。常规稠密模型中，全部参数都会参与对所有输入数据的处理流程。与之不同，稀疏混合\n专家模型具备的稀疏特性，使得计算仅在系统的特定局部展开。也就是说，并非所有参数在处理...",
        "p=0.16\n图 2.10 混合专家模型三种主要类型 [68]\n2.4.1 稀疏混合专家模型\n稀疏混合专家模型，如图2.10(a) 所示，对于每个输入词元，在前向计算中仅激活专家集合中\n的一个子集。 门控网络对专家子集进行选择， 通过计算排名前K 位专家的输出加权和来实现稀疏\n性。这个过程可以形式化的表示为：\nF M oE\nSparse (x; Θ; {Wi}N\ni=1) =\nNX\ni=1\nG(x; Θ)ifi(x; Wi) (2.38)\nG(x; Θ)i = softmax(TopK(g(x; Θ) + Rnoise, K))i (2.39)\nTop-K(g(x; Θ), K)i =\n(\ng(x; Θ)i, g(x; Θ)i的值属于前 K 项\n−∞, 其他\n(2.40)\n其中，g(x; Θ) 表示在进行 softmax 操作之前的门控值，G(x; Θ)i 表示门控网络针对第 i 个专家的\n输出，TopK(·, K) 函数的目标是保持向量的前 K 项不变， 其它维度设置为−∞。 鉴于softmax 函数\n自身所具有的独特性质，当把其中某些项设置为 −∞ 时，这些项所对应的值会近似等同于...",
        "部分专家在专注于做下游任务的同时，另一部分专家专注于将指令与主模型的世界知识对齐，以\n缓解世界知识遗忘。同时局部平衡约束还能防止单个专家组内的专家退化现象，使路由平衡地关\n注于单个专家组的所有专家，防止个别专家长期占据优势，而其他专家未被充分训练或使用。这\n有助于专家之间相互配合以提高下游任务能力。微调后的 LoRAMoE 中的路由能够根据数据类型\n灵活地关注相应的专家，并使专家们相互配合，在保证下游任务表现的同时，也几乎不丧失世界\n知识。\n2.4.3 软混合专家模型\n软混合专家模型， 如图2.10(c) 所示， 门控网络依然根据输入为各个专家分配不同的权重， 但与稠\n密混合专家模型在前向计算中激活所有专家网络不同， 软混合专家模型引入了融合前馈层 （Merged\nFFN） 。 该方法通过门控网络分配的权重对不同专家的参数进行融合， 仅对融合后的前馈层参数进\n行计算。这种设计既能在几乎不增加计算成本的情况下完成计算，又保留了稠密混合专家模型中"
      ],
      "response_time": 3.922050952911377,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 0.5
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.06814385179012217,
        "semantic_similarity": 0.668210506439209
      }
    },
    {
      "sample_id": "0025",
      "question": "FlashAttention 算法是如何减少全局内存消耗的？",
      "ground_truth": "FlashAttention 就提出了不使用中间注意力矩阵，通过存储归一化因子来减少全局内存消耗的方法。",
      "predicted_answer": "FlashAttention 算法通过以下方式减少全局内存消耗：它不使用中间注意力矩阵 S 和 P，而是通过分块写入的方式存储前向传播的 Softmax 归一化因子，并在后向传播中快速重新计算片上注意力[1][2]。这种方式比从全局内存中读取中间注意力矩阵的标准方法更快，从而减少了全局内存的访问量。尽管重新计算会导致 FLOPS 增加，但总体来看，运行速度更快且使用的显存更少[1]。",
      "ground_context": [
        "FlashAttention 就提出了不使用中间注意力矩阵，通过存储归一化因子来减少全局内存消耗的方法。"
      ],
      "retrieved_contexts": [
        "大规模语言模型：从理论到实践 53\n自注意力算法的标准实现将计算过程中的矩阵 S、P 写入全局内存，而这些中间矩阵的大小\n与输入的序列长度有关且为二次型。因此，FlashAttention 就提出了不使用中间注意力矩阵，通过\n存储归一化因子来减少全局内存消耗的方法。FlashAttention 算法并没有将 S、P 整体写入全局内\n存，而是通过分块写入，存储前向传播的 Softmax 归一化因子，在后向传播中快速重新计算片上\n注意力，这比从全局内存中读取中间注意力矩阵的标准方法更快。虽然大幅减少了全局内存的访\n问量， 重新计算也导致FLOPS 增加， 但总体来看运行的速度更快且使用的显存更少。 具体算法如\n代码2.1 所示，其中内层循环和外层循环所对应的计算可以参考图2.9。\n代码 2.1: FlashAttention 算法\n输入: Q, K, V ∈ RN ×d 位于 HBM 中，GPU 芯片中的 SRAM 大小为 M\n输出: O\nBc = ⌈ M\n4d ⌉，Br = min(⌈ M\n4d ⌉, d) // ഡᇂॶնཬčblock size Ď\n在 HBM 中初始化 O = (0)...",
        "来加快计算速度，避免从全局内存中读取和写入注意力矩阵。达成该目标需要做到在不访问整个\n输入的情况下计算 Softmax 函数，并且后向传播中不能存储中间注意力矩阵。在标准 Attention 算\n法中，Softmax 计算按行进行，即在与 V 做矩阵乘法之前，需要完成 Q、K 每个分块中的一整行\n的计算。 在得到Softmax 的结果后， 再与矩阵V 分块做矩阵乘。 而在FlashAttention 中， 将输入分\n割成块，并在输入块上进行多次传递，以增量的方式执行 Softmax 计算。",
        "52 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n图 2.8 NVIDIA GPU 的整体内存结构示意图\n前文介绍了自注意力机制的原理， 在GPU 中进行计算时， 传统的方法还需要引入两个中间矩\n阵 S 和 P 并存储到全局内存中。具体计算过程如下：\nS = QK, P = Softmax(S), O = P V (2.32)\n按照上述计算过程，需要先从全局内存中读取矩阵 Q 和 K，并将计算好的矩阵 S 写入全局内存，\n然后从全局内存中获取矩阵 S，计算 Softmax 得到矩阵 P ，再将其写入全局内存，最后读取矩阵\nP 和矩阵 V ，计算得到矩阵 O。这样的过程会极大地占用显存的带宽。在自注意力机制中，GPU\n的计算速度比内存速度快得多，因此计算效率越来越受全局内存访问的制约。\nFlashAttention[57] 利用 GPU 硬件中的特殊设计， 针对全局内存和共享存储的I/O 速度的不同， 尽\n可能地避免从 HBM 中读取或写入注意力矩阵。FlashAttention 的目标是尽可能高效地使用 SRAM\n来加快计算速度，避免从全局内存中读取和写入注意力矩阵。达成...",
        "54 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n图 2.9 FlashAttention 计算流程图 [57]\nPyTorch 2.0 已经支持 FlashAttention，使用 torch.backends.cuda.enable_flash_sdp() 函数可以启\n用或者关闭 FlashAttention。\n3. 多查询注意力\n多查询注意力（Multi Query Attention）[58] 是多头注意力的一种变体。它的特点是，在多查询\n注意力中不同的注意力头共享一个键和值的集合，每个头只单独保留了一份查询参数，因此键和\n值的矩阵仅有一份，这大幅减少了显存占用，使其更高效。由于多查询注意力改变了注意力机制\n的结构，因此模型通常需要从训练开始就支持多查询注意力。文献 [59] 的研究结果表明，可以通\n过对已经训练好的模型进行微调来添加多查询注意力支持，仅需要约 5% 的原始训练数据量就可\n以达到不错的效果。 包括Falcon[60]、SantaCoder[61]、StarCoder[62] 在内的很多模型都采用了多查询\n注意力。\n以 LLM Foundry 为例，..."
      ],
      "response_time": 5.135913133621216,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.2222222222222222,
        "rouge2": 0.0,
        "rougeL": 0.2222222222222222,
        "bleu": 0.06876137090877835,
        "semantic_similarity": 0.8738150596618652
      }
    },
    {
      "sample_id": "0026",
      "question": "如何删除隐私数据？",
      "ground_truth": "删除隐私数据最直接的方法是采用基于规则的算法，BigScience ROOTS Corpus 在构建过程中就采用了基于命名实体识别的方法，利用命名实体识别算法检测姓名、地址、电话号码等个人信息内容并进行删除或者替换。",
      "predicted_answer": "一种直接的方法是采用基于规则的算法，比如BigScience ROOTS Corpus在构建过程中使用了基于命名实体识别的方法来检测并删除或替换姓名、地址、电话号码等个人信息内容。这种方法利用了基于Transformer的模型和机器翻译技术，能够处理多种语言的文本以消除其中的隐私信息。[128][127]",
      "ground_context": [
        "删除隐私数据最直接的方法是采用基于规则的算法，BigScience ROOTS Corpus 在构建过程中就采用了基于命名实体识别的方法，利用命名实体识别算法检测姓名、地址、电话号码等个人信息内容并进行删除或者替换。"
      ],
      "retrieved_contexts": [
        "70 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n3.2.3 隐私消除\n由于绝大多数预训练数据源于互联网， 因此不可避免地会包含涉及敏感或个人信息 （Personally\nIdentifiable Information，PII）的用户生成内容，这可能会增加隐私泄露的风险[127]。如图3.3 所示，\n输入前缀词“East Stroudsburg Stroudsburg” ，语言模型在此基础上补全了姓名、电子邮件地址、电\n话号码、传真号码及实际地址。这些信息都是模型从预训练数据中学习得到的。因此，非常有必\n要从预训练语料库中删除包含个人身份信息的内容。\n图 3.3 从大语言模型中获得隐私数据的例子 [127]\n删除隐私数据最直接的方法是采用基于规则的算法，BigScience ROOTS Corpus [128] 在构建过\n程中就采用了基于命名实体识别的方法，利用命名实体识别算法检测姓名、地址、电话号码等个\n人信息内容并进行删除或者替换。该方法使用了基于 Transformer 的模型，并结合机器翻译技术，\n可以处理超过 100 种语言的文本，消除其中的隐私信息。该方法被..."
      ],
      "response_time": 4.688979864120483,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.6666666666666666,
        "rouge2": 0.5714285714285715,
        "rougeL": 0.6666666666666666,
        "bleu": 0.38413185281396645,
        "semantic_similarity": 0.9014853239059448
      }
    },
    {
      "sample_id": "0027",
      "question": "什么模型被称为软混合专家模型？",
      "ground_truth": "软混合专家模型（SoftMoE）",
      "predicted_answer": "软混合专家模型（SoftMoE）被称为软混合专家模型[1][3][4]。",
      "ground_context": [
        "（c）软混合专家模型（SoftMoE）"
      ],
      "retrieved_contexts": [
        "Y\n（c）软混合专家模型（SoftMoE）\n<latexit sha1_base64=\"17zqk6qKa6kdejpo5MShoZgkkWA=\">AAAB/XicbVC7SgNBFL0bXzG+opY2g0GwWnYlRBshaGMZwTwgWcLsZDYZMzO7zMwKYQn+gq32dmLrt9j6JU6SLTR64MLhnHs5lxMmnGnjeZ9OYWV1bX2juFna2t7Z3SvvH7R0nCpCmyTmseqEWFPOJG0aZjjtJIpiEXLaDsfXM7/9QJVmsbwzk4QGAg8lixjBxkqt5NJzq36/XPFcbw70l/g5qUCORr/81RvEJBVUGsKx1l3fS0yQYWUY4XRa6qWaJpiM8ZB2LZVYUB1k82+n6MQqAxTFyo40aK7+vMiw0HoiQrspsBnpZW8m/uuFYinZRBdBxmSSGirJIjhKOTIxmlWBBkxRYvjEEkwUs78jMsIKE2MLK9lS/OUK/pLWmevX3NpttVK/yuspwhEcwyn4cA51uIEGNIHAPTzBM7w4j8...",
        "x\n<latexit sha1_base64=\"IVqKn81Phb8FhcRP6SaEHpTS2es=\">AAAB+HicbVDLTgJBEOzFF+IL9ehlIzHxRHaNQY9ELx4hkYeBDZkdemHCzOxmZtYECV/gVe/ejFf/xqtf4gB7ULCSTipV3alOhQln2njel5NbW9/Y3MpvF3Z29/YPiodHTR2nimKDxjxW7ZBo5ExiwzDDsZ0oJCLk2ApHtzO/9YhKs1jem3GCgSADySJGibFS/aFXLHllbw53lfgZKUGGWq/43e3HNBUoDeVE647vJSaYEGUY5TgtdFONCaEjMsCOpZII1MFk/ujUPbNK341iZUcad67+vpgQofVYhHZTEDPUy95M/NcLxVKyia6DCZNJalDSRXCUctfE7qwFt88UUsPHlhCqmP3dpUOiCDW2q4ItxV+uYJU0L8p+pVypX5aqN1k9eTiBUzgHH66gCndQgwZQQHiGF3h1npw35935WKzmnOzmGP7A+fwB8QeTjg...",
        "62 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n可使用基于梯度的训练方法的优势。这个过程可以形式化的表示为：\nF M oE\nSof t (x; Θ; {Wi}N\ni=1) = fmerged(x;\nNX\ni=1\nG(x; Θ)iWi) (2.43)\nG(x; Θ)i = softmax(g(x; Θ))i = exp(g(x; Θ)i)PN\nj exp(g(x; Θ)j)\n(2.44)\n其中，fmerged 表示融合前向层，其结构与其余专家网络 fi 的结构相同。SMEAR 算法[85] 就采用\n了这种软混合专家结构。\n软混合专家模型始终只计算单个专家的输出， 其计算成本可能与单专家稀疏混合模型相当， 明\n显低于稠密混合专家模型。 但是， 软混合专家模型的平均操作仍然会产生不可忽视的计算成本。 为\n了量化这一成本，文献 [85] 分析了 SMEAR 算法的计算复杂度。假设专家网络架构是一个从 d 维\n激活值投射到 m 维向量的稠密计算， 随后经过非线性变换， 再附加一个从m 维投射回 d 维的稠密\n计算。为简便起见，这里忽略成本相对较小的非线性变换成本。假定输入是一个长...",
        "部分专家在专注于做下游任务的同时，另一部分专家专注于将指令与主模型的世界知识对齐，以\n缓解世界知识遗忘。同时局部平衡约束还能防止单个专家组内的专家退化现象，使路由平衡地关\n注于单个专家组的所有专家，防止个别专家长期占据优势，而其他专家未被充分训练或使用。这\n有助于专家之间相互配合以提高下游任务能力。微调后的 LoRAMoE 中的路由能够根据数据类型\n灵活地关注相应的专家，并使专家们相互配合，在保证下游任务表现的同时，也几乎不丧失世界\n知识。\n2.4.3 软混合专家模型\n软混合专家模型， 如图2.10(c) 所示， 门控网络依然根据输入为各个专家分配不同的权重， 但与稠\n密混合专家模型在前向计算中激活所有专家网络不同， 软混合专家模型引入了融合前馈层 （Merged\nFFN） 。 该方法通过门控网络分配的权重对不同专家的参数进行融合， 仅对融合后的前馈层参数进\n行计算。这种设计既能在几乎不增加计算成本的情况下完成计算，又保留了稠密混合专家模型中",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n..."
      ],
      "response_time": 4.084034204483032,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.4,
        "rouge2": 0.0,
        "rougeL": 0.4,
        "bleu": 0.250737833894674,
        "semantic_similarity": 0.8675466775894165
      }
    },
    {
      "sample_id": "0028",
      "question": "哪些大语言模型采用了稀疏混合专家架构？",
      "ground_truth": "Switch Transformer、DeepSeekMoE、AdaMoE、Yuan 2.0-M32、OpenMoE、Qwen1.5-MoE-A2.7B",
      "predicted_answer": "采用了稀疏混合专家架构的大语言模型包括 Mixtral-8x7B[66]、Switch Transformer[67]、DeepSeekMoE[74]、AdaMoE[75]、Yuan 2.0-M32[76]、OpenMoE[77] 和 Qwen1.5-MoE-A2.7B[78]。[2][3][68]",
      "ground_context": [
        "此外，众多大语言模型也都采用了稀疏混合专家架构，包括Switch Transformer、DeepSeekMoE、AdaMoE、Yuan 2.0-M32、OpenMoE、Qwen1.5-MoE-A2.7B等。"
      ],
      "retrieved_contexts": [
        "大规模语言模型：从理论到实践 59\n声项 Rnoise 是训练稀疏混合专家层的一种常用策略， 一方面， 它能够为模型创造更多的探索空间，\n促使不同专家模块之间展开多样化的尝试与协作，挖掘出潜在的优化路径；另一方面，通过打破\n可能出现的局部最优情况，提高了整个混合专家训练过程的稳定性[67]。\n由 Mixtral AI 公司推出的 Mixtral-8x7B 模型[66] 就采用了稀疏混合专家方式， 与早期的Mistral\n7B 模型[73] 共享基础架构。 但是，Mixtral-8x7B 模型使用了稀疏混合专家层代替每个 Transformer 块\n中的前馈层， 每个稀疏混合专家层包含8 个专家网络， 门控网络每次激活2 个专家。 但是在Mixtral-\n8x7B 模型中没有引入噪声项 Rnoise， 每个专家网络则使用了SwiGLU 结构[46]。 由于采用了稀疏混\n合专家方式，虽然 Mixtral-8x7B 模型的总参数量大约 560 亿，但是每次仅使用 130 亿个活跃参数。\n并且，Mixtral-8x7B 模型在很多基准测试中， 展现出了优于或等同于包含了700 亿参数的 Lla...",
        "70B[37] 的性能。 此外， 众多大语言模型也都采用了稀疏混合专家架构， 包括Switch Transformer[67]、\nDeepSeekMoE[74]、AdaMoE[75]、Y uan 2.0-M32[76]、OpenMoE[77]、Qwen1.5-MoE-A2.7B[78] 等。 更多\n相关模型可以参考文献 [68]。\n共享FNNFNN1FNN2FNN3\n门控网络\n共享专家模型（SharedExpert）",
        "60 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n并运行。因此，在并行计算中可以有效利用上述特性。例如， Megablocks[79] 将 MoE 层的前馈网\n络运算转换为大型稀疏矩阵乘法，极大地提高了执行速度，并且能够很好地处理不同专家分配到\n的数量不等的词元情况。 此外，MoE 层可以通过标准的模型并行技术分布到多个 GPU 上， 还可以\n借助专家并行（Expert Parallelism，EP）[80] 实现特殊的分区策略。\n2.4.2 稠密混合专家模型\n稠密混合专家模型，如图2.10(b) 所示，对于每个输入词元，在前向计算中激活所有专家网络\n{f1, ..., fN }。门控网络根据输入赋予专家不同的权重。这个过程可以形式化的表示为：\nF M oE\nDense(x; Θ; {Wi}N\ni=1) =\nNX\ni=1\nG(x; Θ)ifi(x; Wi) (2.41)\nG(x; Θ)i = softmax(g(x; Θ))i = exp(g(x; Θ)i)PN\nj exp(g(x; Θ)j)\n(2.42)\n由于稠密混合专家模型在前向计算过程中会激活所有参数， 不能降低模...",
        "型采用稠密混合专家结构的并不多， 主要包括EvoMoE[81]、MoLE[82]、LoRAMoE[83] 以及 DS-MoE[84]\n等。\n虽然稠密混合专家模型需要使用全部参数进行计算，并不能减少模型计算时间，但是研究人\n员却发现， 如果能够将LoRA 方法和 MoE 相结合， 可以在占用很少GPU 显存的同时， 减少微调数\n据的大规模扩增与模型世界知识维持之间存在的冲突。有监督微调是大语言模型应用的一个关键\n步骤，当模型需要与更广泛的下游任务保持一致，或者希望显著提高在特定任务上的表现时，大\n规模增加微调数据通常成为解决方案。然而当指令数据的大规模扩增可能会破坏大语言模型中之\n前储存的世界知识，即世界知识遗忘。LoRAMoE[83] 采用融合混合专家和 LoRA 插件的思想，插\n件形式确保了在训练阶段冻结主模型，保证了主模型世界知识的完整性。",
        "x2\n图 2.11 共享专家模型 [68]\n稀疏混合专家模型中采用常规的门控策略时，分配给不同专家的词元可能需要一些共有知识\n或信息才能处理。 因此， 多个专家可能会在各自的参数中获取同样的知识， 进而导致专家参数出现\n冗余。如果构建专门用于捕捉并整合不同情境下共有知识的共享专家，那么其他专家之间的参数\n冗余情况将可能得到缓解。这种冗余情况的缓解，有助于构建一个参数利用更高效且专家专业性\n更强的模型。因此，DeepSeekMoE[74] 提出了分离 Ks 个专家作为共享专家的思路。无论门控网络\n所给出的结果如何， 每个词元都将被确定性地分配给这些共享专家， 如图2.11 所示， 深色块Shared\nFFN 为共享专家，所有输入都会分配给共享专家。为保持计算成本恒定，其他经门控网络分配的\n专家中被激活专家的数量将减少 Ks 个。\n稀疏混合专家模型中的 MoE 层对于并行计算也十分友好，能更便捷地在单个 GPU 上实现高\n效计算。常规稠密模型中，全部参数都会参与对所有输入数据的处理流程。与之不同，稀疏混合\n专家模型具备的稀疏特性，使得计算仅在系统的特定局部展开。也就是说，并非所有参数在处理..."
      ],
      "response_time": 5.248821496963501,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 0.5
      },
      "answer_quality_metrics": {
        "rouge1": 0.7142857142857143,
        "rouge2": 0.45,
        "rougeL": 0.7142857142857143,
        "bleu": 0.151448820090482,
        "semantic_similarity": 0.7525153160095215
      }
    },
    {
      "sample_id": "0029",
      "question": "NVIDIA H100 中全局内存的容量是多少？",
      "ground_truth": "80GB",
      "predicted_answer": "NVIDIA H100 中全局内存的容量是 80GB [1]。",
      "ground_context": [
        "NVIDIA H100 中全局内存有 80GB 空间"
      ],
      "retrieved_contexts": [
        "存容量很大。所有线程都可以访问全局内存，而本地内存只能由当前线程访问。 NVIDIA H100\n中全局内存有 80GB 空间，其访问速度虽然可以达到 3.35TB/s，但当全部线程同时访问全局内\n存时，其平均带宽仍然很低。共享存储和寄存器位于 GPU 芯片上，因此容量很小，并且只有在\n同一个 GPU 线程块 （Thread Block） 内的线程才可以并行访问共享存储， 而寄存器仅限于同一个\n线程内部访问。虽然 NVIDIA H100 中每个 GPU 线程块在流式多处理器（ Stream Multi-processor，\nSM）上可以使用的共享存储容量仅有 228KB，但是其速度比全局内存的访问速度快很多。",
        "大规模语言模型：从理论到实践 51\n[Dk, b/2] 的随机矩阵 R，LSH 函数的定义为\nh(x) = arg max([ xR; −xR]) (2.31)\n当 hqi = hkj 时，qi 才可以与相应的 Key-V alue对进行交互。\n2. FlashAttention\nNVIDIA GPU 中的不同类型的内存 （显存） 有不同的速度、 大小及访问限制。 这主要取决于\n它们物理上是在 GPU 芯片内部还是在板卡 RAM 存储芯片上。 GPU 显存分为全局内存（ Global\nMemory） 、本地内存（Local Memory） 、共享存储（Shared Memory，SRAM） 、寄存器（Register） 、\n常量内存 （Constant Memory） 、 纹理内存 （Texture Memory） 六大类。 图2.8 为 NVIDIA GPU 的整\n体内存结构示意图。全局内存、本地内存、共享存储和寄存器具有读写能力。全局内存和本地\n内存使用的高带宽显存 （High Bandwidth Memory，HBM） 位于板卡RAM 存储芯片上， 该部分内\n存容量很大。所有线程...",
        "大规模语言模型：从理论到实践 105\n通过以下命令行启动上述程序：\nCUDA_VISIBLE_DEVICES=0,1 python -m torch.distributed.launch --nproc_per_node=2 main.py\n4.2.2 模型并行\n模型并行往往用于解决单节点内存不足的问题。 以包含1750 亿个参数的 GPT-3 模型为例， 如\n果模型中每一个参数都使用 32 位浮点数表示， 那么模型需要占用700GB 内存。 如果使用16 位浮\n点数表示， 那么每个模型副本需要占用350GB 内存。2022 年 3 月 NVIDIA 发布的 H100 加速卡仅\n支持 80GB 显存，无法将整个模型完整放入其中。模型并行可以从计算图角度，用以下两种形式\n进行切分。\n（1）按模型的层切分到不同设备，即层间并行或算子间并行（Inter-operator Parallelism），也\n称之为流水线并行（Pipeline Parallelism，PP）。\n（2） 将计算图层内的参数切分到不同设备， 即层内并行或算子内并行（Intra-operator Parallelism）\n...",
        "用数千个计算设备花费几十天。分布式训练系统需要克服计算墙、显存墙、通信墙等挑战，以确\n保集群内的所有资源得到充分利用，从而加速训练过程并缩短训练周期。\n• 计算墙：单个计算设备所能提供的计算能力与大语言模型所需的总计算量之间存在巨大差\n异。2022 年 3 月发布的 NVIDIA H100 SXM 的单卡 FP16 算力只有 2000 TFLOPS（Floating\nPoint Operations Per Second ） ， 而GPT-3 需要 314 ZFLOPS 的总计算量， 两者相差了8 个数量\n级。\n• 显存墙： 单个计算设备无法完整存储一个大语言模型的参数。GPT-3 包含 1750 亿个参数， 如\n果在推理阶段采用 FP32 格式进行存储，则需要 700GB 的计算设备内存空间，而 NVIDIA\nH100 GPU 只有 80GB 显存。\n• 通信墙： 分布式训练系统中各计算设备之间需要频繁地进行参数传输和同步。 由于通信的延\n迟和带宽限制，这可能成为训练的瓶颈。在 GPT-3 的训练过程中，如果分布式系统中存在\n128 个模型副本，那么在每次迭代过程中至少需要传输 89..."
      ],
      "response_time": 2.936997890472412,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.4,
        "rouge2": 0.0,
        "rougeL": 0.4,
        "bleu": 0.018724372764461875,
        "semantic_similarity": 0.6106727719306946
      }
    },
    {
      "sample_id": "0030",
      "question": "稀疏混合专家模型的p值是多少？",
      "ground_truth": "p=0.16",
      "predicted_answer": "p=0.16[4]",
      "ground_context": [
        "p=0.16"
      ],
      "retrieved_contexts": [
        "Y\n（b）稠密混合专家模型（DenseMoE）\n<latexit sha1_base64=\"17zqk6qKa6kdejpo5MShoZgkkWA=\">AAAB/XicbVC7SgNBFL0bXzG+opY2g0GwWnYlRBshaGMZwTwgWcLsZDYZMzO7zMwKYQn+gq32dmLrt9j6JU6SLTR64MLhnHs5lxMmnGnjeZ9OYWV1bX2juFna2t7Z3SvvH7R0nCpCmyTmseqEWFPOJG0aZjjtJIpiEXLaDsfXM7/9QJVmsbwzk4QGAg8lixjBxkqt5NJzq36/XPFcbw70l/g5qUCORr/81RvEJBVUGsKx1l3fS0yQYWUY4XRa6qWaJpiM8ZB2LZVYUB1k82+n6MQqAxTFyo40aK7+vMiw0HoiQrspsBnpZW8m/uuFYinZRBdBxmSSGirJIjhKOTIxmlWBBkxRYvjEEkwUs78jMsIKE2MLK9lS/OUK/pLWmevX3NpttVK/yuspwhEcwyn4cA51uIEGNIHAPTzBM7w4...",
        "p=0.28\n<latexit sha1_base64=\"2wDbko0etWTaF+OOQTtkDmzbXxM=\">AAAB/XicbZA9SwNBEIbn4leMX1FLm8UgWIU7kWgjBG0sI5gPSI6wt9lL1uzuHbt7QjiCf8FWezux9bfY+kvcXK7QxBcGXt6ZYYYniDnTxnW/nMLK6tr6RnGztLW9s7tX3j9o6ShRhDZJxCPVCbCmnEnaNMxw2okVxSLgtB2Mb2b99iNVmkXy3kxi6gs8lCxkBBsbteIrt+rV+uWKW3UzoWXj5aYCuRr98ndvEJFEUGkIx1p3PTc2foqVYYTTaamXaBpjMsZD2rVWYkG1n2bfTtGJTQYojJQtaVCW/t5IsdB6IgI7KbAZ6cXeLPy3F4iFyya89FMm48RQSeaHw4QjE6EZCjRgihLDJ9Zgopj9HZERVpgYC6xkoXiLCJZN68zyq9buziv16xxPEY7gGE7Bgwuowy00oAkEHuAZXuDVeXLenHfnYz5acPKdQ...",
        "p=0.28\n<latexit sha1_base64=\"2wDbko0etWTaF+OOQTtkDmzbXxM=\">AAAB/XicbZA9SwNBEIbn4leMX1FLm8UgWIU7kWgjBG0sI5gPSI6wt9lL1uzuHbt7QjiCf8FWezux9bfY+kvcXK7QxBcGXt6ZYYYniDnTxnW/nMLK6tr6RnGztLW9s7tX3j9o6ShRhDZJxCPVCbCmnEnaNMxw2okVxSLgtB2Mb2b99iNVmkXy3kxi6gs8lCxkBBsbteIrt+rV+uWKW3UzoWXj5aYCuRr98ndvEJFEUGkIx1p3PTc2foqVYYTTaamXaBpjMsZD2rVWYkG1n2bfTtGJTQYojJQtaVCW/t5IsdB6IgI7KbAZ6cXeLPy3F4iFyya89FMm48RQSeaHw4QjE6EZCjRgihLDJ9Zgopj9HZERVpgYC6xkoXiLCJZN68zyq9buziv16xxPEY7gGE7Bgwuowy00oAkEHuAZXuDVeXLenHfnYz5acPKdQ...",
        "p=0.16\n（a）稀疏混合专家模型（SparseMoE）\nAdd+NormalizeMergedFFN\nFNN1FNN2FNN3FNN4\n门控网络\n<latexit sha1_base64=\"HNgBwgBsr9ynSNzmxxjzG8mCdC8=\">AAAB+HicbVDLTgJBEOzFF+IL9ehlIjHxRHaNQY9ELx4hkUcCGzI79MKE2dnNzKwRCV/gVe/ejFf/xqtf4gB7ULCSTipV3alOBYng2rjul5NbW9/Y3MpvF3Z29/YPiodHTR2nimGDxSJW7YBqFFxiw3AjsJ0opFEgsBWMbmd+6wGV5rG8N+ME/YgOJA85o8ZK9cdeseSW3TnIKvEyUoIMtV7xu9uPWRqhNExQrTuemxh/QpXhTOC00E01JpSN6AA7lkoaofYn80en5MwqfRLGyo40ZK7+vpjQSOtxFNjNiJqhXvZm4r9eEC0lm/Dan3CZpAYlWwSHqSAmJrMWSJ8rZEaMLaFMcfs7YUOqKDO2q4ItxVuuYJU0L...",
        "p=0.16\n图 2.10 混合专家模型三种主要类型 [68]\n2.4.1 稀疏混合专家模型\n稀疏混合专家模型，如图2.10(a) 所示，对于每个输入词元，在前向计算中仅激活专家集合中\n的一个子集。 门控网络对专家子集进行选择， 通过计算排名前K 位专家的输出加权和来实现稀疏\n性。这个过程可以形式化的表示为：\nF M oE\nSparse (x; Θ; {Wi}N\ni=1) =\nNX\ni=1\nG(x; Θ)ifi(x; Wi) (2.38)\nG(x; Θ)i = softmax(TopK(g(x; Θ) + Rnoise, K))i (2.39)\nTop-K(g(x; Θ), K)i =\n(\ng(x; Θ)i, g(x; Θ)i的值属于前 K 项\n−∞, 其他\n(2.40)\n其中，g(x; Θ) 表示在进行 softmax 操作之前的门控值，G(x; Θ)i 表示门控网络针对第 i 个专家的\n输出，TopK(·, K) 函数的目标是保持向量的前 K 项不变， 其它维度设置为−∞。 鉴于softmax 函数\n自身所具有的独特性质，当把其中某些项设置为 −∞ 时，这些项所对应的值会近似等同于..."
      ],
      "response_time": 3.5057027339935303,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 1.0,
        "mrr": 0.25
      },
      "answer_quality_metrics": {
        "rouge1": 0.8571428571428571,
        "rouge2": 0.8,
        "rougeL": 0.8571428571428571,
        "bleu": 0.23376641384792204,
        "semantic_similarity": 0.8983139991760254
      }
    },
    {
      "sample_id": "0031",
      "question": "在自然语言处理的模型中，如何处理未登录词的表示，并且这个处理对模型性能有什么影响？",
      "ground_truth": "在传统的自然语言处理模型中，未登录词（OOV）会被给予一个默认的通用表示，即使用一个标识为'[UNK]'的向量。词表示模型在训练过程中将'[UNK]'的向量作为词表示矩阵的一部分进行更新。当词表过小时，未登录词的比例较高，可能会影响模型的性能；而当词表过大时，低频词的词向量难以有效训练，亦会对模型性能产生负面影响。",
      "predicted_answer": "在自然语言处理的模型中，处理未登录词（Out-of-Vocabulary, OOV）的一个有效方法是使用词元化技术，特别是子词词元化[2][3]。这种方法首先构建一个包含完整单词及其部分信息（如词根、词缀等）的词元词表。对于输入的词序列，词元表示模型将其拆分为词表内的词元，然后查询每个词元的向量表示，最终重组为词元表示序列[2][3]。\n\n使用词元化的处理方式有助于显著减少未登录词的影响，提升模型的整体性能。具体而言，相比于传统的基于固定词表的词表示模型，词元表示模型能够更好地覆盖大部分输入词汇，同时减轻由于词表大小选择不当带来的数据稀疏问题[2]。这样不仅提高了模型对新词汇的学习能力，还减少了因未登录词导致的性能下降[1]。",
      "ground_context": [
        "在深度学习模型中，词表示模型会预先在词表中加入一个默认的 '[UNK]' （unknown）标识，表示未知词，并在训练的过程中将[UNK] 的向量作为词表示矩阵的一部分一起训练。",
        "当词表过小时，未登录词的比例较高，影响模型性能；当词表过大时，大量低频词出现在词表中，这些词的词向量很难。"
      ],
      "retrieved_contexts": [
        "3.2.4 词元切分\n传统的自然语言处理通常以单词为基本处理单元， 模型都依赖预先确定的词表V， 在对输入词\n序列编码时， 这些词表示模型只能处理词表中存在的词。 因此， 使用时， 如果遇到不在词表中的未登\n录词， 模型无法为其生成对应的表示， 只能给予这些未登录词（Out-of-V ocabulary，OOV） 一个默认\n的通用表示。 在深度学习模型中， 词表示模型会预先在词表中加入一个默认的 “[UNK]” （unknown）\n标识，表示未知词，并在训练的过程中将[UNK] 的向量作为词表示矩阵的一部分一起训练，通过\n引入某些相应机制来更新 [UNK] 向量的参数。使用时，对全部未登录词使用 [UNK] 向量作为表\n示向量。此外，基于固定词表的词表示模型对词表大小的选择比较敏感。当词表过小时，未登录\n词的比例较高，影响模型性能；当词表过大时，大量低频词出现在词表中，这些词的词向量很难",
        "大规模语言模型：从理论到实践 71\n得到充分学习。理想模式下，词表示模型应能覆盖绝大部分的输入词，并避免词表过大所造成的\n数据稀疏问题。\n为了缓解未登录词问题，一些工作通过利用亚词级别的信息构造词表示向量。一种直接的解\n决思路是为输入建立字符级别表示，并通过字符向量的组合获得每个单词的表示，以解决数据稀\n疏问题。 然而， 单词中的词根、 词缀等构词模式往往跨越多个字符， 基于字符表示的方法很难学习\n跨度较大的模式。 为了充分学习这些构词模式， 研究人员提出了子词词元化（Subword Tokenization）\n方法，试图缓解上文介绍的未登录词问题。词元表示模型会维护一个词元词表，其中既存在完整\n的单词，也存在形如“c” “re” “ing”等单词的部分信息，称为子词（Subword）。词元表示模型\n对词表中的每个词元计算一个定长向量表示，供下游模型使用。对于输入的词序列，词元表示模\n型将每个词拆分为词表内的词元。例如， 将单词“reborn”拆分为“re”和“born” 。模型随后查询\n每个词元的表示，将输入重新组成词元表示序列。当下游模型需要计算一个单词或词组的表示时，\n可以将对应...",
        "可以将对应范围内的词元表示合成需要的表示。因此，词元表示模型能够较好地解决自然语言处\n理系统中未登录词的问题。词元分析（Tokenization）是将原始文本分割成词元序列的过程。词元\n切分也是数据预处理中至关重要的一步。\n字节对编码（Byte Pair Encoding，BPE）[129] 是一种常见的子词词元算法。该算法采用的词表\n包含最常见的单词及高频出现的子词。使用时，常见词通常位于BPE 词表中，而罕见词通常能被\n分解为若干个包含在 BPE 词表中的词元， 从而大幅减小未登录词的比例。BPE 算法包括以下两个\n部分。\n（1）词元词表的确定。\n（2）全词切分为词元及词元合并为全词的方法。\nBPE 中词元词表的计算过程如图3.4 所示。 首先， 确定数据库中全词的词表和词频， 然后将每\n个单词切分为单个字符的序列，并在序列最后添加符号“</w>”作为单词结尾的标识。例如，单\n词“low”被切分为序列“l␣o␣w␣</w>” 。所切分出的序列元素称为字节，即每个单词都切分为字\n节的序列。之后，按照每个字节序列的相邻字节对和单词的词频，统计每个相邻字节对的出现频\n率，合并出现频率最高..."
      ],
      "response_time": 7.412355184555054,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.15384615384615383,
        "rouge2": 0.0,
        "rougeL": 0.15384615384615383,
        "bleu": 0.055980023707813416,
        "semantic_similarity": 0.8109694719314575
      }
    },
    {
      "sample_id": "0032",
      "question": "Mixtral-8x7B模型在性能上如何与Huatuo-26M数据集相关的医疗问答对进行比较？",
      "ground_truth": "Mixtral-8x7B模型在很多基准测试中展现出了优于或等同于包含700亿参数的Llama-2-70B的性能，而Huatuo-26M数据集是规模最大的中文医疗问答数据集之一，包含逾2600万条高质量的医疗问答对，提供了疾病、症状、治疗方法以及药物信息等方面的知识。",
      "predicted_answer": "根据提供的文档，我无法回答这个问题。文档中未提及Mixtral-8x7B模型或其与Huatuo-26M数据集的相关性能比较。",
      "ground_context": [
        "Mixtral-8x7B模型在很多基准测试中，展现出了优于或等同于包含了700亿参数的Llama-2-70B的性能。",
        "Huatuo-26M是目前规模最大的中文医疗问答数据集之一，包含逾2600万条高质量的医疗问答对，涵盖疾病、症状、治疗方法以及药物信息等诸多方面。"
      ],
      "retrieved_contexts": [
        "使用了可开放获取的医学百科全书和医学教科书数据。Huatuo-26M[112] 是目前规模最大的中文医\n疗问答数据集之一， 该数据集包含逾2600 万条高质量的医疗问答对， 涵盖疾病、症状、治疗方法\n以及药物信息等诸多方面。MedDialog[113] 是一个多语言的医疗对话数据集， 包含中文和英文的医"
      ],
      "response_time": 4.463335037231445,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.6153846153846153,
        "rouge2": 0.36363636363636365,
        "rougeL": 0.6153846153846153,
        "bleu": 0.05235810430005038,
        "semantic_similarity": 0.7446211576461792
      }
    },
    {
      "sample_id": "0033",
      "question": "大语言模型训练中通用数据的重要性是什么，以及目前业界对预训练数据配比有何共识？",
      "ground_truth": "大语言模型训练中通用数据占比非常高，包括网页、对话文本、书籍等多种类型，提供了大规模且多样的训练数据。然而，业界截至2025年2月对预训练数据的配比尚未达成广泛的共识。",
      "predicted_answer": "大语言模型训练中通用数据的重要性体现在它提供了大规模且多样化的训练数据，有助于模型获得多样化的语言知识并增强其泛化能力[1][2][3]。目前业界关于预训练数据的配比还没有达成广泛的共识[1]。",
      "ground_context": [
        "通用数据在大语言模型训练数据中占比非常高，主要包括网页、对话文本、书籍、代码、百科等不同类型的数据，为大语言模型提供了大规模且多样的训练数据。",
        "截至2025年2月，业界关于预训练数据的配比还没达成广泛的共识。"
      ],
      "retrieved_contexts": [
        "64 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n型的任务解决能力。图3.1 给出了一些典型的大语言模型所使用数据类型的分布情况。可以看到，\n不同的大语言模型在训练数据类型分布上的差距很大， 截至2025 年 2 月， 业界关于预训练数据的\n配比还没达成广泛的共识。\n图 3.1 典型的大语言模型所使用数据类型的分布情况 [18]\n3.1.1 通用数据\n通用数据在大语言模型训练数据中占比非常高，主要包括网页、对话文本、书籍、代码、百\n科等不同类型的数据，为大语言模型提供了大规模且多样的训练数据。\n网页（Webpage）是通用数据中数量最多的一类。随着互联网的大规模普及，人们通过网站、\n论坛、 博客、App 创造了海量的数据。 根据2016 年 Google 公开的数据， 其搜索引擎索引处理了超\n过 130 万亿个网页数据。网页数据所包含的海量内容，使语言模型能够获得多样化的语言知识并\n增强其泛化能力[11, 19]。爬取和处理海量网页内容并不是一件容易的事情，因此一些研究人员构建\n了 ClueWeb09[89]、ClueWeb12[90]、SogouT-16[91]、Com...",
        "始数据有 45TB，过滤后仅保留了 570GB 的数据。通过词元方式对上述数据进行切分，大约包含\n5000 亿个词元。为了保证模型使用更多高质量数据进行训练，在GPT-3 训练时，根据数据来源的\n不同，设置不同的采样权重。在完成 3000 亿个词元的训练时，英文 Wikipedia 的数据平均训练轮\n数为 3.4 次，而 CommonCrawl 和 Books 2 仅有 0.44 次和 0.43 次。由于 CommonCrawl 数据集的\n过滤过程烦琐复杂，Meta 公司的研究人员在训练 OPT[29] 模型时采用了混合 RoBERTa[86]、Pile[87]\n和 PushShift.io Reddit[88] 数据的方法。 由于这些数据集中包含的绝大部分数据都是英文数据， 因此\nOPT 也从 CommonCrawl 数据集中抽取了部分非英文数据加入训练数据。\n大语言模型预训练所需的数据来源大体上分为通用数据和专业数据两大类。通用数据（General\nData）包括网页、图书、新闻、对话文本等[14, 29, 39]。通用数据具有规模大、多样性和易获取等特\n点，因此支持大语言模型的语...",
        "3. 大语言模型预训练数据\n在预训练阶段，大语言模型从海量“高质量”文本数据中学习广泛的知识，随后这些知识存\n储在其模型参数当中。通过预训练使得大语言模型具备了一定程度的语言理解和生成能力。因此，\n如何构造海量“高质量”数据对于大语言模型预训练具有至关重要的作用。研究表明，预训练数\n据需要涵盖各种类型的文本，也需要覆盖尽可能多的领域、语言、文化和视角，从而提高大语言\n模型的泛化能力和适应性。当前大模型预训练使用的语料库涵盖网页内容、学术资料、百科、社\n交媒体和书籍等文本内容，同时也包含来自不同领域的文本内容，比如法律文件、年度财务报告、\n医学教科书等其他特定领域的数据。\n本章将介绍常见的大语言模型预训练数据的来源、处理方法、预训练数据对大语言模型影响\n的分析及开源数据集等。\n3.1 数据来源\n文献 [13] 介绍了 OpenAI 训练 GPT-3 使用的主要数据来源，包含经过过滤的 CommonCrawl\n数据集[19]、WebText 2、Books 1、Books 2 及英文 Wikipedia 等数据集。其中 CommonCrawl 的原\n始数据有 45TB，过滤后仅保留了 57...",
        "大规模语言模型：从理论到实践 161\n图 5.3 LLaMA 65B 模型使用 LIMA [42] 训练效果对比\n文献 [190] 研究也表明， 在模型构建过程中， 数据工程起着至关重要的作用， 可以通过提升数\n据集的多样性，显著增强模型的泛化能力。训练数据多样性的提升，可以从多个方面着手，例如\n使用来自不同源头、具备不同特征且呈现不同分布的数据。此外，实验结果也说明，在数据选择\n环节，多样性有着不可忽视的作用。对比随机选择、均匀选择这两种常见方式，具备多样性的数\n据选择策略展现出明显优势。此外，相较于单纯聚焦于挑选高质量数据，若能将数据质量与多样\n性标准有机结合，模型也可以达到更好的效果[223]。\n在问答任务方面，大语言模型的预训练依托于多样化的语料库来开展，这些语料库包含了多\n种类型的内容，并且涵盖了丰富的世界知识。大语言模型在预训练完成后，大量的知识被编码进\n了模型的参数之中。而通过监督微调的方式，就能够把这些已经编码进参数的知识有效地应用于\n问答任务里。然而，针对大语言模型的问答任务能力提升，存在着三个亟待解决的关键问题： （1）\n指令微调阶段，究竟需要多少数据量，才能使大语...",
        "160 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n3. 数据对结果影响\n大语言模型经过指令微调，可以完成多种类型的任务。指令微调数据对于模型结果有着重要\n的影响。本节分别以通用和问题任务为例，讨论指令微调数据与模型效果之间的关系。\n针对通用任务，文献 [42] 提出了“表层对齐假设” （Superficial Alignment Hypothesis ） 。该假\n设指出，模型所具备的知识与能力，绝大部分是在预训练阶段积累和形成的，而指令微调的关键\n作用在于，引导模型掌握在与用户互动过程中应当运用何种格式的子分布。如果这一假设是正确\n的， 进一步推导可得， 人们可以用相当少的示例集便能对预训练语言模型实现充分且有效的微调。\n[220]。\n为此，LIMA[42] 专门收集了一个数据集，该数据集涵盖了 1000 个提示以及与之对应的回复。\n在这个数据集中，输出（也就是回复）部分在风格方面是相互对齐的，不过输入（即提示）却呈\n现出多样化的特点。具体来说，LIMA 所期望获取的输出内容，是那种带有帮助性的、符合人工\n智能助手风格的内容。为了收集到这样的示例，研究人员从多个来源采样..."
      ],
      "response_time": 4.410063028335571,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.3333333333333333,
        "rouge2": 0.0,
        "rougeL": 0.3333333333333333,
        "bleu": 0.25434645929578503,
        "semantic_similarity": 0.8626844882965088
      }
    },
    {
      "sample_id": "0034",
      "question": "团队使用了哪两种方法对数据集进行去重？",
      "ground_truth": "MinHash 和 URL",
      "predicted_answer": "团队使用了MinHashLSH方法和不同的语言停用词和标记词列表来对数据集进行去重[1][2]。",
      "ground_context": [
        "团队利用 MinHash 和 URL 对数据集进行全面去重，并按语言独立进行。"
      ],
      "retrieved_contexts": [
        "团队利用 MinHash 和 URL 对数据集进行全面去重，并按语言独立进行。其中，MinHashLSH[165]\n方法用于过滤相似文档，它基于 MinHash[158] 的多个哈希函数和 Jaccard 相似度，结合局部敏感哈",
        "据集中删除。使用一系列全面的指标，包括单词数量、字符和单词重复比率等。同时高困惑度分\n数的文档也会被视为噪声排除。由于重复信息会对训练大语言模型产生不利影响， CulturaX 研究\n团队利用不同语言的停用词和标记词列表计算比率以删除文档，还通过 FastText 获取语言识别置\n信度辅助过滤。\n(4) 文档优化：由于 mC4 和 OSCAR 的文档是从互联网上抓取的 HTML 页面中提取的，其中\n很大一部分可能带有抓取和提取错误， 包括长JavaScript 行和无关内容。 因此， 对于每个文档， 文\n档优化步骤的目标是通过一系列操作去除其噪声或不相关的部分。首先，去除每个文档末尾的短\n行，因为这些行通常包含页脚细节或来自网站的无用信息。其次，删除包含JavaScript（JS）关键\n词列表中的单词（例如“<script>” ）的行，以避免不相关和非语言信息。\n(5) 冗余去除：尽管进行了全面的数据清洗， 但由于信息在网络上重新发布、 对同一文章的多\n次引用、样板内容和抄袭等各种原因，剩余数据集仍可能包含大量重复数据，这会导致大语言模\n型记忆和泛化能力受到影响，因此数据去重对保证训练..."
      ],
      "response_time": 3.5681333541870117,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0.0,
        "bleu": 0.009565665109256835,
        "semantic_similarity": 0.5979161262512207
      }
    },
    {
      "sample_id": "0035",
      "question": "数据质量对大语言模型训练效果的影响是什么？",
      "ground_truth": "数据质量通常被认为是影响大语言模型训练效果的关键因素之一。",
      "predicted_answer": "数据质量对大语言模型训练效果有着重要影响。大量的低质量甚至是重复的数据可能导致训练过程不稳定，使得模型训练不收敛[1][2][3]。高质量的数据能够帮助模型更好地学习和掌握语言知识，在自然语言生成和理解任务上表现出更佳的效果[3][4]。同时，去除有害信息和噪音，以及确保数据的新鲜度也是保证数据质量的重要方面[1][5]。总体而言，使用清洗过的高质量数据训练的语言模型通常能获得更好的性能[1][3][5]。",
      "ground_context": [
        "数据质量通常被认为是影响大语言模型训练效果的关键因素之一。"
      ],
      "retrieved_contexts": [
        "数据质量通常被认为是影响大语言模型训练效果的关键因素之一。大量重复的低质量数据甚\n至导致训练过程不稳定， 造成模型训练不收敛[122, 139]。 现有的研究表明， 训练数据的构建时间、 包\n含噪声或有害信息情况、 数据重复率等因素， 都对语言模型性能产生较大影响[115, 122, 124, 140]。 目前\n业界普遍的共识是语言模型在经过清洗的高质量数据上训练可以得到更好的性能。\n¬ M，即 Million，表示百万。\n­ B，即 Billion，表示十亿。",
        "大规模语言模型：从理论到实践 79\n图 3.6 LLaMA 模型在问答和常识推理任务上的效果演变 [34]\n文献 [138] 对不同任务类型所依赖的语言模型训练数量进行了分析。 针对分类探查 （Classifier\nProbing） 、信息论探查（Info-theoretic Probing） 、无监督相对可接受性判断（Unsupervised Relative\nAcceptability Judgment）及应用于自然语言理解任务的微调（Fine-tuning on NLU Tasks）这四类任\n务， 基于不同量级预训练数据的RoBERTa[86] 模型进行了实验验证和分析。 分别针对预训练了1M¬、\n10M、100M 和 1B­个词元的 RoBERTa 模型进行能力分析。研究发现，仅对模型进行10M∼100M\n个词元的训练，就可以获得可靠的语法和语义特征。然而，需要更多的训练数据才能获得足够的\n常识知识和其他技能，并在典型的下游自然语言理解任务中取得较好的结果。\n3.3.2 数据质量\n数据质量通常被认为是影响大语言模型训练效果的关键因素之一。大量重复的低质量数据甚\n至导致训练过程不...",
        "17 亿个参数的模型，针对下游少样本任务的性能进行了分析。使用相同超参数，对使用原始数据\n集和经过质量筛选后的数据训练得到的模型效果进行了对比，实验结果如图 3.8 所示。可以看到，\n使用高质量数据训练的模型在自然语言生成和自然语言理解任务上表现更好。特别是，高质量数\n据对自然语言生成任务的影响大于自然语言理解任务。这可能是因为自然语言生成任务通常需要\n生成高质量的语言， 过滤预训练语料库对语言模型的生成能力至关重要。 文献[116] 的研究强调了\n预训练数据的质量在下游任务的性能中也扮演着关键角色。\nGoogle Research 的研究人员针对数据构建时间、文本质量、是否包含有害信息进行了系统的\n研究[141]。 他们使用包含不同时间、 毒性水平、 文本质量和领域的数据， 训练了28 个具有 15 亿个\n参数的仅解码器（Decoder-only）结构的语言模型。研究结果表明，大语言模型训练数据的时间、\n内容过滤方法及数据源对下游模型行为具有显著影响。",
        "数规模的大语言模型上进行实验的成本非常高，很多结论是在百亿甚至十亿规模的语言模型上进\n行的实验，其结果并不能完整地反映数据对大语言模型的影响。此外，一些观点仍处于猜想阶段，\n需要进一步验证。请各位读者甄别判断。\n3.3.1 数据规模\n随着大语言模型参数规模的增加， 为了有效地训练模型， 需要收集足够数量的高质量数据[34, 132]。\n在针对模型参数规模、训练数据量及总计算量与模型效果之间关系的研究 [132] 被提出之前，大部\n分大语言模型训练所采用的训练数据量相较于 LLaMA 等最新的大语言模型都少很多。 表3.1 给出\n了模型参数量与训练数据量的对比。 在Chinchilla 模型被提出之前， 大部分大语言模型都在着重提\n升模型的参数量，所使用的训练数据量都在 3000 亿个词元左右，LaMDA 模型使用的训练参数量\n仅有 1370 亿个。虽然 Chinchilla 模型的参数量不足 LaMDA 模型的一半，但是训练数据的词元数\n达到 1.4 万亿个，是 LaMDA 模型的 8 倍多。",
        "3. 大语言模型预训练数据\n在预训练阶段，大语言模型从海量“高质量”文本数据中学习广泛的知识，随后这些知识存\n储在其模型参数当中。通过预训练使得大语言模型具备了一定程度的语言理解和生成能力。因此，\n如何构造海量“高质量”数据对于大语言模型预训练具有至关重要的作用。研究表明，预训练数\n据需要涵盖各种类型的文本，也需要覆盖尽可能多的领域、语言、文化和视角，从而提高大语言\n模型的泛化能力和适应性。当前大模型预训练使用的语料库涵盖网页内容、学术资料、百科、社\n交媒体和书籍等文本内容，同时也包含来自不同领域的文本内容，比如法律文件、年度财务报告、\n医学教科书等其他特定领域的数据。\n本章将介绍常见的大语言模型预训练数据的来源、处理方法、预训练数据对大语言模型影响\n的分析及开源数据集等。\n3.1 数据来源\n文献 [13] 介绍了 OpenAI 训练 GPT-3 使用的主要数据来源，包含经过过滤的 CommonCrawl\n数据集[19]、WebText 2、Books 1、Books 2 及英文 Wikipedia 等数据集。其中 CommonCrawl 的原\n始数据有 45TB，过滤后仅保留了 57..."
      ],
      "response_time": 5.503469228744507,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.02421291658474481,
        "semantic_similarity": 0.8537901639938354
      }
    },
    {
      "sample_id": "0036",
      "question": "LLaMA 模型训练使用了哪些类型的数据来源？",
      "ground_truth": "包括网页、代码、论文、图书等。",
      "predicted_answer": "LLaMA 模型训练使用了以下几种类型的数据来源：CommonCrawl、C4、GitHub、Wikipedia、Books、arXiv、Stack Exchange以及MassiveText等[1][2]。对于后者，实验中特别关注了Wikipedia和GitHub这两个数据集的采样权重[1]。",
      "ground_context": [
        "可以看到，LLaMA 模型训练混合了大量不同来源的数据，包括网页、代码、论文、图书等。"
      ],
      "retrieved_contexts": [
        "大规模语言模型：从理论到实践 83\n表 3.2 LLaMA 模型训练所使用的数据集 [37]\n数据集 采样概率 训练轮数 存储空间\nCommonCrawl 67.0% 1.10 3.3 TB\nC4 15.0% 1.06 783 GB\nGitHub 4.5% 0.64 328 GB\nWikipedia 4.5% 2.45 83 GB\nBooks 4.5% 2.23 85 GB\narXiv 2.5% 1.06 92 GB\nStack Exchange 2.0% 1.03 78 GB\nGopher 模型[115] 在训练过程中进行了对数据分布的消融实验，以便验证混合来源对下游任务\n的影响。 针对MassiveText 子集设置了不同权重的数据组合， 并用于训练语言模型。 利用Wikitext103、\nLambada、C4 和 Curation Corpus 测试不同权重组合训练得到的语言模型在下游任务上的性能。 为了\n限制数据组合分布范围， 实验中固定了Wikipedia 和 GitHub 两个数据集的采样权重。 对于Wikipedia，\n要求对训练数据进行完整的学习， 因此将采样权重固定为...",
        "大规模语言模型：从理论到实践 161\n图 5.3 LLaMA 65B 模型使用 LIMA [42] 训练效果对比\n文献 [190] 研究也表明， 在模型构建过程中， 数据工程起着至关重要的作用， 可以通过提升数\n据集的多样性，显著增强模型的泛化能力。训练数据多样性的提升，可以从多个方面着手，例如\n使用来自不同源头、具备不同特征且呈现不同分布的数据。此外，实验结果也说明，在数据选择\n环节，多样性有着不可忽视的作用。对比随机选择、均匀选择这两种常见方式，具备多样性的数\n据选择策略展现出明显优势。此外，相较于单纯聚焦于挑选高质量数据，若能将数据质量与多样\n性标准有机结合，模型也可以达到更好的效果[223]。\n在问答任务方面，大语言模型的预训练依托于多样化的语料库来开展，这些语料库包含了多\n种类型的内容，并且涵盖了丰富的世界知识。大语言模型在预训练完成后，大量的知识被编码进\n了模型的参数之中。而通过监督微调的方式，就能够把这些已经编码进参数的知识有效地应用于\n问答任务里。然而，针对大语言模型的问答任务能力提升，存在着三个亟待解决的关键问题： （1）\n指令微调阶段，究竟需要多少数据量，才能使大语...",
        "46 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n4. 模型整体框架\n基于上述模型和网络结构可以实现解码器层，根据自回归方式利用训练数据进行模型训练的\n过程与 2.2.3 节介绍的过程基本一致。不同规模的 LLaMA 模型使用的超参数如表2.1 所示。由于\n大语言模型的参数量非常大， 并且需要大量的数据进行训练， 因此仅利用单个GPU 很难完成训练，\n需要依赖分布式模型训练框架（第 4 章将详细介绍相关内容） 。",
        "78 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\nDopt ∝ C 0.51 (3.3)\n图 3.5 在同等计算量情况下，训练损失随参数量的变化情况 [132]\nLLaMA[34] 模型在训练时采用了与文献 [132] 相符的训练策略。研究发现，70 亿个参数的语\n言模型在训练超过 1 万亿个词元后，性能仍在持续增长。因此，Meta 的研究人员在 LLaMA-2[37]\n模型训练中，进一步增大了训练数据量，训练数据量达到 2 万亿个词元。LLaMA-3[135] 模型训练\n中，则是进一步将训练数据量增大到了惊人的 15 万亿个词元。Qwen2.5[136] 的 720 亿参数的开源\n版本，也使用了 18 万亿个词元进行了训练。文献 [132] 给出了不同参数量的 LLaMA 模型在训练\n期间， 随着训练数据量的增加， 模型在问答和常识推理任务上的效果演变， 如图3.6 所示。研究人\n员分别在 TriviaQA、HellaSwag、NaturalQuestions、SIQA、WinoGrande、PIQA 这 6 个数据集上进\n行了测试。可以看到，随着训练数据量的增加，模型...",
        "大规模语言模型：从理论到实践 79\n图 3.6 LLaMA 模型在问答和常识推理任务上的效果演变 [34]\n文献 [138] 对不同任务类型所依赖的语言模型训练数量进行了分析。 针对分类探查 （Classifier\nProbing） 、信息论探查（Info-theoretic Probing） 、无监督相对可接受性判断（Unsupervised Relative\nAcceptability Judgment）及应用于自然语言理解任务的微调（Fine-tuning on NLU Tasks）这四类任\n务， 基于不同量级预训练数据的RoBERTa[86] 模型进行了实验验证和分析。 分别针对预训练了1M¬、\n10M、100M 和 1B­个词元的 RoBERTa 模型进行能力分析。研究发现，仅对模型进行10M∼100M\n个词元的训练，就可以获得可靠的语法和语义特征。然而，需要更多的训练数据才能获得足够的\n常识知识和其他技能，并在典型的下游自然语言理解任务中取得较好的结果。\n3.3.2 数据质量\n数据质量通常被认为是影响大语言模型训练效果的关键因素之一。大量重复的低质量数据甚\n至导致训练过程不..."
      ],
      "response_time": 4.676486015319824,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.008006388824329778,
        "semantic_similarity": 0.6169729232788086
      }
    },
    {
      "sample_id": "0037",
      "question": "Chinchilla 语言模型具有多少个参数？",
      "ground_truth": "700 亿个参数",
      "predicted_answer": "Chinchilla 语言模型具有 700 亿个参数[1]。",
      "ground_context": [
        "最终确定 Chinchilla 语言模型具有 700 亿个参数，使用了 1.4 万亿个词元进行训练。"
      ],
      "retrieved_contexts": [
        "放， 即模型大小加倍则训练词元数量也应该加倍。 为了验证该分析结果， 他们使用与Gopher 语言\n模型训练相同的计算资源，根据上述理论预测了 Chinchilla 语言模型的最优参数量与词元数量组\n合。最终确定 Chinchilla 语言模型具有 700 亿个参数，使用了 1.4 万亿个词元进行训练。通过实验\n发现，Chinchilla 在很多下游评估任务中都显著地优于 Gopher（280B） 、GPT-3（175B） 、Jurassic-1\n（178B）及 Megatron-Turing NLG（530B） 。\n图3.5 给出了在同等计算量情况下， 训练损失随参数量的变化情况。 针对9 种不同的训练参数\n量设置，使用不同词元数量的训练数据，训练不同大小的模型参数量，使得最终训练所需浮点运\n算数达到预定目标。 对于每种训练量预定目标， 图3.5(a) 所示为平滑后的训练损失与参数量之间的\n关系。可以看到，训练损失值存在明显的低谷，这意味着对于给定训练计算量目标，存在一个最\n佳模型参数量和训练数据量配置。利用这些训练损失低谷的位置，还可以预测更大的模型的最佳\n模型参数量和训练词元数量...",
        "数规模的大语言模型上进行实验的成本非常高，很多结论是在百亿甚至十亿规模的语言模型上进\n行的实验，其结果并不能完整地反映数据对大语言模型的影响。此外，一些观点仍处于猜想阶段，\n需要进一步验证。请各位读者甄别判断。\n3.3.1 数据规模\n随着大语言模型参数规模的增加， 为了有效地训练模型， 需要收集足够数量的高质量数据[34, 132]。\n在针对模型参数规模、训练数据量及总计算量与模型效果之间关系的研究 [132] 被提出之前，大部\n分大语言模型训练所采用的训练数据量相较于 LLaMA 等最新的大语言模型都少很多。 表3.1 给出\n了模型参数量与训练数据量的对比。 在Chinchilla 模型被提出之前， 大部分大语言模型都在着重提\n升模型的参数量，所使用的训练数据量都在 3000 亿个词元左右，LaMDA 模型使用的训练参数量\n仅有 1370 亿个。虽然 Chinchilla 模型的参数量不足 LaMDA 模型的一半，但是训练数据的词元数\n达到 1.4 万亿个，是 LaMDA 模型的 8 倍多。",
        "大规模语言模型：从理论到实践 77\n表 3.1 模型参数量与训练数据量的对比\n模型名称 参数量（个） 训练数据量（个词元）\nLaMDA[15] 1370 亿 1680 亿\nGPT-3[39] 1750 亿 3000 亿\nJurassic [133] 1780 亿 3000 亿\nGopher [115] 2800 亿 3000 亿\nMT-NLG 530B [134] 5300 亿 2700 亿\nChinchilla[132] 700 亿 14000 亿\nFalcon[60] 400 亿 10000 亿\nLLaMA[34] 630 亿 14000 亿\nLLaMA-2[37] 700 亿 20000 亿\nLLaMA-3[135] 4050 亿 150000 亿\nQwen2.5[136] 720 亿 180000 亿\nGLM-4[137] 1300 亿 100000 亿\nDeepMind 的研究人员在文献 [132] 中描述了他们训练 400 多个语言模型后得出的分析结果\n（模型的参数量从 7000 万个到 160 亿个，训练数据量从 5 亿个词元到 5000 亿个词元） 。研究发现，\n如果希..."
      ],
      "response_time": 2.8533363342285156,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.5,
        "rouge2": 0.0,
        "rougeL": 0.5,
        "bleu": 0.11609514815067874,
        "semantic_similarity": 0.7511911392211914
      }
    },
    {
      "sample_id": "0038",
      "question": "arXiv 上的论文是用什么编写的？",
      "ground_truth": "LaTeX",
      "predicted_answer": "arXiv 上的论文是用 LaTeX 编写的。[1]",
      "ground_context": [
        "arXiv 上的论文是用 LaTeX 编写的，其中公式、符号、表格等内容的表示非常适合语言模型学习。"
      ],
      "retrieved_contexts": [
        "主要集中在数学、计算机科学和物理领域。arXiv 上的论文是用 LaTeX 编写的，其中公式、符号、\n表格等内容的表示非常适合语言模型学习。\n（6）GitHub 是一个大型的开源代码库，对于语言模型完成代码生成、代码补全等任务具有非\n常重要的作用。\n（7）FreeLaw 是一个非营利项目，为法律领域的学术研究提供访问和分析工具。CourtListener\n是 FreeLaw 项目的一部分，包含美国联邦和州法院的数百万条法律意见，并提供批量下载服务。\n（8）Stack Exchange 是一个围绕用户提供问题和答案的网站集合。 Stack Exchange Data Dump\n包含了 Stack Exchange 网站集合中所有用户贡献的内容的匿名数据集。 它是截至2023 年 9 月公开\n可用的最大的问题-答案对数据集之一，包括编程、园艺、艺术等主题。\n（9）USPTO Backgrounds 是美国专利商标局授权的专利背景部分的数据集，来源于其公布的\n批量档案。由于专利通常包含任务背景介绍，给出了发明的背景和技术领域的概述，建立了问题\n空间的框架，因此该数据集包含了大量关于应用主题的...",
        "大规模语言模型：从理论到实践 85\n（1）Pile-CC 是基于 CommonCrawl 的数据集，该数据集通过在 Web Archive 文件上使用 jus-\nText[143] 的方法进行提取，比直接使用 WET 文件产生更高质量的输出。\n（2）PubMed Central（PMC）是由美国国家生物技术信息中心（ NCBI）运营的 PubMed 生物\n医学在线资源库的一个子集，PubMed 是由美国国家医学图书馆运营的生物医学文章在线存储库，\n提供对近 500 万份出版物的开放全文访问。\n（3）Books 3 是一个图书数据集，来自 Shawn Presser 提供的 Bibliotik。Bibliotik 由小说和非小\n说类书籍组成，几乎是图书数据集（BookCorpus 2）数据量的十倍。\n（4）OpenWebText2 （OWT2）是一个基于 WebText[11] 和 OpenWebTextCorpus 的通用数据集。\n它包括来自多种语言的文本内容、网页文本元数据，以及多个开源数据集和开源代码库。\n（5）arXiv 是一个自 1991 年开始运营的论文预印版本发布服务平台。..."
      ],
      "response_time": 2.681173801422119,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.5,
        "rouge2": 0.0,
        "rougeL": 0.5,
        "bleu": 0.018724372764461875,
        "semantic_similarity": 0.5687898397445679
      }
    },
    {
      "sample_id": "0039",
      "question": "arXiv 是什么？",
      "ground_truth": "arXiv 是一个自 1991 年开始运营的论文预印版本发布服务平台。",
      "predicted_answer": "根据提供的文档，我无法回答这个问题。",
      "ground_context": [
        "arXiv 是一个自 1991 年开始运营的论文预印版本发布服务平台。发布在 arXiv 上的论文主要集中在数学、计算机科学和物理领域。arXiv 上的论文是用 LaTeX 编写的，其中公式、符号、表格等内容的表示非常适合语言模型学习。"
      ],
      "retrieved_contexts": [],
      "response_time": 2.226731061935425,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.01691967020615708,
        "semantic_similarity": 0.38962239027023315
      }
    },
    {
      "sample_id": "0040",
      "question": "CulturaX 研究团队使用了哪两种语言识别工具？",
      "ground_truth": "cld3 和 FastText",
      "predicted_answer": "CulturaX 研究团队在处理 mC4 和 OSCAR 数据集时，最初使用了 cld3 和 FastText 这两种不同的语言识别工具[1]。但由于 cld3 在语言检测方面表现不佳，导致 mC4 中存在许多语言检测错误，因此最终决定使用 FastText 对 mC4 中的文档语言进行重新预测[1][2]。",
      "ground_context": [
        "在处理mC4 和 OSCAR 数据集时， 一个较为突出的问题是二者分别使用了cld3 和 FastText 这两种不同的语言识别工具。"
      ],
      "retrieved_contexts": [
        "92 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n练。二者组合后，为后续处理提供了多达 135 亿份文档。其中，mC4 占比 66%，OSCAR 23.01 占\n比 11%，OSCAR 22.01 占比 7%，OSCAR 21.09 占比 9%，OSCAR 20.19 占比 7%。\n基于 mC4 和 OSCAR 合并后的数据集，CulturaX 研究团队通过一系列数据处理步骤来构造高\n质量的多语言数据集， 包括语言识别、 基于URL 的过滤、 基于指标的清洗、 文档优化、 冗余去除。\n具体清洗工作如下：\n(1) 语言识别： 在处理mC4 和 OSCAR 数据集时， 一个较为突出的问题是二者分别使用了cld3\n和 FastText 这两种不同的语言识别工具。此前的研究已经证实，cld3 在语言检测方面的表现远逊\n于 FastText， 这使得mC4 中出现了大量的语言检测错误[154]。 因此，CulturaX 团队使用 FastText 对\nmC4 中的文档语言重新进行预测。 若文档的预测语言与mC4 中原本提供的语言不一致， 那么该文\n档将从数据集中剔除。这样做的目的在...",
        "档将从数据集中剔除。这样做的目的在于避免那些会使 cld3 和 FastText 语言检测器产生混淆的文\n档，因为这些文档极有可能给数据带来噪声干扰。\n(2) 基于 URL 的过滤：为了降低数据中的有害信息， CulturaX 研究团队使用了图卢兹大学\n（University of Toulouse） 提供的最新UT1 URL 和域名黑名单， 将有毒和有害页面从数据中删除。 该\n列表包含来自色情、抱怨和黑客攻击等不同主题的网站，名单每周更新两到三次。目前该黑名单\n包含超过 370 万条由人类和机器 （如搜索引擎、 已知地址和索引） 共同贡献的记录[163]。mC4 数据\n集之前未使用过该黑名单进行过滤。OSCAR 数据集虽然使用过该黑名单进行数据清洗， 但是可以\n根据更新的名单进一步进行清洗。\n(3) 基于指标的清洗：受 ROOTS 语料库数据处理启发，CulturaX 数据集构建中也利用各种数\n据集指标的分布来识别和过滤异常文档。 每个指标为数据集中的文档提供量化特定属性的单一值，\n根据指标值及其范围确定阈值，将其分为正常和异常范围，异常范围的文档被视为噪声，并从数\n据集中删除。使用...",
        "据集中删除。使用一系列全面的指标，包括单词数量、字符和单词重复比率等。同时高困惑度分\n数的文档也会被视为噪声排除。由于重复信息会对训练大语言模型产生不利影响， CulturaX 研究\n团队利用不同语言的停用词和标记词列表计算比率以删除文档，还通过 FastText 获取语言识别置\n信度辅助过滤。\n(4) 文档优化：由于 mC4 和 OSCAR 的文档是从互联网上抓取的 HTML 页面中提取的，其中\n很大一部分可能带有抓取和提取错误， 包括长JavaScript 行和无关内容。 因此， 对于每个文档， 文\n档优化步骤的目标是通过一系列操作去除其噪声或不相关的部分。首先，去除每个文档末尾的短\n行，因为这些行通常包含页脚细节或来自网站的无用信息。其次，删除包含JavaScript（JS）关键\n词列表中的单词（例如“<script>” ）的行，以避免不相关和非语言信息。\n(5) 冗余去除：尽管进行了全面的数据清洗， 但由于信息在网络上重新发布、 对同一文章的多\n次引用、样板内容和抄袭等各种原因，剩余数据集仍可能包含大量重复数据，这会导致大语言模\n型记忆和泛化能力受到影响，因此数据去重对保证训练...",
        "度上说明高质量数据集对语言模型具有重要的影响。\n3.4.4 CulturaX\nCulturaX[159] 是一个可以用于预训练的多语言数据集， 涵盖167 种语言， 包含6.3 万亿个词元。\n它通过整合 mC4[160]（3.1.0 版本）和 OSCAR[161–163]（20.19、21.09、22.01 以及 23.01 版本）数据\n集，并经过语言识别、URL 过滤、基于度量的清洗、文档精炼以及数据去重等一系列严格的数据\n处理步骤，有效解决了现有多语言数据集存在的语言识别不准确、文档级去重缺失、数据清理不\n彻底等问题。该数据集具有多语言、开源、大规模和高质量的特点，旨在提升多语言场景下模型\n训练的数据质量，推动多语言学习的研究与发展，为训练高性能的多语言大语言模型提供了有力\n的数据支持，有助于打破训练数据不透明的现状。\nmC4 最初是为训练多语言编码器-解码器模型 mT5[160] 而创建， 涵盖101 种语言， 从Common-\nCrawl 的 71 个月度快照中获取数据，经过去除短行页面、不良词汇页面及重复行去除等处理，其\n语言识别借助 cld3[164] 工具。OSCAR 数..."
      ],
      "response_time": 4.404492616653442,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.2857142857142857,
        "rouge2": 0.16666666666666669,
        "rougeL": 0.2857142857142857,
        "bleu": 0.018971121556566745,
        "semantic_similarity": 0.6754677295684814
      }
    },
    {
      "sample_id": "0041",
      "question": "ROOTS数据集包含多少种语言？",
      "ground_truth": "59种语言",
      "predicted_answer": "ROOTS数据集包含59种语言，其中包括46种自然语言和13种编程语言[1][2]。",
      "ground_context": [
        "ROOTS（Responsible Open-science Open-collaboration Text Sources ）数据集是 BigScience 项目在训练具有 1760 亿个参数的 BLOOM 大语言模型时使用的数据集。该数据集包含 46 种自然语言和 13 种编程语言，总计 59 种语言，整个数据集的大小约 1.6TB。"
      ],
      "retrieved_contexts": [
        "该数据集的加入可以帮助语言模型建模电子邮件通信的特性。\nPile 中不同数据子集所占比例及训练时的采样权重有很大不同， 高质量的数据会有更高的采样权\n重。 例如，Pile-CC 数据集包含227.12GB 数据， 整个训练周期中采样1 轮。 虽然Wikipedia (English)\n数据集仅有6.38GB 的数据， 但是整个训练周期中采样3 轮。 具体的采样权重和采样轮数可以参考文\n献 [87]。\n3.4.2 ROOTS\nROOTS（Responsible Open-science Open-collaboration Text Sources ）数据集[128] 是 BigScience\n项目在训练具有 1760 亿个参数的 BLOOM 大语言模型时使用的数据集。该数据集包含 46 种自然\n语言和 13 种编程语言，总计 59 种语言，整个数据集的大小约 1.6TB。ROOTS 数据集中各语言所\n占比例如图3.13 所示。图中左侧是以语言家族的字节为单位表示的自然语言占比树状图，其中欧\n亚大陆语言占据了绝大部分（1321.89GB） 。右侧橙色矩形对应的是印度尼西亚语（18GB） ...",
        "如图3.14 所示。其中，中文数据主要由WuDao Corpora 和 OSCAR[154] 组成。在虚拟抓取方面，由\n于很多语言的现有公开数据集较少， 因此这些语言的网页信息是十分重要的资源补充。 在ROOTS\n数据集中， 采用CommonCrawl 网页镜像， 选取了614 个域名， 从这些域名下的网页中提取文本内\n容补充到数据集中， 以提升语言的多样性。 在GitHub 代码方面， 针对程序语言，ROOTS 数据集采\n用了与 AlphaCode[101] 相同的方法：从 BigQuery 公开数据集中选取文件长度在 100 到 20 万字符，\n字母符号占比在 15% 至 65%，最大行数在 20 至 1000 行的代码。训练大语言模型时，网页数据对\n于数据的多样性和数据量支撑起到重要的作用[2, 19]，ROOTS 数据集中包含了 OSCAR 21.09 版本，\n对应的是 CommonCrawl 2021 年 2 月的快照，占整体 ROOTS 数据集规模的 38%。",
        "大规模语言模型：从理论到实践 87\n位的编程语言分布的华夫饼图（Waffle Plot） ，一个正方形大约对应3 万个文件。\n图 3.13 ROOTS 数据集中各语言所占比例 [128]\nROOTS 中的数据主要来自四个方面：公开数据、虚拟抓取、 GitHub 代码和网页数据。在公\n开数据方面，BigScience Data Sourcing 工作组的目标是收集尽可能多的各种类型的数据，包括自\n然语言处理数据集和各类型文档数据集。为此，还设计了 BigScience Catalogue[153] 用于管理和分\n享大型科学数据集，Masader Repository 用于收集阿拉伯语和文化资源的开放数据存储库。 在收集\n原始数据集的基础上，进一步从语言和统一表示方面对收集的文档进行规范化处理。识别数据集\n所属语言并分类存储，将所有数据都按照统一的文本和元数据结构进行表示。由于数据种类繁多，\nROOTS 数据集并没有公开其所包含数据集的情况，但是提供了 Corpus Map 及 Corpus Description\n工具， 以便查询各类数据集占比和数据情况。 在ROOTS 数据集中， 中文...",
        "88 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n图 3.14 在 ROOTS 数据集中，中文数据集的种类及所占比例\n在数据准备完成后， 还要进行清洗、 过滤、 去重及隐私信息删除等工作，ROOTS 数据集处理\n流程如图3.15 所示。 整个处理工作并非完全依赖自动计算， 而是采用人工与自动相结合的方法。 针\n对数据中存在的一些非自然语言的文本，例如预处理错误、SEO 页面或垃圾邮件（包括色情垃圾\n邮件） ，构建ROOTS 数据集时会进行一定的处理。首先，定义一套质量指标，其中高质量的文本\n被定义为 “由人类撰写， 面向人类” （written by humans for humans） ， 不区分内容 （专业人员根据来\n源对内容进行选择）或语法正确性的先验判断。所使用的指标包括字母重复度、单词重复度、特\n殊字符、困惑度等。完整的指标列表可以参考文献[128]。这些指标根据来源的不同，进行了两种\n主要的调整：针对每种语言单独选择参数，如阈值等；人工浏览每个数据来源，以确定哪些指标\n最可能识别出非自然语言。其次，针对冗余信息，采用 SimHash 算法[155]，计算文档的..."
      ],
      "response_time": 3.7556920051574707,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.2857142857142857,
        "rouge2": 0.0,
        "rougeL": 0.2857142857142857,
        "bleu": 0.05229171211257016,
        "semantic_similarity": 0.6742241382598877
      }
    },
    {
      "sample_id": "0042",
      "question": "在训练大语言模型时，为什么多语言场景下的网络爬虫数据集比精选数据集更具优势？",
      "ground_truth": "多语言场景下，网络爬虫数据集可以高效收集多种语言的数据，尽管其原始数据质量参差不齐，但经过清洗后可以很好应用于大语言模型训练。而精选数据集通常有限，可能无法覆盖多种语言的需求。",
      "predicted_answer": "在网络爬虫数据集中，虽然原始数据的质量可能参差不齐，但在经过清洗处理后，仍然能够有效应用于大语言模型的训练[1]。此外，相比于依赖精选数据集的方式，网络爬虫数据集能够在多语言场景下更高效地收集到多种语言的数据，从而提升模型的泛化能力[1]。",
      "ground_context": [
        "在多语言场景下，网络爬虫数据集更具优势，它有助于高效收集多语言数据。",
        "在训练大语言模型之前，构建一个准备充分的预训练语料库尤为重要。"
      ],
      "retrieved_contexts": [
        "管道，对 166 种不同语言的网页数据进行分类和过滤。区别于以往依赖精选数据集（如 The Pile\n和 BookCorpus）训练大语言模型的做法。在多语言场景下，网络爬虫数据集更具优势，它有助于\n高效收集多语言数据。尽管其原始数据质量参差不齐，但经清洗后可以很好应用于大语言模型训"
      ],
      "response_time": 4.655517101287842,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.17255060927378105,
        "semantic_similarity": 0.9290811419487
      }
    },
    {
      "sample_id": "0043",
      "question": "在多语言数据集的构建过程中，如何确保数据的质量和去重，特别是在处理来自网页的数据时？",
      "ground_truth": "CulturaX研究团队使用一系列全面的指标，包括单词数量、字符和单词重复比率等来删除噪声文档，同时利用不同语言的停用词和标记词列表计算比率以删除文档。此外，通过FastText获取语言识别信度辅助过滤。对于网页数据，尽管原始数据质量参差不齐，但经过清洗后，这些数据可以很好地应用于大语言模型训练。冗余去除是保证训练数据质量的关键，CulturaX研究团队利用MinHash和URL对数据集进行全面去重，并按语言独立进行。",
      "predicted_answer": "在多语言数据集的构建过程中，为了确保数据质量和去除冗余信息，特别是处理来自网页的数据时，可以采取以下措施：\n\n1. 定义一套质量指标，如字母重复度、单词重复度、特殊字符、困惑度等[1]。\n2. 针对每种语言单独选择合适的参数，如阈值等。\n3. 通过人工浏览每个数据来源，确定哪些指标最有可能识别出非自然语言。\n4. 使用SimHash算法计算文档的相似性，从而识别并去除冗余信息[1][155]。",
      "ground_context": [
        "CulturaX研究团队利用不同语言的停用词和标记词列表计算比率以删除文档，还通过FastText获取语言识别信度辅助过滤。",
        "在多语言场景下，网络爬虫数据集更具优势，它有助于高效收集多语言数据。"
      ],
      "retrieved_contexts": [
        "88 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n图 3.14 在 ROOTS 数据集中，中文数据集的种类及所占比例\n在数据准备完成后， 还要进行清洗、 过滤、 去重及隐私信息删除等工作，ROOTS 数据集处理\n流程如图3.15 所示。 整个处理工作并非完全依赖自动计算， 而是采用人工与自动相结合的方法。 针\n对数据中存在的一些非自然语言的文本，例如预处理错误、SEO 页面或垃圾邮件（包括色情垃圾\n邮件） ，构建ROOTS 数据集时会进行一定的处理。首先，定义一套质量指标，其中高质量的文本\n被定义为 “由人类撰写， 面向人类” （written by humans for humans） ， 不区分内容 （专业人员根据来\n源对内容进行选择）或语法正确性的先验判断。所使用的指标包括字母重复度、单词重复度、特\n殊字符、困惑度等。完整的指标列表可以参考文献[128]。这些指标根据来源的不同，进行了两种\n主要的调整：针对每种语言单独选择参数，如阈值等；人工浏览每个数据来源，以确定哪些指标\n最可能识别出非自然语言。其次，针对冗余信息，采用 SimHash 算法[155]，计算文档的..."
      ],
      "response_time": 5.815614700317383,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0.0,
        "bleu": 0.017054796821111714,
        "semantic_similarity": 0.7495508790016174
      }
    },
    {
      "sample_id": "0044",
      "question": "如何通过加强数学问题数据集的方式提升大语言模型在数学任务上的能力？",
      "ground_truth": "通过在Pile数据集中专门增加数学问题数据集，期望增强通过Pile数据集训练的语言模型的数学能力。",
      "predicted_answer": "文档中并未具体提及如何通过加强数学问题数据集的方式来提升大语言模型在数学任务上的能力。因此，根据提供的文档，我无法回答这个问题。",
      "ground_context": [
        "大语言模型在数学任务上的表现较差， 这可能是由于训练集中缺乏数学问题。因此，Pile 数据集中专门增加了数学问题数据集，期望增强通过 Pile 数据集训练的语言模型的数学能力。",
        "pair_freqs = compute_pair_freqs(splits)"
      ],
      "retrieved_contexts": [
        "型的参数量巨大，在不同任务上都进行微调需要消耗大量的计算资源，因此预训练微调范式不再\n适用于大语言模型。 研究人员发现， 通过语境学习（In-Context Learning，ICL） 等方法， 直接使用\n大语言模型，就可以在很多任务的少样本场景中取得很好的效果。此后，研究人员提出了面向大\n语言模型的提示词（Prompt）学习方法，以及模型即服务范式（Model as a Service ，MaaS） 、指令\n微调（Instruction Tuning）等方法， 在不同任务中都取得了很好的效果。 与此同时，Google、Meta、\nBigScience、 百度、 华为等公司和研究机构纷纷发布了PaLM[14]、LaMDA[15]、T0[16] 等不同大语言\n模型。2022 年年底 ChatGPT 的出现， 将大语言模型的能力进行了充分的展现， 也引发了大语言模\n型研究的热潮。\nKaplan 等人在文献 [17] 中提出了缩放法则（Scaling Laws），指出模型的性能依赖于模型的规\n模， 包括参数量、 数据集大小和计算量， 模型的效果会随着三者的指数增加而平稳提升。 如图1.1 所...",
        "大规模语言模型：从理论到实践 161\n图 5.3 LLaMA 65B 模型使用 LIMA [42] 训练效果对比\n文献 [190] 研究也表明， 在模型构建过程中， 数据工程起着至关重要的作用， 可以通过提升数\n据集的多样性，显著增强模型的泛化能力。训练数据多样性的提升，可以从多个方面着手，例如\n使用来自不同源头、具备不同特征且呈现不同分布的数据。此外，实验结果也说明，在数据选择\n环节，多样性有着不可忽视的作用。对比随机选择、均匀选择这两种常见方式，具备多样性的数\n据选择策略展现出明显优势。此外，相较于单纯聚焦于挑选高质量数据，若能将数据质量与多样\n性标准有机结合，模型也可以达到更好的效果[223]。\n在问答任务方面，大语言模型的预训练依托于多样化的语料库来开展，这些语料库包含了多\n种类型的内容，并且涵盖了丰富的世界知识。大语言模型在预训练完成后，大量的知识被编码进\n了模型的参数之中。而通过监督微调的方式，就能够把这些已经编码进参数的知识有效地应用于\n问答任务里。然而，针对大语言模型的问答任务能力提升，存在着三个亟待解决的关键问题： （1）\n指令微调阶段，究竟需要多少数据量，才能使大语...",
        "210 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n解决数据受限的问题。大语言模型可通过强化学习中的奖励机制，学习如何探索不同的推理路径，\n从而扩大训练数据的范围。但此前的相关研究成果未达到理想的竞争效果，Kimi k1.5 旨在探索一\n条更有效的技术路线。\n1. 技术路线\n强化学习提示数据集构建： RL 提示数据集的质量和多样性对强化学习的有效性至关重要。\nKimi k1.5 定义了高质量 RL 提示集的三个关键属性：\n多样覆盖：提示应涵盖 STEM、编程和一般推理等广泛学科，以增强模型的适应性和跨领域\n应用能力。为此，采用自动过滤器选择需要丰富推理且易于评估的问题，数据集来源广泛，包括\n不同领域的问题以及纯文本和图像 - 文本问答数据。\n平衡难度：提示集应包含不同难度级别的问题，以促进模型的逐步学习并防止过度拟合。利\n用模型自身能力自适应评估提示难度， 通过SFT 模型多次生成答案计算通过率作为难度代理指标，\n并开发标签系统按领域和学科分类提示，实现难度平衡。\n准确评估能力：提示应能被验证者客观可靠地评估，确保基于正确推理衡量模型性能。为避\n免奖励操纵，排除易出现验证...",
        "据集，会对大语言模型在问答任务上的表现产生怎样的影响？ （3）不同的大语言模型在指令微调\n阶段，对于数据的需求方面存在着怎样的差异呢？\n针对上述问题， 文献[224] 给出了详细的分析。 研究人员使用了ENTITYQUESTIONS[225]， 这\n是一个包含维基百科上 24 个不同话题知识的问答数据集。选择了其中 12 个与地点相关的原始训\n练集作为训练数据， 将它们对应的测试集作为测试集， 并将剩余12 个话题的测试集作为领域外测\n试集。通过设计的多模板补全机制，能够可靠地评估大语言模型对不同知识的记忆程度。利用该\n机制，根据其知识记忆水平将训练和测试集均进行了 5 个级别的划分。\n文献 [224] 中将训练数据划分为六个不同的数据量级别，从 60 个样本到完整数据集不等，并\n通过从 12 个话题中均匀抽样来构建训练集。实验结果表明，仅需 60 个训练样本的指令微调，就\n足以使大语言模型高效执行问答任务，并展现出强大的泛化能力。如图5.4所示。无论基础模型或\n记忆水平如何，大语言模型在使用较少训练样本时的表现优于使用 960 个或全部样本。增加训练",
        "OpenCLIP[305] 则需数百卡 GPU 集群运行数天至数周。 文献[304] 提出通过构建高效的数据处理管\n道，可实现性能提升，同时避免成本大幅上升。\n如图 7.12 所示，数据是训练多模态大语言模型的核心要素之一。构建一个多样且平衡的数据\n集对于模型学习覆盖足够多概念的良好世界模型至关重要。清除大型数据集中常见的重复数据同\n样重要，这不仅能够节省大量计算资源，还能降低模型过度记忆的风险。与此同时，数据剪枝也\n是数据处理的重要环节，确保文本描述与图像内容高度相关，有助于模型更好地理解和对齐多模\n态信息。可以通过改进模型对视觉语义关联（Grounding）能力来增强对图文关系的理解，并通过\n引入人类偏好优化对齐效果。 在OCR 任务中， 使用专门的增强技术可以进一步提升文本读取和翻\n译能力。通过结合高效数据处理、合理的模型架构选择和针对性优化策略，可以显著提升多模态\n大语言模型的训练效果和应用能力。"
      ],
      "response_time": 3.843290090560913,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.09428509488626016,
        "semantic_similarity": 0.7564398050308228
      }
    },
    {
      "sample_id": "0045",
      "question": "混合并行将哪些并行策略混合使用？",
      "ground_truth": "混合并行将多种并行策略如数据并行、流水线并行和张量并行等混合使用。",
      "predicted_answer": "混合并行将数据并行、流水线并行和张量并行等多种并行策略混合使用[1][3]。",
      "ground_context": [
        "混合并行将多种并行策略如数据并行、流水线并行和张量并行等混合使用。"
      ],
      "retrieved_contexts": [
        "4.2.3 混合并行\n混合并行将多种并行策略如数据并行、流水线并行和张量并行等混合使用。通过结合不同的\n并行策略，混合并行可以充分发挥各种并行策略的优点，最大限度地提高计算性能和效率。针对\n千亿规模的大语言模型，通常，在每个服务器内部使用张量并行策略，由于该策略涉及的网络通\n信量较大，因此需要利用服务器内部的不同计算设备之间的高速通信带宽。通过流水线并行，将\n模型的不同层划分为多个阶段，每个阶段由不同的机器负责计算。这样可以充分利用多台机器的\n计算能力， 并通过机器之间的高速通信传递计算结果和中间数据， 以提高整体的计算速度和效率。\n最后，在外层叠加数据并行策略，以增加并发数量，加快整体训练速度。通过数据并行，将训练\n数据分发到多组服务器上进行并行处理，每组服务器处理不同的数据批次。这样可以充分利用多\n台服务器的计算资源，并增加训练的并发度，从而加快整体训练速度。\nBLOOM 使用 Megatron-DeepSpeed[134] 框架进行训练，主要包含两个部分： Megatron-LM 提",
        "def to_dist_tensor(t): return distribute_tensor(t, mesh, rowwise_placement)\nmod._apply(to_dist_tensor)\nsharded_module = distribute_module(MyModule(), mesh, partition_fn =shard_params)\ndef shard_fc(mod_name, mod, mesh):\nrowwise_placement = [Shard(0),Replicate()]\nif mod_name == \"fc1\":\nmod.weight = torch.nn.Parameter(distribute_tensor(mod.weight, mesh, rowwise_placement))\n# ᄝ൐Ⴈൈაభ૫ shard_paramsॖၛ࿊ᄴః၂\nsharded_module = distribute_module(MyModule(), mesh, partition_fn =shard_fc)\n4.2.3 混合并行\n混合并行将多种并行策略...",
        "通过这一设计，FM8-LM 的 FP8 混合精度优化器在训练过程中，每个参数仅消耗 6 字节的内存：\n2|{z}\n主权重\n+ 1|{z}\n梯度\n+ 1 + 2|{z}\nAdam 状态\n= 6字节 (10.20)\n3. FP8 分布式并行训练\n训练大语言模型需要分布式学习策略， 以实现跨多GPU 的并行化。 常用的策略包括数据并行\n（Data Parallelism） 、张量并行（Tensor Parallelism） 、流水线并行（Pipeline Parallelism）以及序列并\n行（Sequence Parallelism） 。每种并行策略都有其优点，并在现有系统中以互补的方式使用。对于\n这些策略的 FP8 支持而言，数据并行和流水线并行无需进行任何特定的修改，因为在将数据批次\n或模型层拆分到不同设备时，这两种策略并不涉及额外的 FP8 计算和通信。\n张量并行将模型的单个层划分到多个设备上，使得权重、梯度和激活张量的分片分布在不同\n的 GPU 上， 而不是集中在单个GPU 上。 为了在张量并行中支持FP8，FP8-LM 将分片的权重和激\n活张量转换为 FP8 格式，用于线性层的计..."
      ],
      "response_time": 3.818451166152954,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.4717991357336539,
        "semantic_similarity": 0.9450365304946899
      }
    },
    {
      "sample_id": "0046",
      "question": "大语言模型是基于什么结构的？",
      "ground_truth": "Transformer 结构",
      "predicted_answer": "大语言模型通常采用类似于GPT的架构，基于Transformer结构构建[2][4]。这种模型使用仅由解码器组成的网络结构，采用自回归方式构建语言模型[2]。",
      "ground_context": [
        "大语言模型都是以Transformer 结构为基础"
      ],
      "retrieved_contexts": [
        "2. 大语言模型基础\n语言模型的核心目标是对自然语言的概率分布进行建模，这一任务在自然语言处理研究中占\n据重要地位，是其基础性工作之一。大量研究围绕这一目标，从不同角度展开了探索，包括 n 元\n语言模型（n-gram Language Models ） 、神经语言模型和预训练语言模型等。这些研究在不同发展\n阶段对自然语言处理任务产生了深远影响。 随着基于Transformer 架构的语言模型不断发展， 以及\n预训练-微调范式在各类自然语言处理任务中取得突破性成果，自 2020 年 OpenAI 发布 GPT-3 以\n来，大语言模型的研究逐步深入。尽管大语言模型参数规模庞大，并且通过有监督微调和强化学\n习可以完成众多任务，其理论基础仍然离不开对语言建模的核心研究。\n本章首先介绍 Transformer 结构， 并在此基础上讲解生成式预训练语言模型GPT、 大语言模型\n的网络结构、注意力机制优化及相关实践。关于 n 元语言模型、神经语言模型及其他预训练语言\n模型的内容，可参考《自然语言处理导论》第 6 章[4]。\n2.1 Transformer 结构\nTransformer 结构[44] ...",
        "==================================================\n2.3 大语言模型的结构\n当前， 绝大多数大语言模型都采用类似GPT 的架构， 使用基于Transformer 结构构建的仅由解\n码器组成的网络结构， 采用自回归的方式构建语言模型， 但是在位置编码、 层归一化位置、 激活函数\n等细节上各有不同。 文献[13] 介绍了 GPT-3 模型的训练过程， 包括模型架构、 训练数据组成、 训练过\n程及评估方法。 由于GPT-3 并没有开放源代码， 根据论文直接重现整个训练过程并不容易， 因此文\n献[29] 介绍了根据 GPT-3 的描述复现的过程， 构造并开源了系统OPT（Open Pre-trained Transformer\nLanguage Models） 。MetaAI 也仿照 GPT-3 的架构开源了 LLaMA 模型[34]， 公开评测结果及利用该模\n型 进 行 有 监 督 微 调 后 的 模 型 都 有 非 常 好 的 表 现。 GPT-3 模 型 之",
        "vi 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n\u001a\u000f\u0012\u000f\u0013ࠩٳ318\n\u001a\u000f\u0012\u000f\u0014ׄ321\n9.2 模块化检索增强生成架构 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 322\n\u001a\u000f\u0013\u000f\u0012෬ႄଆॶ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 324\n\u001a\u000f\u0013\u000f\u0013߄328\n\u001a\u000f\u0013\u000f\u0014෬ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ...",
        "1. 绪论\n大语言模型是一种由包含数百亿个及以上参数的深度神经网络构建的语言模型，通常使用自\n监督学习方法通过大量无标注文本进行训练。2018 年以来，Google、OpenAI、Meta、百度、华为\n等公司和研究机构相继发布了 BERT[1]、GPT[2] 等多种模型， 这些模型在几乎所有自然语言处理任\n务中都表现出色。2019 年，大语言模型呈现爆发式的增长，特别是 2022 年 11 月 ChatGPT（Chat\nGenerative Pre-trained Transformer）的发布，引起了全世界的广泛关注。用户可以使用自然语言与\n系统交互，实现问答、分类、摘要、翻译、聊天等从理解到生成的各种任务。大语言模型展现出\n了强大的对世界知识的掌握和对语言的理解能力。\n本章主要介绍大语言模型的基本概念、发展历程和构建流程。\n1.1 大语言模型的基本概念\n使用语言是人类与其他动物最重要的区别之一， 而人类的多种智能也与此密切相关， 逻辑思维\n以语言的形式表达， 大量的知识也以文字的形式记录和传播。 如今， 互联网上已经拥有数万亿个网页\n的资源， 其中大部分信息都是用自然语言描述的。 ...",
        "10 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n1.3 大语言模型的构建流程\n根据 OpenAI 联合创始人 Andrej Karpathy 在微软 Build 2023 大会上公开的信息，OpenAI 使用\n的大语言模型构建流程如图1.3 所示， 主要包含四个阶段：预训练、 有监督微调、 奖励建模和强化\n学习。这四个阶段都需要不同规模的数据集及不同类型的算法，会产出不同类型的模型，所需要\n的资源也有非常大的差别。\n图 1.3 OpenAI 使用的大语言模型构建流程\n预训练（Pretraining） 阶段需要利用海量的训练数据 （数据来自互联网网页、 维基百科、 书籍、\nGitHub、 论文、 问答网站等） ， 构建包含数千亿甚至数万亿单词的具有多样性的内容。 利用由数千块\n高性能 GPU 和高速网络组成的超级计算机， 花费数十天完成深度神经网络参数训练， 构建基础模\n型 （Base Model） 。 基础模型对长文本进行建模， 使模型具有语言生成能力， 根据输入的提示词， 模\n型可以生成文本补全句子。有一部分研究人员认为，语言模型建模过程中隐含地构建了包括事实\n性知识..."
      ],
      "response_time": 4.165387392044067,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.33333333333333337,
        "rouge2": 0.0,
        "rougeL": 0.33333333333333337,
        "bleu": 0.014783348734456997,
        "semantic_similarity": 0.566165566444397
      }
    },
    {
      "sample_id": "0047",
      "question": "1F1B 交错式调度模式要求微批次的数量是流水线阶段的什么倍数？",
      "ground_truth": "整数倍",
      "predicted_answer": "微批次的数量是流水线阶段的整数倍。[1]",
      "ground_context": [
        "1F1B 交错式调度模式要求微批次的数量是流水线阶段的整数倍。"
      ],
      "retrieved_contexts": [
        "大规模语言模型：从理论到实践 107\n1F1B 交错式调度模式要求微批次的数量是流水线阶段的整数倍。每个设备不仅负责连续多\n个层的计算， 还可以处理多个层的子集， 这些子集被称为模型块。 具体而言， 在之前的模式中， 设\n备 1 可能负责层 1∼4， 设备2 负责层 5∼8， 依此类推。 在新的模式下， 设备1 可以处理层 1、2、9、\n10，设备 2 处理层 3、4、11、12，依此类推。在这种模式下，每个设备在流水线中被分配到多个\n阶段。例如，设备 1 可能参与热身阶段、前向计算阶段和后向计算阶段的某些子集任务。每个设\n备可以并行执行不同阶段的计算任务，从而更好地利用流水线并行的优势。这种模式不仅在内存\n消耗方面表现出色，还能提高计算效率，使大型模型的并行系统能够更高效地完成计算任务。\n图 4.8 1F1B 流水线并行策略样例 [171]\nPyTorch 中也包含了实现流水线的 API 函数 Pipe，具体实现参考“ torch.distributed.pipeline.\nsync.Pipe”类。可以使用这个 API 构造一个模型，其包含两个线性层，分别放置在两个计算设备\n中的样例..."
      ],
      "response_time": 2.638908863067627,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.018724372764461875,
        "semantic_similarity": 0.5243733525276184
      }
    },
    {
      "sample_id": "0048",
      "question": "What is the default value for 'local_rank' in the argument parser?",
      "ground_truth": "0",
      "predicted_answer": "The default value for 'local_rank' in the argument parser is 0. [2]",
      "ground_context": [
        "parser.add_argument('--local_rank', default =0, type=int, help='node rank for distributed training' )"
      ],
      "retrieved_contexts": [
        "from dataset import Cityscaples\n# ҕඔഡᇂ\nparser = argparse.ArgumentParser(description='DeepLab')\nparser.add_argument('-j', '--workers', default =4, type=int, metavar ='N',\nhelp='number of data loading workers (default: 4)' )\nparser.add_argument('--epochs', default =100, type=int, metavar ='N',\nhelp='number of total epochs to run' )\nparser.add_argument('--start-epoch', default =0, type=int, metavar ='N',\nhelp='manual epoch number (useful on restarts)' )\nparser.add_argument('-b', '--batch-size', def...",
        "metavar='N')\nparser.add_argument('--local_rank', default =0, type=int,\nhelp='node rank for distributed training' )\nargs = parser.parse_args()\ntorch.distributed.init_process_group(backend=\"nccl\") #߄\nprint(\"Use GPU: {} for training\" .format(args.local_rank))\n#ଆ྘\nmodel = DeepLab()\ntorch.cuda.set_device(args.local_rank) #భཁव\nmodel = model.cuda()\nmodel = torch.nn.parallel.DistributedDataParallel(model, device_ids =[args.local_rank],\noutput_device=args.local_rank, find_unused_parameters =True) # ඔऌѩྛ\n..."
      ],
      "response_time": 2.764186143875122,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.14285714285714288,
        "rouge2": 0.0,
        "rougeL": 0.14285714285714288,
        "bleu": 0.013609139615996616,
        "semantic_similarity": 0.5057512521743774
      }
    },
    {
      "sample_id": "0049",
      "question": "单个计算设备在推理阶段采用 FP32 格式进行存储时，需要多少GB的计算设备内存空间来存储GPT-3的参数？",
      "ground_truth": "700GB",
      "predicted_answer": "单个计算设备在推理阶段采用 FP32 格式进行存储时，需要 700GB 的计算设备内存空间来存储 GPT-3 的参数 [1]。",
      "ground_context": [
        "如果在推理阶段采用 FP32 格式进行存储，则需要 700GB 的计算设备内存空间"
      ],
      "retrieved_contexts": [
        "用数千个计算设备花费几十天。分布式训练系统需要克服计算墙、显存墙、通信墙等挑战，以确\n保集群内的所有资源得到充分利用，从而加速训练过程并缩短训练周期。\n• 计算墙：单个计算设备所能提供的计算能力与大语言模型所需的总计算量之间存在巨大差\n异。2022 年 3 月发布的 NVIDIA H100 SXM 的单卡 FP16 算力只有 2000 TFLOPS（Floating\nPoint Operations Per Second ） ， 而GPT-3 需要 314 ZFLOPS 的总计算量， 两者相差了8 个数量\n级。\n• 显存墙： 单个计算设备无法完整存储一个大语言模型的参数。GPT-3 包含 1750 亿个参数， 如\n果在推理阶段采用 FP32 格式进行存储，则需要 700GB 的计算设备内存空间，而 NVIDIA\nH100 GPU 只有 80GB 显存。\n• 通信墙： 分布式训练系统中各计算设备之间需要频繁地进行参数传输和同步。 由于通信的延\n迟和带宽限制，这可能成为训练的瓶颈。在 GPT-3 的训练过程中，如果分布式系统中存在\n128 个模型副本，那么在每次迭代过程中至少需要传输 89...",
        "大规模语言模型：从理论到实践 117\n图 4.15 FP32、FP16 和 BF16 的表示\n混合精度优化的过程如图 4.16 所示。Adam 优化器状态包括采用 FP32 保存的模型参数备份，\n一阶动量和二阶动量也都采用 FP32 格式存储。 假设模型参数量为Φ， 模型参数和梯度都是用FP16\n格式存储， 则共需要2Φ + 2Φ + (4Φ + 4Φ + 4Φ) = 16 Φ 字节存储。其中，Adam 状态占比 75%。动\n态损失缩放在反向传播前，将损失变化（dLoss）手动增大 2K 倍，因此反向传播时得到的激活函\n数梯度不会溢出；反向传播后，将权重梯度缩小 2K 倍，恢复正常值。举例来说，有 75 亿个参数\n的模型，如果用 FP16 格式，只需要 15GB 计算设备内存，但是在训练阶段，模型状态实际上需要\n耗费 120GB 内存。计算卡内存占用中除了模型状态，还有剩余状态（Residual States） ，包括激活\n值（Activation） 、各种临时缓冲区（Buffer）及无法使用的显存碎片（Fragmentation）等。可以使\n用激活值检查点（Activation Ch..."
      ],
      "response_time": 4.471971273422241,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.33333333333333337,
        "rouge2": 0.0,
        "rougeL": 0.33333333333333337,
        "bleu": 0.0092991042878441,
        "semantic_similarity": 0.5404314994812012
      }
    },
    {
      "sample_id": "0050",
      "question": "What does the DistributedSampler class require to be available?",
      "ground_truth": "Requires distributed package to be available",
      "predicted_answer": "The `DistributedSampler` class requires the distributed package to be available [1]。",
      "ground_context": [
        "if not dist.is_available(): raise RuntimeError(\"Requires distributed package to be available\" )"
      ],
      "retrieved_contexts": [
        "102 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\nclass DistributedSampler(Sampler):\ndef __init__(self, dataset, num_replicas =None, rank =None, shuffle =True, seed =0):\nif num_replicas is None:\nif not dist.is_available():\nraise RuntimeError(\"Requires distributed package to be available\" )\nnum_replicas = dist.get_world_size()\nif rank is None:\nif not dist.is_available():\nraise RuntimeError(\"Requires distributed package to be available\" )\nrank = dist.get_rank()\nself.dataset = dataset #ࠢ\nself.num_replicas = num_rep..."
      ],
      "response_time": 2.4393670558929443,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.7058823529411764,
        "rouge2": 0.5333333333333333,
        "rougeL": 0.7058823529411764,
        "bleu": 0.22894156860669912,
        "semantic_similarity": 0.7846457958221436
      }
    },
    {
      "sample_id": "0051",
      "question": "1F1B 流水线并行策略引入了什么机制？",
      "ground_truth": "任务调度机制",
      "predicted_answer": "1F1B 流水线并行策略引入了任务调度机制，使得下游设备能够在等待上游计算的同时执行其他可并行的任务，从而提高设备的利用率[1]。",
      "ground_context": [
        "1F1B 流水线并行策略引入了任务调度机制，使得下游设备能够在等待上游计算的同时执行其他可并行的任务，从而提高设备的利用率。"
      ],
      "retrieved_contexts": [
        "同时在计算设备 1 中并行计算 F12。 相比于最原始的流水线并行方法，GPipe 流水线方法可以有效\n减少并行气泡。\n图 4.7 GPipe 策略流水线并行样例 [170]\n虽然 GPipe 策略可以减少一定的并行气泡，但是只有当一个小批次中所有的前向计算都完成\n时，才能执行后向计算。因此，还是会产生很多并行气泡，从而降低系统的并行效率。Megatron-\nLM[171] 采用了 1F1B 流水线并行策略， 即一个前向通道和一个后向通道。1F1B 流水线并行策略引\n入了任务调度机制，使得下游设备能够在等待上游计算的同时执行其他可并行的任务，从而提高\n设备的利用率。1F1B 给出了非交错式和交错式两种调度模式，如图4.8 所示。\n1F1B 非交错式调度模式可分为三个阶段。 首先是热身阶段， 在计算设备中进行不同数量的前\n向计算。接下来的阶段是前向-后向阶段，计算设备按顺序执行一次前向计算，然后进行一次后向\n计算。最后一个阶段是后向阶段，计算设备完成最后一次后向计算。相比于GPipe 策略，1F1B 非\n交错式调度模式在节省内存方面表现得更好。然而，它需要与 GPipe 策略一样的时间来...",
        "大规模语言模型：从理论到实践 107\n1F1B 交错式调度模式要求微批次的数量是流水线阶段的整数倍。每个设备不仅负责连续多\n个层的计算， 还可以处理多个层的子集， 这些子集被称为模型块。 具体而言， 在之前的模式中， 设\n备 1 可能负责层 1∼4， 设备2 负责层 5∼8， 依此类推。 在新的模式下， 设备1 可以处理层 1、2、9、\n10，设备 2 处理层 3、4、11、12，依此类推。在这种模式下，每个设备在流水线中被分配到多个\n阶段。例如，设备 1 可能参与热身阶段、前向计算阶段和后向计算阶段的某些子集任务。每个设\n备可以并行执行不同阶段的计算任务，从而更好地利用流水线并行的优势。这种模式不仅在内存\n消耗方面表现出色，还能提高计算效率，使大型模型的并行系统能够更高效地完成计算任务。\n图 4.8 1F1B 流水线并行策略样例 [171]\nPyTorch 中也包含了实现流水线的 API 函数 Pipe，具体实现参考“ torch.distributed.pipeline.\nsync.Pipe”类。可以使用这个 API 构造一个模型，其包含两个线性层，分别放置在两个计算设备\n中的样例...",
        "， 也称之为张量并行（Tensor Parallelism，TP） 。 两节点模型并行训练系统样例如图4.5 所示， 图4.5(a)\n为流水线并行， 模型的不同层被切分到不同的设备中； 图4.5(b) 为张量并行， 同一层中的不同参数\n被切分到不同的设备中进行计算。\n图 4.5 两节点模型并行训练系统样例\n1. 流水线并行\n流水线并行是一种并行计算策略，将模型的各个层分段处理，并将每个段分布在不同的计算设\n备上， 使得前后阶段能够流水式、 分批工作。 流水线并行通常应用于大语言模型的并行系统中， 以有\n效解决单个计算设备内存不足的问题。 图4.6 给出了一个由四个计算设备组成的流水线并行系统， 包\n含前向计算和后向计算。其中F1、F2、F3、F4 分别代表四个前向路径，位于不同的设备上；而B4、\nB3、B2、B1 则代表逆序的后向路径， 也分别位于四个不同的设备上。 从图4.6 中可以看出， 计算图中"
      ],
      "response_time": 3.373143434524536,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.01780895112194094,
        "semantic_similarity": 0.62727952003479
      }
    },
    {
      "sample_id": "0052",
      "question": "数据并行和GPipe策略在并行计算中有什么主要区别？",
      "ground_truth": "数据并行系统中，每个计算设备都有整个神经网络模型的模型副本，进行前向计算和梯度同步，而GPipe策略在前向计算完成后才能执行后向计算，且可能产生并行气泡。虽然GPipe减少了并行气泡，但在执行后向计算时仍然依赖于前向计算的完成。",
      "predicted_answer": "数据并行和GPipe策略的主要区别在于它们如何减少并行计算中的“并行气泡”：\n\n- 数据并行通过在多个计算设备之间分布整个模型副本来进行前向计算，每个设备处理一部分数据样本。这种方式允许所有设备几乎同时进行前向计算，减少了因等待前向计算完成而导致的设备空闲时间[3]。\n\n- 而GPipe策略则是通过将小批次进一步划分为更小的微批次，逐个处理这些微批次的数据，实现高效的流水线并行。这种方法虽然也能减少并行气泡，但仍然依赖于小批次内所有前向计算完成后才能开始相应的后向计算[1][2]。",
      "ground_context": [
        "在数据并行系统中，每个计算设备都有整个神经网络模型的模型副本，进行迭代时，每个计算设备只分配一个批次数据样本的子集。",
        "GPipe策略可以减少一定的并行气泡，但是只有当一个小批次中所有的前向计算都完成时，才能执行后向计算。"
      ],
      "retrieved_contexts": [
        "同时在计算设备 1 中并行计算 F12。 相比于最原始的流水线并行方法，GPipe 流水线方法可以有效\n减少并行气泡。\n图 4.7 GPipe 策略流水线并行样例 [170]\n虽然 GPipe 策略可以减少一定的并行气泡，但是只有当一个小批次中所有的前向计算都完成\n时，才能执行后向计算。因此，还是会产生很多并行气泡，从而降低系统的并行效率。Megatron-\nLM[171] 采用了 1F1B 流水线并行策略， 即一个前向通道和一个后向通道。1F1B 流水线并行策略引\n入了任务调度机制，使得下游设备能够在等待上游计算的同时执行其他可并行的任务，从而提高\n设备的利用率。1F1B 给出了非交错式和交错式两种调度模式，如图4.8 所示。\n1F1B 非交错式调度模式可分为三个阶段。 首先是热身阶段， 在计算设备中进行不同数量的前\n向计算。接下来的阶段是前向-后向阶段，计算设备按顺序执行一次前向计算，然后进行一次后向\n计算。最后一个阶段是后向阶段，计算设备完成最后一次后向计算。相比于GPipe 策略，1F1B 非\n交错式调度模式在节省内存方面表现得更好。然而，它需要与 GPipe 策略一样的时间来...",
        "106 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n的下游设备 （Downstream Device） 需要长时间持续处于空闲状态， 等待上游设备 （Upstream Device）\n计算完成， 才能开始计算自身的任务。 这种情况导致设备的平均使用率大幅降低， 形成了模型并行气\n泡（Model Parallelism Bubble），也称为流水线气泡（Pipeline Bubble）。\n图 4.6 流水线并行样例\n朴素流水线策略所产生的并行气泡，使得系统无法充分利用计算资源，降低了系统整体的计\n算效率。为了减少并行气泡，文献 [170] 提出了 GPipe 方法，将小批次（Mini-batch）进一步划分\n成更小的微批次（Micro-batch），利用流水线并行方法，每次处理一个微批次的数据。在当前阶\n段计算完成得到结果后，将该微批次的结果发送给下游设备，同时开始处理后一个微批次的数据，\n这样可以在一定程度上减少并行气泡。图4.7 给出了 GPipe 策略流水线并行样例。前向 F1 计算被\n拆解为 F11、F12、F13、F14，在计算设备 1 中计算完成 F11 后，会...",
        "大规模语言模型：从理论到实践 101\n4.2.1 数据并行\n在数据并行系统中， 每个计算设备都有整个神经网络模型的模型副本（Model Replica） ， 进行\n迭代时，每个计算设备只分配一个批次数据样本的子集，并根据该批次样本子集的数据进行网络\n模型的前向计算。 假设一个批次的训练样本数为N ， 使用M 个计算设备并行计算， 每个计算设备\n会分配到 N /M 个样本。前向计算完成后，每个计算设备都会根据本地样本计算损失误差，得到\n梯度 Gi（i 为加速卡编号） ，并将本地梯度Gi 进行广播。所有计算设备需要聚合其他加速卡给出\n的梯度值，然后使用平均梯度 (PN\ni=1 Gi)/N 对模型进行更新，完成该批次训练。图4.4 给出了由\n两个计算设备组成的数据并行训练系统样例。\n图 4.4 由两个计算设备组成的数据并行训练系统样例\n数据并行训练系统可以通过增加计算设备，有效提升整体训练吞吐量，即 每秒全局批次数\n（Global Batch Size Per Second ）。与单个计算设备训练相比，其最主要的区别在于反向计算中的\n梯度需要在所有计算设备中进行同步，以保证每个计算设备上最终..."
      ],
      "response_time": 7.193150043487549,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.5714285714285715,
        "rouge2": 0.0,
        "rougeL": 0.5714285714285715,
        "bleu": 0.07733540245474821,
        "semantic_similarity": 0.8973722457885742
      }
    },
    {
      "sample_id": "0053",
      "question": "在张量并行与1F1B交错式调度模式中，如何解决大规模语言模型中设备间的计算和存储问题？",
      "ground_truth": "张量并行需要根据模型的具体结构和算子类型，将参数切分到不同设备，并保证数学一致性。对于大语言模型，嵌入式表示算子的参数存储需求很高，因此可以按词维度切分，以减少每个设备的显存需求。同时，1F1B交错式调度模式通过让每个设备处理多个层的子集，从而更高效地利用计算资源，提升了内存消耗和计算效率。",
      "predicted_answer": "张量并行主要用于解决大规模语言模型中由于参数过多导致的存储问题。对于嵌入式表示层，可以通过按词维度切分参数来减小每台设备所需的存储空间[1]。1F1B交错式调度模式则通过交错分配模型的不同部分给不同的设备，提高了内存利用率和计算效率[2]。这两种方法分别从存储和计算角度缓解了大规模语言模型中的设备间问题。",
      "ground_context": [
        "张量并行需要根据模型的具体结构和算子类型，解决如何将参数切分到不同设备，以及如何保证切分后的数学一致性这两个问题。",
        "在新的模式下，设备1 可以处理层 1、2、9、10，设备 2 处理层 3、4、11、12，依此类推。"
      ],
      "retrieved_contexts": [
        "张量并行需要根据模型的具体结构和算子类型，解决如何将参数切分到不同设备，以及如何\n保证切分后的数学一致性这两个问题。 大语言模型都是以Transformer 结构为基础，Transformer 结\n构主要由嵌入式表示（Embedding） 、矩阵乘（MatMul）和交叉熵损失（Cross Entropy Loss）计算\n构成。这三种类型的算子有较大的差异，需要设计对应的张量并行策略 [169] 才可以实现将参数切\n分到不同的设备。\n对于嵌入式表示算子，如果总的词表数非常大，会导致单计算设备显存无法容纳 Embedding\n层参数。举例来说，如果词表数量是 64000，嵌入式表示维度为 5120，类型采用 32 位精度浮点\n数， 那么整层参数需要的显存大约为64000 × 5120 × 4/1024/1024 = 1250 MB， 反向梯度同样需要\n1250MB 显存， 仅仅存储就需要将近2.5GB。 对于嵌入表示层的参数， 可以按照词维度切分， 每个\n计算设备只存储部分词向量，然后通过汇总各个设备上的部分词向量，得到完整的词向量。图4.9\n给出了单节点 Embedding 和两节点 E...",
        "大规模语言模型：从理论到实践 107\n1F1B 交错式调度模式要求微批次的数量是流水线阶段的整数倍。每个设备不仅负责连续多\n个层的计算， 还可以处理多个层的子集， 这些子集被称为模型块。 具体而言， 在之前的模式中， 设\n备 1 可能负责层 1∼4， 设备2 负责层 5∼8， 依此类推。 在新的模式下， 设备1 可以处理层 1、2、9、\n10，设备 2 处理层 3、4、11、12，依此类推。在这种模式下，每个设备在流水线中被分配到多个\n阶段。例如，设备 1 可能参与热身阶段、前向计算阶段和后向计算阶段的某些子集任务。每个设\n备可以并行执行不同阶段的计算任务，从而更好地利用流水线并行的优势。这种模式不仅在内存\n消耗方面表现出色，还能提高计算效率，使大型模型的并行系统能够更高效地完成计算任务。\n图 4.8 1F1B 流水线并行策略样例 [171]\nPyTorch 中也包含了实现流水线的 API 函数 Pipe，具体实现参考“ torch.distributed.pipeline.\nsync.Pipe”类。可以使用这个 API 构造一个模型，其包含两个线性层，分别放置在两个计算设备\n中的样例...",
        "392 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n序列并行 序列并行张量并行A和B：参数（FP8），Y和Z：激活值\n图 10.11 采用 FP8 张量和序列并行的 Transformer 层 [530]\n零冗余优化器 （Zero Redundancy Data Parallelism，ZeRO）[173–175] 也是大模型训练中的另一种\n常用分布式学习技术。ZeRO 的核心思想是将模型状态分片到各设备，使每个设备仅保存训练步\n骤所需数据 （如主权重、 梯度和优化器状态） 的一部分。 为了减少内存消耗，ZeRO 方法通常将单\n个张量分割为多个子张量，并将其分布到不同的设备上。\n直接将 FP8 应用于 ZeRO 也是不可行的，因为难以处理与 FP8 分片相关的缩放因子。每个张\n量的缩放因子需要与 FP8 分片一起分发。为了解决这一问题，FP8-LM 实现了一种新的 FP8 分布\n方案，该方案将整个张量分布到设备上，而不是像ZeRO 那样将张量分割为多个子张量进行分布。\nFP8 张量的分布采用贪婪策略，具体过程如算法 1 所述。具体来说，我们的方法首先根据张量状\n态的大小对..."
      ],
      "response_time": 5.253609895706177,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.5,
        "rouge2": 0.0,
        "rougeL": 0.5,
        "bleu": 0.07798619945711663,
        "semantic_similarity": 0.9262887239456177
      }
    },
    {
      "sample_id": "0054",
      "question": "在分布式系统中，如何处理多个计算设备的输出以确保最终结果与单个计算设备一致？",
      "ground_truth": "在分布式系统中，多个计算设备的输出需要进行合并，最终得到与单个计算设备等价的计算结果。这是因为多个计算设备并行执行各自的子任务，从而加速整体计算。",
      "predicted_answer": "在分布式系统中，为了确保多个计算设备的输出与单个计算设备的结果一致，通常需要对各个计算设备的输出进行合并。具体而言，在完成了各自分配的任务后，不同计算设备会产生各自的计算结果。然后，通过某种聚合机制（如平均值、最大值或其他统计方法），将这些局部结果合并成全局结果。这样做的目的是使得分布式系统能够模拟单一强大计算设备的效果，进而提高整体计算效率并加速训练过程[1]。",
      "ground_context": [
        "一个模型训练任务往往会有大量的训练样本作为输入，可以利用一个计算设备完成，也可以将整个模型的训练任务拆分成多个子任务，分发给不同的计算设备，实现并行计算。此后，还需要对每个计算设备的输出进行合并，最终得到与单个计算设备等价的计算结果。",
        "由于同一个服务器内部的多个计算设备之间可能并不共享内存，因此无论这些计算设备是处于一个服务器还是多个服务器中，其系统架构都属于分布式系统范畴。"
      ],
      "retrieved_contexts": [
        "GPU） 、张量处理器（Tensor Processing Unit ，TPU） ，也可以是神经网络处理器（Neural network\nProcessing Unit，NPU） 。由于同一个服务器内部的多个计算设备之间可能并不共享内存，因此无\n论这些计算设备是处于一个服务器还是多个服务器中，其系统架构都属于分布式系统范畴。一个\n模型训练任务往往会有大量的训练样本作为输入，可以利用一个计算设备完成，也可以将整个模\n型的训练任务拆分成多个子任务，分发给不同的计算设备，实现并行计算。此后，还需要对每个\n计算设备的输出进行合并，最终得到与单个计算设备等价的计算结果。由于每个计算设备只需要\n负责子任务，并且多个计算设备可以并行执行，因此其可以更快速地完成整体计算，并最终实现\n对整个计算过程的加速。\n促使人们设计分布式训练系统的一个最重要的原因是单个计算设备的算力已经不足以支撑模\n型训练。 图4.2 给出了机器学习模型对于算力的需求以及同期单个计算设备能够提供的算力。 机器\n学习模型快速发展，从 2013 年 AlexNet 被提出开始，到 2022 年拥有 5400 亿个参数的 PaLM 模型...",
        "100 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n加，这些问题变得更加突出。同时，当大型集群进行长时间训练时，设备故障可能会影响或中断\n训练，对分布式系统的问题处理也提出了很高的要求。\n4.2 分布式训练的并行策略\n分布式训练系统的目标是将单节点模型训练转换成等价的分布式并行模型训练。对于大语言\n模型来说，训练过程就是根据数据和损失函数，利用优化算法对神经网络模型参数进行更新的过\n程。 单个计算设备模型训练系统的结构如图4.3 所示， 其主要由数据和模型两个部分组成。 训练过\n程由多个数据小批次（Mini-batch）完成。图中数据表示一个数据小批次。训练系统会利用数据小\n批次根据损失函数和优化算法计算梯度，从而对模型参数进行修正。针对大语言模型多层神经网\n络的执行过程，可以由一个 计算图（Computational Graph ）表示。这个图有多个相互连接的算子\n（Operator） ，每个算子实现一个神经网络层（Neural Network Layer ） ，而参数则代表了这个层在训\n练中所更新的权重。\nा䃎ツ \nा \n䃎ツ \n图 4.3 单个计算设备模型训练系统的..."
      ],
      "response_time": 5.289945840835571,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.10653844945566907,
        "semantic_similarity": 0.885193407535553
      }
    },
    {
      "sample_id": "0055",
      "question": "什么是训练的批量大小？",
      "ground_truth": "GLOBAL_BATCH_SIZE",
      "predicted_answer": "在训练过程中，批量大小指的是每次从训练集中抽取用于计算梯度和更新模型参数的数据样本数量。具体来说，在文档中提到的 PPO 小批量大小 `ppo_mini_batch_size` 设置为 64，表示每次从训练集中选取 64 个样本组成一个小批量[2]。而 `log_prob_micro_batch_size_per_gpu` 则指每个 GPU 上用于计算对数概率的微批量大小，在这里设置为 8，表示每个 GPU 每次处理 8 个样本的微批量[1]。",
      "ground_context": [
        "\"train_batch_size\": GLOBAL_BATCH_SIZE"
      ],
      "retrieved_contexts": [
        "避免因数据量过大导致的显存溢出问题。 log_prob_micro_batch_size_per_gpu 表示每个 GPU\n上用于计算对数概率的微批次大小。在强化学习中，对数概率用于计算策略梯度，它反映了模型\n在当前策略下采取某个动作的概率。将这个参数设置为 8，即每个 GPU 在计算对数概率时，每次\n处理 8 个样本的微批次，这样可以优化计算过程，提高训练效率。",
        "大规模语言模型：从理论到实践 219\n动相关领域的发展，也为开发者在大模型强化学习领域的创新提供了支持。本节将介绍使用 verl\n框架进行大模型中强化学习的实践。\n1. 训练脚本与参数配置\n以推理任务为例，我们按照官方教程选用 Qwen2.5-0.5B-Instruct 模型在 GSM8K 数据集上进\n行强化学习训练。下面是一些关键的 RL 训练参数：\n在近端策略优化算法中，ppo_mini_batch_size 表示小批次的大小。在训练过程中，我们并\n不会一次性使用整个训练集来更新模型参数，而是将训练集划分为多个小批次。这个参数设置\n为 64，意味着每次从训练集中选取 64 个样本组成一个小批次，用于计算梯度和更新演员模型\n的参数。通过使用小批次，可以减少内存的占用，并且在一定程度上提高训练的稳定性和效率。\nppo_micro_batch_size_per_gpu 指的是每个 GPU 上的微批次大小。在多 GPU 训练环境下，为\n了更高效地利用 GPU 资源， 会将小批次进一步划分为微批次。 这里设置为4， 表示每个GPU 每次\n处理 4 个样本的微批次。 这种细粒度的划分有助于在G..."
      ],
      "response_time": 5.75213098526001,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.1739130434782609,
        "rouge2": 0.09523809523809525,
        "rougeL": 0.1739130434782609,
        "bleu": 0.0035097590837790244,
        "semantic_similarity": 0.4929960370063782
      }
    },
    {
      "sample_id": "0056",
      "question": "GLOBAL_BATCH_SIZE定义了什么？",
      "ground_truth": "GLOBAL_BATCH_SIZE定义了全局的批次大小。",
      "predicted_answer": "GLOBAL_BATCH_SIZE定义了全局的批次大小，这通常是所有GPU加起来的总批次大小。[1]",
      "ground_context": [
        "GLOBAL_BATCH_SIZE定义了全局的批次大小。这通常是所有 GPU 加起来的总批次大小。"
      ],
      "retrieved_contexts": [
        "大规模语言模型：从理论到实践 139\n4. DeepSpeed 设置\n在配置代码的开始， 定义了两个关键参数GLOBAL_BA TCH_SIZE和 MICRO_BA TCH_ SIZE。\nGLOBAL_BA TCH_SIZE定义了全局的批次大小。这通常是所有 GPU 加起来的总批次大小。MI-\nCRO_BA TCH_SIZE定义了每块 GPU 上的微批次大小。因为微批次处理每次只加载并处理一小部\n分数据，所以可以帮助大语言模型在有限的 GPU 内存中运行。训练配置函数 get_train_ds_config\n主要包括以下内容。\n（1）ZeRO 优化配置：ZeRO 是 DeepSpeed 提供的一种优化策略，旨在减少训练中的冗余并加\n速模型的训练。其中的参数，如 offload_param 和 offload_optimizer，允许用户选择是否将模型参\n数或优化器状态卸载到 CPU。\n（2）混合精度训练：通过设置 FP16 字段，使模型可以使用 16 位浮点数进行训练，加速训练\n过程并减少内存使用。\n（3）梯度裁剪：通过 gradient_clipping 字段，可以防止训练过程中出现..."
      ],
      "response_time": 3.504617214202881,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.7499999999999999,
        "rouge2": 0.6666666666666666,
        "rougeL": 0.7499999999999999,
        "bleu": 0.3979182783756331,
        "semantic_similarity": 0.9181808233261108
      }
    },
    {
      "sample_id": "0057",
      "question": "ZeRO-3 配置参数的 stage 是多少？",
      "ground_truth": "3",
      "predicted_answer": "ZeRO-3 配置参数的 stage 设置为 3。[1]",
      "ground_context": [
        "以下是 DeepSpeed 使用 ZeRO-3 配置参数的样例：{\"zero_optimization\": {\"stage\": 3,},"
      ],
      "retrieved_contexts": [
        "大规模语言模型：从理论到实践 133\n以下是 DeepSpeed 使用 ZeRO-3 配置参数的样例：\n{\n\"zero_optimization\": {\n\"stage\": 3,\n},\n\"fp16\": {\n\"enabled\": true\n},\n\"optimizer\": {\n\"type\": \"AdamW\",\n\"params\": {\n\"lr\": 0.001,\n\"betas\": [\n0.8,\n0.999\n],\n\"eps\": 1e-8,\n\"weight_decay\": 3e-7\n}\n},\n...\n}\n如果希望在 ZeRO-3 的基础上继续使用 ZeRO-Infinity 将优化器状态和计算转移到 CPU 中， 则\n可以在配置文件中按照如下方式配置：\n{\n\"zero_optimization\": {\n\"stage\": 3,\n\"offload_optimizer\": {\n\"device\": \"cpu\"\n}\n},\n...\n}\n甚至可以进一步将模型参数也装载到 CPU 内存中，在配置文件中按照如下方式配置："
      ],
      "response_time": 2.79128098487854,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.33333333333333337,
        "rouge2": 0.0,
        "rougeL": 0.33333333333333337,
        "bleu": 0.01758542189440898,
        "semantic_similarity": 0.45186352729797363
      }
    },
    {
      "sample_id": "0058",
      "question": "如何初始化进程组？",
      "ground_truth": "dist.init_process_group(\"gloo\", rank =rank, world_size =world_size)",
      "predicted_answer": "在初始化进程中，使用 `deepspeed.init_distributed()` 函数实现每个进程与其他进程的同步，从而初始化分布式环境。[1]",
      "ground_context": [
        "dist.init_process_group(\"gloo\", rank =rank, world_size =world_size)"
      ],
      "retrieved_contexts": [
        "142 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n5. DeepSpeed 初始化\n设置 DeepSpeed 的配置参数后， 可以利用DeepSpeed 进行模型训练的初始化， 初始化流程如下。\n（1）确定运行的设备：首先，检查代码是否有指定的本地 GPU（通过 args.local_rank） 。如果\n没有指定，则程序默认使用 CUDA 设备。否则，它会为进程设置指定的 GPU。\n（2） 初始化分布式后端： 在分布式训练中， 使用deepspeed.init_distributed() 函数实现每个进程\n与其他进程的同步，初始化分布式环境。\n（3）获取当前进程的全局排序：在分布式训练中，使用torch.distributed.get_rank() 函数获得每\n个进程的唯一排序或 ID。\n（4）设置 DeepSpeed 配置：根据用户参数（如是否进行 offload、使用哪个 Zero Stage 等）构\n建一个 DeepSpeed 配置字典，来决定训练设置。\n（5） 同步所有工作进程： 使用torch.distributed.barrier() 确保在进一步的初始化之..."
      ],
      "response_time": 3.5390055179595947,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.13333333333333333,
        "rouge2": 0.0,
        "rougeL": 0.13333333333333333,
        "bleu": 0.04343820806912152,
        "semantic_similarity": 0.5303052067756653
      }
    },
    {
      "sample_id": "0059",
      "question": "DeepSpeed 支持哪些类型的 ZeRO 分片机制？",
      "ground_truth": "ZeRO-0、ZeRO-1、ZeRO-2、ZeRO-3 以及 ZeRO-Infinity。",
      "predicted_answer": "DeepSpeed 支持以下类型的 ZeRO 分片机制：ZeRO-0、ZeRO-1、ZeRO-2、ZeRO-3 以及 ZeRO-Infinity [1][2]。",
      "ground_context": [
        "DeepSpeed 中也支持多种类型 ZeRO 的分片机制，包括ZeRO-0、ZeRO-1、ZeRO-2、ZeRO-3 以及 ZeRO-Infinity。"
      ],
      "retrieved_contexts": [
        "132 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n{\n\"optimizer\": {\n\"type\": \"OneBitAdam\",\n\"params\": {\n\"lr\": 0.001,\n\"betas\": [\n0.8,\n0.999\n],\n\"eps\": 1e-8,\n\"weight_decay\": 3e-7,\n\"freeze_step\": 400,\n\"cuda_aware\": false,\n\"comm_backend_name\": \"nccl\"\n}\n}\n...\n}\nDeepSpeed 中也支持多种类型 ZeRO 的分片机制， 包括ZeRO-0、ZeRO-1、ZeRO-2、ZeRO-3 以及\nZeRO-Infinity。ZeRO-0 禁用所有类型的分片， 仅将DeepSpeed 当作分布式数据并行使用；ZeRO-1\n对优化器状态进行分片，占用内存为原始的 1/4，通信容量与数据并行性相同；ZeRO-2 对优化器\n状态和梯度进行分片，占用内存为原始的 1/8，通信容量与数据并行性相同；ZeRO-3 对优化器状\n态、梯度及模型参数进行分片，内存减少与数据并行度和复杂度成线性关系，同时通信容...",
        "数）和 MT-NLG[134]（5400 亿个参数）都采用 DeepSpeed 框架完成训练。\nDeepSpeed 的主要优势在于支持大规模神经网络模型、提供了更多的优化策略和工具。Deep-\nSpeed 通过实现三种并行方法的灵活组合， 即ZeRO 支持的数据并行、 流水线并行和张量并行， 可\n以应对不同工作负载的需求。 特别是通过3D 并行性的支持，DeepSpeed 可以处理具有万亿个参数\n的超大规模模型。DeepSpeed 还引入了 ZeRO-Offload，使单个 GPU 能够训练比其显存容量大 10\n倍的模型。为了充分利用 CPU 和 GPU 的内存来训练大语言模型， DeepSpeed 还扩展了 ZeRO-2。\n此外，DeepSpeed 还提供了稀疏注意力核 （Sparse Attention Kernel） ， 支持处理包括文本、 图像和语\n音等长序列输入的模型。DeepSpeed 还集成了 1 比特 Adam 算法（1-bit Adam） ，该算法可以只使\n用原始 Adam 算法 1/5 的通信量，达到与 Adam 类似的收敛率，显著提高分布式训练的效率，降\n低通信开...",
        "态的大小对其进行排序，然后根据每个 GPU 的剩余内存大小将张量分发到不同的 GPU。分布遵\n循一个原则：剩余内存较大的 GPU 优先接收新的分布张量。\n通过这种方式，可以将张量的缩放因子与张量一并顺利分发，同时降低通信和计算复杂性。\n图10.12展示了在包含和不包含缩放因子的情况下，ZeRO 张量分片方式的差异。ZeRO 张量分区方\n式可以分为两种：有缩放因子和无缩放因子。左图展示了原始的高精度 ZeRO 方法，其中一个张\n量被分割成多个分区后分配到不同的设备上。右图展示了提出的 FP8 ZeRO 方法，该方法将每个\n张量的完整副本分配到设备上，同时保留并考虑张量的缩放因子。",
        "大规模语言模型：从理论到实践 133\n以下是 DeepSpeed 使用 ZeRO-3 配置参数的样例：\n{\n\"zero_optimization\": {\n\"stage\": 3,\n},\n\"fp16\": {\n\"enabled\": true\n},\n\"optimizer\": {\n\"type\": \"AdamW\",\n\"params\": {\n\"lr\": 0.001,\n\"betas\": [\n0.8,\n0.999\n],\n\"eps\": 1e-8,\n\"weight_decay\": 3e-7\n}\n},\n...\n}\n如果希望在 ZeRO-3 的基础上继续使用 ZeRO-Infinity 将优化器状态和计算转移到 CPU 中， 则\n可以在配置文件中按照如下方式配置：\n{\n\"zero_optimization\": {\n\"stage\": 3,\n\"offload_optimizer\": {\n\"device\": \"cpu\"\n}\n},\n...\n}\n甚至可以进一步将模型参数也装载到 CPU 内存中，在配置文件中按照如下方式配置："
      ],
      "response_time": 3.517545700073242,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.8333333333333333,
        "rouge2": 0.8181818181818181,
        "rougeL": 0.8333333333333333,
        "bleu": 0.5283419944690572,
        "semantic_similarity": 0.7867786288261414
      }
    },
    {
      "sample_id": "0060",
      "question": "What optimizer class is used in the document?",
      "ground_truth": "torch.optim.Adam",
      "predicted_answer": "根据提供的文档，我无法回答这个问题。",
      "ground_context": [
        "optimizer_class=torch.optim.Adam, # ЇልਔAdam"
      ],
      "retrieved_contexts": [],
      "response_time": 1.6868870258331299,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.0,
        "semantic_similarity": 0.4491402208805084
      }
    },
    {
      "sample_id": "0061",
      "question": "如何在使用 ZeRO-3 的基础上配置优化器以便将状态转移到 CPU 中，并且选择适当的优化器类型？",
      "ground_truth": "在使用 ZeRO-3 的基础上，可以通过在配置文件中设置 'offload_optimizer' 为 'cpu' 来将优化器状态转移到 CPU 中。同时，可以根据是否选择了 offload 选项来决定使用 DeepSpeedCPUAdam 还是 FusedAdam 作为优化器。",
      "predicted_answer": "要在使用 ZeRO-3 的基础上配置优化器以便将状态转移到 CPU 中，您可以在配置文件中设置 `offload_optimizer` 参数并将 `device` 设置为 `\"cpu\"`，具体配置如下所示：\n\n```json\n{\n    \"zero_optimization\": {\n        \"stage\": 3,\n        \"offload_optimizer\": {\n            \"device\": \"cpu\"\n        }\n    },\n    ...\n}\n```\n\n为了选择合适的优化器类型，您可以考虑以下几种情况。如果需要使用传统的 AdamW 优化器并且不特别需要 OneBitAdam 的特性，则可以直接沿用文档1中的配置。如果需要利用 OneBitAdam 优化器的特点，比如冻结步数等特性，则可以参考文档2中的配置。因此，您的选择应基于具体的项目需求来决定。例如，如果您不需要 OneBitAdam 的特殊功能，可以选择文档1中的配置；若需要这些功能，则采用文档2中的配置。[1][2]",
      "ground_context": [
        "\"zero_optimization\": { \"stage\": 3, \"offload_optimizer\": { \"device\": \"cpu\" } }",
        "AdamOptimizer = DeepSpeedCPUAdam if args.offload else FusedAdam"
      ],
      "retrieved_contexts": [
        "大规模语言模型：从理论到实践 133\n以下是 DeepSpeed 使用 ZeRO-3 配置参数的样例：\n{\n\"zero_optimization\": {\n\"stage\": 3,\n},\n\"fp16\": {\n\"enabled\": true\n},\n\"optimizer\": {\n\"type\": \"AdamW\",\n\"params\": {\n\"lr\": 0.001,\n\"betas\": [\n0.8,\n0.999\n],\n\"eps\": 1e-8,\n\"weight_decay\": 3e-7\n}\n},\n...\n}\n如果希望在 ZeRO-3 的基础上继续使用 ZeRO-Infinity 将优化器状态和计算转移到 CPU 中， 则\n可以在配置文件中按照如下方式配置：\n{\n\"zero_optimization\": {\n\"stage\": 3,\n\"offload_optimizer\": {\n\"device\": \"cpu\"\n}\n},\n...\n}\n甚至可以进一步将模型参数也装载到 CPU 内存中，在配置文件中按照如下方式配置：",
        "132 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n{\n\"optimizer\": {\n\"type\": \"OneBitAdam\",\n\"params\": {\n\"lr\": 0.001,\n\"betas\": [\n0.8,\n0.999\n],\n\"eps\": 1e-8,\n\"weight_decay\": 3e-7,\n\"freeze_step\": 400,\n\"cuda_aware\": false,\n\"comm_backend_name\": \"nccl\"\n}\n}\n...\n}\nDeepSpeed 中也支持多种类型 ZeRO 的分片机制， 包括ZeRO-0、ZeRO-1、ZeRO-2、ZeRO-3 以及\nZeRO-Infinity。ZeRO-0 禁用所有类型的分片， 仅将DeepSpeed 当作分布式数据并行使用；ZeRO-1\n对优化器状态进行分片，占用内存为原始的 1/4，通信容量与数据并行性相同；ZeRO-2 对优化器\n状态和梯度进行分片，占用内存为原始的 1/8，通信容量与数据并行性相同；ZeRO-3 对优化器状\n态、梯度及模型参数进行分片，内存减少与数据并行度和复杂度成线性关系，同时通信容..."
      ],
      "response_time": 7.179154396057129,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.39999999999999997,
        "rouge2": 0.24242424242424243,
        "rougeL": 0.3428571428571429,
        "bleu": 0.0799771286088748,
        "semantic_similarity": 0.8881768584251404
      }
    },
    {
      "sample_id": "0062",
      "question": "在使用分布式训练时，如何选择合适的数据采样器？",
      "ground_truth": "在分布式训练环境中，需要使用 DistributedSampler 确保每个进程或节点获得数据的一个不重复的子集，而在单机环境中则可以使用普通的随机或顺序采样器。",
      "predicted_answer": "在使用分布式训练时，应选择 `DistributedSampler` 进行数据采样。该采样器适用于分布式训练环境，能够实现数据的有效划分与分配，从而支持高效的并行训练过程。[1]",
      "ground_context": [
        "使用 PyTorch 和 transformers 库来设置预训练模型的数据加载器， 以实现在单机或多机分布式训练环境中对数据的加载和采样。",
        "通过检查 args.local_rank 是否为 −1，代码会选择使用普通的采样器（单机）还是分布式采样器 （多机） 。DistributedSampler 确保在分布式训练环境中， 每个进程或节点都能获得数据的一个不重复的子集。"
      ],
      "retrieved_contexts": [
        "大规模语言模型：从理论到实践 135\npip install deepspeed\n1. 训练数据配置\n使用 PyTorch 和 transformers 库来设置预训练模型的数据加载器， 以实现在单机或多机分布式\n训练环境中对数据的加载和采样。需要导入的模块如下。\n• DataLoader 是 PyTorch 提供的工具，用于从数据集加载数据到模型进行训练或评估。\n• RandomSampler 和 SequentialSampler 是 PyTorch 提供的两种采样器。RandomSampler 随机采\n样数据，而 SequentialSampler 顺序采样数据。\n• DistributedSampler 是用于分布式训练的数据采样器。\n• default_data_collator 是 transformers 库提供的默认数据收集器，用于将多个样本整合为一个\n批量数据。\n• create_pretrain_dataset 是一个自定义函数，用于创建预训练数据集。\n通过检查 args.local_rank 是否为 −1，代码会选择使用普通的采样器（单机）还是分布式采样\n器 （多...",
        "重复的子集，这使得分布式训练变为可能。而在单机环境中，使用常规的随机或顺序采样器即可。\n具体代码如下所示："
      ],
      "response_time": 4.108269929885864,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.6666666666666666,
        "rouge2": 0.0,
        "rougeL": 0.6666666666666666,
        "bleu": 0.04717450626232371,
        "semantic_similarity": 0.8603115081787109
      }
    },
    {
      "sample_id": "0063",
      "question": "在使用参数服务器架构进行模型训练时，如何确保模型能够正确处理文本的长度和填充设置？",
      "ground_truth": "在参数服务器架构中，训练服务器需要提供大量计算资源，同时在加载和配置模型时，需要确保词元分析器能够处理各种文本的长度，并进行适当的填充设置。",
      "predicted_answer": "在使用参数服务器架构进行模型训练时，虽然文档中未详细说明具体的填充设置方法，但可以参考对LLaMA模型的处理方式来确保模型能够正确处理文本的长度和填充设置。具体来说，在加载LLaMA模型时，如果词元分析器尚未指定填充符号，则应将其设置为[PAD]，并将填充行为设置为在句子右侧进行[1]。同时，还需为模型配置设置结束符号和填充符号的ID，以及调整模型的词汇表嵌入大小使其成为8的倍数。这样可以确保模型在处理不同长度的文本时能正确应用填充和截断操作[1]。",
      "ground_context": [
        "参数服务器需要提供充足的内存资源和通信资源，训练服务器需要提供大量的计算资源。",
        "使用 transformers 库加载和配置 LLaMA 模型及其相关的词元分析器，从 transformers 库中导入 LLaMA 模型、相应的词元分析器和模型配置后，使用from_pretrained 方法加载预训练的 LLaMA 模型、词元分析器和配置。"
      ],
      "retrieved_contexts": [
        "2. 模型载入\n使用 transformers 库加载和配置 LLaMA 模型及其相关的词元分析器。从 transformers 库中导\n入 LLaMA 模型、 相应的词元分析器和模型配置后， 使用from_pretrained 方法加载预训练的 LLaMA\n模型、词元分析器和配置。为了确保词元分析器可以处理各种文本的长度，还需要进行填充设置。\n如果词元分析器还没有指定填充符号，则将其设置为 [PAD]，并确定填充行为发生在句子的右侧。\n此外，为了保证模型能够正确地处理句子结束和填充，还为模型配置设置了结束符号和填充符号\n的 ID。最后，为了优化模型在硬件上的性能，还需要调整模型的词汇表嵌入大小，使其成为8 的\n倍数。 通过这些步骤， 可以成功地加载并配置LLaMA 模型， 为后续的训练任务做好准备。 具体代\n码如下：",
        "大规模语言模型：从理论到实践 123\n参数服务器架构的分布式训练过程可以细分为同步训练和异步训练两种模式。\n• 同步训练： 训练服务器在完成一个小批次的训练后， 将梯度推送给参数服务器。 参数服务器\n在收到所有训练服务器的梯度后，进行梯度聚合和参数更新。\n• 异步训练： 训练服务器在完成一个小批次的训练后， 将梯度推送给参数服务器。 参数服务器\n不再等待接收所有训练服务器的梯度，而是直接基于已收到的梯度进行参数更新。\n在同步训练的过程中，参数服务器会等待所有训练服务器完成当前小批次的训练，有诸多的\n等待或同步机制，导致整个训练速度较慢。异步训练去除了训练过程中的等待机制，训练服务器\n可以独立进行参数更新， 极大地加快了训练速度。 引入异步更新的机制会导致训练效果有所波动。\n应根据具体情况和需求选择适合的训练模式。\n4.3.3 去中心化架构\n去中心化 （Decentralized Network ）架构采用集合通信实现分布式训练系统。在去中心化\n架构中，没有中央服务器或控制节点，而是由节点之间进行直接通信和协调。这种架构的好处\n是可以减少通信瓶颈，提高系统的可扩展性。由于节点之间可以并行...",
        "122 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n图 4.19 NVIDIA HGX H100 8-GPU NVLink 和 NVSwitch 连接框图 [167]\n4.3.2 参数服务器架构\n参数服务器（Parameter Server，PS）架构的分布式训练系统中有两种服务器角色：训练服务\n器和参数服务器。参数服务器需要提供充足的内存资源和通信资源，训练服务器需要提供大量的\n计算资源。图4.20 为参数服务器的分布式训练集群的示意图。该集群包括两个训练服务器和两个\n参数服务器。 假设有一个可分为两个参数分区的模型， 每个分区由一个参数服务器负责参数同步。\n在训练过程中， 每个训练服务器都拥有完整的模型， 将分配到此服务器的训练数据集切片 （Dataset\nShard）并进行计算，将得到的梯度推送到相应的参数服务器。参数服务器会等待两个训练服务器\n都完成梯度推送，再计算平均梯度并更新参数。之后，参数服务器会通知训练服务器拉取最新的\n参数，并开始下一轮训练迭代。\n图 4.20 参数服务器的分布式训练集群的示意图 [167]"
      ],
      "response_time": 6.464420795440674,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.041724470174958984,
        "semantic_similarity": 0.7719961404800415
      }
    },
    {
      "sample_id": "0064",
      "question": "如何计算训练中的 TFLOPs?",
      "ground_truth": "train_tflops = train_flops_per_iteration / (e2e_time * gpus_per_model * (10**12))",
      "predicted_answer": "训练中的 TFLOPs 计算公式为：\\[ \\text{TFLOPs} = \\frac{\\text{train\\_flops\\_per\\_iteration}}{\\text{e2e\\_time} \\times \\text{gpus\\_per\\_model} \\times 10^{12}} \\] 其中，`train_flops_per_iteration` 是每次迭代的浮点运算次数，`e2e_time` 是端到端时间（秒），`gpus_per_model` 是每种模型使用的 GPU 数量。[1]",
      "ground_context": [
        "train_tflops = train_flops_per_iteration / (e2e_time * gpus_per_model * (10**12))"
      ],
      "retrieved_contexts": [
        "checkpoint_activations_factor, batch_size, seq_length, hf_config)\ntrain_tflops = train_flops_per_iteration / (e2e_time * gpus_per_model *\n(10**12))\nparam_string = f\"{params_in_billions:.3f} B\" if params_in_billions != 0 else \"NA\"\nprint(\nf\"Model Parameters: {param_string}, Latency: {e2e_time:.2f}s,\nTFLOPs: {train_tflops:.2f}, Samples/sec: {samples_per_second:.2f},\nTime/seq {e2e_time/batch_size:.2f}s, Batch Size: {batch_size},\nSequence Length: {seq_length}\"\n)\ndef save_hf_format(model, tokenizer, a..."
      ],
      "response_time": 5.427295923233032,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.5909090909090909,
        "rouge2": 0.4285714285714285,
        "rougeL": 0.5454545454545454,
        "bleu": 0.09401052130353031,
        "semantic_similarity": 0.837751567363739
      }
    },
    {
      "sample_id": "0065",
      "question": "在生成任务输入和输出的步骤中，如何处理非分类任务？",
      "ground_truth": "对于非分类任务，使用输入优先的方法，先根据任务产生输入，再根据任务指令和输入生成输出。",
      "predicted_answer": "在生成任务输入和输出的步骤中，对于非分类任务，采用“输入优先”的方法：首先根据任务生成输入，然后基于任务指令和生成的输入来生成输出。[1]",
      "ground_context": [
        "对于非分类任务，使用输入优先的方法，先根据任务产生输入，再根据任务指令和输入生成输出。"
      ],
      "retrieved_contexts": [
        "154 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n分类。\n步骤 3：生成任务输入和输出\n通过步骤 1，语言模型已经生成了面向新任务的指令，然而指令数据中还没有相应的输入和\n输出。 本步骤将为此前生成的指令生成输入和输出， 让指令数据变得完整。 与之前的步骤相同， 本\n步骤同样使用语境学习，使用来自其他任务的“指令” “输入” “输出”上下文示例做提示，预训\n练模型就可以为新任务生成输入–输出对。针对不同的任务类别， 分别使用“输入优先”或“输出\n优先”方法：对于非分类任务，使用输入优先的方法，先根据任务产生输入，再根据任务指令和\n输入生成输出；而对于分类任务，为了避免模型过多地生成某些特定类别的输入（而忽略其他的\n类别） ， 使用输出优先的方法， 先产生所有可能的输出标签， 再根据任务指令和输出， 补充相应的\n输入。\n“输入优先”提示模板如下所示：\nCome up with examples for the following tasks. Try to generate multiple examples when possible. If\n,→ the task d...",
        "Task 5: {instruction for existing task 5}\nTask 6: {instruction for existing task 6}\nTask 7: {instruction for existing task 7}\nTask 8: {instruction for existing task 8}\nTask 9:\n步骤 2：确定指令是否代表分类任务\n由于后续对于分类任务和非分类任务有两种不同的处理方法，因此需要在本步骤对指令是否\n为分类任务进行判断，同样是利用拼接几个上下文示例的方法让模型自动判断任务类型是否是"
      ],
      "response_time": 4.037906885147095,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.16676755647470037,
        "semantic_similarity": 0.9509384036064148
      }
    },
    {
      "sample_id": "0066",
      "question": "指令微调在模型获取各类关键能力的进程中发挥着什么作用？",
      "ground_truth": "不可或缺的作用",
      "predicted_answer": "指令微调在模型获取各类关键能力的进程中发挥着不可或缺的作用，它能使模型具备指令遵循（Instruction Following）能力，能够完成各类预先设计的任务，并且可以在零样本情况下处理诸多下游任务。[2]",
      "ground_context": [
        "然而，指令微调在模型获取各类关键能力的进程中却发挥着不可或缺的作用。"
      ],
      "retrieved_contexts": [
        "据构造的难度。\n5.1.4 指令微调训练策略\n尽管从整体流程来看，指令微调的步骤并不繁杂，其训练代码甚至与预训练阶段的代码大体\n相同，然而，指令微调在模型获取各类关键能力的进程中却发挥着不可或缺的作用。此外，开源",
        "5. 指令微调\n指令微调又称有监督微调，是指在预训练大语言模型的基础上，通过使用有标注的自然语言\n形式的数据， 对模型参数进行微调， 使模型具备指令遵循（Instruction Following） 能力， 能够完成\n各类预先设计的任务，并可以在零样本情况下处理诸多下游任务。经过海量数据预训练后的语言\n模型虽然具备了大量的 “知识” ， 但是由于其训练时的目标仅是进行下一个词的预测， 因此不能够\n理解并遵循人类自然语言形式的指令。为了使模型具有理解并响应人类指令的能力，还需要使用\n指令数据对其进行调整。如何构造指令数据，如何高效低成本地进行指令微调训练，以及如何在\n语言模型基础上进一步扩大上下文等问题，是大语言模型在指令微调阶段的核心。\n本章先介绍大语言模型指令微调训练方法，在此基础上介绍高效模型微调及模型上下文窗口\n扩展方法，最后介绍指令微调的代码实践。\n5.1 指令微调训练\n指令微调具体训练过程并不复杂，主要分为如下三个步骤： （ 1）针对每一项任务去明确地\n定义相应的自然语言形式的指令或者提示，这些指令或提示对任务目标以及输出要求进行清晰描\n述； （2）把训练数据调整成包含指令以...",
        "练数据对预训练模型进行微调操作。从训练方式的角度来看，指令微调与预训练大体上较为相似，\n不过指令微调的目标函数往往只是针对输出部分来计算损失。\n本节将从指令微调数据构造、数据评估和影响、指令微调训练策略以及开源指令微调数据等\n方面进行介绍。\n5.1.1 指令微调数据\n指令微调数据通常由文本对构成，包含“指令输入”与“答案输出”两个关键部分。 “指令输\n入” ， 是指人们向模型提出的各类请求， 包含定义精准、 清晰的指令或者提示信息， 其核心作用在\n于详细阐释任务的目标究竟是什么，以及明确规定输出需要满足的各项要求。指令涵盖的范畴极\n为广泛，包括问题回答、信息分类、内容总结、文本改写等。 “答案输出” ，则是期望模型依据所",
        "下游任务进行微调。 这些模型参数量大都在10 亿个以上， 由于微调的计算量很大， 这类模型的影\n响力在当时相较 BERT 类模型有不小的差距。\n能力探索阶段集中于 2019 年至 2022 年，由于大语言模型很难针对特定任务进行微调，研究\n人员开始探索在不针对单一任务进行微调的情况下如何发挥大语言模型的能力。2019 年，Radford\n等人在文献 [11] 中使用 GPT-2 模型研究了大语言模型在零样本情况下的任务处理能力。 在此基础\n上，Brown 等人在 GPT-3[13] 模型上研究了通过语境学习进行少样本学习的方法，将不同任务的少\n量有标注的实例拼接到待分析的样本之前输入语言模型，语言模型根据实例理解任务并给出正确\n的结果。基于 GPT-3 的语境学习在 TriviaQA、WebQS、CoQA 等评测集合中都展示出了非常强的\n能力，在有些任务中甚至超过了此前的有监督方法。上述方法不需要修改语言模型的参数，模型\n在处理不同任务时无须花费大量计算资源进行模型微调。仅依赖语言模型本身，其性能在很多任\n务上仍然很难达到有监督学习 （Supervised Learning） 的效果，...",
        "（1）指令微调：使用精选的人类回答来微调预训练语言模型以应对各种查询。\n（2）奖励模型微调：使用一个包含人类对同一查询的多个答案打分的数据集来训练一个独立\n的奖励模型。\n（3）基于人类反馈的强化学习（Reinforcement Learning from Human Feedback ，RLHF）训练：\n利用近端策略优化（Proximal Policy Optimization，PPO） 算法， 根据奖励模型的奖励反馈进一步微\n调 SFT 模型。\n本节只针对步骤 （1） 指令微调的实践进行介绍， 对于奖励模型微调和RLHF 训练的实践会在\n后续对应章节中详细介绍。\n图 5.10 基于 DeepSpeed-Chat 训练类 ChatGPT 对话模型的三个步骤 [244]\nDeepSpeed-Chat 具有以下三大核心功能。"
      ],
      "response_time": 4.332461833953857,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.024128958361671914,
        "semantic_similarity": 0.5471594929695129
      }
    },
    {
      "sample_id": "0067",
      "question": "Flan 2022 在构建过程中使用了哪些技术？",
      "ground_truth": "任务混合和输入反转等技术。",
      "predicted_answer": "Flan 2022 在构建过程中使用了任务混合和输入反转等技术[1]。具体来说，任务混合将来自不同任务的示例混合在一起进行训练；而输入反转是指将原始输入中的某些元素或部分进行反转或重新排列，以生成新的输入，从而增强模型的泛化能力和鲁棒性。",
      "ground_context": [
        "Flan 2022 构建过程中还使用了任务混合和输入反转等技术。"
      ],
      "retrieved_contexts": [
        "了四个不同的指令输入模板，包括零样本、少量样本、CoT 模板。Flan 2022 构建过程中还使用了\n任务混合和输入反转等技术。输入反转（Input Inversion）是指将原始输入中的某些元素或部分进\n行反转或重新排列，以生成新的输入，用于增强模型的泛化能力和鲁棒性。例如，在对话任务中，\n将对话历史中的上下文和响应进行反转，以测试模型在不同输入顺序下的表现。在代码生成任务\n中，可以将代码和问题进行反转，在链式推理任务（Chain-of-Thought，CoT）中，将查询、答案\n和解释进行反转。任务混合（Task Mixing）则将来自不同任务的示例混合在一起进行训练，其目\n标旨在增强模型的泛化能力和适应不同任务的能力。\n文献 [188] 针对提升大语言模型在开放领域命名实体识别中的能力进行了研究。通过整合 54\n个现有的中英文命名实体识别数据集， 并经过两步规范化， 构建了B2NERD 数据集。 研究指出， 整\n合多个现有数据集的主要挑战在于实体定义的不一致性和模糊性。 例如， 有些数据集会区分 “时代\n广场” 这样的地点和 “巴黎” 这样的地缘政治实体， 而另一些数据集则将两者统...",
        "大规模语言模型：从理论到实践 151\n源数据集的质量情况，同时还要对数据的格式进行标准化处理，这涉及多方面细致的工作以及对\n不同数据原有特点的把握等，操作起来较为复杂且容易出现遗漏等情况。此外，大部分已有数据\n集都是为传统自然语言处理任务准备，并没有包含多样性的提示词，如何构造大量多样性且语义\n相同的提示词也是需要解决的难点。目前已经很多指令微调数据集采用这种方式进行构建。\nOIG（Open Instruction Generation）[184] 是一个大型指令微调数据集， 由LAION 社区成员创建，\n包含 30 个数据集和 4300 万条指令，包含使用来自多种数据源的数据增强创建的指令。它不仅涵\n盖标准数据集（如 Natural Questions 和 Natural Instructions） ，还涵盖与对话、总结、教育等相关的\n数据。Flan 2022[185] 数据集则是由五个部分组成， 分别是Flan 2021[186]、T0[16]、SUPER-NA TURAL\nINSTRUCTIONS[187]） 、CoT 数据集和对话数据集。它涵盖了多达 1836 个数据集。每个指..."
      ],
      "response_time": 4.017458200454712,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.08247524661589911,
        "semantic_similarity": 0.7816540598869324
      }
    },
    {
      "sample_id": "0068",
      "question": "What happens if args.global_rank is 0?",
      "ground_truth": "save_hf_format(model, tokenizer, args)",
      "predicted_answer": "If `args.global_rank` is 0, several actions are taken:\n\n- In the first code snippet, the `model` and `tokenizer` are saved in HF format [1].\n- If `args.zero_stage` is set to 3, the model is saved with Zero Optimization stage 3 settings [1].\n- The `msg` is printed using the `print_rank_0` function, which only prints if the rank is less than or equal to 0 [2].\n\n[1][2]",
      "ground_context": [
        "if args.global_rank == 0:\nsave_hf_format(model, tokenizer, args)"
      ],
      "retrieved_contexts": [
        "if args.global_rank == 0:\nsave_hf_format(model, tokenizer, args)\nif args.zero_stage == 3:\n#ႿZero Stage 3 đૄॶGPUඔ\nsave_zero_three_model(model,\nargs.global_rank,\nargs.output_dir,\nzero_stage=args.zero_stage)\ndef print_rank_0(msg, rank =0):\nif rank <= 0 :\nprint(msg)\n#ႨႿյႆZero Stage 1ބStage 2๽๳ਈ\ndef print_throughput(hf_model, args, e2e_time, rank =0):\nif rank <= 0 :\nhf_config = hf_model.config\nnum_layers, hidden_size, vocab_size = get_hf_configs(hf_config)\ngpus_per_model = torch.distributed.get_world...",
        "start = time.time()\nbatch = to_device(batch, device)\noutputs = model(**batch, use_cache =False)\nloss = outputs.loss\nif args.print_loss:\nprint(\nf\"Epoch: {epoch}, Step: {step},\nRank: {torch.distributed.get_rank()}, loss = {loss}\"\n)\nmodel.backward(loss)\nmodel.step()\nend = time.time()\nif torch.distributed.get_rank() == 0:\nprint_throughput(model.model, args, end - start,\nargs.global_rank)\nif args.output_dir is not None:\nprint_rank_0('saving the final model ... ' , args .global_rank)\nmodel = convert_l..."
      ],
      "response_time": 4.251802444458008,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.13157894736842105,
        "rouge2": 0.027027027027027025,
        "rougeL": 0.07894736842105264,
        "bleu": 0.005273370131615928,
        "semantic_similarity": 0.6333999633789062
      }
    },
    {
      "sample_id": "0069",
      "question": "在进行指令微调时，如何确保模型的输出符合预期的要求？",
      "ground_truth": "在进行指令微调时，确保模型的输出符合预期的要求，可以通过构造包含精准、清晰的指令输入的数据对来实现。这些指令输入详细阐释了任务的目标，并明确规定了输出需要满足的各项要求。同时，在模型训练的过程中，可以通过保存模型的不同阶段，并利用打印功能来监控训练过程中的各种指标，例如通过打印吞吐量来评估训练效率。",
      "predicted_answer": "在进行指令微调时，可以通过以下方式确保模型的输出符合预期的要求：\n\n1. 针对每一项任务明确地定义相应的自然语言形式的指令或提示，这些指令或提示应清晰描述任务目标以及输出要求[2]。\n2. 将训练数据调整成包含指令的形式，以便模型能够理解和响应这些指令[2]。\n3. 构建高质量的指令微调数据集，该数据集应该包含多样化的指令输入和对应的正确答案输出，从而帮助模型更好地学习不同类型的指令及其预期结果[3]。",
      "ground_context": [
        "指令微调数据通常由文本对构成，包含“指令输入”与“答案输出”两个关键部分。",
        "在进行指令微调时，确保模型的输出符合预期的要求，可以通过构造包含精准、清晰的指令输入的数据对来实现。",
        "save_hf_format(model, tokenizer, args) 和 save_zero_three_model(model, args.global_rank, args.output_dir, zero_stage=args.zero_stage) 这类函数可以帮助保存模型的不同阶段，从而监控训练过程中的各种指标。"
      ],
      "retrieved_contexts": [
        "大规模语言模型：从理论到实践 149\nUser:\n൉હ൞ AIĤ\nAssistant:\nᆦ,ఖb\nUser:\nĤ\nAssistant:\nPython\nUser:\nູ൉હ൐Ⴈ Python ଻Ĥ\nAssistant:\nႿնଆ྘টඪđPythonე࿽bཞ TensorFlowބPyTorchਔ Python१b\n通过这样的对话数据，可以把最后一轮“Assistant”回答前的所有数据当做“输入” ，最后一\n轮“Assistant”回答做为“输出” ，模型针对这样的“输入-输出”微调后，就能够具备理解上下文\n对话并给出符合期望输出的能力。\n可以看到， 指令微调数据由一系列文本对构成， 其中每一对都涵盖了 “指令输入” 与 “答案输\n出” 两个关键部分。 乍一看， 指令微调数据构造并不复杂， 但其实构建指令微调数据集是极具挑战\n性的任务，复杂性在诸多层面均有体现。在数据收集阶段，获取高质量指令数据集需耗费大量时\n间与资源，既要广泛招募参与者，精心规划有效的收集策略，还要全力保证收集到的数据兼具多\n样性与高质量。收集来的数据后续必经重写与筛选流程，研究人员常运用深度演化、广度演化策\n略以及主题多...",
        "5. 指令微调\n指令微调又称有监督微调，是指在预训练大语言模型的基础上，通过使用有标注的自然语言\n形式的数据， 对模型参数进行微调， 使模型具备指令遵循（Instruction Following） 能力， 能够完成\n各类预先设计的任务，并可以在零样本情况下处理诸多下游任务。经过海量数据预训练后的语言\n模型虽然具备了大量的 “知识” ， 但是由于其训练时的目标仅是进行下一个词的预测， 因此不能够\n理解并遵循人类自然语言形式的指令。为了使模型具有理解并响应人类指令的能力，还需要使用\n指令数据对其进行调整。如何构造指令数据，如何高效低成本地进行指令微调训练，以及如何在\n语言模型基础上进一步扩大上下文等问题，是大语言模型在指令微调阶段的核心。\n本章先介绍大语言模型指令微调训练方法，在此基础上介绍高效模型微调及模型上下文窗口\n扩展方法，最后介绍指令微调的代码实践。\n5.1 指令微调训练\n指令微调具体训练过程并不复杂，主要分为如下三个步骤： （ 1）针对每一项任务去明确地\n定义相应的自然语言形式的指令或者提示，这些指令或提示对任务目标以及输出要求进行清晰描\n述； （2）把训练数据调整成包含指令以...",
        "练数据对预训练模型进行微调操作。从训练方式的角度来看，指令微调与预训练大体上较为相似，\n不过指令微调的目标函数往往只是针对输出部分来计算损失。\n本节将从指令微调数据构造、数据评估和影响、指令微调训练策略以及开源指令微调数据等\n方面进行介绍。\n5.1.1 指令微调数据\n指令微调数据通常由文本对构成，包含“指令输入”与“答案输出”两个关键部分。 “指令输\n入” ， 是指人们向模型提出的各类请求， 包含定义精准、 清晰的指令或者提示信息， 其核心作用在\n于详细阐释任务的目标究竟是什么，以及明确规定输出需要满足的各项要求。指令涵盖的范畴极\n为广泛，包括问题回答、信息分类、内容总结、文本改写等。 “答案输出” ，则是期望模型依据所"
      ],
      "response_time": 5.741254568099976,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 0.3333333333333333
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.14010865439734388,
        "semantic_similarity": 0.8405353426933289
      }
    },
    {
      "sample_id": "0070",
      "question": "在训练模型中，如何结合选择子集的硬掩码或软权重与损失函数的优化来提高模型的性能？",
      "ground_truth": "在训练模型时，可以通过选择子集的硬掩码或软权重来优化模型参数，并结合带有自监督语言建模损失的双层优化问题，从而提高模型的性能。在片段A中，模型在训练过程中计算损失并进行反向传播，而在片段B中，优化问题的形式化表示表明可以通过负对数似然损失来优化所选子集的模型参数。通过这种方式，可以更有效地训练模型。",
      "predicted_answer": "在训练模型中，可以通过以下方式结合选择子集的硬掩码或软权重与损失函数的优化来提高模型的性能：\n\n1. 从数据集 \\( S \\) 中选择合适的子集 \\( S_b \\)，并通过硬掩码或软权重的形式化表示来优化模型参数 \\( \\theta \\)[1]。\n2. 对于每个数据样本 \\( x_i \\)，最小化负对数似然 \\( NLL_i \\) 来优化模型参数 \\( \\theta \\)，公式如下：\n\\[ NLL_i = -\\frac{1}{|x_i|} \\sum_{j=1}^{|x_i|} \\log P(x_i(j)|x_i(<j); \\theta) \\]\n这里 \\( |x_i| \\) 是数据样本 \\( x_i \\) 的长度，\\( P(x_i(j)|x_i(<j); \\theta) \\) 是给定前 \\( j-1 \\) 个元素的情况下生成第 \\( j \\) 个元素的概率[1]。\n\n这种优化过程包括内循环和外循环，外循环的目标是最小化整个子集上的损失函数，而内循环则是最小化单个数据样本上的损失函数[1]。通过这种方式，可以在保持模型灵活性的同时，有效地利用有限的数据资源，从而提高模型性能。",
      "ground_context": [
        "loss = outputs.loss",
        "从 S 中选择子集的硬掩码或软权重；内循环用于优化在 Sb 上的模型参数 θ。",
        "model.backward(loss)",
        "可以将带有自监督语言建模损失的双层优化问题，按照如下方法形式化表示："
      ],
      "retrieved_contexts": [
        "从 S 中选择子集的硬掩码或软权重；2) 内循环用于优化在 Sb 上的模型参数 θ。 可以将带有自监督\n语言建模损失的双层优化问题，按照如下方法形式化表示：\nSb = arg min\nS′\nb⊂S,|Sb|=b\nX\nxi∈S′\nb,θ=θ∗\nN LLA|Q\ni (5.4)\ns.t.θ∗ = arg min\nθ\nX\nxi∈S′\nb\nN LLA|Q\ni (5.5)\nN LLi = 1\n|xi|\n|xi|X\nj=1\n− log P (xi(j)|xi(<j); θ) (5.6)\n其中 N LLi 表示针对每个数据 xi 的负对数似然（Negative Log Likelihood） ，可以使用较小的模型\n进行学习，比如 MPT 125M[219] 等。",
        "OpenCLIP[305] 则需数百卡 GPU 集群运行数天至数周。 文献[304] 提出通过构建高效的数据处理管\n道，可实现性能提升，同时避免成本大幅上升。\n如图 7.12 所示，数据是训练多模态大语言模型的核心要素之一。构建一个多样且平衡的数据\n集对于模型学习覆盖足够多概念的良好世界模型至关重要。清除大型数据集中常见的重复数据同\n样重要，这不仅能够节省大量计算资源，还能降低模型过度记忆的风险。与此同时，数据剪枝也\n是数据处理的重要环节，确保文本描述与图像内容高度相关，有助于模型更好地理解和对齐多模\n态信息。可以通过改进模型对视觉语义关联（Grounding）能力来增强对图文关系的理解，并通过\n引入人类偏好优化对齐效果。 在OCR 任务中， 使用专门的增强技术可以进一步提升文本读取和翻\n译能力。通过结合高效数据处理、合理的模型架构选择和针对性优化策略，可以显著提升多模态\n大语言模型的训练效果和应用能力。",
        "批量解码或并行解码）以降低内存访问成本。这些优化策略将在后续章节中进一步探讨。\n10.2 模型优化\n模型优化是提升大语言模型推理效率的重要手段，主要集中在优化模型结构和模型压缩两方\n面。 模型结构优化通过设计高效的模型结构直接提升效率， 包括高效FFN 设计、 注意力机制优化、\nMoE 架构设计、Transformer 代替架构设计等， 这些内容大部分都在本书第二章大语言模型基础部\n分进行了介绍。模型压缩则涵盖了多种技术， 旨在通过修改模型的数据表示（例如量化） 、 改变其\n架构（例如稀疏化、结构优化等）或者知识蒸馏来提高预训练模型的推理效率。"
      ],
      "response_time": 9.496052742004395,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.039215686274509796,
        "rouge2": 0.0,
        "rougeL": 0.039215686274509796,
        "bleu": 0.08489669031583633,
        "semantic_similarity": 0.8621909618377686
      }
    },
    {
      "sample_id": "0071",
      "question": "如何在命名实体识别和情感分类任务中确保数据的多样性和有效性？",
      "ground_truth": "在命名实体识别中，通过均匀选择每种实体类型的样本并强调语义多样性，确保数据的多样性。而在情感分类任务中，针对每个类标签生成相应的输入，确保对情感的准确分类。",
      "predicted_answer": "为了确保命名实体识别(NER)和情感分类任务中数据的多样性和有效性，可以从以下几个方面入手：\n\n1. 标准化实体定义：系统地标准化所有数据集中的实体定义，解决不同数据集之间的定义冲突，确保每个实体类型都有明确且可区分的标签[2]。\n2. 强调语义多样性：选择文本相似度低的样本来增加数据多样性，在54个中英双语命名实体识别数据集中应用这种方法，有助于提高模型的性能[1]。\n3. 避免过度标注：注意不要对常见实体进行过多标注，而是适当平衡长尾实体的样本数量，避免因缺乏多样性而导致模型过拟合[2]。\n\n通过以上措施可以在一定程度上提升命名实体识别和情感分类任务中数据的有效性和多样性。",
      "ground_context": [
        "均匀选择每种实体类型的样本，同时强调语义多样性，通过选择文本相似度较低的样本来确保数据的多样性。",
        "Given the classification task definition and the class labels, generate an input that corresponds to each of the class labels."
      ],
      "retrieved_contexts": [
        "策略来减少冗余。具体而言，均匀选择每种实体类型的样本，同时强调语义多样性，通过选择文\n本相似度较低的样本来确保数据的多样性。 最终， 在54 个中英双语命名实体识别数据集中应用该\n方法，得到了 B2NERD，这是一个包含 16 个主要领域、400 多种实体类型的高级命名实体识别数",
        "广场” 这样的地点和 “巴黎” 这样的地缘政治实体， 而另一些数据集则将两者统一标注为 “LOC” 。\n如果直接使用未经处理的混合数据，大语言模型在训练中可能会与这些不一致的数据对齐，导致\n模型记住特定数据集的标注规则， 并在推理时对常见实体类型产生混淆。 此外， 合并数据集还容易\n引入大量冗余数据。许多数据集对常见实体进行了过多标注，而对长尾实体的样本标注较少。这\n种缺乏多样性的情况可能使大语言模型出现过拟合现象，并进一步导致知识遗忘和泛化能力下降\n的问题。\n为了解决数据集合并中的定义歧义以及数据冗余等问题，文献 [188] 提出了一种多数据集合\n并方法，如图5.1所示。该方法分为两个步骤，第一步是系统地标准化所有收集到的数据集中的实\n体定义。针对不同数据集中存在的不一致实体定义，方法通过基于模型的交叉验证和基于规则的\n筛选自动检测这些定义冲突。随后，根据特定原则为每种独特的实体类型分配明确且可区分的标\n签， 以消除模糊性。 在此阶段， 构建了一个通用的实体分类体系， 涵盖了常见实体类型， 并为新的\nNER 任务提供了标签命名的指导依据。 第二步则通过采用一种基于类别和语义多样性的数..."
      ],
      "response_time": 6.9990434646606445,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.04339426345012933,
        "semantic_similarity": 0.8254355788230896
      }
    },
    {
      "sample_id": "0072",
      "question": "find_and_replace 会根据什么参数从基础模型中找出模块？",
      "ground_truth": "config 中的参数",
      "predicted_answer": "`find_and_replace` 会根据 `config` 中的参数从基础模型的 `named_parameters` 中找出包含指定名称的模块， 默认查找的是“q”和“v”，即注意力模块的 Q 和 V 矩阵。[1]",
      "ground_context": [
        "find_and_replace 会根据 config 中的参数从基础模型的 named_parameters 中找出包含指定名称的模块（默认为“q” “v” ，即注意力模块的Q 和 V 矩阵）"
      ],
      "retrieved_contexts": [
        "大规模语言模型：从理论到实践 171\n_find_and_replace 会根据 config 中的参数从基础模型的 named_parameters 中找出包含指定名称的\n模块（默认为“q” “v” ，即注意力模块的Q 和 V 矩阵） ，创建一个新的自定义类Linear 模块，并\n替换原来的。\nclass Linear(nn.Linear, LoraLayer):\n# LoraҪᇏ\ndef __init__(\nself,\nadapter_name: str,\nin_features: int,\nout_features: int,\nr: int = 0,\nlora_alpha: int = 1,\nlora_dropout: float = 0.0,\nfan_in_fan_out: bool = False,\nis_target_conv_1d_layer: bool = False,\n**kwargs,\n):\ninit_lora_weights = kwargs.pop(\"init_lora_weights\", True)\nnn.Linear.__init__(self, in_fe..."
      ],
      "response_time": 4.226665019989014,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.16666666666666669,
        "rouge2": 0.0,
        "rougeL": 0.16666666666666669,
        "bleu": 0.024584895567495123,
        "semantic_similarity": 0.6021020412445068
      }
    },
    {
      "sample_id": "0073",
      "question": "线性插值法的优点是什么？",
      "ground_truth": "线性插值法具有良好的数值稳定性，并且不需要修改模型架构，只需要少量微调即可将 LLaMA.",
      "predicted_answer": "线性插值法得到的结果平滑且数值稳定[1]。此外，它可以通过修改位置编码来直接扩展大语言模型的上下文窗口大小，而不改变模型架构，使得模型更容易适应[2]。",
      "ground_context": [
        "线性插值法具有良好的数值稳定性（具体推导请参考文献 [242]），并且不需要修改模型架构，只需要少量微调（例如，在 pile 数据集上进行 1000 步的微调）即可将 LLaMA"
      ],
      "retrieved_contexts": [
        "大规模语言模型：从理论到实践 177\n将所有三角函数视为基函数 ϕj(s) := e isθj ，可以将式 (5.16) 展开为\na(s) = Re\n2\n4\nd/2−1X\nj=0\nhjeisθj\n3\n5 (5.17)\n其中 s 是查询和键之间的相对距离，hj := ( q2j + iq2j+1)(k2j − ik2j+1) 是取决于查询和键的复系\n数。作为基函数的三角函数具有非常强的拟合能力，基本上可以拟合任何函数，因此在不训练的\n情况下， 对于预训练2048 的上下文窗口总会存在与 [0, 2048] 中的小函数值相对应但在 [0, 2048] 之\n外的区域中大很多的系数 hj（键和查询） ，如图5.9(a) 所示，但线性插值法得到的结果平滑且数值\n稳定，如图5.9(b) 所示。\n图 5.9 不同相对距离下外推法和线性插值法的注意力分数比较。\n因此，可以利用位置插值修改式 (5.15) 的位置编码函数：\nf ′(x, m) = f\n\u0012\nx, mL\nL′\n\u0013\n(5.18)\n这种方法对齐了位置索引和相对距离的范围， 减小了上下文窗口扩展对注意力得分计算的影响， 使\n得模型更容易适应。线性...",
        "28 。 通常情况下， 对于n 个注意头， 斜率值是\n从 2\n−8\nn 开始，并使用相同的值作为其比率。ALiBi 的计算过程如图5.7 所示。\nm\n图 5.7 ALiBi 计算过程示例\nALiBi 对最近性具有归纳偏差， 它对远程查询–键对之间的注意力分数进行惩罚， 随着键和查询\n之间的距离增加， 惩罚也增加。不同的注意力头以不同的速率增加其惩罚， 这取决于斜率幅度。实\n验证明，这组斜率参数适用于各种文本领域和模型尺寸，不需要在新的数据和架构上调整斜率值。\n5.3.2 插值法\n不同的预训练大语言模型使用不同的位置编码，修改位置编码意味着重新训练，因此对于已\n训练的模型，通过修改位置编码扩展上下文窗口大小的适用性仍然有限。为了不改变模型架构而\n直接扩展大语言模型上下文窗口大小， 文献[242] 提出了位置插值法， 使现有的预训练大语言模型\n（包括 LLaMA、Falcon、Baichuan 等） 能直接扩展上下文窗口。 其关键思想是， 直接缩小位置索引，\n使最大位置索引与预训练阶段的上下文窗口限制相匹配。线性插值法的示意图如图5.8 所示。"
      ],
      "response_time": 4.201402425765991,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0.0,
        "bleu": 0.040529331319480916,
        "semantic_similarity": 0.8345935344696045
      }
    },
    {
      "sample_id": "0074",
      "question": "如何获取评估数据？",
      "ground_truth": "return self.raw_datasets[\"eval\"]",
      "predicted_answer": "根据提供的文档，我无法回答这个问题。",
      "ground_context": [
        "return self .raw_datasets[\"eval\"]"
      ],
      "retrieved_contexts": [],
      "response_time": 2.808849573135376,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.0,
        "semantic_similarity": 0.369901180267334
      }
    },
    {
      "sample_id": "0075",
      "question": "在第一轮训练中，Rank 4 的损失是多少？",
      "ground_truth": "1.6396484375",
      "predicted_answer": "在第一轮训练中，Rank 4 的损失是 2.005859375 [1]。",
      "ground_context": [
        "running - Rank: 4, Epoch 1/2, Step 1/341, trained samples: 128/341, Loss 1.6396484375"
      ],
      "retrieved_contexts": [
        "running - Rank: 4, Epoch 1/2, Step 1/341, trained samples: 128/341, Loss 1.6396484375\nrunning - Rank: 6, Epoch 1/2, Step 1/341, trained samples: 128/341, Loss 1.94140625\n...\nrunning - Rank: 4, Epoch 1/2, Step 341/341, trained samples: 43584/341, Loss 2.005859375\nrunning - Rank: 5, Epoch 1/2, Step 341/341, trained samples: 43584/341, Loss 1.6533203125\nrunning - ***** Evaluating perplexity, Epoch 1/2 *****\nrunning - Rank: 7, Epoch 1/2, Step 341/341, trained samples: 43584/341, Loss 2.076171875\nrun..."
      ],
      "response_time": 2.8905084133148193,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.28571428571428575,
        "rouge2": 0.0,
        "rougeL": 0.28571428571428575,
        "bleu": 0.0,
        "semantic_similarity": 0.5012513995170593
      }
    },
    {
      "sample_id": "0076",
      "question": "ChatDoctor指令数据集的规模是多少？",
      "ground_truth": "11.5 万",
      "predicted_answer": "ChatDoctor指令数据集的规模是11.5万条。[1]",
      "ground_context": [
        "ChatDoctor 德克萨斯大学西南医学中心 11.5 万 医疗 公开"
      ],
      "retrieved_contexts": [
        "大规模语言模型：从理论到实践 167\n表 5.3 部分开源领域指令微调数据集的汇总信息\n指令数据集名称 发布单位 指令数据集规模（条） 领域 是否公开\nChatDoctor 德克萨斯大学西南医学中心 11.5 万 医疗 公开\nDISC-Med-SFT 复旦大学 46.49 万 医疗 公开\nHuatuo-26M 香港中文大学等 265 万 医疗 公开\nMedDialog 加州大学圣地亚哥分校 366 万 医疗 公开\nMedical Meadow 亚琛大学医院等 16 万 医疗 公开\nBELLE School Math 贝壳研究院 24.85 万 数学 公开\nGoat 新加坡国立大学 175 万 数学 公开\nOpenMathInstruct-1 NVIDIA 180 万 数学 公开\nCode Alpaca 20K Sahil Chaudhary 2 万 代码 公开\nCodeContest DeepMind 1.36 万 代码 公开\nCommitPackFT Bigcode 70.21 万 代码 公开\nDISC-Law-SFT 复旦大学 40.3 万 法律 部分\nHanFei 1.0 中国科...",
        "166 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n表 5.2 部分开源通用指令微调数据集的汇总信息\n指令数据集名称 发布单位 指令数据集规模 语言 是否公开\nAlpaca Data Standford Alpaca 5.2 万条 英文 公开\nAya Collection Cohere For AI 等 5.13 亿条 多语言 公开\nAya Dataset Cohere For AI 等 20.4 万条 多语言 公开\nBELLE 贝壳研究院 350 万条 中文 公开\nCOIG 北京智源研究院 19.11 万条 中文 公开\nDialogStudio Salesforce AI 87 个数据集 多语言 公开\nDolly Databricks 1.5 万条 英语 公开\nFirefly Y eungNLP 115 万条 中文 公开\nFlan 2022 Google Research 1836 个数据集 多语言 部分\nInstructionWild V2 新加坡国立大学 11 万条 中英文 公开\nLCCC 清华大学 1200 万条 中文 公开\nLMSYS-Chat-1M 加州大学伯...",
        "含 110,000 个指令。LCCC[183] 是一个中文对话数据集， 包含LCCC-base 和 LCCC-large 两个版本。 其\n中 LCCC-base 采用两阶段数据收集方案，首先挑选专注发布新闻的微博帐号作为高质量用户，再\n收集其微博帖子下方评论并把评论路径视为对话一部分； LCCC-large 则是从包括中国 Chatterbot\n语料库、PTT 闲话语料库等多个开源存储库收集语料库，并与青云语料库、贴吧语料库一同清洗\n后处理成单轮对话数据集。\n2. 现有数据集转换\n收集和改进现有数据集也是一种用于构建指令微调数据集的方法，它涉及整合和修改多个开\n源数据集， 最终将它们合并成一个新数据集用于大模型指令微调。 文献[106] 指出这种构建方式具\n有以下优点： （1）多样性和全面性，生成的数据集具有丰富的数据来源、多样化的任务类型和广\n泛的领域覆盖； （2）规模大，选择的数据集越多，规模越大； （3）节省时间，这种构建方式可以\n减少数据集构建所需的时间。这种数据集构造的主要是难点是质量与格式标准化。需要全面考量"
      ],
      "response_time": 3.411062479019165,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.6666666666666666,
        "rouge2": 0.5,
        "rougeL": 0.6666666666666666,
        "bleu": 0.018724372764461875,
        "semantic_similarity": 0.4303074777126312
      }
    },
    {
      "sample_id": "0077",
      "question": "如何设置ds_accelerator?",
      "ground_truth": "Setting ds_accelerator to cuda (auto detect)",
      "predicted_answer": "可以通过调用`get_accelerator`函数并设置`ds_accelerator`参数来设置加速器。在这个例子中，它被设置为`cuda`（自动检测）。[1][2][3][4]",
      "ground_context": [
        "Setting ds_accelerator to cuda (auto detect)"
      ],
      "retrieved_contexts": [
        "大规模语言模型：从理论到实践 181\n[2023-09-06 21:17:36,034] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator\nto cuda (auto detect)\nDetected CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 but ignoring it because one or several of --include/\n--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't\npass any of these arguments to deepspeed.\n[2023-09-06 21:17:37,575] [INFO] [runner.py:555:main] cmd = /opt/miniconda3/envs/baichuan_sft/bin/\npython -u -m deepspeed....",
        "188 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n[2023-09-07 10:31:52,575] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator\nto cuda (auto detect)\n[2023-09-07 10:31:57,019] [WARNING] [runner.py:196:fetch_hostfile] Unable to find hostfile, will\nproceed with training with local resources only.\nDetected CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7: setting --include=localhost:0,1,2,3,4,5,6,7\n...\nrunning - ***** Running training *****\nrunning - ***** Evaluating perplexity, Epoch 0/2 *****\nrunning - pp...",
        "[2023-09-06 21:17:41,099] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator\nto cuda (auto detect)\n[2023-09-06 21:17:43,194] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed\nnot yet implemented\n[2023-09-06 21:17:43,194] [INFO] [comm.py:594:init_distributed] cdb=None\n[2023-09-06 21:17:43,194] [INFO] [comm.py:625:init_distributed] Initializing TorchBackend in\nDeepSpeed with backend nccl\nDownloading pytorch_model.bin: 0%| | 0.00/2.63G [00:00<?, ?B/s]\nDow...",
        "python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.\n1 --master_port=29500 --enable_each_rank_log=None main.py --model_name_or_path facebook/opt-1.3b\n--gradient_accumulation_steps 8 --lora_dim 128 --zero_stage 0 --enable_tensorboard --tensorboard_pa\nth /root/workspace/DeepSpeed-Chat/output/actor-models/1.3b --deepspeed --output_dir /root/workspace\n/DeepSpeed-Chat/output/actor-models/1.3b\n[2023-09-06 21:17:38,322] [INFO] [real_accelerator.py:110:get_..."
      ],
      "response_time": 3.896178722381592,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.375,
        "rouge2": 0.14285714285714288,
        "rougeL": 0.375,
        "bleu": 0.02797289658108333,
        "semantic_similarity": 0.801497220993042
      }
    },
    {
      "sample_id": "0078",
      "question": "文献 [239] 中提出了什么方案来平滑化敏感性？",
      "ground_truth": "文献 [239] 中提出了一种新的方案来平滑化敏感性，以及量化其不确定性。",
      "predicted_answer": "文献 [239] 中提出了一种新的方案来平滑化敏感性，并且量化其不确定性。具体来说，他们使用了以下公式：\n\n\\[\n\\bar{I}(t)(w_{ij}) = \\beta_1 \\bar{I}(t-1) + (1 - \\beta_1) I(t)(w_{ij})\n\\]\n\n\\[\n\\bar{U}(t)(w_{ij}) = \\beta_2 \\bar{U}(t-1) + (1 - \\beta_2) |I(t)(w_{ij}) - \\bar{I}(t)(w_{ij})|\n\\]\n\n\\[\ns(t)(w_{ij}) = \\frac{\\bar{I}(t)}{\\bar{U}(t)}\n\\]\n\n通过这些公式计算得到的平滑后的参数敏感性被认为是效果最优的。[1]",
      "ground_context": [
        "然而，根据文献 [239] 中的实验结果，该敏感性度量受限于小批量采样带来的高方差和不确定性，因此并不完全可靠。相应地，文献 [239] 中提出了一种新的方案来平滑化敏感性，以及量化其不确定性。"
      ],
      "retrieved_contexts": [
        "大规模语言模型：从理论到实践 173\n其中，wij 表示可训练参数。该式估计了某个参数变为 0 后， 损失函数值的变化。因此，I(wij) 越\n大，表示模型对该参数越敏感，这个参数也就越应该被保留。然而，根据文献 [239] 中的实验结\n果，该敏感性度量受限于小批量采样带来的高方差和不确定性，因此并不完全可靠。相应地，文\n献 [239] 中提出了一种新的方案来平滑化敏感性，以及量化其不确定性。\n¯I (t)(wij) = β1 ¯I (t−1) + (1 − β1)I (t)(wij) (5.10)\n¯U (t)(wij) = β2 ¯U (t−1) + (1 − β2)|I (t)(wij) − ¯I (t)(wij)| (5.11)\ns(t)(wij) = ¯I (t) ¯U (t) (5.12)\n通过实验对上述几种重要性定义方法进行对比，发现由式 (5.11) 计算得到的重要性分数，即平滑\n后的参数敏感性，效果最优。故最终的重要性分数计算式为\nSk,i = s(λk,i) + 1\nd1\nd1X\nj=1\ns(Pk,ji ) + 1\nd2\nd2X\nj=1\ns(Qk,ij ) (5.1..."
      ],
      "response_time": 6.864907264709473,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.03636363636363636,
        "rouge2": 0.0,
        "rougeL": 0.03636363636363636,
        "bleu": 0.07930210337695492,
        "semantic_similarity": 0.8139530420303345
      }
    },
    {
      "sample_id": "0079",
      "question": "get_peft_model 函数的功能是什么？",
      "ground_truth": "该函数封装了基础模型并得到一个 PeftModel 类的模型。",
      "predicted_answer": "`get_peft_model`函数用于将LoRA（Low-Rank Adaptation）配置应用到给定的模型上，从而实现模型的知识蒸馏或微调[1]。在这个例子中，它被用来将LoRA配置应用于一个名为`bigscience/mt0-large`的大规模序列到序列的语言模型[1]。",
      "ground_context": [
        "接下来介绍 peft 库对 LoRA 的实现， 也就是上述代码中get_peft_model 函数的功能。 该函数封装了基础模型并得到一个 PeftModel 类的模型。"
      ],
      "retrieved_contexts": [
        "大规模语言模型：从理论到实践 169\nfrom transformers import AutoModelForSeq2SeqLM\nfrom peft import get_peft_config, get_peft_model, LoraConfig, TaskType\nmodel_name_or_path = \"bigscience/mt0-large\"\ntokenizer_name_or_path = \"bigscience/mt0-large\"\npeft_config = LoraConfig(\ntask_type=TaskType.SEQ_2_SEQ_LM, inference_mode =False, r =8, lora_alpha =32, lora_dropout =0.1\n)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name_or_path)\nmodel = get_peft_model(model, peft_config)\n接下来介绍 peft 库对 LoRA 的实现， 也就是上述代码中get_pe..."
      ],
      "response_time": 4.605196714401245,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0.0,
        "bleu": 0.012364970228436708,
        "semantic_similarity": 0.6433334350585938
      }
    },
    {
      "sample_id": "0080",
      "question": "Alpaca数据集的规模是多少条?",
      "ground_truth": "5.2 万条",
      "predicted_answer": "Alpaca数据集的规模是5.2万条。[1]",
      "ground_context": [
        "Alpaca Data Standford Alpaca 5.2 万条 英文 公开"
      ],
      "retrieved_contexts": [
        "166 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n表 5.2 部分开源通用指令微调数据集的汇总信息\n指令数据集名称 发布单位 指令数据集规模 语言 是否公开\nAlpaca Data Standford Alpaca 5.2 万条 英文 公开\nAya Collection Cohere For AI 等 5.13 亿条 多语言 公开\nAya Dataset Cohere For AI 等 20.4 万条 多语言 公开\nBELLE 贝壳研究院 350 万条 中文 公开\nCOIG 北京智源研究院 19.11 万条 中文 公开\nDialogStudio Salesforce AI 87 个数据集 多语言 公开\nDolly Databricks 1.5 万条 英语 公开\nFirefly Y eungNLP 115 万条 中文 公开\nFlan 2022 Google Research 1836 个数据集 多语言 部分\nInstructionWild V2 新加坡国立大学 11 万条 中英文 公开\nLCCC 清华大学 1200 万条 中文 公开\nLMSYS-Chat-1M 加州大学伯..."
      ],
      "response_time": 3.449025869369507,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.6666666666666666,
        "rouge2": 0.5,
        "rougeL": 0.6666666666666666,
        "bleu": 0.04774753205784679,
        "semantic_similarity": 0.6496869325637817
      }
    },
    {
      "sample_id": "0081",
      "question": "在训练过程中，如何使用不同的位置编码方法来增强模型的外推能力，并且在训练中如何对OPT模型进行微调和奖励模型的训练？",
      "ground_truth": "位置编码的外推能力来源于相对位置信息，而在训练OPT模型时，通过train.py脚本对OPT-1.3b模型进行监督微调和对OPT-350m模型进行奖励模型的训练。",
      "predicted_answer": "根据提供的文档，我无法回答这个问题。文档中提到了几种位置编码方法及其在外推能力方面的应用，但并未详细讨论OPT模型的微调过程以及奖励模型的训练方式。[1][2]",
      "ground_context": [
        "位置编码的长度外推能力来源于位置编码中表征相对位置信息的部分，相对位置信息不同于绝对位置信息， 对于训练时的依赖较少。",
        "表示通过 train.py 脚本进行步骤一和步骤二的训练，分别对OPT-1.3b 模型进行监督微调和对 OPT-350m 模型进行奖励模型的训练。"
      ],
      "retrieved_contexts": [
        "进的位置编码和插值法。\n5.3.1 具有外推能力的位置编码\n位置编码的长度外推能力来源于位置编码中表征相对位置信息的部分，相对位置信息不同于\n绝对位置信息， 对于训练时的依赖较少。 位置编码的研究一直是基于Transformer 结构模型的重点。\n2017 年 Transformer 结构[12] 提出时， 介绍了两种位置编码， 一种是Naive Learned Position Embedding，\n也就是 BERT 模型中使用的位置编码；另一种是 Sinusoidal Position Embedding ，通过正弦函数为\n每个位置向量提供一种独特的编码。这两种最初的形式都是绝对位置编码的形式，依赖于训练过\n程中的上下文窗口大小，在推理时基本不具有外推能力。随后，2021 年提出的 RoPE[48] 在一定程\n度上缓解了绝对位置编码外推能力弱的问题。关于 RoPE 位置编码的具体细节， 已在2.3.1 节进行\n了介绍， 这里不再赘述。 后续在T5 架构[243] 中， 研究人员又提出了T5 Bias Position Embedding， 直",
        "量不得超过 2048 个。这会限制模型对长文本的理解和表达能力。当涉及长时间对话或长文档摘要\n时，传统的上下文窗口大小可能无法捕捉到全局语境，从而导致信息丢失或模糊的建模结果。\n为了更好地满足长文本需求，有必要探索如何扩展现有的大语言模型，使其能够有效地处理\n更大范围的上下文信息。具体来说，扩展语言模型的长文本建模能力主要有以下方法。\n• 增加上下文窗口的微调： 采用直接的方式， 即通过使用一个更大的上下文窗口来微调现有的\n预训练 Transformer，以适应长文本建模需求。\n• 位置编码：改进的位置编码，如ALiBi[240]、LeX[241] 等能够实现一定程度上的长度外推。这\n意味着它们可以在小的上下文窗口上进行训练，在大的上下文窗口上进行推理。\n• 插值法：将超出上下文窗口的位置编码通过插值法压缩到预训练的上下文窗口中。\n文献 [242] 指出， 采用增大上下文窗口微调的方式训练的模型， 对上下文的适应速度较慢。 在\n经过了超过 10000 个批次的训练后， 模型上下文窗口只有小幅度的增长， 从2048 增加到 2560。 实\n验结果显示，这种朴素的方法在扩展到更大的上下文窗...",
        "大语言模型：从理论到实践 245\n在推理过程中，视觉标记对位置编码的占用仍然是一个不可忽视的问题。事实上，视觉标记的长\n度通常比文本标记高出 500 倍以上，在具有位置感知的视觉语言模型中，这会消耗大量的位置嵌\n入资源。鉴于视觉专家本身已经包含位置编码信息，为每个视觉标记再次分配视觉大语言模型的\n位置嵌入显得冗余且低效。为了解决这一问题，MouSi 模型提出了一种二维可训练图像位置编码\n方法，通过直接在视觉标记中引入可训练的二维位置编码，避免了对视觉大语言模型位置嵌入的\n额外占用。这种方法不仅有效解决了多视觉专家导致的超长序列问题，还减少了位置编码的冗余\n分配，从而优化了视觉标记的处理效率，为多模态模型的可扩展性提供了重要支持。\n7.3 多模态大语言模型训练策略\n深度神经网络缩放法则（Scaling Law）为多模态大语言模型的训练策略提供了重要参考。以\n往业界普遍采用增加计算资源和模型规模的方式来提升性能，然而，根据文献 [304] 的研究成果，\n优化数据处理环节亦可带来突破性进展。以 CLIP 为例，其采用 4 亿张图像进行训练，开源版本\nOpenCLIP[305] 则需数百卡 G..."
      ],
      "response_time": 4.114363193511963,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.3333333333333333,
        "rouge2": 0.2,
        "rougeL": 0.3333333333333333,
        "bleu": 0.07562263205281951,
        "semantic_similarity": 0.8218741416931152
      }
    },
    {
      "sample_id": "0082",
      "question": "在使用 DeepSpeed 进行模型训练时，如何设置训练步骤和模型参数，以便同时满足片段A和片段B中的要求？",
      "ground_truth": "可以通过命令行参数设置训练步骤和模型参数，例如在片段A中提到的 'deepspeed main.py --data_path <my_data>/my_dataset' 和在片段B中提到的 '--step 训练步骤参数' 来实现。",
      "predicted_answer": "要设置训练步骤和模型参数以同时满足片段A和片段B中的要求，可以按照以下步骤操作：\n\n1. **初始化 DeepSpeed**：首先，你需要初始化 DeepSpeed，包括确定运行的设备、初始化分布式后端、获取当前进程的全球排序等。这部分可以通过调用 `deepspeed.init_distributed()` 完成 [2]。\n\n2. **准备模型**：创建并加载模型。例如，如果你正在使用 BAIchuan 7B 模型，你可以使用 `create_hf_model` 来实例化模型对象 [4]。此外，还需要设置模型的一些配置参数，比如调整 token embedding 尺寸等 [5]。\n\n3. **模型训练**：进入训练循环，这通常涉及几个关键步骤：\n    - 打印当前周期和总周期数。\n    - 将数据批次移动到对应的 GPU 上。\n    - 对批次进行前向传播计算损失。\n    - 计算梯度并通过更新模型参数来进行反向传播。\n    - 主进程中打印吞吐量以监控训练进度 [1]。\n\n4. **保存模型**：训练结束后，可以选择保存模型状态及配置。你可以使用 `save_hf_format` 函数或其他合适的保存方法 [1]。\n\n综上所述，通过上述步骤，你可以有效地设置训练步骤和模型参数，从而满足片段A和片段B的要求。",
      "ground_context": [
        "--data_path <my_data>/my_dataset",
        "--step 训练步骤参数， 表示运行哪个步骤， 可选参数为1、2、3。"
      ],
      "retrieved_contexts": [
        "144 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n6. 模型训练\n借助 DeepSpeed 框架实现对模型的训练，训练步骤大致分为以下几个阶段。\n（1） 训练前的准备： 使用print_rank_0 函数输出当前的训练状态。 该函数确保只有指定的进程\n（通常是主进程）会打印消息，避免了多进程环境下的重复输出。在开始训练之前，对模型进行一\n次评估，计算模型的困惑度。\n（2） 训练循环： 每个周期的开始， 都会打印当前周期和总周期数。 在每次迭代中， 数据批次先被\n移动到相应的 GPU 设备， 接着模型对这个批次进行前向传播计算损失。 使用model.backward(loss)\n计算梯度，并使用 model.step() 更新模型参数。对于主进程，还会使用 print_throughput 函数打印\n吞吐量，这有助于了解模型的训练速度和效率。\n（3）保存模型：如果指定了输出目录，则模型的状态和配置将被保存。模型可以在不同的格\n式中保存，例如 HuggingFace 的模型格式或 DeepSpeed 的 Zero Stage 3 特定格式。save_hf_format\n函数...",
        "142 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n5. DeepSpeed 初始化\n设置 DeepSpeed 的配置参数后， 可以利用DeepSpeed 进行模型训练的初始化， 初始化流程如下。\n（1）确定运行的设备：首先，检查代码是否有指定的本地 GPU（通过 args.local_rank） 。如果\n没有指定，则程序默认使用 CUDA 设备。否则，它会为进程设置指定的 GPU。\n（2） 初始化分布式后端： 在分布式训练中， 使用deepspeed.init_distributed() 函数实现每个进程\n与其他进程的同步，初始化分布式环境。\n（3）获取当前进程的全局排序：在分布式训练中，使用torch.distributed.get_rank() 函数获得每\n个进程的唯一排序或 ID。\n（4）设置 DeepSpeed 配置：根据用户参数（如是否进行 offload、使用哪个 Zero Stage 等）构\n建一个 DeepSpeed 配置字典，来决定训练设置。\n（5） 同步所有工作进程： 使用torch.distributed.barrier() 确保在进一步的初始化之...",
        "- സ੻\n- utils #ඔ९\n- data #ս઒\n- model #ࡱ\nmodule #ࡱ\nds_utils.py # DeepSpeedܱ\nutils.py #ඔ\n- train.py #Ӯ࿞਀ೆ१\n当需要完整微调一个模型时（包含所有步骤） ，可以直接运行train.py 程序。训练中主要调\n整如下参数。",
        "model = create_hf_model(AutoModelForCausalLM,\nargs.model_name_or_path,\ntokenizer,\nds_config,\ndisable_dropout=args.disable_dropout)\n#ູᆦӻBaichuan 7Bս઒\ntokenizer = BaiChuanTokenizer.from_pretrained(args.model_name_or_path)\nmodel = create_hf_model(BaiChuanForCausalLM,\nargs.model_name_or_path,\ntokenizer,\nds_config,\ndisable_dropout=args.disable_dropout)\n最后，在训练脚本中将 model_name_or_path 参数修改为 Baichuan 7B 的模型路径即可开始\n模型的训练。训练脚本中以 DeepSpeed-Chat 中的 run_llama2_7b.sh 为模板进行修改：",
        "model = LlamaForCausalLM.from_pretrained(model_name_or_path, config =model_config)\nmodel.config.end_token_id = tokenizer.eos_token_id\nmodel.config.pad_token_id = model.config.eos_token_id\nmodel.resize_token_embeddings(int(\n8 *\nmath.ceil(len(tokenizer) / 8.0 ))) # ഡᇂՍіնཬູ8Пඔ\n3. 优化器设置\nDeepSpeed 库提供了高效的优化器算法，如 DeepSpeedCPUAdam 和 FusedAdam，这些算法经\n过特殊优化以提高在大规模数据和模型上的训练速度。优化器配置主要包含以下几个方面。\n（1） 参数分组： 通过get_optimizer_grouped_parameters 函数将模型参数分为两组， 一组使用权\n重衰减，另一组则不使用。这种参数分组有助于正则化模型，防止过拟合，并允许对特定参数应\n用不同的学习设置..."
      ],
      "response_time": 9.031879425048828,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.14634146341463414,
        "rouge2": 0.0,
        "rougeL": 0.14634146341463414,
        "bleu": 0.028296544602042593,
        "semantic_similarity": 0.8102662563323975
      }
    },
    {
      "sample_id": "0083",
      "question": "Kimi k1.5 的多模态数据包括哪五类?",
      "ground_truth": "字幕、图像-文本交错数据、OCR、知识和一般问题回答五类。",
      "predicted_answer": "Kimi k1.5 的多模态数据包括字幕、图像-文本交错数据、OCR、知识和一般问题回答五类[1]。",
      "ground_context": [
        "作为多模态模型，Kimi k1.5 的多模态数据包括字幕、图像-文本交错数据、OCR、知识和一般问题回答五类。"
      ],
      "retrieved_contexts": [
        "如一般问答、 编程、 数学和科学等。 此外， 还构建了100 万个文本 - 视觉示例， 涵盖图表解读、OCR\n等多种类别。\n多模态数据：作为多模态模型，Kimi k1.5 的多模态数据包括字幕、 图像-文本交错数据、OCR、\n知识和一般问题回答五类。对每类数据进行了针对性处理：\n标题数据整合开源和内部数据，并严格限制合成数据比例，同时进行质量控制和图像分辨率\n调整。 图像-文本交错数据则考虑开源数据集并构建自建数据， 还通过数据重排序确保图像和文本\n顺序正确。OCR 数据来源多样， 包括公开数据和自建数据集， 并进行数据增强以提高模型的OCR\n能力。",
        "免奖励操纵，排除易出现验证错误的问题类型，并通过特定方法识别和移除易被操纵的提示。\n此外，为提高模型的图像推理能力，数据还来源于现实世界数据、合成视觉推理数据和文本\n渲染数据这三个类别。\n预训练数据集的构建与处理： Kimi k1.5 的预训练数据集涵盖英语、 中文、 代码、 数学与推理\n以及知识数据五个领域，以确保数据多样性。为保证数据高质量，采用多种清洗方法：\n针对英语和中文文本数据，建立多维质量过滤框架，包括基于规则的过滤、基于 FastText 的\n分类、基于嵌入的相似性分析和基于大模型的质量评估，最后通过动态采样率对不同质量的文档\n进行处理。\n对于代码数据， 对纯代码数据和文本-代码交错数据分别进行处理， 前者遵循BigCode 方法进\n行预处理和采样调整，后者采用基于句向量的方法召回高质量数据。数学与推理数据通过开发专\n门的数据清洗程序和 OCR 模型， 以及两阶段数据清洗过程， 提高数据质量。 知识数据通过精心策\n划，利用内部语言模型添加多维标签，并实施复杂的过滤和采样管道，优化数据组成。\n微调数据集的构建： Kimi k1.5 的 SFT 数据集包含约 100 万个文本...",
        "210 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n解决数据受限的问题。大语言模型可通过强化学习中的奖励机制，学习如何探索不同的推理路径，\n从而扩大训练数据的范围。但此前的相关研究成果未达到理想的竞争效果，Kimi k1.5 旨在探索一\n条更有效的技术路线。\n1. 技术路线\n强化学习提示数据集构建： RL 提示数据集的质量和多样性对强化学习的有效性至关重要。\nKimi k1.5 定义了高质量 RL 提示集的三个关键属性：\n多样覆盖：提示应涵盖 STEM、编程和一般推理等广泛学科，以增强模型的适应性和跨领域\n应用能力。为此，采用自动过滤器选择需要丰富推理且易于评估的问题，数据集来源广泛，包括\n不同领域的问题以及纯文本和图像 - 文本问答数据。\n平衡难度：提示集应包含不同难度级别的问题，以促进模型的逐步学习并防止过度拟合。利\n用模型自身能力自适应评估提示难度， 通过SFT 模型多次生成答案计算通过率作为难度代理指标，\n并开发标签系统按领域和学科分类提示，实现难度平衡。\n准确评估能力：提示应能被验证者客观可靠地评估，确保基于正确推理衡量模型性能。为避\n免奖励操纵，排除易出现验证...",
        "作为负样本（包括错误长响应和正确但超长响应） ，形成成对偏好数据用于DPO 训练，DPO 细节\n参见下文 [245]。\n4. 长到短强化学习：在标准 RL 训练后，选择性能和 token 效率平衡最佳的模型作为基础模\n型，进行单独的长到短 RL 训练，应用长度惩罚方案惩罚超长响应。\n改进的策略优化：Kimi k1.5 推导出带有长推理链的强化学习公式， 并采用在线镜像下降的变\n体进行策略优化。该算法通过以下方式进一步改进：\n1. 采样策略：采用课程采样和优先级采样策略。课程采样从简单任务开始训练，逐渐过渡到\n困难任务，利用数据的难度标签提高训练效率；优先级采样跟踪每个问题的成功率，按比例采样\n问题，使模型专注于薄弱领域。\n2. 长度惩罚：针对 RL 训练期间模型响应长度增加的问题，引入长度奖励限制 token 长度增\n长。在正确答案中提倡简短回答并惩罚较长回答，对错误答案的长回答明确惩罚。为缓解长度惩\n罚在训练初期对训练速度的影响，采用逐渐增加长度惩罚的方式。\n3. 训练架构及工程框架\nKimi k1.5 模型的训练分为三个阶段：\n1. 视觉语言预训练阶段：模型最初仅在语言数据上训练，...",
        "大规模语言模型：从理论到实践 211\n2. 算法创新\n长上下文扩展： Kimi k1.5 将 RL 的上下文窗口扩展到 128k， 实验表明， 上下文长度与模型解\n决问题的能力强相关，增加上下文长度可提升模型在困难推理基准测试中的性能。为解决长上下\n文带来的计算量增加问题，采用部分回放（ partial rollouts）技术，通过重用之前轨迹的大部分来\n采样新轨迹，减少计算开销。具体操作时，部分展开系统将长响应分解为多个段，在多个迭代中\n逐步处理，加快训练速度。\nLong2short 的上下文压缩策略\n长上下文模型虽性能强大，但测试时 token 消耗较多。Kimi k1.5 提出多种方法将长上下文模\n型的思维先验转移到短上下文模型，以提高短上下文模型的性能：\n1. 模型合并： 通过简单平均长上下文模型和短上下文模型的权重， 获得无需训练的新模型， 有\n助于保持泛化能力。\n2. 最短拒绝采样： 基于模型对同一问题生成响应长度变化大的特点， 对同一问题多次采样， 选\n择最短的正确响应。\n3. DPO ：利用长上下文模型生成多个响应样本，将最短正确解决方案作为正样本，较长响应\n作为负样本..."
      ],
      "response_time": 3.3835396766662598,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.33333333333333337,
        "rouge2": 0.0,
        "rougeL": 0.33333333333333337,
        "bleu": 0.5761436789607578,
        "semantic_similarity": 0.7390801906585693
      }
    },
    {
      "sample_id": "0084",
      "question": "RLOO算法是基于哪个算法发展而来的？",
      "ground_truth": "REINFORCE算法",
      "predicted_answer": "RLOO算法是基于REINFORCE算法发展而来的[1][2]。",
      "ground_context": [
        "REINFORCE Leave-One-Out（RLOO）算法是在 REINFORCE 算法基础上发展而来的一种改进算法"
      ],
      "retrieved_contexts": [
        "θn+1 = arg max\nθ\nJPPO(θn)\n7: 通过最小化均方误差来更新价值函数：\nϕn+1 = arg min\nϕ\nL(ϕn)\n8: end for\n6.2.5 RLOO\nREINFORCE Leave-One-Out（RLOO）算法是在 REINFORCE 算法基础上发展而来的一种改\n进算法，它主要针对 REINFORCE 算法梯度估计方差较高的问题，通过利用多个在线样本构建更\n有效的基线来降低方差，从而提升算法性能。\n1. 算法原理\nRLOO 的核心在于改进基线的构建方式。在 REINFORCE 算法中，通常使用简单的移动平均\n基线，这种基线在处理复杂环境和多样本情况时存在一定局限性。RLOO 则利用每次采样得到的\n多个样本之间的关系，为每个样本单独构建基线。",
        "大规模语言模型：从理论到实践 203\n假设在一次训练中，从策略 πθ(a|s) 中采样得到 k 个独立同分布的样本 y(1), · · · , y(k)\ni.i.d\n∼\nπθ(.|x)，对于每个样本 y(i)，其对应的奖励为 R(y(i), x)。RLOO 构建的基线为除 y(i) 之外的其他\nk − 1 个样本奖励的平均值，即 1\nk−1\nP\nj̸=i R(y(j), x)。\n基于此，RLOO 的策略梯度估计公式为：\n1\nk\nkX\ni=1\n2\n4R\n\u0000\ny(i), x\n\u0001\n− 1\nk − 1\nX\nj̸=i\nR\n\u0000\ny(j), x\n\u0001\n3\n5 ∇ log π\n\u0000\ny(i)|x\n\u0001\n(6.30)\n这个公式的含义是，对每个样本的奖励减去用其他样本构建的基线，再乘以该样本动作概率的对\n数梯度，最后对所有样本的结果进行平均，以此来估计策略梯度。\n2. 算法步骤\nRLOO 算法的实现步骤在 REINFORCE 算法基础上有所扩展：\n(1) 初始化策略参数：同 REINFORCE 算法，随机初始化策略网络参数 θ。\n(2) 采样多组轨迹： 使用当前策略πθ(a|s) 与环境交互， 每次收集k 条轨...",
        "多组样本集 {(y(m)\n(1) , · · · , y(m)\n(k) )}M\nm=1，其中 m 表示组数，M 为总的组数。\n(3) 计算 RLOO 基线和梯度估计：对于每组样本 (y(m)\n(1) , · · · , y(m)\n(k) )，为每个 y(m)\n(i) 计算 RLOO 基\n线 1\nk−1\nP\nj̸=i R(y(m)\n(j) , x)，并计算相应的策略梯度估计值：\nˆ∇θJ(θ)m = 1\nk\nkX\ni=1\n2\n4R\n\u0010\ny(m)\n(i) , x\n\u0011\n− 1\nk − 1\nX\nj̸=i\nR\n\u0010\ny(m)\n(j) , x\n\u0011\n3\n5 ∇ log π\n\u0010\ny(m)\n(i) |x\n\u0011\n(6.31)\n(4) 更新参数： 将多组样本的梯度估计值进行平均， 得到最终的梯度估计值， 然后沿梯度方向更\n新策略参数：\nˆ∇θJ(θ) = 1\nM\nMX\nm=1\nˆ∇θJ(θ)m (6.32)\nθ ← θ + α ˆ∇θJ(θ) (6.33)\n其中 α 为学习率。\n(5) 重复迭代：重复步骤 2 - 4 直至策略收敛。\n3. 与 REINFORCE 算法对比\n与 REINFORCE 算法对比，RL...",
        "204 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n(1) 方差降低效果：REINFORCE 算法使用简单基线 （如移动平均基线） ， 在降低方差方面效果有\n限。 而RLOO 通过利用多个样本构建动态基线， 能更有效地降低梯度估计的方差。 例如， 在\n实验中，RLOO 在相同训练条件下， 其奖励方差明显低于REINFORCE 算法， 这使得RLOO\n在优化过程中更加稳定，能够更快地收敛到较优的策略。\n(2) 样本利用效率：REINFORCE 算法在更新策略时，每个样本主要用于自身的梯度计算，样本\n之间的信息利用不足。RLOO 则充分利用了多个样本之间的关系， 每个样本不仅用于自身的\n梯度计算， 还参与构建其他样本的基线， 大大提高了样本的利用效率。实验表明， 在相同采\n样预算下，RLOO 能够实现更好的优化效果，如在多个数据集和模型上的实验显示，RLOO\n在胜率和奖励优化方面均优于 REINFORCE 算法。\n(3) 计算复杂度： 虽然RLOO 在样本利用和方差降低上具有优势， 但它的计算复杂度相对REIN-\nFORCE 算法有所增加。在构建基线时，RLOO 需要对每个样...",
        "随着样本数量 k 的增加， 计算量会相应增大。 不过， 在实际应用中， 由于其在性能上的显著\n提升，这种计算复杂度的增加在可接受范围内。\n4. 算法特性分析\nRLOO 算法在继承 REINFORCE 算法优点的同时，有效改进了其部分缺陷。它通过多样本构\n建基线的方式，降低了梯度估计的方差，提高了策略更新的稳定性和准确性，使得算法在复杂环\n境和大规模任务中表现更优。然而，RLOO 算法也并非完美无缺。在处理大规模样本时，其计算\n复杂度的增加可能会成为限制因素，需要消耗更多的计算资源和时间。此外，RLOO 算法对样本\n的独立性假设较为依赖，如果样本之间存在较强的相关性，可能会影响基线的有效性，进而影响\n算法性能。 在实际应用中， 需要根据具体问题的特点和资源情况， 合理选择是否使用RLOO 算法。\n6.2.6 GRPO\n图 6.2 GRPO 算法流程图（需要重画）"
      ],
      "response_time": 3.552159547805786,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.4,
        "rouge2": 0.0,
        "rougeL": 0.4,
        "bleu": 0.036422694300114855,
        "semantic_similarity": 0.7043595314025879
      }
    },
    {
      "sample_id": "0085",
      "question": "随着样本数量 k 的增加，计算量会如何变化？",
      "ground_truth": "计算量会相应增大。",
      "predicted_answer": "随着样本数量 \\( k \\) 的增加，计算量会相应增大[1]。尽管如此，在实际应用中，这种计算复杂度的增加仍在可接受范围内[1]。",
      "ground_context": [
        "随着样本数量 k 的增加，计算量会相应增大。"
      ],
      "retrieved_contexts": [
        "随着样本数量 k 的增加， 计算量会相应增大。 不过， 在实际应用中， 由于其在性能上的显著\n提升，这种计算复杂度的增加在可接受范围内。\n4. 算法特性分析\nRLOO 算法在继承 REINFORCE 算法优点的同时，有效改进了其部分缺陷。它通过多样本构\n建基线的方式，降低了梯度估计的方差，提高了策略更新的稳定性和准确性，使得算法在复杂环\n境和大规模任务中表现更优。然而，RLOO 算法也并非完美无缺。在处理大规模样本时，其计算\n复杂度的增加可能会成为限制因素，需要消耗更多的计算资源和时间。此外，RLOO 算法对样本\n的独立性假设较为依赖，如果样本之间存在较强的相关性，可能会影响基线的有效性，进而影响\n算法性能。 在实际应用中， 需要根据具体问题的特点和资源情况， 合理选择是否使用RLOO 算法。\n6.2.6 GRPO\n图 6.2 GRPO 算法流程图（需要重画）",
        "204 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n(1) 方差降低效果：REINFORCE 算法使用简单基线 （如移动平均基线） ， 在降低方差方面效果有\n限。 而RLOO 通过利用多个样本构建动态基线， 能更有效地降低梯度估计的方差。 例如， 在\n实验中，RLOO 在相同训练条件下， 其奖励方差明显低于REINFORCE 算法， 这使得RLOO\n在优化过程中更加稳定，能够更快地收敛到较优的策略。\n(2) 样本利用效率：REINFORCE 算法在更新策略时，每个样本主要用于自身的梯度计算，样本\n之间的信息利用不足。RLOO 则充分利用了多个样本之间的关系， 每个样本不仅用于自身的\n梯度计算， 还参与构建其他样本的基线， 大大提高了样本的利用效率。实验表明， 在相同采\n样预算下，RLOO 能够实现更好的优化效果，如在多个数据集和模型上的实验显示，RLOO\n在胜率和奖励优化方面均优于 REINFORCE 算法。\n(3) 计算复杂度： 虽然RLOO 在样本利用和方差降低上具有优势， 但它的计算复杂度相对REIN-\nFORCE 算法有所增加。在构建基线时，RLOO 需要对每个样..."
      ],
      "response_time": 3.4681830406188965,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.059273053487055624,
        "semantic_similarity": 0.7731160521507263
      }
    },
    {
      "sample_id": "0086",
      "question": "强化学习算法对模型性能优化的有效性是谁证明的？",
      "ground_truth": "Diamond 等",
      "predicted_answer": "Diamond等人证明了强化学习算法对模型性能优化的有效性[1]。",
      "ground_context": [
        "Diamond 等，也展现出强大的推理能力，证明了强化学习算法对模型性能优化的有效性。"
      ],
      "retrieved_contexts": [
        "Diamond 等，也展现出强大的推理能力，证明了强化学习算法对模型性能优化的有效性。\n• 自我进化过程： 训练过程中， 模型的思考时间和生成回答的长度不断增加， 这并非外部调整\n所致， 而是模型在强化学习环境中自我改进的结果。 随着测试时计算量的增加， 模型自发出\n现复杂行为， 如反思先前步骤、 探索多种解题方法等， 显著提升了推理能力， 使其能够处理\n更具挑战性的任务。\n• 顿悟时刻： 训练过程中出现 “aha moment” ， 模型在中间版本学会重新评估初始解题方法， 分\n配更多思考时间， 这一行为不仅体现了模型推理能力的提升， 也展示了强化学习可带来意想\n不到的复杂结果，凸显了强化学习在激发模型智能方面的潜力。",
        "找食物、住宿还是娱乐。\n现代强化学习之父 Richard Sutton 在 《苦涩的教训 （The Bitter Lesson） 》 中指出， 过去70 年人\n工智能研究领域最重要的一堂课是，只有通用的、可规模化扩展的方法才是最终有效的，而且优\n势巨大。 因此， 结合OpenAI 的研究实践， 强化学习在大语言模型中的优势可重新归纳为以下三个\n维度：\n（1）摆脱局部最优束缚的全局优化能力。监督学习依赖词元级精确标注，本质上将人类先验"
      ],
      "response_time": 3.3842031955718994,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.6666666666666666,
        "rouge2": 0.0,
        "rougeL": 0.6666666666666666,
        "bleu": 0.034430441335820114,
        "semantic_similarity": 0.5169724822044373
      }
    },
    {
      "sample_id": "0087",
      "question": "DeepSeek-R1 在 AIME 2024 上的 Pass@1 得分是多少？",
      "ground_truth": "79.8%",
      "predicted_answer": "DeepSeek-R1 在 AIME 2024 上的 Pass@1 得分为 79.8% [2]。",
      "ground_context": [
        "DeepSeek-R1 在多个推理任务中表现出色，在 AIME 2024 上 Pass@1 得分达到 79.8%，略超 OpenAI-o1-1217。"
      ],
      "retrieved_contexts": [
        "晰呈现。\n不使用结果或过程神经奖励模型，因其在大规模强化学习中可能出现奖励黑客问题，且重新训练\n奖励模型会增加计算资源需求并使训练流程复杂化。\n1.3 训练模板 设计简单训练模板， 要求DeepSeek-R1-Zero 先产生推理过程， 再给出最终答案。 模\n板为：用户提出问题，助手先在脑海中思考推理过程，然后提供答案，推理过程和答案分别包含\n在 <think> </think> 和 <answer> </answer> 标签内，训练时 prompt 会被具体推理问题替换。通过\n这种模板，在避免内容特定偏差的同时，引导模型遵循指定结构进行推理，便于观察模型在强化\n学习过程中的自然发展。\n1.4 性能、自我进化过程与顿悟时刻\n• 性能： 在AIME 2024 基准测试中，DeepSeek-R1-Zero 的平均 pass@1 分数从初始的 15.6% 显\n著提升至 71.0%，达到与 OpenAI-o1-0912 相当的性能水平。使用多数投票后，分数进一步\n提升至 86.7%，超过 OpenAI-o1-0912。在其他推理相关基准测试中，如 MA TH-500、GPQA\nDiamond ...",
        "动了相关领域的研究发展。\n4.2 模型性能卓越 DeepSeek-R1 在多个推理任务中表现出色， 在AIME 2024 上 Pass@1 得分达到\n79.8%，略超 OpenAI-o1-1217；在 MA TH-500上得分高达 97.3%，与 OpenAI-o1-1217 相当且远超\n其他模型。在编码相关任务中，于 Codeforces 竞赛中获得 2029 Elo 评级，超越 96.3% 的人类参与\n者；在知识类基准测试如 MMLU、GPQA Diamond 等任务中，也取得了优异成绩，展现出强大的\n知识掌握和推理应用能力。\n6.3.2 Kimi k1.5\n基于下一个 token 预测的语言模型预训练遵循缩放定律，即按比例增加模型参数和数据规模\n可提升模型智能程度。然而，这种方法严重依赖高质量训练数据的数量。在实际应用中，可用的\n高质量数据往往有限，这限制了模型性能的进一步提升。将强化学习与大语言模型相结合，有望",
        "出的同时，更符合用户需求和安全标准。\n3. 蒸馏：赋予小模型推理能力\n为使更小的模型具备类似 DeepSeek-R1 的推理能力，使用在 DeepSeek-R1 训练过程中收集的\n800k 样本，对 Qwen 和 Llama 等开源模型进行直接微调。实验发现，这种简单的蒸馏方法能显著\n提升小模型的推理能力。在实验中， 选择Qwen2.5-Math-1.5B、Qwen2.5-Math-7B 等多种模型作为\n基础模型，仅对蒸馏模型进行 SFT，未引入 RL 阶段，以突出蒸馏技术的有效性，后续 RL 阶段\n的探索留给研究社区。 结果显示， 蒸馏后的小模型在多个推理基准测试中表现优异， 如DeepSeek-\nR1-Distill-Qwen-7B 在 AIME 2024 上的成绩超越了部分强大的基线模型。\n4. 总结\n4.1 强化学习训练创新 DeepSeek-R1-Zero 首次验证了大语言模型的推理能力可通过纯强化学习\n激发，无需监督微调作为前期步骤。这种创新训练方式使模型能够自主探索思维链以解决复杂问\n题， 展现出自我验证、 反思和生成长思维链等能力， 为大语言模型推理能力提升开辟了新路径..."
      ],
      "response_time": 3.221240282058716,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.3636363636363636,
        "rouge2": 0.2222222222222222,
        "rougeL": 0.3636363636363636,
        "bleu": 0.014238348381961592,
        "semantic_similarity": 0.5056397318840027
      }
    },
    {
      "sample_id": "0088",
      "question": "环境会如何根据智能体的行为给予反馈？",
      "ground_truth": "通常以奖励的形式。",
      "predicted_answer": "环境会根据智能体采取的动作给出下一个状态及当前动作带来的奖励[1]。此外，智能体会通过持续获取环境反馈或监控环境变化，并基于反馈信息动态调整行动计划[3]。环境反馈可用于评估智能体一系列动作的整体效果，而中间反馈则着重考察每一步操作的即时表现[2]。",
      "ground_context": [
        "环境会根据智能体的行为给予反馈，通常以奖励的形式。"
      ],
      "retrieved_contexts": [
        "大规模语言模型：从理论到实践 191\n智能体和环境。 在强化学习过程中， 智能体与环境不断交互。 智能体在环境中获取某个状态后， 会\n根据该状态输出一个动作， 也称为决策。 动作会在环境中执行， 环境会根据智能体采取的动作， 给\n出下一个状态及当前动作带来的奖励。智能体的目标就是尽可能多地从环境中获取奖励。本节将\n介绍强化学习的基本概念、强化学习与有监督学习的区别，以及在大语言模型中基于人类反馈的\n强化学习流程。\n图 6.1 强化学习基本框架\n在现实生活中，经常会遇到需要通过探索和试错来学习的情境。例如，孩子学会骑自行车的\n过程或是教宠物狗如何玩飞盘。宠物狗一开始对如何抓飞盘一无所知，但每当它成功抓住飞盘时，\n都可以给予它一定的奖励。这种通过与环境交互，根据反馈来学习最佳行为的过程正是强化学习\n的核心思想。通过宠物狗学习抓飞盘的例子，可以引出一些强化学习中的基本概念。\n（1）智能体与环境：在宠物狗学习抓飞盘的场景中，宠物狗就是一个智能体（Agent），它做\n出决策（Decision）并执行动作。它所在的场景， 包括飞盘的飞行轨迹和速度， 以及其他可能的因\n素，构成了环境（Environ...",
        "能够根据环境反馈和人类反馈动态调整操作策略，从而不断优化工具的使用方法。\n在实际操作中，环境反馈通常通过智能体与外部环境交互后所获得的结果进行优化；具体而\n言，结果反馈用于评估智能体一系列动作的整体效果，而中间反馈则着重考察每一步操作的即时\n表现。例如，在 WebShop[393] 场景中，智能体通过对比其购买行为与人类购买行为之间的相似性\n来获得结果反馈，从而评估其表现的有效性。在此基础上，人类反馈强化学习通过模拟人类奖励\n机制，结合强化学习算法优化智能体的策略，以提升其决策能力和执行效果。同时，智能体会将\n每次尝试的结果系统化地记录下来，构建经验库。这一过程不仅使智能体能够积累丰富的操作经\n验，还能逐步提升其对工具的使用熟练度和操作效率。\n探索学习的关键在于通过持续的试探与调整，使智能体在动态环境中不断完善其工具使用能\n力。这种方法不仅赋予智能体更强的适应性与自主性，还为其在多变任务场景中的高效表现提供\n了坚实的技术支持。\n当前研究的重点在于如何通过整合多种学习策略来优化模型性能，从而全面提升大模型智能\n体的表现能力。例如，将示范学习的精确性与探索学习的灵活性相结合，可以显著增强模...",
        "际环境中面临执行困难或效果不佳的问题，特别是在忽略外部数据变化的情况下，智能体可能无\n法有效应对突发事件或异常情况。\n2. 带反馈规划\n带反馈规划（Planning with feedback）是一种更为复杂且灵活的规划方式， 智能体在执行任务\n的过程中能够持续获取环境反馈或监控环境变化，并基于反馈信息动态调整行动计划。在这一模\n式下，智能体不仅会在任务开始前制定初步的执行计划，还能够在任务执行过程中实时监测环境\n变化和任务进展，依据实际情况不断优化和修正计划。带反馈规划强调智能体与环境的交互，通\n过不断更新计划以确保任务的顺利完成。其显著优势在于高度的适应性与灵活性，尤其适用于环\n境复杂且变化频繁的任务场景。ReAct[390] 方法是大模型智能体带反馈规划方法的经典方法， 其核\n心在于将任务执行过程与推理规划过程相结合。在任务执行的每一步中，大模型智能体依据已完\n成的子任务和获得的环境反馈，动态生成当前步骤的子任务及相应的执行动作，并将其在真实环\n境中执行。完成后，环境反馈会被传递回智能体，用于下一步的任务规划。通过这一反复循环的\n过程，ReAct 方法使大模型智能体能够根据环境反...",
        "192 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n6.1.1 强化学习基础概念\n智能体与环境的不断交互过程中，会获得很多观测 oi。针对每一个观测，智能体会采取一个\n动作 ai，也会得到一个奖励 ri。可以定义历史 Ht 是观测、动作、奖励的序列：\nHt = o1, a1, r1, o2, a2, r2, · · · , ot, at, rt (6.1)\n由于智能体在采取当前动作时会依赖它之前得到的历史， 因此可以把环境整体状态St 看作关于历\n史的函数：\nSt = f (Ht) (6.2)\n当智能体能够观察到环境的所有状态时，称环境是完全可观测的（ Fully Observed） ，这时观测ot\n等于 St。当智能体只能看到部分观测时，称环境是部分可观测的（Partially Observed） ，这时观测\n是对状态的部分描述。整个状态空间使用 S 表示。\n在给定的环境中， 有效动作的集合经常被称为动作空间（Action Space）， 使用A 表示。例如\n围棋（Go）这样的环境具有离散动作空间（Discrete Action Space），智能体的动作数量在这个空\n...",
        "提升整体的工作效率和用户体验。\n在人机平等协作范式中，强调智能体与人类作为平等的合作伙伴，共同参与任务的规划与执\n行。这种模式注重智能体的适应性与自主性，通过协同合作实现任务的高效完成。例如，在任务\n执行过程中， 智能体能够主动寻求人类的反馈， 并根据反馈动态调整其行为策略[384]。 与人类主导\n范式不同，这种模式不仅要求智能体具备执行能力，还需要其通过自主学习与优化不断提升自身\n能力。\n随着智能体在环境感知、 推理与决策能力方面的进步， 人机交互的效率与深度也将不断提高。\n通过持续的优化与协同，人类与智能体之间可以实现真正的无缝合作，使智能体成为人类创新和\n效率提升的重要伙伴。\n8.2 大语言模型智能体架构\n智能体可以被视为独立的个体，能够接收并处理外部信息，进而给出响应。大模型智能体基\n本组成如图8.2 所示，主要包含以下几个核心模块：感知模块、规划模块、记忆模块、工具使用模\n块。对于外界输入，智能体借助多模态能力将文字、音频、图像等多种形式的信息转换为机器能\n够理解的表现形式；进而由规划模块对这些信息进行处理，结合记忆模块完成推理、规划等复杂\n任务；智能体可能会利用工具使用模块..."
      ],
      "response_time": 4.710941553115845,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.005608174786994817,
        "semantic_similarity": 0.5670962929725647
      }
    },
    {
      "sample_id": "0089",
      "question": "面向推理的强化学习采用了什么样的大规模强化学习训练过程？",
      "ground_truth": "采用与 DeepSeek-R1-Zero 相同的大规模强化学习训练过程。",
      "predicted_answer": "面向推理的强化学习采用了与 DeepSeek-R1-Zero 相同的大规模强化学习训练过程，该过程在冷启动微调后进行[2][3]。这个过程专注于编码、数学、科学和逻辑推理等推理密集型任务[1]。在训练过程中，针对CoT（Chain-of-Thought）语言混合问题，引入了语言一致性奖励，该奖励根据CoT中目标语言单词的比例进行计算[1]。尽管消融实验显示该奖励会导致模型性能轻微下降，但为了提高生成内容的可读性，仍将它与推理任务准确性奖励结合使用，直到推理任务收敛[1]。",
      "ground_context": [
        "在冷启动微调后，采用与 DeepSeek-R1-Zero 相同的大规模强化学习训练过程，聚焦于编码、数学、科学和逻辑推理等推理密集型任务。"
      ],
      "retrieved_contexts": [
        "识，提升了模型的性能潜力，为后续强化学习训练奠定更好基础。\n2.2 面向推理的强化学习 在冷启动微调后，采用与 DeepSeek-R1-Zero 相同的大规模强化学习训\n练过程，聚焦于编码、数学、科学和逻辑推理等推理密集型任务。针对训练中发现的CoT 语言混\n合问题，引入语言一致性奖励，根据 CoT 中目标语言单词的比例计算。虽然消融实验表明该奖励\n会使模型性能略有下降，但为提升生成内容的可读性，仍将其与推理任务准确性奖励直接相加作\n为最终奖励，持续训练模型直至推理任务收敛。\n2.3 拒绝采样和监督微调 当面向推理的 RL 训练接近收敛时， 利用此时的检查点收集用于后续轮\n次的监督微调数据。\n• 推理数据：通过拒绝采样生成推理轨迹， 扩展数据集。 除基于规则奖励评估的数据外， 纳入\n部分使用生成奖励模型评估的数据， 将真实标注和模型预测输入DeepSeek-V3 进行判断。 同\n时，过滤掉语言混合、长段落和代码块等难以阅读的思维链，每个 prompt 采样多个回答并",
        "大规模语言模型：从理论到实践 207\n模型在不同场景高效运用推理能力， 仍是学界和业界面临的挑战。 此前研究尝试了多种方法， 如基\n于过程的奖励模型、 强化学习以及蒙特卡洛树搜索和波束搜索等搜索算法， 但均未达到与OpenAI\no 系列模型相媲美的通用推理性能。在此背景下，DeepSeek 团队开展了基于纯强化学习提升模型\n推理能力的探索。\n1. DeepSeek-R1-Zero：基于基座模型的强化学习\n1.1 强化学习算法 DeepSeek 的研究人员采用 GRPO 算法进行强化学习， 该算法舍弃了传统Actor-\nCritic 范式中与策略模型规模相当的 critic 模型，通过从一组得分估计基线来优化策略模型。通过\n这种方式，能够提高强化学习的效率，有利于大规模强化学习的开展。\n1.2 奖励建模 采用基于规则的奖励系统，包含两种奖励类型：\n• 准确性奖励： 用于评估模型响应的正确性。 对于有确定性答案的数学问题， 要求模型按指定\n格式输出最终答案以便验证；对于 LeetCode 编程问题，利用编译器根据预定义测试用例生\n成反馈。\n• 格式奖励：促使模型将思考过程置于‘<think...",
        "晰呈现。\n不使用结果或过程神经奖励模型，因其在大规模强化学习中可能出现奖励黑客问题，且重新训练\n奖励模型会增加计算资源需求并使训练流程复杂化。\n1.3 训练模板 设计简单训练模板， 要求DeepSeek-R1-Zero 先产生推理过程， 再给出最终答案。 模\n板为：用户提出问题，助手先在脑海中思考推理过程，然后提供答案，推理过程和答案分别包含\n在 <think> </think> 和 <answer> </answer> 标签内，训练时 prompt 会被具体推理问题替换。通过\n这种模板，在避免内容特定偏差的同时，引导模型遵循指定结构进行推理，便于观察模型在强化\n学习过程中的自然发展。\n1.4 性能、自我进化过程与顿悟时刻\n• 性能： 在AIME 2024 基准测试中，DeepSeek-R1-Zero 的平均 pass@1 分数从初始的 15.6% 显\n著提升至 71.0%，达到与 OpenAI-o1-0912 相当的性能水平。使用多数投票后，分数进一步\n提升至 86.7%，超过 OpenAI-o1-0912。在其他推理相关基准测试中，如 MA TH-500、GPQA\nDiamond ...",
        "（Chain-of-Thought） 的决策序列， 在数学证明、 代码生成等场景中展现出超越监督学习的推理能力。\n相较于传统监督学习， 强化学习框架具有显著优势：在RLHF 范式下， 模型通过生成-反馈的\n闭环机制持续优化，摆脱对标准答案的绝对依赖；在深度推理场景中，强化学习能自主探索最优\n推理路径，通过价值函数估计引导模型突破局部最优解。两类方法都强调对生成文本的整体质量\n把控，前者侧重人类价值对齐，后者专注复杂问题求解，共同构成大语言模型能力进化的核心驱\n动力。\n本章将系统阐述基于人类反馈的强化学习技术体系，解析奖励模型构建、策略优化算法等关\n键组件。同时深入探讨强化学习在深度推理任务中的创新应用，包括思维链强化、过程奖励设计\n等前沿方法。最后通过 verl 实践案例，展示强化学习技术在大语言模型训练中的工程实现与效果\n验证。\n6.1 强化学习概述\n强化学习（Reinforcement Learning，RL）研究的是智能体与环境交互的问题，其目标是使智\n能体在复杂且不确定的环境中最大化 奖励。强化学习基本框架如图6.1 所示，主要由两部分组成：",
        "414 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n图 11.5 “红队”测试整体框架 [563]\n测试用例作为训练数据， 以最大似然估计损失为目标进行学习。 随机抽取90% 的测试用例组成训\n练集， 剩余的测试用例用于验证。 通过一次训练周期来学习pr(x)， 以保持测试用例的多样性并避\n免过拟合。\n（4） 强化学习： 使用强化学习来最大化有害性期望Epr(x)[r(x, y)]。 使用Advantage Actor-Critic\n（A2C）[564] 训练“红队”大语言模型pr(x)。 通过使用有监督学习得到的训练模型进行初始化热启\n动 pr(x)。为了防止强化学习塌陷到单个高奖励，还添加了损失项，使用当前 pr(x) 与初始化分布\n之间的 KL 散度。 最终损失是KL 散度惩罚项和 A2C 损失的线性组合， 使用α ∈ [0, 1] 进行两项之\n间的加权。\n11.2.3 垂直领域评估\n前面几节重点介绍了评估大语言模型整体能力的评估体系。本节将对垂直领域和重点能力的\n细粒度评估展开介绍，主要包括复杂推理、环境交互、特定领域。\n1. 复杂推理\n复杂推理（Complex ..."
      ],
      "response_time": 6.367220878601074,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.375,
        "rouge2": 0.2857142857142857,
        "rougeL": 0.375,
        "bleu": 0.09021032397789189,
        "semantic_similarity": 0.7491564750671387
      }
    },
    {
      "sample_id": "0090",
      "question": "GRPO算法如何在平衡策略的探索与利用的同时优化目标函数？",
      "ground_truth": "GRPO通过对目标函数的优化，能够利用组内奖励信息，同时平衡策略的探索与利用，从而实现高效稳定的训练。",
      "predicted_answer": "GRPO算法通过优化目标函数 \\( J_{GRPO}(\\theta) \\)，能够在利用组内奖励信息的同时平衡策略的探索与利用。具体而言，算法首先从旧策略中抽取出一组输出，然后基于这些输出计算每个动作的时间步上的优势值 \\(\\hat{A}_{i,t}\\)。接着，通过综合考虑优势值和KL散度项来调整当前策略参数，从而优化目标函数。这种方法不仅能够有效利用组内的奖励信息，还能通过控制策略变化的幅度来稳定训练过程，进而实现高效的策略优化[1][2][3]。",
      "ground_context": [
        "优化目标是最大化轨迹的期望回报 J(θ)，即：J(θ) = Eτ ∼P (τ ;θ) [R(τ )]",
        "通过对这个目标函数的优化，GRPO 能够在利用组内奖励信息的同时，平衡策略的探索与利用，实现高效稳定的训练。"
      ],
      "retrieved_contexts": [
        "通过对这个目标函数的优化，GRPO 能够在利用组内奖励信息的同时，平衡策略的探索与利\n用，实现高效稳定的训练。\n3. 算法步骤\n如图6.2所示，GRPO 算法实施的流程如下：\n(1) 初始化策略参数：随机初始化当前策略模型 πθ 的参数 θ 以及旧策略模型 πθold 的参数（通常\n初始值与 πθ 相同） 。\n(2) 抽取组样本：从分布 P (Q) 中采样问题 q，然后根据旧策略 πθold(O|q) 为每个问题 q 抽取 G\n个输出 {oi}G\ni=1。\n(3) 计算优势值和目标函数： 对于每个输出oi 的每个时间步 t， 计算优势值 ˆAi,t ， 并根据目标函",
        "i=1∼πθold (O|q)[ 1\nG\nGX\ni=1\n1\n|oi|\n|oi|X\nt=1\n[min( πθ(oi,t|q, oi,<t)\nπθold(oi,t|q, oi,<t)\nˆAi,t,\nclip\n\u0012 πθ(oi,t|q, oi,<t)\nπθold(oi,t|q, oi,<t) , 1 − ϵ, 1 + ϵ\n\u0013\nˆAi,t)] − βDKL [πθ||πref]]\n(6.34)\n在这个公式中 πθ 代表当前正在优化的策略模型，其参数为 θ，πθold 是旧的策略模型，用于提供参\n考和对比。G 表示组大小，即从旧策略 πθold 中抽取的多个输出 oi 的数量。每个 oi 都是一个完整\n的输出序列，|oi| 表示序列 oi 的长度。 ˆAi,t 是基于组内奖励计算得到的优势值，它衡量了在时间\n步 t 采取动作 oi,t 相对于平均水平的优势程度，用于指导策略的更新。ϵ 和 β 是超参数。ϵ 用于控\n制梯度剪切，防止策略更新幅度过大导致不稳定，β 则控制 KL 散度 DKL 的权重，DKL [πθ||πref]\n用于约束当前策略 πθ 和参考策略 πref 之间的差异，确保策略不会偏...",
        "大规模语言模型：从理论到实践 205\nGroup Relative Policy Optimization （GRPO）是一种基于近端策略优化算法改进而来的优化算\n法， 旨在解决传统PPO 在计算资源和训练稳定性方面的问题。 它通过创新的组奖励机制来估计基\n线，在不依赖独立价值模型的情况下实现高效训练，尤其适用于大型模型的优化。\n1. 算法概述\n传统的近端策略优化算法在训练过程中依赖独立的价值模型来估计奖励和减少方差。 然而， 这\n种方式在处理大型模型时会带来较高的计算成本和内存消耗。 GRPO 则另辟蹊径，它不再使用独\n立的价值模型，而是通过组奖励来估计基线。具体来说， GRPO 从旧策略中抽取多个输出（形成\n组） ， 利用组内奖励信息计算优势值， 以此优化策略。 这种方法避免了对每个样本都进行独立基线\n计算，大大减少了训练资源的消耗，在提升计算效率的同时，增强了训练过程的稳定性。\n2. 算法原理\nGRPO 的核心在于其优化目标函数的设计。目标函数 JGRPO(θ) 旨在最大化策略的期望奖励，\n同时控制策略的变化幅度，确保训练的稳定性：\nJGRPO(θ) =Eq∼P (Q),{oi}...",
        "206 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n数 JGRPO(θ) 的公式计算相应的项。 在计算过程中， 会用到当前策略πθ 和旧策略 πθold 对动作\n的概率估计。\n(4) 更新策略参数： 通过优化目标函数JGRPO(θ) ， 计算梯度并更新当前策略模型πθ 的参数 θ。 通\n常使用随机梯度下降（SGD）或其变种算法来进行参数更新。\n(5) 更新旧策略： 将更新后的当前策略πθ 的参数复制给旧策略模型 πθold ， 为下一轮迭代做准备。\n(6) 重复迭代重复步骤 2 - 5 ，直到达到预设的训练轮数、策略收敛或满足其他停止条件。\n4. 与 PPO 的对比\nPPO 算法通过价值函数来估计奖励，并使用优势函数减少方差，其目标函数为：\nJPPO(θ) = Eq∼P (Q),o∼πθold (O|q)\n\u0014\nmin\n\u0012 πθ(o|q)\nπθold(o|q) A, clip\n\u0012 πθ(o|q)\nπθold(o|q) , 1 − ϵ, 1 + ϵ\n\u0013\nA\n\u0013\u0015\n在这个公式中，依赖一个单独训练的价值函数来计算优势函数 A。而 GRPO 与之不同：\n(1) 计算负担方面：PPO..."
      ],
      "response_time": 5.991945266723633,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.15384615384615385,
        "rouge2": 0.0,
        "rougeL": 0.15384615384615385,
        "bleu": 0.08198198546422224,
        "semantic_similarity": 0.8582937717437744
      }
    },
    {
      "sample_id": "0091",
      "question": "在推理密集型任务的强化学习训练中，RLOO算法相比REINFORCE算法在方差降低和样本利用效率方面表现如何？",
      "ground_truth": "RLOO算法通过构建动态基线，能够更有效地降低梯度估计的方差，从而在优化过程中更加稳定，收敛更快。而在样本利用效率方面，RLOO充分利用多个样本之间的关系，使得每个样本不仅用于自身的梯度计算，还参与构建其他样本的基线，这大大提高了样本的利用效率。相比之下，REINFORCE算法的样本利用效率有限，主要用于自身的梯度计算。",
      "predicted_answer": "在推理密集型任务的强化学习训练中，RLOO算法相比REINFORCE算法表现出以下特点：\n\n- 方差降低方面：RLOO算法通过利用多个样本构建动态基线，能更有效地降低梯度估计的方差。例如，在实验中，即使在相同的训练条件下，RLOO的奖励方差明显低于REINFORCE算法，使其在优化过程中更加稳定，并且能够更快地收敛到较好的策略[1]。\n\n- 样本利用效率方面：RLOO算法充分考虑了样本间的关联性，每个样本不仅用于自身梯度计算，还参与到其他样本基线的构建中，这大大提升了样本的利用率。因此，在相同的采样预算下，RLOO能够在胜率和奖励优化方面取得比REINFORCE算法更好的结果[2][3]。",
      "ground_context": [
        "面向推理的强化学习...聚焦于编码、数学、科学和逻辑推理等推理密集型任务。",
        "RLOO通过利用多个样本构建动态基线，能更有效地降低梯度估计的方差...大大提高了样本的利用效率。"
      ],
      "retrieved_contexts": [
        "204 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n(1) 方差降低效果：REINFORCE 算法使用简单基线 （如移动平均基线） ， 在降低方差方面效果有\n限。 而RLOO 通过利用多个样本构建动态基线， 能更有效地降低梯度估计的方差。 例如， 在\n实验中，RLOO 在相同训练条件下， 其奖励方差明显低于REINFORCE 算法， 这使得RLOO\n在优化过程中更加稳定，能够更快地收敛到较优的策略。\n(2) 样本利用效率：REINFORCE 算法在更新策略时，每个样本主要用于自身的梯度计算，样本\n之间的信息利用不足。RLOO 则充分利用了多个样本之间的关系， 每个样本不仅用于自身的\n梯度计算， 还参与构建其他样本的基线， 大大提高了样本的利用效率。实验表明， 在相同采\n样预算下，RLOO 能够实现更好的优化效果，如在多个数据集和模型上的实验显示，RLOO\n在胜率和奖励优化方面均优于 REINFORCE 算法。\n(3) 计算复杂度： 虽然RLOO 在样本利用和方差降低上具有优势， 但它的计算复杂度相对REIN-\nFORCE 算法有所增加。在构建基线时，RLOO 需要对每个样...",
        "随着样本数量 k 的增加， 计算量会相应增大。 不过， 在实际应用中， 由于其在性能上的显著\n提升，这种计算复杂度的增加在可接受范围内。\n4. 算法特性分析\nRLOO 算法在继承 REINFORCE 算法优点的同时，有效改进了其部分缺陷。它通过多样本构\n建基线的方式，降低了梯度估计的方差，提高了策略更新的稳定性和准确性，使得算法在复杂环\n境和大规模任务中表现更优。然而，RLOO 算法也并非完美无缺。在处理大规模样本时，其计算\n复杂度的增加可能会成为限制因素，需要消耗更多的计算资源和时间。此外，RLOO 算法对样本\n的独立性假设较为依赖，如果样本之间存在较强的相关性，可能会影响基线的有效性，进而影响\n算法性能。 在实际应用中， 需要根据具体问题的特点和资源情况， 合理选择是否使用RLOO 算法。\n6.2.6 GRPO\n图 6.2 GRPO 算法流程图（需要重画）",
        "θn+1 = arg max\nθ\nJPPO(θn)\n7: 通过最小化均方误差来更新价值函数：\nϕn+1 = arg min\nϕ\nL(ϕn)\n8: end for\n6.2.5 RLOO\nREINFORCE Leave-One-Out（RLOO）算法是在 REINFORCE 算法基础上发展而来的一种改\n进算法，它主要针对 REINFORCE 算法梯度估计方差较高的问题，通过利用多个在线样本构建更\n有效的基线来降低方差，从而提升算法性能。\n1. 算法原理\nRLOO 的核心在于改进基线的构建方式。在 REINFORCE 算法中，通常使用简单的移动平均\n基线，这种基线在处理复杂环境和多样本情况时存在一定局限性。RLOO 则利用每次采样得到的\n多个样本之间的关系，为每个样本单独构建基线。",
        "多组样本集 {(y(m)\n(1) , · · · , y(m)\n(k) )}M\nm=1，其中 m 表示组数，M 为总的组数。\n(3) 计算 RLOO 基线和梯度估计：对于每组样本 (y(m)\n(1) , · · · , y(m)\n(k) )，为每个 y(m)\n(i) 计算 RLOO 基\n线 1\nk−1\nP\nj̸=i R(y(m)\n(j) , x)，并计算相应的策略梯度估计值：\nˆ∇θJ(θ)m = 1\nk\nkX\ni=1\n2\n4R\n\u0010\ny(m)\n(i) , x\n\u0011\n− 1\nk − 1\nX\nj̸=i\nR\n\u0010\ny(m)\n(j) , x\n\u0011\n3\n5 ∇ log π\n\u0010\ny(m)\n(i) |x\n\u0011\n(6.31)\n(4) 更新参数： 将多组样本的梯度估计值进行平均， 得到最终的梯度估计值， 然后沿梯度方向更\n新策略参数：\nˆ∇θJ(θ) = 1\nM\nMX\nm=1\nˆ∇θJ(θ)m (6.32)\nθ ← θ + α ˆ∇θJ(θ) (6.33)\n其中 α 为学习率。\n(5) 重复迭代：重复步骤 2 - 4 直至策略收敛。\n3. 与 REINFORCE 算法对比\n与 REINFORCE 算法对比，RL...",
        "大规模语言模型：从理论到实践 203\n假设在一次训练中，从策略 πθ(a|s) 中采样得到 k 个独立同分布的样本 y(1), · · · , y(k)\ni.i.d\n∼\nπθ(.|x)，对于每个样本 y(i)，其对应的奖励为 R(y(i), x)。RLOO 构建的基线为除 y(i) 之外的其他\nk − 1 个样本奖励的平均值，即 1\nk−1\nP\nj̸=i R(y(j), x)。\n基于此，RLOO 的策略梯度估计公式为：\n1\nk\nkX\ni=1\n2\n4R\n\u0000\ny(i), x\n\u0001\n− 1\nk − 1\nX\nj̸=i\nR\n\u0000\ny(j), x\n\u0001\n3\n5 ∇ log π\n\u0000\ny(i)|x\n\u0001\n(6.30)\n这个公式的含义是，对每个样本的奖励减去用其他样本构建的基线，再乘以该样本动作概率的对\n数梯度，最后对所有样本的结果进行平均，以此来估计策略梯度。\n2. 算法步骤\nRLOO 算法的实现步骤在 REINFORCE 算法基础上有所扩展：\n(1) 初始化策略参数：同 REINFORCE 算法，随机初始化策略网络参数 θ。\n(2) 采样多组轨迹： 使用当前策略πθ(a|s) 与环境交互， 每次收集k 条轨..."
      ],
      "response_time": 7.382540702819824,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.42857142857142855,
        "rouge2": 0.33333333333333337,
        "rougeL": 0.42857142857142855,
        "bleu": 0.18095732793160604,
        "semantic_similarity": 0.8926854729652405
      }
    },
    {
      "sample_id": "0092",
      "question": "在策略梯度算法中，如何通过剪切机制解决高方差问题以提高优化的稳定性？",
      "ground_truth": "PPO算法通过引入剪切机制，限制重要性权重在特定范围内，以避免优化不稳定，从而解决高方差问题。",
      "predicted_answer": "在策略梯度算法中，PPO算法通过引入剪切机制来解决高方差问题并提高优化的稳定性。具体而言，PPO算法使用了一个名为“剪切”的机制，通过将重要性采样的权重限制在一个特定范围内来防止权重变得过于极端，从而减少优化过程中的方差。这个范围由超参数ε定义，权重会被裁剪到\\(1-\\epsilon\\)和\\(1+\\epsilon\\)之间[1]。这样可以确保即使两个策略分布存在差异，也能有效降低方差，进而稳定优化过程。\n\n[1]",
      "ground_context": [
        "基于蒙特卡洛采样的 REINFORCE 方法作为经典的策略梯度算法，存在以下显著缺陷：首先，其依赖完整轨迹采样的蒙特卡洛特性导致梯度估计方差过高，这不仅会显著延缓收敛速度，还容易引发策略更新方向的剧烈波动，造成训练过程的不稳定性。",
        "PPO算法引入了剪切机制，通过将权重限制在特定范围内来避免优化不稳定，即：JPPO(θ) = E(s,a)∼πθ′ (a|s) clip(πθ(a|s)/πθ′ (a|s), 1 − ε, 1 + ε) A(s, a)。"
      ],
      "retrieved_contexts": [
        "202 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n前面提到，重要性采样需要保证两个策略分布相似，否则高方差会导致优化不稳定。因此， PPO\n算法引入了剪切机制，通过将权重限制在特定范围内来避免优化不稳定，即：\nJPPO(θ) = E(s,a)∼πθ′ (a|s)\n\u0014\nclip\n\u0012 πθ(a|s)\nπθ′ (a|s) , 1 − ε, 1 + ε\n\u0013\nA(s, a)\n\u0015\n(6.29)\n其中，ε 是超参数（例如可以设置为 0.1 或 0.2） 。Clip 函数裁剪重要性权重的大小，限制权重在\n1 − ε 和 1 + ε 之间。\n2. 算法流程\n综合上面的推导过程，我们可以得到 PPO 算法的流程，如代码 6.1 所示。\n代码 6.1: PPO 算法的流程\n1: 输入: 初始策略参数 θ0，初始价值函数参数 ϕ0\n2: for n = 0, 1, 2, · · · do\n3: 收集轨迹集合 Dn = {τi}，通过在环境中执行策略 πθn\n4: 针对每条轨迹计算回报 Rt\n5: 基于当前的价值函数 Vϕn ，使用广义优势估计方法计算优势 At\n6: 通过最小化策略梯度损失函数...",
        "从分布 P 时的期望。\n不过，两个分布的差异不能过大，否则会导致以下问题：\n(1) 高方差：当分布差异较大时，权重 P (x)\nQ(x) 可能出现极端值，导致估计的期望值方差增大。\n(2) 偏差：为了解决高方差问题，通常需要对权重进行裁剪或限制，这可能引入偏差。\n假设用于与环境交互的智能体策略为 θ′， 用于学习的智能体策略为θ， 应用重要性采样后， 可\n以将策略梯度公式改为异策略的形式，即：\n∇θJ(θ) = E(s,a)∼πθ(a|s) [A(s, a)∇θ log πθ(a|s)]\n= E(s,a)∼πθ′ (a|s)\n\u0014 pθ(s, a)\npθ′ (s, a) A(s, a)∇θ log πθ(a|s)\n\u0015 (6.26)\n其中，pθ(s, a) = πθ(a|s)p(s) 表示状态-动作对出现的概率，状态的概率被认为与策略无关，以便\n进行优化。因此，最终的策略梯度为：\n∇θJ(θ) = E(s,a)∼πθ′ (a|s)\n\u0014 πθ(a|s)\nπθ′ (a|s) A(s, a)∇θ log πθ(a|s)\n\u0015\n(6.27)\n从上述梯度形式反推 PPO 的目标函数为：\nJ(θ) = ...",
        "间步骤， 而是将策略本身参数化（例如用神经网络表示） ， 直接通过梯度上升优化策略参数， 让智\n能体更倾向于选择能带来高回报的动作。简单来说，策略梯度通过反复试验，统计哪些动作在特\n定状态下更容易获得奖励，并像“调整旋钮”一样微调策略，使得这些动作在未来被选中的概率\n逐渐增加。\n这一方法的优势在于能天然处理连续动作、随机策略以及部分观测环境，但也面临梯度估计\n方差大、训练不稳定等挑战。本节将从策略梯度的基础概念出发，回顾经典算法如 REINFORCE，\nPPO 等，并讨论在大模型时代流行的 GRPO，RLOO 等方法。\n6.2.1 策略梯度\n策略梯度方法是强化学习中一类重要的算法，它直接优化策略函数 π(a|s; θ)，以最大化预期\n的回报（累计奖励）R(τ ) =P∞\nt=0 γtrt，其中 θ 是策略的参数。",
        "大规模语言模型：从理论到实践 201\n此，策略梯度方法需要不断地在环境交互中学习，训练效率较低。\n注意，在策略梯度方法中，同一个智能体既负责与环境交互，也负责策略参数更新，这种训\n练方法被称为同策略（On-Policy）训练方法。相反，异策略（Off-Policy）训练方法将这两个职能\n分开，即固定一个智能体与环境交互而不更新，另一个智能体则只负责从采集的数据中学习更新\n参数。这种方式可以重复利用历史数据。然而，由于两个智能体的分布不同，直接更新会导致不\n稳定的训练。 一种思路是调整这两个分布使它们保持一致，重要性采样（Importance Sampling） 就\n是在这种思路下的重要技术。\n1. 算法原理\n假设我们希望计算期望 Ex∼P (x)[f (x)]， 但采样数据来自另一个分布Q(x)， 可以通过设置采样\n数据的权重来修正结果：\nEx∼P (x)[f (x)] = Ex∼Q(x)\n\u0014 P (x)\nQ(x) f (x)\n\u0015\n(6.25)\n从 P 中每次采样一个 xi 并计算 f (xi)，都需要乘上一个重要性权重 P (xi)\nQ(xi) 来修正这两个分布\n的差异，这种方法被称...",
        "制要求每次策略更新后必须重新采样轨迹数据，导致样本利用率低下，难以适应大规模复杂任务\n的需求。虽然策略的随机性天然具备探索优势，但高方差问题可能削弱这一优势对学习效果的促\n进作用。最后，该方法主要适用于小规模离散动作空间场景，对函数近似误差敏感的特性也限制\n了其在连续动作空间或深度强化学习框架中的应用范围。\n6.2.3 广义优势估计\n为了克服蒙特卡洛方法的缺陷 （高方差和完整轨迹依赖） ， 研究者们提出了时序差分方法 （Tem-\nporal Difference Methods，TD） 。 时序差分方法基于动态规划的思想， 通过引入Bootstrapping 机制，\n即利用当前的价值估计来更新自身，而不必等待完整的轨迹结束。这种方法允许在每个时间步进\n行更新，极大地提高了样本效率。\n对于给定的状态 st 和动作 at，时序差分方法的基本更新公式为：\nQ(st, at) ← Q(st, at) + α[rt + γV (st+1)Q(st, at)]\n其中，α 是学习率， 控制更新步长，γ 是折扣因子， 控制未来奖励的权重。 由于只涉及单步奖励和\n下一个状态的估计，TD 方法的方差通常低..."
      ],
      "response_time": 5.947822570800781,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.2222222222222222,
        "rouge2": 0.0,
        "rougeL": 0.2222222222222222,
        "bleu": 0.07325960768235429,
        "semantic_similarity": 0.9126709699630737
      }
    },
    {
      "sample_id": "0093",
      "question": "ppo_mini_batch_size 设置为多少？",
      "ground_truth": "64",
      "predicted_answer": "根据提供的文档，我无法回答这个问题。文档中提到的是 `critic.ppo_micro_batch_size_per_gpu` 而不是 `ppo_mini_batch_size` 。[2]",
      "ground_context": [
        "这个参数设置为 64，意味着每次从训练集中选取 64 个样本组成一个小批次，用于计算梯度和更新演员模型的参数。"
      ],
      "retrieved_contexts": [
        "避免因数据量过大导致的显存溢出问题。 log_prob_micro_batch_size_per_gpu 表示每个 GPU\n上用于计算对数概率的微批次大小。在强化学习中，对数概率用于计算策略梯度，它反映了模型\n在当前策略下采取某个动作的概率。将这个参数设置为 8，即每个 GPU 在计算对数概率时，每次\n处理 8 个样本的微批次，这样可以优化计算过程，提高训练效率。",
        "actor_rollout_ref.rollout.log_prob_micro_batch_size_per_gpu=8\nactor_rollout_ref.rollout.tensor_model_parallel_size=1\nactor_rollout_ref.rollout.gpu_memory_utilization=0.4\nactor_rollout_ref.ref.log_prob_micro_batch_size_per_gpu=4\ncritic.optim.lr=1e-5\ncritic.model.path=Qwen/Qwen2.5-0.5B-Instruct\ncritic.ppo_micro_batch_size_per_gpu=4\nalgorithm.kl_ctrl.kl_coef=0.001\ntrainer.logger=['console']\n+trainer.val_before_train=False\ntrainer.default_hdfs_dir=null\ntrainer.n_gpus_per_node=1\ntrainer.nnodes=1\ntrai..."
      ],
      "response_time": 3.7701313495635986,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0.0,
        "bleu": 0.0,
        "semantic_similarity": 0.30212897062301636
      }
    },
    {
      "sample_id": "0094",
      "question": "在模仿学习中，训练数据包含了什么？",
      "ground_truth": "输入和相应的期望输出，即专家生成的正确答案。",
      "predicted_answer": "在模仿学习中，训练数据包含了输入和相应的期望输出，即专家生成的正确答案。[1]",
      "ground_context": [
        "在模仿学习中，训练数据包含了输入和相应的期望输出，即专家生成的正确答案。"
      ],
      "retrieved_contexts": [
        "大规模语言模型：从理论到实践 217\n其中 σ 是 sigmoid 函数，r 代表参数为 ψ 的奖励模型的值，r (x, y) 表示针对输入提示 x 和输出 y\n预测出的单一标量奖励值。利用标量值可以对一对样本进行打分，分数差值 r(x, yw) − r(x, yl) 反\n映了两条回复的差异程度。例如，在验证集上的分差分布如图 6.4 所示，其中大部分样本能够被\n正确判别，即分差大于 0，但是仍然有一部分样本的分差小于 0，这部分样本模型无法正确分类。\n事实上，在奖励模型建模过程中，由于人类偏好的主观性，数据集噪声是不可避免的问题。\n此外， 文献[249] 引入了模仿学习的思想。 在模仿学习中， 训练数据包含了输入和相应的期望\n输出，即专家生成的正确答案。模型的目标是学习从输入到输出的映射，以便能够在类似的输入\n上生成类似的输出。这种方法对于每一对输出，在输出上引入了自回归的语言模型损失，使模型\n能够在每个句子对中模仿首选的输出。 在实际操作中， 在语言模型损失上引入了系数βrm， 以调节\n其影响。得到如下奖励模型损失：\nL (ψ) = −λE(x,yw,yl)∼Drm [log σ ..."
      ],
      "response_time": 3.494527578353882,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.5086841681174387,
        "semantic_similarity": 0.7871177196502686
      }
    },
    {
      "sample_id": "0095",
      "question": "Drm 表示什么？",
      "ground_truth": "训练数据集的经验分布。",
      "predicted_answer": "Drm 表示训练数据集的经验分布。[1]",
      "ground_context": [
        "其中 Drm 表示训练数据集的经验分布。"
      ],
      "retrieved_contexts": [
        "L (ψ) = −λE(x,yw,yl)∼Drm [log σ (r (x, yw) − r (x, yl))] − βrmE(x,yw)∼Drm [log (r′ (x, yw))] (6.36)\n其中 Drm 表示训练数据集的经验分布。r′ 是与 r 相同的模型， 只有顶层的线性层与r 有所不同， 该\n线性层的维度与词汇表的大小相对应。 在r′ 模型中，r′ (x, yw) 表示在给定输入提示 x 和首选输出\nyw 的条件下的似然概率，这个似然概率表达了模型生成给定输出的可能性。\n另外，还可以引入一个附加项到奖励函数中，该附加项基于学习得到的强化学习策略 πRL\nϕ 与\n初始监督模型 πSFT 之间的 Kullback-Leibler（KL） 散度， 从而引入了一种惩罚机制。 总奖励可以根\n据文献 [250] 通过如下方式表达：\nrtotal = r (x, y) − ηKL\n\u0000\nπRL\nϕ (y|x) , πSFT (y|x)\n\u0001\n(6.37)\n其中 η 代表 KL 奖励系数， 用于调整KL 惩罚的强度。 这个KL 散度项在这里发挥着两个重要的作\n用。首先，它作为一个熵奖励，促进..."
      ],
      "response_time": 2.632782459259033,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.5156626918239822,
        "semantic_similarity": 0.7600151300430298
      }
    },
    {
      "sample_id": "0096",
      "question": "Kimi k1.5 在 AIME 上达到了多少分？",
      "ground_truth": "77.5 分",
      "predicted_answer": "Kimi k1.5 在 AIME 上达到了 77.5 分[2]。",
      "ground_context": [
        "出色的推理性能：在长思维链模式下，Kimi k1.5 在多个基准测试和模态中达到了 SOTA 模型 OpenAI o1 正式版的水平，如在 AIME 上达到 77.5 分，MA TH 500上达到 96.2 分，在 Codeforces"
      ],
      "retrieved_contexts": [
        "212 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n3. 长上下文激活阶段：使用上采样的长上下文冷却数据训练，处理扩展序列任务。通过上采\n样长上下文数据，在训练期间使用不同比例的全注意力数据和部分注意力数据，并逐步增加最大\n序列长度。\n大规模 RL 训练系统：设计大规模 RL 训练系统， 采用部分回放技术优化长CoT RL 训练。 该\n系统通过迭代同步方法操作，每次迭代包括展开阶段和训练阶段。在展开阶段，展开工作进程生\n成响应序列并存储在重放缓冲区；在训练阶段，训练工作进程访问这些经验更新模型权重。系统\n还包含代码执行服务（沙箱） ，用于处理代码相关问题，评估模型在实际编码场景中的输出。\n混合部署框架：利用 Kubernetes Sidecar 容器共享 GPU 资源，实现训练和推理任务的并行执\n行。 在Megatron 和 vLLM 之上实现混合部署框架， 训练到推理阶段的转换耗时不到一分钟， 反之\n约十秒钟。该框架可解决复杂的并行策略、最小化空闲 GPU 资源和实现动态扩展等问题。\n4. 技术优势\n出色的推理性能：在长思维链模式下，Kimi k1.5 在多个基准测试和...",
        "OpenAI o1 正式版的水平，如在 AIME 上达到 77.5 分，MA TH 500上达到 96.2 分，在 Codeforces\n上达到 94 百分位，在 MathVista 上达到 74.9 分。在短思维链模式下，其数学、代码、视觉多模态\n和通用能力也表现出色，大幅超越现有短链思维模型，如在 AIME 上达到 60.8 分，MA TH500上\n达到 94.6 分，LiveCodeBench 上达到 47.3 分，提升幅度高达 550%。\n创新的技术架构通过长上下文扩展和改进的策略优化，Kimi k1.5 建立了一个简单有效的 RL\n框架， 无需依赖复杂技术如蒙特卡罗树搜索、 价值函数和过程奖励模型， 即可实现强大的性能。 同\n时，Long2short 方法有效提升了短 CoT 模型的性能，提高了模型的 token 效率。\n高效的数据处理与训练精心设计的数据工程确保了训练数据的高质量和多样性，多模态数据\n的有效整合使模型能够更好地理解和处理不同类型的信息。优化的训练架构和算法，如部分回放\n技术、混合部署框架等，提高了训练效率和系统的可扩展性。\n6.4 基于人类反馈的强化学习\n6...",
        "动了相关领域的研究发展。\n4.2 模型性能卓越 DeepSeek-R1 在多个推理任务中表现出色， 在AIME 2024 上 Pass@1 得分达到\n79.8%，略超 OpenAI-o1-1217；在 MA TH-500上得分高达 97.3%，与 OpenAI-o1-1217 相当且远超\n其他模型。在编码相关任务中，于 Codeforces 竞赛中获得 2029 Elo 评级，超越 96.3% 的人类参与\n者；在知识类基准测试如 MMLU、GPQA Diamond 等任务中，也取得了优异成绩，展现出强大的\n知识掌握和推理应用能力。\n6.3.2 Kimi k1.5\n基于下一个 token 预测的语言模型预训练遵循缩放定律，即按比例增加模型参数和数据规模\n可提升模型智能程度。然而，这种方法严重依赖高质量训练数据的数量。在实际应用中，可用的\n高质量数据往往有限，这限制了模型性能的进一步提升。将强化学习与大语言模型相结合，有望"
      ],
      "response_time": 2.9790992736816406,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 0.5
      },
      "answer_quality_metrics": {
        "rouge1": 0.4444444444444445,
        "rouge2": 0.2857142857142857,
        "rougeL": 0.4444444444444445,
        "bleu": 0.044249674106354955,
        "semantic_similarity": 0.6752841472625732
      }
    },
    {
      "sample_id": "0097",
      "question": "ImageBind可以支持哪些数据的编码？",
      "ground_truth": "图像、文本、音频、深度、热成像和惯性测量单元（Inertial Measurement Unit，IMU）等多种数据的编码。",
      "predicted_answer": "ImageBind可以支持图像、文本、音频、深度、热成像和惯性测量单元（IMU）等多种数据的编码。[256]",
      "ground_context": [
        "ImageBind[256] 则可以支持图像、 文本、 音频、 深度、 热成像和惯性测量单元 （Inertial Measurement Unit，IMU） 等多种数据的编码。"
      ],
      "retrieved_contexts": [
        "232 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\nImageBind[256] 则可以支持图像、 文本、 音频、 深度、 热成像和惯性测量单元 （Inertial Measurement\nUnit，IMU） 等多种数据的编码。 然而， 在多模态学习中， 将这些模态特征对齐到同一语义空间是\n关键难点。 时间对齐 （如视频帧与字幕的对齐） 和语义对齐 （如图像内容与文本描述的一致性） 要\n求模型具备强大的对齐能力，同时需要在保证语义一致性的基础上，解决模态间特征表达方式差\n异带来的融合挑战。\n此外，多模态数据中的长序列处理能力也是模型架构设计中的瓶颈问题。现有的 Transformer\n架构在处理长序列时，由于自注意力机制的计算复杂度为 O(n2)，随着序列长度增加，内存和计\n算成本会迅速飙升，难以高效处理长时间视频或长篇文本等数据。同时，捕捉长时依赖性也是一\n个挑战。例如，在多模态任务中，视频的全局语义信息可能需要结合其字幕的长时上下文进行建\n模， 而传统的Transformer 模型往往难以在长序列中充分捕获这种全局信息。 因此， 如何设计高效\n的长序列建模机制，同时控...",
        "230 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n能够响应多种模态的输入， 包括音频、3D 点云、 视频及其嵌入空间。此外，ImageBind-LLM 还通\n过仅进行图像-文本对齐训练， 实现了多模态的指令跟随能力。 在训练过程中，ImageBind-LLM 采\n用一个可学习的绑定网络（Bind Network） ，将LLaMA 与 ImageBind 图像编码器的嵌入空间对齐。\n然后，绑定网络转换后的图像特征被添加到 LLaMA 所有层的词语 Token 中，从而通过一种无注\n意力且零初始化的门控机制逐步注入视觉指令。\n4. KOSMOS 系列\nKOSMOS 是微软开发的一系列多模态大语言模型，将语言模型原生支持多模态数据作为目\n标。 通过结合语言理解与视觉感知能力， 为多模态学习提供了另外的解决方案。KOSMOS-1[258] 从\n预训练阶段开始之初，便引入多模态数据，支持文本、图像和语音输入，原生具备处理多模态信\n息的能力。 因此，KOSMOS-1 能够同时胜任语言任务、 感知-语言任务和视觉任务， 包括视觉对话、\nOCR、 简单数学方程求解以及带描述的零样本图像..."
      ],
      "response_time": 3.7419097423553467,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.28571428571428575,
        "rouge2": 0.0,
        "rougeL": 0.28571428571428575,
        "bleu": 0.7232927766551176,
        "semantic_similarity": 0.798658549785614
      }
    },
    {
      "sample_id": "0098",
      "question": "整个系统通过什么实现多角色的协同训练？",
      "ground_truth": "资源池管理",
      "predicted_answer": "整个系统通过资源池管理实现多角色（Actor/Critic/Ref Policy等）的协同训练。[1]",
      "ground_context": [
        "整个系统通过资源池管理实现多角色 （Actor/Critic/Ref Policy 等） 的协同训练， 具有分布式训练、弹性资源调度和可扩展的架构设计等特点。"
      ],
      "retrieved_contexts": [
        "大规模语言模型：从理论到实践 221\n键组件。 整个系统通过资源池管理实现多角色 （Actor/Critic/Ref Policy 等） 的协同训练， 具有分布\n式训练、弹性资源调度和可扩展的架构设计等特点。"
      ],
      "response_time": 3.5520570278167725,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.05022135506220879,
        "semantic_similarity": 0.6696091890335083
      }
    },
    {
      "sample_id": "0099",
      "question": "如何初始化训练器的工作者？",
      "ground_truth": "trainer.init_workers()",
      "predicted_answer": "根据提供的文档，我无法回答这个问题。文档中未提及具体初始化训练器工作者的过程。",
      "ground_context": [
        "trainer.init_workers()"
      ],
      "retrieved_contexts": [
        "reward_fn=reward_fn,\nval_reward_fn=val_reward_fn)\ntrainer.init_workers()\ntrainer.fit()\nif __name__ == '__main__':\nmain()",
        "142 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n5. DeepSpeed 初始化\n设置 DeepSpeed 的配置参数后， 可以利用DeepSpeed 进行模型训练的初始化， 初始化流程如下。\n（1）确定运行的设备：首先，检查代码是否有指定的本地 GPU（通过 args.local_rank） 。如果\n没有指定，则程序默认使用 CUDA 设备。否则，它会为进程设置指定的 GPU。\n（2） 初始化分布式后端： 在分布式训练中， 使用deepspeed.init_distributed() 函数实现每个进程\n与其他进程的同步，初始化分布式环境。\n（3）获取当前进程的全局排序：在分布式训练中，使用torch.distributed.get_rank() 函数获得每\n个进程的唯一排序或 ID。\n（4）设置 DeepSpeed 配置：根据用户参数（如是否进行 offload、使用哪个 Zero Stage 等）构\n建一个 DeepSpeed 配置字典，来决定训练设置。\n（5） 同步所有工作进程： 使用torch.distributed.barrier() 确保在进一步的初始化之..."
      ],
      "response_time": 3.2613885402679443,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.0,
        "semantic_similarity": 0.6099667549133301
      }
    },
    {
      "sample_id": "0100",
      "question": "强化学习的核心在于实现什么?",
      "ground_truth": "智能体策略与价值函数的优化，从而提升性能与稳定性。",
      "predicted_answer": "强化学习的核心在于通过一系列精心设计的计算和控制机制，实现智能体策略与价值函数的优化，从而提升性能与稳定性[1]。",
      "ground_context": [
        "强化学习的核心在于通过一系列精心设计的计算和控制机制，实现智能体策略与价值函数的优化，从而提升性能与稳定性。"
      ],
      "retrieved_contexts": [
        "大规模语言模型：从理论到实践 225\n3.1 优势函数与回报计算 强化学习的核心在于通过一系列精心设计的计算和控制机制，实现智\n能体策略与价值函数的优化，从而提升性能与稳定性。因此 verl 的核心算法模块涵盖多个关键功\n能模块。在系数控制上，有根据 KL 散度动态调整系数的 AdaptiveKLController 和系数固定不\n变的 FixedKLController，并通过 get_kl_controller 依据配置返回对应实例。优势函数与回\n报计算方面，包含计算广义优势估计和回报的 compute_gae_advantage_return，以及针对不同\n算法的优势函数计算方法， 如GRPO、REINFORCE++、ReMax 算法对应的优势计算函数。 奖励计\n算通过 compute_rewards 完成， 依据分数、 对数概率等计算最终奖励。 在损失计算上， 分别有利\n用裁剪技巧限制更新幅度的策略损失计算 comput_policy_loss、保持策略多样性的熵损失计算\ncompute_entropy_loss、 防止过拟合的价值损失计算 compute_value_loss...",
        "大规模语言模型：从理论到实践 191\n智能体和环境。 在强化学习过程中， 智能体与环境不断交互。 智能体在环境中获取某个状态后， 会\n根据该状态输出一个动作， 也称为决策。 动作会在环境中执行， 环境会根据智能体采取的动作， 给\n出下一个状态及当前动作带来的奖励。智能体的目标就是尽可能多地从环境中获取奖励。本节将\n介绍强化学习的基本概念、强化学习与有监督学习的区别，以及在大语言模型中基于人类反馈的\n强化学习流程。\n图 6.1 强化学习基本框架\n在现实生活中，经常会遇到需要通过探索和试错来学习的情境。例如，孩子学会骑自行车的\n过程或是教宠物狗如何玩飞盘。宠物狗一开始对如何抓飞盘一无所知，但每当它成功抓住飞盘时，\n都可以给予它一定的奖励。这种通过与环境交互，根据反馈来学习最佳行为的过程正是强化学习\n的核心思想。通过宠物狗学习抓飞盘的例子，可以引出一些强化学习中的基本概念。\n（1）智能体与环境：在宠物狗学习抓飞盘的场景中，宠物狗就是一个智能体（Agent），它做\n出决策（Decision）并执行动作。它所在的场景， 包括飞盘的飞行轨迹和速度， 以及其他可能的因\n素，构成了环境（Environ...",
        "（Chain-of-Thought） 的决策序列， 在数学证明、 代码生成等场景中展现出超越监督学习的推理能力。\n相较于传统监督学习， 强化学习框架具有显著优势：在RLHF 范式下， 模型通过生成-反馈的\n闭环机制持续优化，摆脱对标准答案的绝对依赖；在深度推理场景中，强化学习能自主探索最优\n推理路径，通过价值函数估计引导模型突破局部最优解。两类方法都强调对生成文本的整体质量\n把控，前者侧重人类价值对齐，后者专注复杂问题求解，共同构成大语言模型能力进化的核心驱\n动力。\n本章将系统阐述基于人类反馈的强化学习技术体系，解析奖励模型构建、策略优化算法等关\n键组件。同时深入探讨强化学习在深度推理任务中的创新应用，包括思维链强化、过程奖励设计\n等前沿方法。最后通过 verl 实践案例，展示强化学习技术在大语言模型训练中的工程实现与效果\n验证。\n6.1 强化学习概述\n强化学习（Reinforcement Learning，RL）研究的是智能体与环境交互的问题，其目标是使智\n能体在复杂且不确定的环境中最大化 奖励。强化学习基本框架如图6.1 所示，主要由两部分组成：",
        "找食物、住宿还是娱乐。\n现代强化学习之父 Richard Sutton 在 《苦涩的教训 （The Bitter Lesson） 》 中指出， 过去70 年人\n工智能研究领域最重要的一堂课是，只有通用的、可规模化扩展的方法才是最终有效的，而且优\n势巨大。 因此， 结合OpenAI 的研究实践， 强化学习在大语言模型中的优势可重新归纳为以下三个\n维度：\n（1）摆脱局部最优束缚的全局优化能力。监督学习依赖词元级精确标注，本质上将人类先验"
      ],
      "response_time": 3.8828635215759277,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.40236057392677443,
        "semantic_similarity": 0.7575907707214355
      }
    },
    {
      "sample_id": "0101",
      "question": "在多模态学习中，KOSMOS-1如何结合语言理解与视觉感知能力，而ImageBind-LLM的训练方法又是如何支持多模态指令跟随能力的？",
      "ground_truth": "KOSMOS-1通过原生支持多模态数据，实现语言理解和视觉感知的结合，能够处理文本、图像和语音输入，从而胜任语言任务、感知-语言任务和视觉任务。而ImageBind-LLM则通过图像-文本对齐训练，利用一个可学习的绑定网络将LLaMA与ImageBind图像编码器的嵌入空间对齐，实现多模态的指令跟随能力。",
      "predicted_answer": "KOSMOS-1 通过结合语言理解和视觉感知能力来支持多模态学习，在预训练阶段就引入了多模态数据，包括文本、图像和语音输入，使其能够处理多模态信息，进而同时胜任语言任务、感知-语言任务和视觉任务，如视觉对话、OCR、简单数学方程求解以及带描述的零样本图像分类等[1][2]。\n\nImageBind-LLM 则是通过仅进行图像-文本对齐训练实现了多模态的指令跟随能力。在训练过程中，它采用了一个可学习的绑定网络，将LLaMA与 ImageBind 图像编码器的嵌入空间对齐，随后绑定网络转换后的图像特征被添加到 LLaMA 所有层的词语 Token 中，通过一种无注意力且零初始化的门控机制逐步注入视觉指令[1]。",
      "ground_context": [
        "KOSMOS 是微软开发的一系列多模态大语言模型，将语言模型原生支持多模态数据作为目标。",
        "KOSMOS-1 从预训练阶段开始之初，便引入多模态数据，支持文本、图像和语音输入。",
        "ImageBind-LLM 通过仅进行图像-文本对齐训练，实现了多模态的指令跟随能力。",
        "在训练过程中，ImageBind-LLM 采用一个可学习的绑定网络，将LLaMA 与 ImageBind 图像编码器的嵌入空间对齐。"
      ],
      "retrieved_contexts": [
        "230 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n能够响应多种模态的输入， 包括音频、3D 点云、 视频及其嵌入空间。此外，ImageBind-LLM 还通\n过仅进行图像-文本对齐训练， 实现了多模态的指令跟随能力。 在训练过程中，ImageBind-LLM 采\n用一个可学习的绑定网络（Bind Network） ，将LLaMA 与 ImageBind 图像编码器的嵌入空间对齐。\n然后，绑定网络转换后的图像特征被添加到 LLaMA 所有层的词语 Token 中，从而通过一种无注\n意力且零初始化的门控机制逐步注入视觉指令。\n4. KOSMOS 系列\nKOSMOS 是微软开发的一系列多模态大语言模型，将语言模型原生支持多模态数据作为目\n标。 通过结合语言理解与视觉感知能力， 为多模态学习提供了另外的解决方案。KOSMOS-1[258] 从\n预训练阶段开始之初，便引入多模态数据，支持文本、图像和语音输入，原生具备处理多模态信\n息的能力。 因此，KOSMOS-1 能够同时胜任语言任务、 感知-语言任务和视觉任务， 包括视觉对话、\nOCR、 简单数学方程求解以及带描述的零样本图像...",
        "OCR、 简单数学方程求解以及带描述的零样本图像分类等。KOSMOS-1 的训练是在大规模的多模\n态语料库上进行的， 包括单模态数据 （例如文本语料库） 、 跨模态配对数据 （例如图像-字幕对） 以\n及交错的多模态数据（例如包含任意交错图像和文本的文档） 。\nKOSMOS-2[259] 采用了与 KOSMOS-1 相同的模型架构， 引入了基于语义和描述的视觉定位任\n务， 使得模型能够更准确地将文本与视觉对象连接， 并实现细粒度的对象级交互。 为了训练Kosmos-\n2， 研究团队构建了GRIT（Grounded Image-Text pairs） 数据集， 包含大量图像和文本对。 这个数据\n集通过将图像中的物体与相应文本描述进行精确匹配，极大地丰富了模型的训练数据，提高了其\n在多模态任务中的表现，尤其在文本密集图像任务中表现出色，能够生成结构化 Markdown 文本。\nKOSMOS-2.5[260] 结合基于 ViT（Vision Transformer）[261] 的视觉编码器和 Transformer 结构的\n解码器， 通过重采样模块进行连接， 实现了高效的多模态数据处理。 这种...",
        "解码器， 通过重采样模块进行连接， 实现了高效的多模态数据处理。 这种统一的模型接口简化了下\n游任务训练，并提升了模型的指令执行能力。KOSMOS-2.5 能够处理文本与图像协作的复杂任务，\n例如生成具有空间感知的文本块或以 Markdown 格式生成结构化文本输出。同时， KOSMOS-2.5\n在文本密集图像的理解上表现优异，支持信息提取、布局分析、视觉问答、截图理解以及用户界\n面自动化等多种任务。\n5. 开源模型\nLLaV A（Large Language and Vision Assistant）[262] 是开源的多模态大语言模型， 通过端到端训\n练方式，将视觉编码器（如 CLIP 的 ViT-L/14）与大语言模型（如 LLaMA、Vicuna）相结合，实\n现了对多模态指令的深刻理解与执行。其架构主要包括三部分： （1）视觉编码器负责提取输入图\n像的特征； （2）语言模型用于理解用户的语言指令并生成响应； （3）跨模态连接器（通常是线性\n层） 将视觉特征与语言模型的输入对齐， 从而实现跨模态信息的融合。 这种设计使得LLaV A能够\n高效处理和理解复杂的多模态任务。\nMini..."
      ],
      "response_time": 7.26718544960022,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.6666666666666666,
        "rouge2": 0.5000000000000001,
        "rougeL": 0.6666666666666666,
        "bleu": 0.2951267635242864,
        "semantic_similarity": 0.9483897686004639
      }
    },
    {
      "sample_id": "0102",
      "question": "在评估模型回答的质量时，如何影响答案提取和人类偏好数据集的设计？",
      "ground_truth": "在评估模型回答的质量时，答案提取方法（如'严格'或'灵活'）会影响最终得分，而人类偏好数据集的设计则通过收集对话的选择与拒绝，确保数据集中的每个配对比较具有相同的权重。使用更有害的回应可能导致对话朝着更有害的方向发展。",
      "predicted_answer": "根据提供的文档，我无法回答这个问题。",
      "ground_context": [
        "answer = extract_solution(solution_str=solution_str, method=method)",
        "收集数据的时候，... 只把数据集中的每个配对比较都当作二选一，并且权重相同"
      ],
      "retrieved_contexts": [],
      "response_time": 2.925416946411133,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 8.457159203181278e-05,
        "semantic_similarity": 0.4311448633670807
      }
    },
    {
      "sample_id": "0103",
      "question": "在深度学习中，如何结合强化学习和掩码预测方法来优化模型的性能？",
      "ground_truth": "强化学习中的PPO（Proximal Policy Optimization）方法可以通过特征转换和损失函数优化来提升模型性能，而掩码预测则通过自编码器的方式在视觉语言模型中增强对图像和文本的理解。这两者结合可以提升模型对缺失信息的恢复能力。",
      "predicted_answer": "虽然文档中没有直接提到结合强化学习和掩码预测的方法，但是我们可以推测一种可能的方式：\n\n首先，可以使用掩码预测方法来处理图像-文本配对数据集，在这种情况下，可以通过掩码某些描述性词汇让模型学习恢复这些被遮蔽的内容[277]。接着，可以引入强化学习来优化这个过程，比如通过设计一个奖励系统，其中准确性奖励可以用来评估模型恢复的描述是否准确，而格式奖励则鼓励模型产生符合预期格式的回答[3]。这样，模型不仅能在掩码预测任务中表现良好，还能通过强化学习不断提高其推理能力和泛化能力。",
      "ground_context": [
        "run_ppo(config)",
        "掩码（Masking）预测方法扮演着重要角色， 它本质上属于自编码器的一种特殊变体。"
      ],
      "retrieved_contexts": [
        "时，系统将各类特征转换为数值向量表示，并通过损失函数优化以使描述内容与图像特征在向量\n空间中相互接近。\n2. 掩码预测\n在深度学习领域， 掩码（Masking）预测方法扮演着重要角色， 它本质上属于自编码器的一种\n特殊变体[277]。 掩码预测在视觉语言模型中应用， 主要体现在两种训练模式上： 基于文本描述来恢\n复图像的缺失部分； 通过遮掩描述性词汇， 让模型从图像中提取并复现这些被遮蔽的语义信息， 如\n图7.3所示。",
        "大规模语言模型：从理论到实践 207\n模型在不同场景高效运用推理能力， 仍是学界和业界面临的挑战。 此前研究尝试了多种方法， 如基\n于过程的奖励模型、 强化学习以及蒙特卡洛树搜索和波束搜索等搜索算法， 但均未达到与OpenAI\no 系列模型相媲美的通用推理性能。在此背景下，DeepSeek 团队开展了基于纯强化学习提升模型\n推理能力的探索。\n1. DeepSeek-R1-Zero：基于基座模型的强化学习\n1.1 强化学习算法 DeepSeek 的研究人员采用 GRPO 算法进行强化学习， 该算法舍弃了传统Actor-\nCritic 范式中与策略模型规模相当的 critic 模型，通过从一组得分估计基线来优化策略模型。通过\n这种方式，能够提高强化学习的效率，有利于大规模强化学习的开展。\n1.2 奖励建模 采用基于规则的奖励系统，包含两种奖励类型：\n• 准确性奖励： 用于评估模型响应的正确性。 对于有确定性答案的数学问题， 要求模型按指定\n格式输出最终答案以便验证；对于 LeetCode 编程问题，利用编译器根据预定义测试用例生\n成反馈。\n• 格式奖励：促使模型将思考过程置于‘<think...",
        "（Chain-of-Thought） 的决策序列， 在数学证明、 代码生成等场景中展现出超越监督学习的推理能力。\n相较于传统监督学习， 强化学习框架具有显著优势：在RLHF 范式下， 模型通过生成-反馈的\n闭环机制持续优化，摆脱对标准答案的绝对依赖；在深度推理场景中，强化学习能自主探索最优\n推理路径，通过价值函数估计引导模型突破局部最优解。两类方法都强调对生成文本的整体质量\n把控，前者侧重人类价值对齐，后者专注复杂问题求解，共同构成大语言模型能力进化的核心驱\n动力。\n本章将系统阐述基于人类反馈的强化学习技术体系，解析奖励模型构建、策略优化算法等关\n键组件。同时深入探讨强化学习在深度推理任务中的创新应用，包括思维链强化、过程奖励设计\n等前沿方法。最后通过 verl 实践案例，展示强化学习技术在大语言模型训练中的工程实现与效果\n验证。\n6.1 强化学习概述\n强化学习（Reinforcement Learning，RL）研究的是智能体与环境交互的问题，其目标是使智\n能体在复杂且不确定的环境中最大化 奖励。强化学习基本框架如图6.1 所示，主要由两部分组成："
      ],
      "response_time": 6.128519296646118,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0.0,
        "bleu": 0.015380019129471637,
        "semantic_similarity": 0.7856941819190979
      }
    },
    {
      "sample_id": "0104",
      "question": "X-VLM 的训练依赖于哪些大规模标注数据集？",
      "ground_truth": "COCO、Visual Genome、SBU 和 Conceptual Captions",
      "predicted_answer": "X-VLM 的训练依赖于多个大规模标注数据集，包括 COCO[316]、Visual Genome[317]、SBU 和 Conceptual Captions[318]。",
      "ground_context": [
        "X-VLM 的训练依赖于多个大规模标注数据集，包括 COCO、Visual Genome、SBU 和 Conceptual Captions。"
      ],
      "retrieved_contexts": [
        "248 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n标注， 它有效地帮助模型理解复杂的视觉与文本关系。X-VLM 的训练依赖于多个大规模标注数据\n集，包括 COCO[316]、Visual Genome[317]、SBU 和 Conceptual Captions[318]，总计包含约 1600 万张\n图像。这些数据集丰富的标注信息为模型提供了大量高质量的视觉语义关联训练样本，使其在图\n文检索、视觉推理、视觉语义对齐以及图像描述等任务中均表现优异，超越了其他现有方法。这\n表明，边界框标注不仅能够提升模型的性能，还为复杂任务提供了更强的泛化能力。\n除了直接利用现成的标注数据集，一些研究者选择通过公开模型生成新的图文对数据集。例\n如，Kosmos-2[259] 使用网络爬取的数据构建了大规模图文对。其方法首先借助 spaCy 从文本中提\n取名词，然后通过基础模型 GLIP[319] 检测与这些名词相关的边界框。随后，使用 spaCy 从文本中\n进一步提取与名词对应的描述，生成能够与检测到的边界框匹配的图文对。这种方法显著扩展了\n标注数据的规模，为提升模型在视觉语义关联任务中的...",
        "模型训练，避免模态间竞争导致的不稳定性。 Chameleon 在大量多样化的数据上进行预训练，包\n括文本、图像 - 文本对以及交错序列等。这种多样化的训练数据使模型能够学习丰富的多模态表\n示，显著提升了其泛化能力和对复杂多模态任务的适应性。\n4. 映射学习\nVLM 的训练通常面临显著的计算开销问题， 依赖庞大的计算资源和海量数据支持。 为解决这\n一问题，映射学习范式提出了一种高效的训练方法，即在现有的大语言模型和视觉特征提取模型\n的基础上进行二次训练，如图 7.5 所示。该方法通过利用开源的大语言模型，重点学习文本模态\n与图像模态之间的映射关系。通过构建这种映射，大语言模型能够适应视觉任务，同时显著降低\n对计算资源的需求。\nA photo Of a bird 大语言模型图像编码器映射网络\n图 7.5 视觉语言模型映射学习范式 [271]\nFrozen [284] 是一种将预训练大语言模型与视觉信息相融合的开创性方法。该方法设计了一种\n简洁高效的特征转换架构， 用于将图像特征映射到文本语义空间。 具体来说， 它采用NF-ResNet-50\n作为图像特征提取的基础模型，并训练了一个特征到语..."
      ],
      "response_time": 4.073689699172974,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.7058823529411764,
        "rouge2": 0.4,
        "rougeL": 0.7058823529411764,
        "bleu": 0.14957644445778928,
        "semantic_similarity": 0.7209202647209167
      }
    },
    {
      "sample_id": "0105",
      "question": "AnyGPT 能够处理哪些模态的输入与输出？",
      "ground_truth": "文本、语音、图像和音乐四种模态",
      "predicted_answer": "AnyGPT 能够处理文本、语音、图像和音乐这四种模态的输入与输出[1][4]。",
      "ground_context": [
        "AnyGPT 统一了文本、语音、图像和音乐四种模态，并实现了任意模态组合的相互转换，为多模态交互提供了一个"
      ],
      "retrieved_contexts": [
        "多模态大语言模型的架构种类繁多，其设计方式根据任务需求和输入输出的模态复杂性而有\n所不同。本节将重点介绍两种具有代表性的多模态模型：一是能够处理任意模态输入与输出的多\n模态大语言模型 AnyGPT[300]， 二是具有多视觉编码器融合架构的眸思 （MouSi）[301]。AnyGPT 通\n过统一的框架实现了跨模态的无缝交互，具备高度灵活的适应性，而眸思则通过集成多个视觉编\n码器，大幅增强了对复杂视觉信息的理解与生成能力。两者在多模态领域均展现出强大的性能和\n应用潜力。\n1. AnyGPT\nAnyGPT 将所有模态的数据转换为统一的离散化表示，并基于大语言模型采用的 Next Token\nPrediction 任务进行统一训练。 基于GPT 的原始架构以及多模态的离散化表示，AnyGPT 统一了文\n本、语音、图像和音乐四种模态，并实现了任意模态组合的相互转换，为多模态交互提供了一个",
        "242 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n统一的框架，如图 7.10 所示。\n语音解码器 图像解码器音乐解码器\n语音编码器 语音编码器音乐编码器\n语音文本图像 音乐\n图 7.10 AnyGPT 模型框架 [300]\nAnyGPT 所提出的统一的多模态生成框架由三个核心组成部分构成：多模态分词器、多模态\n大语言模型以及多模态生成器。具体来说，多模态分词器的作用是将连续的非文本模态数据转换\n为离散的 Token，并将这些 Token 组织成多模态交错序列。随后，大语言模型以 Next Token 预测\n损失为目标，在这些多模态序列上进行统一训练。在推理阶段，生成的多模态 Token 会通过对应\n的生成器解码回原始的模态表示。为了进一步提升生成结果的质量，还可以借助多模态增强模块\n对输出进行后处理，例如声音克隆或图像超分辨率等技术。\nAnyGPT 使用 SEED[302] 作为图像分词器。SEED 由 ViT 编码器、因果 Q-Former、VQ（V ector\nQuantization）码本、多层感知机以及 UNet 解码器组成，其内部码本（Codebook）包含 ...",
        "初始化。 最终， 所有模态的Token 组合形成了一个新的词汇表， 其大小等于所有模态的token 数之\n和。借助特定模态的分词器，能够将多模态数据压缩为离散的 Token 序列。语言模型在这些序列\n上执行 Next Token Prediction 任务进行训练，从而使核心的 LLM 能以自回归的方式自然地统一多\n模态感知、理解、推理和生成等任务。AnyGPT 使用 LLaMA-2 7B 的参数对大语言模型进行初始\n化，除了扩展嵌入矩阵和预测头外，语言模型的其余部分保持不变。\n使用大语言模型生成高质量的多模态数据是一项具有挑战性的任务，因为图像和音频的精确\n表示需要大量存储，导致序列长度显著增加，从而提高了语言模型的计算复杂度。为了解决这一\n问题，AnyGPT 提出了一种两阶段框架，用于高质量多模态数据生成，包括语义信息建模和感知\n信息建模。在语义层面，自回归语言模型生成融合且对齐的多模态 Token 序列；随后，非自回归\n模型将这些多模态语义 Token 转换为高保真的多模态内容，从而在性能和效率之间取得平衡。\n具体来说，在视觉语言建模中使用 SEED 标记，并通过扩散模型将其解码...",
        "大语言模型：从理论到实践 243\n换为一个大小为 500 × 8 的矩阵，其中包含 500 × 1 的语义 Token 和 500 × 8 的声学 Token。\nAnyGPT 使用 Encodec[294] 作为音乐分词器。Encodec 内部包含 4 个码本，每个码本包含 2048\n个词元表示。 具体实现中， 使用一个在音乐数据上预训练的模型， 输入为32kHz 的单声道音频。 编\n码器将输入音频转换为嵌入向量， 随后通过残差向量量化 （RVQ） 进行量化， 使用4 个量化器， 每\n个量化器包含 2048 个码元，从而生成一个总数为 8192 的音乐 Token 表示。对于 5 秒长度的音频，\nEncodec 会将其量化为一个大小为 250 × 4 的码元矩阵。 为了适配语言模型的输入格式， 将这些码\n元按逐帧方式展平成一维序列，便于语言模型预测完整的音乐信息。\n为了将多模态的离散表示纳入预训练的大语言模型， AnyGPT 对模型进行了扩展，具体包括\n将每种模态的 Token 加入到词汇表中，并相应地扩展嵌入层和预测层。新加入的参数均采用随机\n初始化。 最终， 所有模态的Token ...",
        "7. 多模态大语言模型\n2023 年 3 月，GPT-4 的发布标志着大语言模型首次支持视觉模态输入，赋予其理解图像并生\n成相关自然语言内容的能力[65]。一年后，2024 年 5 月推出的 GPT-4o 更进一步，实现了文本、图\n像和语音等多模态信息的深度融合，使 ChatGPT 转型为具备实时语音对话能力的数字个人助理。\nGPT-4o 在视觉和语音交互方面表现尤为突出， 能够查看用户上传的屏幕截图、 照片、 文档或图表，\n并基于这些内容与用户展开对话。大规模预训练范式不仅在语言模型领域取得了突破性成功，也\n显著推动了视觉模型和语音模型在音视频编码、多模态感知等领域的发展。近年来，多模态预训\n练架构逐渐统一到基于 Transformer 的框架之下， 大大促进了大语言模型与其他模态模型之间的深\n度交互与融合，也使得多模态大语言模型成为研究的前沿热点。\n本章将重点介绍多模态大语言模型基础、多模态大语言模型架构、多模态大语言模型训练策\n略以及应用实践。\n7.1 多模态大语言模型基础\n人们日常处理的数据不仅限于文本内容（例如对话、 文章、 指令等语言信息的表达形式） ， 还\n包含视觉模态（..."
      ],
      "response_time": 3.4487502574920654,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.26512298021756175,
        "semantic_similarity": 0.7529333829879761
      }
    },
    {
      "sample_id": "0106",
      "question": "MiniGPT-4 如何减少训练开销？",
      "ground_truth": "MiniGPT-4 将预训练的大语言模型和视觉编码器同时冻结，只需要单独训练线性投影层，使视觉特征和语言模型对齐。",
      "predicted_answer": "为了减少训练开销，MiniGPT-4 将预训练的大语言模型和视觉编码器同时冻结，仅需单独训练线性投影层，使视觉特征和语言模型对齐[1]。",
      "ground_context": [
        "为了减少训练开销、避免全参数微调带来的潜在威胁， MiniGPT-4 将预训练的大语言模型和视觉编码器同时冻结，只需要单独训练线性投影层，使视觉特征和语言模型对齐。"
      ],
      "retrieved_contexts": [
        "# ൐ႨQ-Former໓Ч\nquery_output = self.Qformer.bert(\nquery_embeds=query_tokens,\nencoder_hidden_states=image_embeds,\nencoder_attention_mask=image_atts,\nreturn_dict=True,\n)\n#ࡼQ-Formerൻೆ\ninputs_llama = self.llama_proj(query_output.last_hidden_state)\n#ᇿၩ৯ဃ઒\natts_llama = torch.ones(inputs_llama.size()[:-1], dtype =torch.long).to(image.device)\n#ᇿၩ৯ဃ઒\nreturn inputs_llama, atts_llama\n为了减少训练开销、避免全参数微调带来的潜在威胁， MiniGPT-4 将预训练的大语言模型和\n视觉编码器同时冻结，只需要单独训练线性投影层，使视觉特征和语言模型对齐。如图7.13 所示，\n输入的粉色 logo 在经过一个冻结的视觉编码器模块后， 通过...",
        "大语言模型：从理论到实践 251\n7.4.1 MiniGPT-4 模型架构\nMiniGPT-4 期望将来自预训练视觉编码器的图像信息与大语言模型的文本信息对齐，它的模\n型架构如图7.13 所示，具体来说主要由三个部分构成：预训练的大语言模型 Vicuna[41]、预训练的\n视觉编码器，以及一个单一的线性投影层。\n图 7.13 MiniGPT-4 的模型架构 [252]\n1. Vicuna 模型\nVicuna 是一个基于解码器的大语言模型，它建立在 LLaMA[34] 的基础上，可以执行多种复杂\n语言任务。在 MiniGPT-4 中，它的主要任务是同时理解输入的文本与图像数据，对多个模态的信\n息具有感知理解能力， 生成符合指令的文本描述。 在具体的构建过程中，MiniGPT-4 并不从头开始\n训练大语言模型，而是直接利用现有的 Vicuna-13B 或 Vicuna-7B 版本，冻结所有的参数权重，降\n低计算开销。相关的预训练代码可以参考第 4 章和第 5 章的相关内容。\n2. 视觉编码器\n为了让大语言模型具备良好的视觉感知能力，MiniGPT-4 使用了与 BLIP-2[263] 相同...",
        "大语言模型：从理论到实践 255\n7.4.2 MiniGPT-4 训练策略\n为了获得真正具备多模态能力的大语言模型，MiniGPT-4 提出了一种分为两阶段的训练方法。\n第一阶段，MiniGPT-4 在大量的图像-文本对数据上进行预训练，以获得基础的视觉语言知识。第\n二阶段，MiniGPT-4 使用数量更少但质量更高的图像-文本数据集进行微调，以进一步提高预训练\n模型的生成质量与综合表现。\n1. MiniGPT-4 预训练\n在预训练阶段，MiniGPT-4 希望从大量的图像-文本对中学习视觉语言知识，所以使用了来自\nConceptual Caption [318, 334]、SBU[335] 和 LAION[336] 的组合数据集进行模型预训练。以 Conceptual\nCaption 数据集为例，数据格式如图7.14 所示，包含基本的图像信息与对应的文本描述。",
        "258 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n这段代码实现了整个 MiniGPT-4 模型的前向传播过程， 包括图像和文本的编码、 提示处理、 多\n模态数据编码的连接， 以及最终损失的计算。通过在Conceptual Caption、SBU 等组合数据集上进\n行计算，即可获得预训练的 MiniGPT-4 模型。\n在第一轮训练完成后，MiniGPT-4 获得了关于图像的丰富知识，并且可以根据人类查询提供\n合理的描述。但是它在生成连贯的语句输出方面遇到了困难，例如，可能会产生重复的单词或句\n子、碎片化的句子或者完全不相关的内容。这样的问题降低了 MiniGPT-4 与人类进行真实交流时\n流畅的视觉对话能力。\n2. 高质量数据集构建\n研究人员注意到，预训练的 GPT-3 曾面临类似的问题。虽然在大量的语言数据集上做了预训\n练，但模型并不能直接生成符合用户意图的文本输出。 GPT-3 通过从人类反馈中进行指令微调和\n强化学习，产生了更加人性化的输出。借鉴这一点，研究人员期望预训练的 MiniGPT-4 也可以做\n到与用户意图对齐，增强模型的可用性。\n为此，研究人员精心构建了...",
        "在预训练的基础上， 研究人员使用精心构建的高质量图像-文本对对预训练的 MiniGPT-4 模型\n进行微调。在训练过程中，MiniGPT-4 同样要完成类似的文本描述生成任务，不过具体的任务指\n令不再固定， 而是来自一个更广泛的预定义指令集。例如， “详细描述此图像” 、 “你可以为我描述\n此图像的内容吗” ，或者“解释这张图像为什么有趣” 。微调训练只在训练数据集和文本提示上与\n预训练过程略微不同，在此不再介绍相关的代码实现。\n微调结果表明，MiniGPT-4 能够产生更加自然、 更加流畅的视觉问答反馈。 同时， 这一训练过\n程也是非常高效的，只需要 400 个训练步骤，批量大小为 12，使用单块 NVIDIA A100 80GB GPU\n训练 7 分钟即可完成。\n在微调完成后，研究人员发现 MiniGPT-4 具备其他各种有趣的能力，这是在 GPT-4 的演示中\n没有体现的，例如：通过观察诱人的食物照片，直接生成详细的食谱；识别图像中存在的问题并\n提供相应的解决方案；直接从图像中检索出有关人物、电影或绘画作品的事实信息。如图7.16 所\n示，用户希望 MiniGPT-4 指出输入的..."
      ],
      "response_time": 3.554466962814331,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.8,
        "rouge2": 0.6666666666666666,
        "rougeL": 0.8,
        "bleu": 0.6788295451309077,
        "semantic_similarity": 0.9358550906181335
      }
    },
    {
      "sample_id": "0107",
      "question": "语音到文本模态的转换是目前广泛采用的方法之一吗？",
      "ground_truth": "是的。",
      "predicted_answer": "是的，语音到文本模态的转换是目前广泛采用的方法之一[2]。",
      "ground_context": [
        "语音到文本模态的转换是目前广泛采用的方法之一。"
      ],
      "retrieved_contexts": [
        "在此基础上引入了 LSTM 模块以增强序列分析能力，同时结合 Transformer 架构优化离散符号序\n列的建模能力，从而在多种语音任务中取得了显著的性能提升。\n3. 语音和文本表示融合架构\n获得语音模态信息后， 需要将其与文本模态信息集成， 以便大语言模型进行进行最终推理。 语\n音和文本表示融合主要有两个技术路线：语音模态表示转换到文本模态空间；语音和文本两个模\n态数据融合在同一空间联合表示。\n语音到文本模态的转换是目前广泛采用的方法之一。这种方法充分考虑到大语言模型主要是\n为文本模态设计的特点，通过将语音模态信息投射到文本空间，实现语音与文本模态的直接对齐，\n从而在最大程度上保留大语言模型的能力。 为了实现这一目标， 通常需要引入一个 “连接器” （Con-\nnector）或“投射器” （Projector）来将语音模态特征转换到文本模态特征空间。 在此过程中， 需尽\n量减少语音特征信息的损失，并保证模态转换的平滑性。目前，主要有以下两种实现方式：直接\n投射（Direct Projection）和 Token 映射（Token Mapping） 。\n直接投射方法通过连接器将语音特...",
        "238 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\nS2T 模式实现简单，适用于纯语音到文本的转换任务，但是无法处理更复杂的多模态任务。\nST2T 是目前语音语言模型中最广泛采用的模式。 该模式支持同时输入语音和文本， 其中文本\n通常作为指令或任务提示。模型通过同时处理音频与文本模态的信息，融合两者的特征后生成最\n终的文本输出。这种模式不仅能够支持多任务学习，还能充分发挥大语言模型的强大能力，处理\n更广泛的任务，可以应用于语音翻译、语音情感分析等涉及音频和文本模态的任务。\nST2ST 是一种更高级的模式，模型在输入中结合语音和文本，并在输出中同时生成语音和文\n本。这种模式在解码阶段需要额外的语音合成模块（V ocoder）来生成语音输出。ST2ST 模式不仅\n能够完成基本的语音识别任务，还支持文本语音生成（Text-to-Speech，TTS） 、语音翻译及语音转\n换等复杂任务。\n2. 语音嵌入表示预训练\n语音嵌入表示预训练是一种通过在大规模语音数据上学习语音通用特征表示，进而提升下游\n语音任务性能的关键技术。近年来，基于不同模型架构的预训练方法逐渐成为研究热点，其中主\n...",
        "240 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n……\n音频嵌入\n对齐的音频嵌入\n+文本编码器\n文本编码器\n文本输入\n音频输入\n音频嵌入空间\n转换\n大语言模型文本嵌入空间\n大语言模型输出\n…………\n文本嵌入\n对齐的音频嵌入\n文本嵌入\n连接\n……\n图 7.7 语音和文本表示融合架构直接投射方法 [285]\nToken 映射方法通过将语音特征转换为大语言模型可处理的文本 Token 实现模态转换[297]， 如\n图7.8所示。 具体而言， 语音特征经过投射器或转换器生成与文本Token 对应的表示， 这些符号随后\n与文本的 Token 序列结合，形成一个同时包含语音和文本信息的 Token 序列，并将其输入到 LLM\n中进行统一处理。 该方法不仅能够较好地保留语音特征信息， 还确保了LLM 在处理数据时的连续\n性和一致性。\n映射/转换\n文本词元切分文本输入\n音频词元\n+连接\n音频输入\n文本词元\n音频词元\n文本词元\n连接词元\n…………\n…………\n融合嵌入\n语言模型文本嵌入空间\n大语言模型输出\n…\n文本编码器\n…\n图 7.8 语音和文本表示融合架构 Token 映射方法 [28...",
        "大语言模型：从理论到实践 237\n7.2.2 语音语言模型架构\n语音语言模型（Speech-Language Models, SLM） 是一种结合语音处理与自然语言理解的多模态\n大模型， 旨在实现语音与文本模态的深度融合。 与传统的语音识别后级联文本处理方法不同，SLM\n通过端到端架构直接学习音频特征与语言语义的映射关系，从而增强了模型在开放世界场景中的\n泛化能力。语音语言模型在多模态环境中应用广泛，如语音识别、语音合成、语音翻译、语音交\n互等。\n1. SLM 输入/输出模式\n语音语言模型的输入/输出模式可以根据任务需求分为三种主要类型， 如图7.6所示： 语音到文\n本（Speech-to-Text，S2T） 、语音文本到文本（Speech&Text-to-Text，ST2T）和语音文本到语音文\n本（Speech&Text-to-Speech&Text，ST2ST） 。\n多模态特征抽取多模态信息融合大语言模型推理\nS2T\nST2T\nST2ST\n音频编码器\n音频编码器\n文本编码器\n文本解码器音频输入\n音频输入\n音频输入\n文本指令\n文本输出变换模型音频嵌入 对齐的音频特征音频嵌入空间文本嵌...",
        "大语言模型：从理论到实践 241\nToken 空间中直接融入语音模态信息，实现语音和文本的深度融合。该方法通过增加 Token 空间，\n在原有文本 Token 的基础上新增语音 Token，形成扩展的 Token 空间[298–300]，如图7.9所示。具体\n而言，首先从语音特征中提取信息并生成语音 Token；然后将这些语音 Token 与文本 Token 结合，\n形成一个新的输入 Token 序列； 最后将该序列作为LLM 的输入， 直接进行语音和文本模态的联合\n建模。这种方法通过在大语言模型的 Token 空间中引入语音信息，最大程度地保留了语音的原始\n特征，同时有效避免了模态转换过程中可能出现的信息损失问题。\n映射/转换\n文本词元切分文本输入 融合嵌入\n融合嵌入空间音频词元\n大语言模型输出+连接\n…音频输入\n文本词元\n文本编码器\n大语言模型嵌入空间\n扩展词汇修改嵌入矩阵音频词元\n文本词元\n连接词元\n文本-音频编码器\n…………\n…………\n…\n图 7.9 语音和文本表示融合架构语音文本 Token 空间融合方法 [285]\n7.2.3 多模态大语言模型架构\n多模态大语言模型的架构种..."
      ],
      "response_time": 3.645981550216675,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.032801828835855064,
        "semantic_similarity": 0.5714655518531799
      }
    },
    {
      "sample_id": "0108",
      "question": "这张图片呈现了什么？",
      "ground_truth": "这是一张站着的鸟的照片。",
      "predicted_answer": "根据提供的文档，我无法回答这个问题。",
      "ground_context": [
        "This is a photo of a standing bird."
      ],
      "retrieved_contexts": [],
      "response_time": 2.884263515472412,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.027895820162122397,
        "semantic_similarity": 0.4560100734233856
      }
    },
    {
      "sample_id": "0109",
      "question": "DataComp 框架的目标是什么？",
      "ground_truth": "旨在构建能够在 38 项下游任务中表现卓越的图像-文本数据组合。",
      "predicted_answer": "DataComp 框架的目标是在38项下游任务中表现卓越的图像-文本数据组合的质量评估[271]。具体来说，它旨在构建一个包含128万至128亿对图像-文本样本的噪声网络数据库，并系统性地探索多种数据筛选策略[271]。",
      "ground_context": [
        "该框架基于标准化的CLIP 架构与预训练参数， 旨在构建能够在 38 项下游任务中表现卓越的图像-文本数据组合。"
      ],
      "retrieved_contexts": [
        "A photo of a birdA photo of a dog\n数据\n视觉语义关联视觉语义对齐\n图 7.12 多模态大语言模型训练策略 [271]\n本节主要从数据处理、视觉语义关联、文本对齐等方面进行介绍。\n7.3.1 数据处理\n在多模态大语言模型的训练中，数据质量对模型性能起着至关重要的作用。高效的数据处理\n与筛选策略能够显著提升模型的学习效果及其在下游任务中的泛化能力。为评估基础数据集的质\n量， 研究团队提出了DataComp 框架[305]。 该框架基于标准化的CLIP 架构与预训练参数， 旨在构建\n能够在 38 项下游任务中表现卓越的图像-文本数据组合。DataComp 构建了一个包含 128 万至 128\n亿对图像-文本样本的噪声网络数据库，并系统性地探索了多种数据筛选策略。研究表明，剪枝优\n化是提升跨模态大语言模型效果的关键技术手段，为高效能模型的训练提供了重要支持。\n数据剪枝的方法可以分为三类：(1) 使用启发式方法去除低质量样本；(2) 基于预训练 VLM 的\n打分方法对图文对进行排序，丢弃对齐较差的样本；(3) 创建多样化且平衡的数据集。\n启发式方法可以分为单模态...",
        "据集，DataComp 提出了从多样化设计的数据集中进行采样的策略。 具体而言， 采样方法主要分为\n基于文本和基于图像两种： 基于文本的采样方法保留与ImageNet 类别相关联的图文对描述； 而基\n于图像的采样则通过利用 OpenAI CLIP 的 ViT-L/14模型对图像进行编码， 并借助FAISS 工具将大\n规模噪声图像聚类为 100,000 个组，然后根据 ImageNet 训练样本的嵌入，选择与这些样本最相近\n的聚类，从而生成具有多样性的图像数据集。尽管这些方法能有效提升数据的多样性，但它们对\nImageNet 等语义数据集的依赖可能会引入类别偏倚，从而限制模型在新下游任务中的泛化能力。\n此外，MetaCLIP[314] 提出了另一种方法， 利用来自Wikipedia 和 WordNet 的 500,000 个查询作为元\n数据，构建覆盖广泛概念的预训练数据分布。通过“平衡采样”算法，MetaCLIP 限制每个查询的\n样本数量（最多 20,000 个） ，在概念的多样性与代表性之间寻求平衡，从而进一步提升模型的泛\n化能力。\n7.3.2 视觉语义关联\n视觉语义关联是多模态大语言..."
      ],
      "response_time": 4.492746829986572,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.2857142857142857,
        "rouge2": 0.0,
        "rougeL": 0.2857142857142857,
        "bleu": 0.2356808688936941,
        "semantic_similarity": 0.750706672668457
      }
    },
    {
      "sample_id": "0110",
      "question": "什么方法将RLHF从文本领域适配到视觉语言任务？",
      "ground_truth": "事实增强 RLHF（Factually Augmented RLHF）",
      "predicted_answer": "创新方法——事实增强 RLHF（Factually Augmented RLHF ）将RLHF从文本领域适配到了视觉语言任务[1]。该方法通过在奖励模型中加入图像标题和真实多选题的额外事实信息，减少了奖励滥用问题。",
      "ground_context": [
        "创新方法——事实增强 RLHF（Factually Augmented RLHF ） 。该方法将RLHF 从文本领域适配到视觉语言任务，通过在奖励模型中加入图像标题和真实多选题的额外事实信息，减少奖励滥用问题。"
      ],
      "retrieved_contexts": [
        "创新方法——事实增强 RLHF（Factually Augmented RLHF ） 。该方法将RLHF 从文本领域适配到\n视觉语言任务，通过在奖励模型中加入图像标题和真实多选题的额外事实信息，减少奖励滥用问\n题。LLaV A-RLHF还利用 GPT-4 生成的训练数据及人工编写的图文对进一步提升其通用能力。在\nLLaV A-Bench中，其性能达到了 GPT-4 的 94%，在专注于减少幻觉的 MMHAL-BENCH 中，相较\n基线模型提升了 60%。\n2. 富含文本信息的图像理解\n富含文本信息的图像（Text-rich Image，如电影海报、书籍封面、文档扫描等）不仅需要模型\n理解视觉内容，还需要解析其中包含的细粒度文本信息，并与视觉语义进行有效关联。传统的多\n模态大语言模型在处理这类任务时往往面临文本识别能力不足、分辨率限制以及上下文信息捕获\n不充分等问题。 为应对这些挑战， 近年来涌现出一系列创新方法和模型， 包括LLaV AR、Monkey、\nLumos 等，它们专注于提升文本丰富图像的理解能力。",
        "练。通过将预训练的 Vicuna 语言模型编码器与 CLIP ViT-L/14 视觉编码器的输出融合到相同的维\n度空间，LLaV A在合成指令跟随任务和 Science QA 基准测试中表现出显著的改进。LLaV A 1.5[320]\n在 LLaV A的基础上进一步优化了多模态文本对齐能力。其改进包括引入跨模态全连接多层感知\n机（MLP）层，并结合视觉问答（VQA）指令数据进行训练。LLaV A 1.5仅使用 60 万条图文对数\n据，在 8 张 A100 GPU 上约一天即可完成训练。LLaV A-NeXT (v1.6)[330] 在 LLaV A 1.5的基础上进\n行了多方面的改进，进一步推动了多模态文本对齐的性能。通过将全图和小图块的视觉特征分别\n输入视觉编码器，并将其拼接后处理，提高了图像分辨率的利用效率。优化了视觉指令调优数据\n集，新增了更好的视觉推理、OCR、世界知识和逻辑推理样本。\n由于高质量视觉指令调优数据的稀缺， LLaV A等模型可能在视觉和文本模态对齐上存在偏\n差， 甚至生成幻觉性输出。 为了解决这一问题，LLaV A-RLHF[262] 提出了基于人类反馈强化学习...",
        "大语言模型：从理论到实践 249\n加贴合人类需求。此外，多模态大模型处理文本丰富的图像理解面临特定挑战，相关领域也涌现\n出大量研究，推动了技术的持续发展。本节针对上述内容进行介绍。\n1. 多模态指令微调与 RLHF\n多模态指令微调通过在包含指令、 输入和期望响应的监督数据上对多模态文本对齐进行优化，\n从而提升模型理解和执行复杂指令的能力。与大规模的预训练数据集相比，指令微调数据集的规\n模通常较小， 其样本数量从几千到一百万不等[328]。 代表性的视觉语言模型如LLaV A、InstructBLIP\n和 OpenFlamingo[329] 均引入了指令微调技术，显著提升了多模态任务的表现。\nRLHF 则专注于通过人类反馈使模型输出更符合人类偏好。 具体来说，RLHF 首先通过训练一\n个奖励模型来评估模型响应的质量， 捕捉人类偏好的特征。 借助这一奖励模型，RLHF 能有效模拟\n人类偏好，从而减少对人工标注的依赖。随后，通过奖励模型对多模态大语言模型进行微调，使\n其生成的响应更加贴合人类期望。\nLLaV A[320] 通过指令微调提升了多模态对话能力，采用了 15 万条合成视觉指令样本进..."
      ],
      "response_time": 4.522867679595947,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.8,
        "rouge2": 0.7499999999999999,
        "rougeL": 0.8,
        "bleu": 0.12936981168384865,
        "semantic_similarity": 0.7670227289199829
      }
    },
    {
      "sample_id": "0111",
      "question": "Qformer使用的基础模型是什么？",
      "ground_truth": "BertLMHeadModel",
      "predicted_answer": "Qformer使用的基础模型是BERT[1]。",
      "ground_context": [
        "Qformer = BertLMHeadModel(config=encoder_config)"
      ],
      "retrieved_contexts": [
        "大语言模型：从理论到实践 253\ndef init_Qformer(cls, num_query_token, vision_width, cross_attention_freq =2):\n#֥BERTଆ྘஥ᇂQ-Former\nencoder_config = BertConfig.from_pretrained(\"bert-base-uncased\")\n#؇\nencoder_config.encoder_width = vision_width\nencoder_config.query_length = num_query_token\n# ᄝBERTҭᇿၩ৯Ҫ\nencoder_config.add_cross_attention = True\nencoder_config.cross_attention_freq = cross_attention_freq\n#֥BERTଆ྘ቔູQ-Formerଆॶ\nQformer = BertLMHeadModel(config=encoder_config)\n#༢\nquery_tokens = nn.Parameter(\ntorch.zeros..."
      ],
      "response_time": 3.258589267730713,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0.0,
        "bleu": 0.0,
        "semantic_similarity": 0.5156816244125366
      }
    },
    {
      "sample_id": "0112",
      "question": "在代码中，如何计算损失（loss）?",
      "ground_truth": "loss = outputs.loss",
      "predicted_answer": "在给定的文档中，损失（loss）是通过交叉熵损失函数计算的。具体来说，在文档2中的代码片段展示了如何计算损失：\n\n```python\nloss = F.cross_entropy(preds.view(-1, preds.size(-1)), \n                       results, ignore_index = target_pad)\n```\n\n这里，`preds` 是模型预测的结果，`results` 是目标结果，`ignore_index = target_pad` 表示忽略填充索引的目标值。[2]",
      "ground_context": [
        "loss = outputs.loss"
      ],
      "retrieved_contexts": [
        "start = time.time()\nbatch = to_device(batch, device)\noutputs = model(**batch, use_cache =False)\nloss = outputs.loss\nif args.print_loss:\nprint(\nf\"Epoch: {epoch}, Step: {step},\nRank: {torch.distributed.get_rank()}, loss = {loss}\"\n)\nmodel.backward(loss)\nmodel.step()\nend = time.time()\nif torch.distributed.get_rank() == 0:\nprint_throughput(model.model, args, end - start,\nargs.global_rank)\nif args.output_dir is not None:\nprint_rank_0('saving the final model ... ' , args .global_rank)\nmodel = convert_l...",
        "src = batch.English.transpose(0,1)\ntrg = batch.French.transpose(0,1)\n#ე\n#ყҩ\ntrg_input = trg[:, : -1]\n# ൫๭ყҩֆՍ\ntargets = trg[:, 1:].contiguous().view(-1)\n#ඔটᇅቔဃ઒\nsrc_mask, trg_mask = create_masks(src, trg_input)\npreds = model(src, trg_input, src_mask, trg_mask)\noptim.zero_grad()\nloss = F.cross_entropy(preds.view(-1, preds .size(-1)),\nresults, ignore_index =target_pad)\nloss.backward()\noptim.step()\ntotal_loss += loss.data[0]\nif (i + 1) % print_every == 0:\nloss_avg = total_loss / print_every\nprint(\"...",
        "从 S 中选择子集的硬掩码或软权重；2) 内循环用于优化在 Sb 上的模型参数 θ。 可以将带有自监督\n语言建模损失的双层优化问题，按照如下方法形式化表示：\nSb = arg min\nS′\nb⊂S,|Sb|=b\nX\nxi∈S′\nb,θ=θ∗\nN LLA|Q\ni (5.4)\ns.t.θ∗ = arg min\nθ\nX\nxi∈S′\nb\nN LLA|Q\ni (5.5)\nN LLi = 1\n|xi|\n|xi|X\nj=1\n− log P (xi(j)|xi(<j); θ) (5.6)\n其中 N LLi 表示针对每个数据 xi 的负对数似然（Negative Log Likelihood） ，可以使用较小的模型\n进行学习，比如 MPT 125M[219] 等。"
      ],
      "response_time": 5.154270172119141,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.14285714285714285,
        "rouge2": 0.0,
        "rougeL": 0.14285714285714285,
        "bleu": 0.007392977433111122,
        "semantic_similarity": 0.6180027723312378
      }
    },
    {
      "sample_id": "0113",
      "question": "MouSi模型如何结合不同视觉专家的信息，以更好地识别图像中的文本，并且如何通过ChatGPT提高文本描述的质量？",
      "ground_truth": "MouSi模型通过引入多个视觉专家（如CLIP、SAM和Layout Mv3专家）编码处理图像信息，并利用多视觉融合网络整合这些输出序列，以完成视觉问答和光学字符识别任务。同时，ChatGPT被用作自动化的文本质量评估者，检查生成的文本描述，修正其中的语义和语法错误，从而提高描述的质量。",
      "predicted_answer": "MouSi模型通过一个多视觉融合网络来统一处理来自不同视觉专家（如CLIP专家、SAM专家、Layout Mv3专家及其他专家）的不同视觉标记，从而更好地识别图像中的文本[1][2]。这个融合网络能够压缩和融合多通道视觉信息，并将其与视觉输入标记对齐[3]。\n\n另外，研究人员利用ChatGPT的强大语言理解和生成能力作为文本质量评估者，对生成的图像-文本对进行检查和改进，以修正其中的语义、语法错误或结构问题，从而提升文本描述的质量[3]。",
      "ground_context": [
        "基于 MouSi 模型，当用户上传一张描绘风媒花授粉过程的图片并询问“哪些球果产生花粉？”时，该图片依次经过 CLIP 专家、SAM 专家、Layout Mv3 专家及其他专家的编码处理，产生多组不同的视觉标记。",
        "研究人员利用ChatGPT强大的语言理解和生成能力，让其作为一个自动化的文本质量评估者，对生成的5000个图像-文本对进行检查。"
      ],
      "retrieved_contexts": [
        "244 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n嵌入表示\n Poly-Expert 融合网络\nOCR专家分割专家 其他专家\nLayoutLMv3图像编码器SAM图像编码器 任意模态编码器\n视觉语言模型\n图片描述生成 OCR\n哪些松果产⽣花粉？\nCLIP图像编码器\n语义专家\n图片问答 其他\n图 7.11 眸思（MouSi）模型框架 [301]\n基于 MouSi 模型，当用户上传一张描绘风媒花授粉过程的图片并询问“哪些球果产生花粉？ ”\n时，该图片依次经过 CLIP 专家、SAM 专家、Layout Mv3 专家及其他专家的编码处理，产生多组\n不同的视觉标记。随后，一个多视觉融合网络压缩融合多通道视觉信息，并将其与视觉输入标记\n对齐。用户的问题通过大语言模型的嵌入层被处理成文本标记。最终，MouSi 通过对视觉语言标\n记进行处理，完成 VQA（视觉问答）和 OCR（光学字符识别）任务，从图片中识别答案文本， 生\n成正确答案“雄性球果产生花粉。 ”\n由于不同视觉专家的输出序列在维度和数量上往往存在差异，因此需要设计融合网络来统一\n处理这些输出。 为了更好地整合多专家信息，M...",
        "具体来说，在视觉语言建模中使用 SEED 标记，并通过扩散模型将其解码为高质量图像。在\n语音生成任务中，采用 SoundStorm 模型生成声学 Token，随后将其解码为原始音频数据。对于音\n乐生成，使用 Encodec 标记以捕捉高频细节，并通过 Encodec 解码器将其重构为高保真的音频数\n据。 通过这种设计，AnyGPT 在显著减少语音序列长度的同时， 能够生成高质量的多模态数据， 从\n而在生成效果和计算效率之间实现了良好的平衡。\n2. 眸思（MouSi）\n当前的视觉语言模型经常遭遇单视觉编码器组件能力不足和视觉 Token 过长等挑战。这些挑\n战会限制模型准确理解繁复的视觉信息和过长的上下文信息。解决这些难题对于提高 VLM 的性\n能和可用性至关重要。\n为解决上述问题， 多模态大模型眸思 （MouSi）[301] 提出了使用多专家技术以协同各视觉编码\n器的能力， 这些能力包括图像文本匹配， 光学字符识别， 图像分割等。 该技术引入一个融合网络使\n得来自不同视觉专家的输出得到统一，同时弥合了视觉编码器和预训练 LLM 之间的差异。此外，\n还提出了二维可训练图像位置编码方法， ...",
        "（2）提供更高质量的描述： 预训练的MiniGPT-4 并不能生成高质量的文本描述， 仍然存在较多\n的错误和噪声，例如不连贯的陈述、重复的单词或句子。因此，研究人员利用ChatGPT 强大的语\n言理解和生成能力，让其作为一个自动化的文本质量评估者，对生成的 5000 个图像-文本对进行\n检查。期望通过这步操作修正文本描述中的语义、语法错误或结构问题。该步操作使用 ChatGPT\n自动改进描述。具体的提示模板如下：",
        "影融合网络和 Q-Former 融合网络。 然而， 在实际应用中， 多个视觉专家输出的大量视觉标记不仅\n增加了视觉语言模型的计算成本和内存使用率，还可能超过推理过程中最大序列长度的限制。为\n了解决这一问题，MouSi 模型提出了多补丁-单标记投影方法， 以按比例减少每个专家的输出标记\n数量。具体而言，由于图像信号具有局部性和稀疏性属性，用一个标记表示相邻的多个补丁是合\n理的。这种方法通过对局部视觉信息进行压缩，将多个补丁映射为单个标记，从而实现了多通道\n视觉信号的高效传输。通过多补丁-单标记投影，不仅有效降低了视觉信号传输的冗余，还减少了\n视觉大语言模型后续处理的计算成本，显著提高了推理效率，为多视觉专家的高效整合提供了切\n实可行的解决方案。\n尽管通过多补丁-单标记操作或在 Q-Former 中定义少量查询可以显著减少视觉标记的数量， 但"
      ],
      "response_time": 5.944416284561157,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.7499999999999999,
        "rouge2": 0.5714285714285714,
        "rougeL": 0.7499999999999999,
        "bleu": 0.14436462706471034,
        "semantic_similarity": 0.9317485094070435
      }
    },
    {
      "sample_id": "0114",
      "question": "如何通过多补丁-单标记投影方法提高多模态大语言模型在视觉和语音处理中的效率？",
      "ground_truth": "多补丁-单标记投影方法能够有效降低视觉信号传输的冗余，并减少视觉大语言模型后续处理的计算成本，从而显著提高推理效率。与此同时，语音语言模型通过端到端架构直接学习音频特征与语言语义的映射关系，增强了模型在开放世界场景中的泛化能力。这两者结合可以实现视觉与语音模态的高效融合与处理。",
      "predicted_answer": "通过多补丁-单标记投影方法，MouSi 模型能够有效降低视觉信号传输的冗余，减少视觉大语言模型后续处理的计算成本，从而显著提高推理效率[1]。这种方法利用图像信号的局部性和稀疏性属性，将多个补丁压缩成单个标记，实现多通道视觉信号的高效传输[1]。此外，该方法还能减少位置编码的冗余分配，进一步优化视觉标记的处理效率，为多模态模型的可扩展性提供支持[2]。",
      "ground_context": [
        "MouSi 模型提出了多补丁-单标记投影方法， 以按比例减少每个专家的输出标记数量。",
        "语音语言模型通过端到端架构直接学习音频特征与语言语义的映射关系，从而增强了模型在开放世界场景中的泛化能力。"
      ],
      "retrieved_contexts": [
        "影融合网络和 Q-Former 融合网络。 然而， 在实际应用中， 多个视觉专家输出的大量视觉标记不仅\n增加了视觉语言模型的计算成本和内存使用率，还可能超过推理过程中最大序列长度的限制。为\n了解决这一问题，MouSi 模型提出了多补丁-单标记投影方法， 以按比例减少每个专家的输出标记\n数量。具体而言，由于图像信号具有局部性和稀疏性属性，用一个标记表示相邻的多个补丁是合\n理的。这种方法通过对局部视觉信息进行压缩，将多个补丁映射为单个标记，从而实现了多通道\n视觉信号的高效传输。通过多补丁-单标记投影，不仅有效降低了视觉信号传输的冗余，还减少了\n视觉大语言模型后续处理的计算成本，显著提高了推理效率，为多视觉专家的高效整合提供了切\n实可行的解决方案。\n尽管通过多补丁-单标记操作或在 Q-Former 中定义少量查询可以显著减少视觉标记的数量， 但",
        "具体来说，在视觉语言建模中使用 SEED 标记，并通过扩散模型将其解码为高质量图像。在\n语音生成任务中，采用 SoundStorm 模型生成声学 Token，随后将其解码为原始音频数据。对于音\n乐生成，使用 Encodec 标记以捕捉高频细节，并通过 Encodec 解码器将其重构为高保真的音频数\n据。 通过这种设计，AnyGPT 在显著减少语音序列长度的同时， 能够生成高质量的多模态数据， 从\n而在生成效果和计算效率之间实现了良好的平衡。\n2. 眸思（MouSi）\n当前的视觉语言模型经常遭遇单视觉编码器组件能力不足和视觉 Token 过长等挑战。这些挑\n战会限制模型准确理解繁复的视觉信息和过长的上下文信息。解决这些难题对于提高 VLM 的性\n能和可用性至关重要。\n为解决上述问题， 多模态大模型眸思 （MouSi）[301] 提出了使用多专家技术以协同各视觉编码\n器的能力， 这些能力包括图像文本匹配， 光学字符识别， 图像分割等。 该技术引入一个融合网络使\n得来自不同视觉专家的输出得到统一，同时弥合了视觉编码器和预训练 LLM 之间的差异。此外，\n还提出了二维可训练图像位置编码方法， ...",
        "大语言模型：从理论到实践 245\n在推理过程中，视觉标记对位置编码的占用仍然是一个不可忽视的问题。事实上，视觉标记的长\n度通常比文本标记高出 500 倍以上，在具有位置感知的视觉语言模型中，这会消耗大量的位置嵌\n入资源。鉴于视觉专家本身已经包含位置编码信息，为每个视觉标记再次分配视觉大语言模型的\n位置嵌入显得冗余且低效。为了解决这一问题，MouSi 模型提出了一种二维可训练图像位置编码\n方法，通过直接在视觉标记中引入可训练的二维位置编码，避免了对视觉大语言模型位置嵌入的\n额外占用。这种方法不仅有效解决了多视觉专家导致的超长序列问题，还减少了位置编码的冗余\n分配，从而优化了视觉标记的处理效率，为多模态模型的可扩展性提供了重要支持。\n7.3 多模态大语言模型训练策略\n深度神经网络缩放法则（Scaling Law）为多模态大语言模型的训练策略提供了重要参考。以\n往业界普遍采用增加计算资源和模型规模的方式来提升性能，然而，根据文献 [304] 的研究成果，\n优化数据处理环节亦可带来突破性进展。以 CLIP 为例，其采用 4 亿张图像进行训练，开源版本\nOpenCLIP[305] 则需数百卡 G..."
      ],
      "response_time": 5.801173686981201,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.29280295226273195,
        "semantic_similarity": 0.84136962890625
      }
    },
    {
      "sample_id": "0115",
      "question": "在直接投射方法中，如何将语音特征与文本模态结合以生成输出，并与S2T模式有何不同？",
      "ground_truth": "直接投射方法通过连接器将语音特征映射到大语言模型的文本模态嵌入空间，生成的语音嵌入向量与输入文本的嵌入向量拼接，形成一个融合语音和文本信息的新嵌入向量。而S2T模式则以语音作为输入，通常用于自动语音识别任务，不需要文本编码器，而是通过音频编码器提取特征并生成文本输出。",
      "predicted_answer": "在直接投射方法中，首先通过连接器将语音特征映射到大语言模型的文本模态嵌入空间[1]。具体来说，语音特征经过编码器提取并生成特征张量，然后通过投射器转化为与文本模态对齐的嵌入向量[1]。生成的语音嵌入向量与输入文本的嵌入向量拼接，形成融合了语音和文本信息的新嵌入向量，再将其输入到大语言模型中进行处理[1]。\n\n相比之下，S2T模式仅接受语音输入并生成对应的文本输出，不涉及文本模态的融合处理[4]。这种模式主要用于自动语音识别任务，不考虑文本指令或其他文本信息的影响。而直接投射方法则能够在大语言模型中同时整合语音和文本信息，使其能够处理更多样化的任务类型，如语音翻译和情感分析等[2][3]。",
      "ground_context": [
        "直接投射方法通过连接器将语音特征映射到大语言模型的文本模态嵌入空间，生成的语音嵌入向量与输入文本的嵌入向量拼接，形成一个融合语音和文本信息的新嵌入向量。",
        "S2T 是最基础的模式，模型以语音作为输入，并生成对应的文本输出。这种模式通常用于自动语音识别（Automatic Speech Recognition，ASR）任务。"
      ],
      "retrieved_contexts": [
        "直接投射方法通过连接器将语音特征映射到大语言模型的文本模态嵌入空间[295, 296]， 如图7.7所\n示。语音特征经过编码器提取，生成包含语音信息的特征张量。该张量随后通过投射器转化为与\n文本模态对齐的嵌入向量。生成的语音嵌入向量与输入文本的嵌入向量拼接，形成一个融合语音\n和文本信息的新嵌入向量，并将其输入到大语言模型中进行处理。此外，一些研究者采用隐式投\n射方式，通过调整原始编码器的参数，在训练过程中直接完成语音到文本模态的映射，无需额外\n的连接器。",
        "在此基础上引入了 LSTM 模块以增强序列分析能力，同时结合 Transformer 架构优化离散符号序\n列的建模能力，从而在多种语音任务中取得了显著的性能提升。\n3. 语音和文本表示融合架构\n获得语音模态信息后， 需要将其与文本模态信息集成， 以便大语言模型进行进行最终推理。 语\n音和文本表示融合主要有两个技术路线：语音模态表示转换到文本模态空间；语音和文本两个模\n态数据融合在同一空间联合表示。\n语音到文本模态的转换是目前广泛采用的方法之一。这种方法充分考虑到大语言模型主要是\n为文本模态设计的特点，通过将语音模态信息投射到文本空间，实现语音与文本模态的直接对齐，\n从而在最大程度上保留大语言模型的能力。 为了实现这一目标， 通常需要引入一个 “连接器” （Con-\nnector）或“投射器” （Projector）来将语音模态特征转换到文本模态特征空间。 在此过程中， 需尽\n量减少语音特征信息的损失，并保证模态转换的平滑性。目前，主要有以下两种实现方式：直接\n投射（Direct Projection）和 Token 映射（Token Mapping） 。\n直接投射方法通过连接器将语音特...",
        "240 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n……\n音频嵌入\n对齐的音频嵌入\n+文本编码器\n文本编码器\n文本输入\n音频输入\n音频嵌入空间\n转换\n大语言模型文本嵌入空间\n大语言模型输出\n…………\n文本嵌入\n对齐的音频嵌入\n文本嵌入\n连接\n……\n图 7.7 语音和文本表示融合架构直接投射方法 [285]\nToken 映射方法通过将语音特征转换为大语言模型可处理的文本 Token 实现模态转换[297]， 如\n图7.8所示。 具体而言， 语音特征经过投射器或转换器生成与文本Token 对应的表示， 这些符号随后\n与文本的 Token 序列结合，形成一个同时包含语音和文本信息的 Token 序列，并将其输入到 LLM\n中进行统一处理。 该方法不仅能够较好地保留语音特征信息， 还确保了LLM 在处理数据时的连续\n性和一致性。\n映射/转换\n文本词元切分文本输入\n音频词元\n+连接\n音频输入\n文本词元\n音频词元\n文本词元\n连接词元\n…………\n…………\n融合嵌入\n语言模型文本嵌入空间\n大语言模型输出\n…\n文本编码器\n…\n图 7.8 语音和文本表示融合架构 Token 映射方法 [28...",
        "文本指令\n音频编码器\n文本编码器\n音频嵌入音频嵌入空间\n文本嵌入文本嵌入空间\n音频嵌入音频嵌入空间\n文本嵌入文本嵌入空间\n融合特征统一嵌入空间\n融合特征统一嵌入空间\n文本解码器文本输出\n文本解码器文本输出\n语音合成音频输出\n图 7.6 语音语言模型输入输出模式 [285]\nS2T 是最基础的模式，模型以语音作为输入，并生成对应的文本输出。这种模式通常用于自\n动语音识别（Automatic Speech Recognition，ASR）任务。模型架构中包含一个音频编码器，用于\n提取语音信号中的特征，而由于输入中没有文本模态，因此不需要文本编码器。这种模式通常采\n用解码器架构， 通过一个特征转换模块将音频特征映射到文本嵌入空间， 以生成精准的文本输出。",
        "238 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\nS2T 模式实现简单，适用于纯语音到文本的转换任务，但是无法处理更复杂的多模态任务。\nST2T 是目前语音语言模型中最广泛采用的模式。 该模式支持同时输入语音和文本， 其中文本\n通常作为指令或任务提示。模型通过同时处理音频与文本模态的信息，融合两者的特征后生成最\n终的文本输出。这种模式不仅能够支持多任务学习，还能充分发挥大语言模型的强大能力，处理\n更广泛的任务，可以应用于语音翻译、语音情感分析等涉及音频和文本模态的任务。\nST2ST 是一种更高级的模式，模型在输入中结合语音和文本，并在输出中同时生成语音和文\n本。这种模式在解码阶段需要额外的语音合成模块（V ocoder）来生成语音输出。ST2ST 模式不仅\n能够完成基本的语音识别任务，还支持文本语音生成（Text-to-Speech，TTS） 、语音翻译及语音转\n换等复杂任务。\n2. 语音嵌入表示预训练\n语音嵌入表示预训练是一种通过在大规模语音数据上学习语音通用特征表示，进而提升下游\n语音任务性能的关键技术。近年来，基于不同模型架构的预训练方法逐渐成为研究热点，其中主\n..."
      ],
      "response_time": 7.532623767852783,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.25,
        "rouge2": 0.0,
        "rougeL": 0.25,
        "bleu": 0.24435472628298174,
        "semantic_similarity": 0.9408479928970337
      }
    },
    {
      "sample_id": "0116",
      "question": "思维链提示方式的目的是什么？",
      "ground_truth": "提升模型的推理能力。",
      "predicted_answer": "思维链提示方式的目的是引导模型在回答问题之前先输出推理过程，从而提升其在推理任务上的表现[2][3]。这种方法可以让模型展示出更多的推理步骤，提高答案的准确性和可解释性[1]。",
      "ground_context": [
        "思维链（Chain-of-Thought，CoT）提示方式...使得模型不仅输出最终结果，还输出中间步骤，从而提升模型的推理能力。"
      ],
      "retrieved_contexts": [
        "276 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n个问题，Google Brain 的研究人员提出了思维链（Chain-of-Thought，CoT） 提示方式[395]， 除了将问\n题输入模型， 还将类似题目的解题思路和步骤输入模型， 使得模型不仅输出最终结果， 还输出中间\n步骤， 从而提升模型的推理能力。 研究人员甚至提出了零样本思维链（Zero-shot Chain-of-Thought，\nZero-shot CoT） 提示方式， 只需要简单地告知模型 “让我们一步一步思考 （Let’s think step by step） ”\n[397]，模型就能够自动输出中间步骤。\n思维链提示方式如图8.3 所示， 标准少样本提示 （Standard Few-shot Prompting） 技术在给模型\n的输入里面提供了 k 个 [问题，答案] 对，以及当前问题，由模型输出答案。而思维链提示在给模\n型的输入里面提供了 k 个 [问题，思维链，提示] 元组及当前问题，引导模型在回答问题之前先输\n出推理过程。 可以看到在标准少样本提示下， 模型通常直接给出答案， 但是由于缺少推...",
        "对环境和任务有深刻理解， 从而生成并优化任务执行计划， 制定合理的行动步骤以实现既定目标。\n然而，仅仅通过扩大语言模型的规模，并不能显著提升推理（Reasoning）能力，如常识推理、逻\n辑推理、数学推理等。通过示例（Demonstration）或者明确指导模型在面对问题时如何逐步思考，\n促使模型在得出最终答案之前生成中间的推理步骤，可以显著提升其在推理任务上的表现。这种\n方法被称为思维链提示（Chain-of-Thought Prompting）[395]。同样地，面对复杂任务或问题时，大\n语言模型可以展现出良好的规划 （Planning） 能力。 通过引导模型首先将复杂的问题分解为多个较\n为简单的子问题，然后逐一解决这些子问题，可使模型得出最终答案，这种策略被称为由少至多\n提示[396]。 本节将重点介绍如何利用思维链提示和由少至多提示这两种方式， 提升大语言模型的推\n理规划能力。\n1. 思维链提示\n语言模型在推理能力方面的表现一直未能令人满意，一些研究人员认为这可能是因为此前的\n模式是直接让模型输出结果，而忽略了其中的思考过程。人类在解决包括数学应用题在内的、涉\n及多步推理的问题...",
        "接给出的答案准确率不高，也缺乏解释。而在思维链提示下，模型输出推理步骤，在一定程度上\n降低了推理难度，最终结果的准确率有所提升，同时具备了一定的可解释性。\n图 8.3 思维链提示方式 [395]\n文献 [395] 使用了人工构造的思维链。 然而， 通过实验发现， 使用由不同人员编写的符号推理\n示例在准确率上存在高达 28.2% 的差异，而改变范例的顺序在大多数任务中则只产生了不到 2%\n的变化。 因此， 如果能够自动构建具有良好问题和推理链的范例， 则可以大幅度提升推理效果。 文\n献 [398] 发现， 仅通过搜索相似问题并将其对应的推理过程作为范例对于效果提升而言作用十分有\n限， 但是问题和推理链示例的多样性对于自动构建范例至关重要。 因此， 上海交通大学和Amazon\nWeb Services 的研究人员提出了 Auto-CoT[398] 方法，通过采集具有多样性的问题和生成推理链来\n构建范例。Auto-CoT 算法的整体过程如图8.4 所示。Auto-CoT 包括以下两个主要阶段。\n（1）问题聚类：将给定数据集中的问题划分为几个簇（Cluster） 。",
        "j ， 则构\n建提示输入 [Q : q(i)\nj , A : [ P ]]，其中 [P ] 是一个单一提示“让我们一步一步思考” 。将这个提示输\n入使用 Zero-Shot CoT[397] 的大语言模型中，得到由解释 r(i)\nj 和提取的答案 a(i)\nj 组成的推理链。最\n终得到范例 d(i)\nj = [Q : q(i)\nj , A : r(i)\nj ◦ a(i)\nj ]。如果 r(i)\nj 中的推理步骤小于 5 步，并且 q(i)\nj 中的词元\n小于 60 个，则将 d(i)\nj 纳入 d(i)。\n此外， 一些研究人员提出了对思维链提示的改进方法， 例如从训练样本中选取推理最复杂的样\n本 来 形 成 示 例 样 本， 被 称 为Complex-CoT[400]。 也 有 研 究 人 员 指 出 可 以 从 问 题 角 度 考 虑 优\n化思维链提示，通过将复杂的、模糊的、低质量的问题优化为模型更易理解的、高质量的问题，\n进一步提升思维链提示的性能，这一方法被称为 Self-Polish[401]。"
      ],
      "response_time": 4.251612186431885,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.01418431346807848,
        "semantic_similarity": 0.6698784828186035
      }
    },
    {
      "sample_id": "0117",
      "question": "大模型智能体的核心能力包括哪些？",
      "ground_truth": "感知、规划、记忆以及工具使用",
      "predicted_answer": "大模型智能体的核心能力包括感知、规划、记忆以及工具使用[1][2]。",
      "ground_context": [
        "大模型智能体的核心能力涵盖了感知、规划、记忆以及工具使用，这些能力使其能够弥补传统大模型无法与外部世界交互的局限性。"
      ],
      "retrieved_contexts": [
        "知环境中的适应能力；而教程学习与示范学习的结合，则能够为模型理解复杂工具操作提供双重\n支持。 这种多策略融合不仅提升了模型的学习效率， 还为处理更复杂的多工具任务开辟了新路径。\n8.3 大模型智能体训练\n大模型智能体的核心能力涵盖了感知、规划、记忆以及工具使用，这些能力使其能够弥补传\n统大模型无法与外部世界交互的局限性。然而，大语言模型在最初的设计中并不具备这些核心能\n力。大语言模型主要依赖于大规模的文本数据训练，擅长语言生成和理解，但无法直接使用外部",
        "提升整体的工作效率和用户体验。\n在人机平等协作范式中，强调智能体与人类作为平等的合作伙伴，共同参与任务的规划与执\n行。这种模式注重智能体的适应性与自主性，通过协同合作实现任务的高效完成。例如，在任务\n执行过程中， 智能体能够主动寻求人类的反馈， 并根据反馈动态调整其行为策略[384]。 与人类主导\n范式不同，这种模式不仅要求智能体具备执行能力，还需要其通过自主学习与优化不断提升自身\n能力。\n随着智能体在环境感知、 推理与决策能力方面的进步， 人机交互的效率与深度也将不断提高。\n通过持续的优化与协同，人类与智能体之间可以实现真正的无缝合作，使智能体成为人类创新和\n效率提升的重要伙伴。\n8.2 大语言模型智能体架构\n智能体可以被视为独立的个体，能够接收并处理外部信息，进而给出响应。大模型智能体基\n本组成如图8.2 所示，主要包含以下几个核心模块：感知模块、规划模块、记忆模块、工具使用模\n块。对于外界输入，智能体借助多模态能力将文字、音频、图像等多种形式的信息转换为机器能\n够理解的表现形式；进而由规划模块对这些信息进行处理，结合记忆模块完成推理、规划等复杂\n任务；智能体可能会利用工具使用模块...",
        "而实现适应性行动。总体而言，感知模块的多模态扩展不仅能够帮助智能体更全面地理解并适应\n外部环境，还将显著提升其在复杂任务中的执行能力。未来研究的核心将聚焦于赋予大模型更强\n的多模态理解能力，以进一步增强其感知与决策水平。这一领域的突破将为大模型智能体的全面\n发展奠定重要基础。\n8.2.2 规划模块\n规划模块是大模型智能体的核心，其主要职责是通过对环境与任务的深刻理解，生成并优化\n任务执行计划，制定合理的行动步骤以实现既定目标。研究表明，大模型的推理与规划能力随着\n模型参数规模和训练数据量的增加呈现出显著的阶跃式提升 [62]。尤其是在模型参数量达到数百\n亿级别时，即使缺乏直接与任务相关的数据，大模型也能够通过在输入提示中加入包含任务中间\n推理步骤的示例，或通过引导模型逐步输出推理过程，逐步构建任务的解决方案。将大模型作为\n规划模块的核心，充分发挥其强大的推理能力和丰富的知识库，可以在复杂且动态的环境中实现",
        "8. 大模型智能体\n一直以来， 实现通用类人智能都是人类不懈追求的目标，智能体， 也是在该背景下提出的。 早\n期的智能体主要是基于强化学习实现的，不仅计算成本高，需要用大量的数据训练，而且难以实\n现知识迁移。随着大模型的发展，其在诸多领域展现出惊人的语义处理能力，能够快速生成文本、\n回答问题， 甚至完成一些复杂的知识推理任务。 研究人员开始思考如何将大模型与智能体结合， 从\n而突破大模型本身不具备与外部世界联系， 无法感知外部环境以及调用外部工具的问题。 同时， 智\n能体借助大模型强大的多模态理解与生成优势，可以快速处理信息、规划行动。智能体与大模型\n结合展现出了强大的能力，因此近年来大模型智能体受到了越来越多的关注并在很多应用领域取\n得了很好的实践结果。\n本章将重点介绍智能体的发展、大语言模型智能体架构，最后以 LangChain 为例介绍大语言\n模型智能体实践。\n8.1 智能体基础\n“智能体” （Agent）也称为智能代理，这一概念源远流长，其历史渊源可上溯至亚里士多德、\n休谟等先哲的相关论述。 从哲学维度剖析， “智能体”意指具备行动潜能的实体， 而“代理”一词，\n则侧重于对这种...",
        "度学习的兴起，深度强化学习应运而生，它融合了强化学习与深度神经网络技术 [358, 359]，促使智\n能体具备从高维输入数据中学习复杂策略的能力，得以在未知环境里自如探索、自主学习，进而\n在从电子游戏竞技到机器人操控等诸多领域广泛渗透， 产生了AlphaGo[360]、DQN[361] 等一系列重\n要成果。但是，强化学习智能体依旧面临着训练周期冗长、采样效率欠佳以及稳定性不足等棘手\n难题，尤其在错综复杂的现实世界场景应用中，这些短板更加凸显[355]。\n2023 以来，大模型异军突起，其所展现出的惊人的能力引发广泛瞩目，基于大模型构建的智\n能体也日益备受瞩目[362–365]。 大模型智能体具有感知、 决策、 行动和记忆的能力， 通过感知模块捕\n获周围环境的信息，利用大模型进行推理和决策，通过执行器实施具体行动，同时还能存储和管\n理记忆， 以支持持续学习和适应动态环境[366]。 大模型智能体将大模型作为智能体的核心中枢， 即\n大脑或控制器的关键构成要素，同时借助多模态感知、工具运用等策略，全方位拓展智能体的感\n知范畴与行动边界。凭借思维链、问题分解等技术手段，大模型智能体得以彰显出可..."
      ],
      "response_time": 3.624089479446411,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.2981792160679168,
        "semantic_similarity": 0.6211161613464355
      }
    },
    {
      "sample_id": "0118",
      "question": "智能体的概念核心聚焦于什么？",
      "ground_truth": "个体的自主性",
      "predicted_answer": "智能体概念的核心聚焦于个体的自主性，即赋予其实现意志、抉择判断并付诸行动的能力，从而使其脱离单纯的被动反应模式。[1][2]",
      "ground_context": [
        "尤为关键的是，智能体概念的核心聚焦于个体的自主性，即赋予其运用意志、抉择判断以及付诸行动的能力，使之摆脱了单纯被动回应外部刺激的模式。"
      ],
      "retrieved_contexts": [
        "则侧重于对这种行动潜能的施行与展现[337]。 智能体的范畴颇为广泛， 既涵盖人类个体， 亦囊括物\n理世界以及虚拟空间中的其他各类实体。尤为关键的是，智能体概念的核心聚焦于个体的自主性，\n即赋予其运用意志、抉择判断以及付诸行动的能力，使之摆脱了单纯被动回应外部刺激的模式。\n本节将从智能体的发展历史和大模型智能体应用范式角度介绍智能体发展的大体历程以及大\n模型智能体在实际应用中的具体范式。\n8.1.1 智能体发展历史\n自 20 世纪 80 年代中后期起， 人工智能研究人员开展了智能体相关研究[338–341]。 与此同时， 智\n能体的内涵也历经演变，与哲学意义上的智能体逐渐有所区别。就人工智能范畴而言，智能体本\n质上是一种计算实体[342, 343]。由于哲学范畴内容关于智能体的定义涉及意识、欲望等概念，这些",
        "8. 大模型智能体\n一直以来， 实现通用类人智能都是人类不懈追求的目标，智能体， 也是在该背景下提出的。 早\n期的智能体主要是基于强化学习实现的，不仅计算成本高，需要用大量的数据训练，而且难以实\n现知识迁移。随着大模型的发展，其在诸多领域展现出惊人的语义处理能力，能够快速生成文本、\n回答问题， 甚至完成一些复杂的知识推理任务。 研究人员开始思考如何将大模型与智能体结合， 从\n而突破大模型本身不具备与外部世界联系， 无法感知外部环境以及调用外部工具的问题。 同时， 智\n能体借助大模型强大的多模态理解与生成优势，可以快速处理信息、规划行动。智能体与大模型\n结合展现出了强大的能力，因此近年来大模型智能体受到了越来越多的关注并在很多应用领域取\n得了很好的实践结果。\n本章将重点介绍智能体的发展、大语言模型智能体架构，最后以 LangChain 为例介绍大语言\n模型智能体实践。\n8.1 智能体基础\n“智能体” （Agent）也称为智能代理，这一概念源远流长，其历史渊源可上溯至亚里士多德、\n休谟等先哲的相关论述。 从哲学维度剖析， “智能体”意指具备行动潜能的实体， 而“代理”一词，\n则侧重于对这种...",
        "面向研究创新的智能体则专注于科学探索、技术研发和创新性问题的解决。这类智能体需要\n具备强大的推理能力与创新思维，因此推理与决策模块在其设计中尤为关键 [372]。例如，在化学、\n数学等领域，ChemCrow[373] 和 FunSearch[374] 等基于大模型的智能体已经展现了在自动化任务执",
        "6. 智能体\n智能体的核心思想是使用大语言模型来选择要执行的一系列动作。在链中，操作序列是硬编\n码在代码中的。 在智能体中， 需要将大语言模型用作推理引擎， 以确定要采取哪些动作， 以及以何\n种顺序采取这些动作。智能体通过将大语言模型与动作列表结合，自动选择最佳的动作序列，从\n而实现自动化决策和行动。智能体可以用于许多不同类型的应用程序，例如自动化客户服务、智\n能家居等。LangChain 显示的智能体仅是智能体的简化方案。LangChain 中的智能体由如下几个核\n心组件构成。\n• Agent：决定下一步该采取什么操作的类，由大语言模型和提示词驱动。提示词可以包括智\n能体的个性（有助于使其以某种方式做出回应） 、智能体的背景上下文（有助于提供所要求\n完成的任务类型的更多上下文信息） 、激发更好的推理的提示策略。\n• Tools： 智能体调用的工具。 这里有两个重要的考虑因素， 一是为智能体提供正确的工具访问\n权限；二是用对智能体最有帮助的方式描述工具。\n• Toolkits： 一组旨在一起使用以完成特定任务的工具集合， 加载方便。 通常一个工具集合中有\n3 ∼ 5 个工具。\n• Ag...",
        "262 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n对于计算实体来说很难定义和度量 [344]，我们所能直接观测到的仅仅是计算实体的外在行为表现。\n因而，包括艾伦・图灵在内的诸多人工智能研究者提议，暂且搁置有关智能体是否“真正”在思\n考，又或者是否真正持有“思想”这类问题的探讨 [345]。研究人员转而采用诸如自主性、响应性、\n主动性以及社交性等其他特性， 用以辅助阐释智能体[342]。 从根本上来说， 人工智能领域的智能体\n与哲学意义层面的智能体并非同一概念，人工智能领域的智能体是智能体哲学概念于人工智能语\n境下的具象化呈现。\n自 20 世纪 90 年代开始，人工智能领域智能体研究开始更快速发展，从整体上看智能体技术\n的发展与人工智能发展紧密相关，可以粗略地划分为以下三个阶段：符号智能体、基于强化学习\n的智能体、以及基于大模型的智能体。\n在人工智能发展的早期阶段， 符号智能体扮演着关键角色， 主要关注转导、 表征和推理问题[346]。\n具体而言，转导问题侧重于将来自环境的低层次感知数据，诸如传感器读取的数据，转换为高层\n次的符号表示；表征和推理问题，则聚焦选择和设计适当..."
      ],
      "response_time": 4.093338251113892,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.031694503197185774,
        "semantic_similarity": 0.6014341115951538
      }
    },
    {
      "sample_id": "0119",
      "question": "ToolLLM 提出了什么方法来提升开源大模型的工具使用能力？",
      "ground_truth": "ToolLLM 提出了通过构建 ToolBench 数据集，为3000 余种工具（涵盖16000多个 API）自动生成任务指令，并利用深度优先搜索算法自动化构建解决方案路径，从而对开源大模型进行微调，显著提升其基于教程学习的工具使用能力。",
      "predicted_answer": "ToolLLM 提出了通过构建 ToolBench 数据集并利用深度优先搜索算法自动化构建解决方案路径的方法来提升开源大模型的工具使用能力[1]。此外，该方法还通过 API 检索器推荐最适合的 API，以进一步优化工具选择与操作过程[1]。",
      "ground_context": [
        "ToolLLM 提出了通过构建 ToolBench 数据集，为3000 余种工具（涵盖16000多个 API）自动生成任务指令，并利用深度优先搜索算法自动化构建解决方案路径，从而对开源大模型进行微调，显著提升其基于教程学习的工具使用能力。"
      ],
      "retrieved_contexts": [
        "大语言模型：从理论到实践 271\n成教程学习任务，现有的开源大模型却因其上下文理解能力的不足，难以通过教程学习有效掌握\n工具使用技能。\n针对这一问题，ToolLLM[392] 提出了通过构建 ToolBench 数据集， 为3000 余种工具 （涵盖16000\n多个 API）自动生成任务指令，并利用深度优先搜索算法自动化构建解决方案路径，从而对开源\n大模型进行微调，显著提升其基于教程学习的工具使用能力。此外，该方法还通过 API 检索器推\n荐最适合的 API，以进一步优化工具选择与操作过程，成功解决了开源大模型在依赖工具手册提\n示语句进行学习时效果受限的问题。\n教程学习的显著优势在于其系统性与全面性。大模型能够通过详细的文档深入学习工具的功\n能与操作方法，从而赋予智能体更为全面且强大的工具使用能力。这种学习方式不仅能够帮助智\n能体高效掌握工具，还为其在复杂任务场景中的灵活应用奠定了坚实基础。\n3. 探索学习\n探索学习 （Exploratory Learning） 是一种通过自主尝试与实验来掌握工具使用的方法。 在这一\n过程中，智能体通过自主探索和反复试验，逐步学习工具的操作技巧及其最佳...",
        "272 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n工具，也不能很好对任务进行多步骤的规划。同时大语言模型构建之初也没有考虑记忆和使用用\n户全部对话历史。为了弥补这些不足，研究者们开始系统地研究如何提升大语言模型解决上述问\n题的能力。本节将重点介绍大语言模型工具使用能力提升、推理规划能力提升以及长期记忆构建\n与应用的策略方法。\n8.3.1 工具学习\n大模型工具学习 （Tool Learning） 是指通过让大语言模型学会使用各种工具的调用方式， 进而\n利用合适的工具去实现特定的功能需求。 例如， 用户输入“请告诉我上海今天的天气。 ”具备工具\n使用能力的大语言模型会给出如下响应：\n1. ്љ಩ༀো྘ğ ฿గҰ࿘಩ༀb\n2.Ⴈ฿గ APIğႨ฿గ APIༀ APIčೂ WeatherMapෂҰ࿘ҕඔĎ b\nresponse = requests.get(\"https://api.weathermap/data/2.5/weather\",\nparams={\n\"q\": \"Shanghai\",\n\"date\": \"2025-1-6\",\n\"appid\": \"your_api_key\"...",
        "用轨迹， 但其中约17% 的轨迹存在工具使用错误。 这些错误的轨迹会对利用其进行训练的模型带\n来了显著的负面影响。此外，通过对 ToolLLaMA-2-7B-v2 和 NexusRaven-13B-v2 的实验结果进行\n分析发现，当模型选择了错误的工具时，通常会选择一个与正确工具具有相同前缀的工具。进一\n步研究表明，通过手动纠正模型第一个错误预测的词元，模型往往能够生成正确的后续词元。这\n一现象说明， 某些关键词元 （Key Tokens） 对于任务的成功至关重要。 研究还表明， 模型在工具调\n用中的错误可以根据工具类型、参数以及内容分为有限的几种类别。这为后续针对性地优化工具\n学习数据集和提升模型性能提供了重要参考。\n根据上述分析，文献 [394] 提出的 TL-Training 方法通过错误数据影响缓解、关键词元优先级\n排序以及强化学习策略有效缓解了上述问题。在有监督微调阶段，其核心目标是使大型语言模型\n（LLM） 与训练数据的分布保持一致。 然而， 训练数据中的错误交互路径可能对模型的决策产生负\n面影响， 进而增加工具调用错误的概率。 为了解决这一问题，TL-Training 设..."
      ],
      "response_time": 4.4827516078948975,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.5454545454545454,
        "rouge2": 0.22222222222222224,
        "rougeL": 0.5454545454545454,
        "bleu": 0.326136741934044,
        "semantic_similarity": 0.9131845235824585
      }
    },
    {
      "sample_id": "0120",
      "question": "感知模块负责从环境中获取哪些形式的信息？",
      "ground_truth": "文本、视觉、听觉等多种形式的信息",
      "predicted_answer": "感知模块负责从环境中获取文本、视觉、听觉等多种形式的信息[1]。",
      "ground_context": [
        "感知模块负责从环境中获取文本、 视觉、 听觉等多种形式的信息, 并将其传递给其他模块进行处理。"
      ],
      "retrieved_contexts": [
        "266 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n观察天空，你觉得明天会下⾬吗？如果会下⾬的话，请给我⼀把⾬伞。\n环境\n感知模块\n从⽬前的天⽓情况和⽹上的天⽓预报来看，明天很可能会下⾬。这是你要的⾬伞。\n推理规划\n记忆模块\n⻓期记忆\n短期记忆\n输⼊\n智能体\n调⽤API …\n⼯具使⽤模块示范学习教程学习探索学习\n规划模块制定决策\n图 8.2 智能体框架结构 [370]\n8.2.1 感知模块\n感知模块负责从环境中获取文本、 视觉、 听觉等多种形式的信息, 并将其传递给其他模块进行\n处理。多模态感知能力对于大模型智能体的发展至关重要。通过整合这些多样化的输入，智能体\n能够深入理解其所处的环境，做出更明智的决策，在复杂多变的任务中发挥出色表现。赋予大模\n型智能体多模态感知能力已成为一个重要的研究方向，除了常见的输入形式之外，触觉反馈、手\n势以及 3D 雷达等其他潜在输入也可以丰富智能体的感知范围，使其在复杂环境中保持灵活、全\n面的感知能力。\n文本作为人类与世界交互的核心载体， 在大模型智能体的发展中扮演着重要角色。 同时， 文本作\n为承载数据、 信息和知识的主要媒介， 也是人机...",
        "视觉感知方面的性能。\n在音频感知方面，声音信息是外界环境中不可或缺的重要组成部分，为大模型智能体赋予听\n觉感知能力， 能够显著增强其对交互内容、 环境状况乃至潜在危险的感知能力。 目前， 已有多种针\n对音频处理的模型和方法被开发， 但这些模型通常仅在特定任务中表现优异[288, 387, 388]。 鉴于大模\n型智能体在工具使用方面的强大能力，研究人员提出了一种直观的方案，即通过将大模型作为控\n制中心，级联调用现有的工具集或模型库以感知音频信息，从而实现多模态感知的高效融合。然\n而，与视觉感知类似，这种通过外部模型进行听觉感知的方法仍存在信息丢失的隐患。因此，如\n何将听觉感知能力直接融入大模型体系，成为当前亟待解决的重要研究课题。\n此外，感知模块的发展还应涵盖其他潜在的输入形式，如触觉、嗅觉等，以进一步拓展大模\n型智能体的感知能力。未来的智能体可能具备更加丰富的感官系统，能够像人类一样感知并理解\n多样化的现实世界信息。例如，通过配备特定的触觉和嗅觉器官，智能体可以在与物体交互时获\n取更为详尽的信息；同时，其还能够对环境中的温度、湿度、光照强度等要素进行精准感知，从\n而实现适应性行动。总..."
      ],
      "response_time": 3.4580790996551514,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.401577332834242,
        "semantic_similarity": 0.7274181842803955
      }
    },
    {
      "sample_id": "0121",
      "question": "无反馈规划指的是什么？",
      "ground_truth": "无反馈规划（Planning without feedback）指在规划阶段一次性生成完整的任务和子任务拆分计划，并严格按照该计划逐步执行，而不根据外界变化进行实时调整。",
      "predicted_answer": "无反馈规划是指在规划阶段一次性生成完整的任务和子任务拆分计划，并严格按照该计划逐步执行，而不根据外界变化进行实时调整[1]。在这种模式下，大模型智能体会在任务开始前，根据当前环境和任务要求生成一个完整的执行方案，并在执行过程中始终遵循初始计划。主要优势在于其执行效率较高，适用于环境相对稳定、变化较少的任务场景[1][2]。",
      "ground_context": [
        "无反馈规划（Planning without feedback）指在规划阶段一次性生成完整的任务和子任务拆分计划，并严格按照该计划逐步执行，而不根据外界变化进行实时调整。"
      ],
      "retrieved_contexts": [
        "268 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n快速决策，并灵活应对各种变化。目前，这一领域的研究主要集中于无反馈规划与带反馈规划两\n大方向，为探索大模型在规划能力上的潜力提供了重要的研究路径。\n1. 无反馈规划\n无反馈规划（Planning without feedback ） 指在规划阶段一次性生成完整的任务和子任务拆分\n计划，并严格按照该计划逐步执行，而不根据外界变化进行实时调整。在这种模式下，大模型智\n能体会在任务开始前，根据当前环境和任务要求生成一个完整的执行方案，并在执行过程中始终\n遵循初始计划。无反馈规划的主要优势在于其执行效率较高，适用于环境相对稳定、变化较少的\n任务场景。 例如， 在文档生成任务中， 智能体可以根据预先设定的主题、 段落结构和内容要求， 生\n成包含所有预定义内容的完整文章，并在生成过程中不因外部反馈而修改文章内容。目前，无反\n馈规划的典型方法是将思维链推理技术扩展至智能体领域[389]。 在这种方法中， 大模型智能体能够\n利用思维链推理技术预先生成完成任务所需的所有子任务拆分计划，并为每个子任务设计相应的\n执行动作，以便在真实环境中逐...",
        "在实际应用中，通常将无反馈规划与带反馈规划相结合，以兼顾效率与灵活性。例如，在自\n主配送系统中，可以首先利用无反馈规划生成初步的配送路线，并在实际执行过程中通过带反馈\n规划进行实时调整，以应对突发情况和动态变化。通过融合无反馈规划的高效性与带反馈规划的\n适应性，规划模块赋予了大模型自主智能体灵活且高效的决策能力，使其能够在多样化的任务环\n境中表现出色，从而完成复杂的任务目标。",
        "际环境中面临执行困难或效果不佳的问题，特别是在忽略外部数据变化的情况下，智能体可能无\n法有效应对突发事件或异常情况。\n2. 带反馈规划\n带反馈规划（Planning with feedback）是一种更为复杂且灵活的规划方式， 智能体在执行任务\n的过程中能够持续获取环境反馈或监控环境变化，并基于反馈信息动态调整行动计划。在这一模\n式下，智能体不仅会在任务开始前制定初步的执行计划，还能够在任务执行过程中实时监测环境\n变化和任务进展，依据实际情况不断优化和修正计划。带反馈规划强调智能体与环境的交互，通\n过不断更新计划以确保任务的顺利完成。其显著优势在于高度的适应性与灵活性，尤其适用于环\n境复杂且变化频繁的任务场景。ReAct[390] 方法是大模型智能体带反馈规划方法的经典方法， 其核\n心在于将任务执行过程与推理规划过程相结合。在任务执行的每一步中，大模型智能体依据已完\n成的子任务和获得的环境反馈，动态生成当前步骤的子任务及相应的执行动作，并将其在真实环\n境中执行。完成后，环境反馈会被传递回智能体，用于下一步的任务规划。通过这一反复循环的\n过程，ReAct 方法使大模型智能体能够根据环境反..."
      ],
      "response_time": 5.120371341705322,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0.0,
        "bleu": 0.2799456685182945,
        "semantic_similarity": 0.8888169527053833
      }
    },
    {
      "sample_id": "0122",
      "question": "大语言模型在视觉感知领域的表现如何？",
      "ground_truth": "尽管大语言模型在理解和处理多轮对话方面展现了卓越的性能，但仍然无法处理视觉模态信息。",
      "predicted_answer": "大语言模型在视觉感知领域展现出了卓越的性能[65]，但在处理视觉模态信息方面仍有局限性。为了改善这一点，可以通过将大语言模型与视觉编码器相结合，并增加一个可学习的接口层来增强大语言模型对视觉信息的感知能力[387]。这样可以提高大语言模型在视觉感知方面的性能。",
      "ground_context": [
        "尽管大语言模型在理解和处理多轮对话方面展现了卓越的性能，但仍然无法处理视觉模态信息。"
      ],
      "retrieved_contexts": [
        "大语言模型：从理论到实践 267\n陌生场景下，提升智能体的文本感知能力显得尤为重要。\n在视觉感知领域，尽管大语言模型在理解和处理多轮对话方面展现了卓越的性能[65]，但仍然\n无法处理视觉模态信息。 视觉输入通常包含丰富的环境信息， 例如物体的属性、 空间关系以及场景\n布局。将视觉信息与其他模态数据相结合，能够使智能体对外部环境的理解更加全面且精准 [255]。\n为了赋予智能体理解视觉信息的能力，一种直接的方法是将视觉输入通过图像描述生成技术转换\n为对应的文本描述[386]。 这种方法的优点在于其高度的可解释性， 并且无需为生成描述进行额外训\n练，从而显著节约计算资源。然而，此方法在转换过程中可能会丢失大量潜在信息，导致视觉信\n息的表达不完整。为解决上述问题，研究人员尝试将大语言模型与视觉编码器相结合，并通过增\n加一个可学习的接口层来对齐视觉编码与大模型的语言理解能力，从而增强大模型对视觉信息的\n感知能力[387]。 这一方法有效降低了大模型在学习视觉语言对齐任务中的负担， 并显著提升了其在\n视觉感知方面的性能。\n在音频感知方面，声音信息是外界环境中不可或缺的重要组成部分，为大模型智能体...",
        "大语言模型：从理论到实践 245\n在推理过程中，视觉标记对位置编码的占用仍然是一个不可忽视的问题。事实上，视觉标记的长\n度通常比文本标记高出 500 倍以上，在具有位置感知的视觉语言模型中，这会消耗大量的位置嵌\n入资源。鉴于视觉专家本身已经包含位置编码信息，为每个视觉标记再次分配视觉大语言模型的\n位置嵌入显得冗余且低效。为了解决这一问题，MouSi 模型提出了一种二维可训练图像位置编码\n方法，通过直接在视觉标记中引入可训练的二维位置编码，避免了对视觉大语言模型位置嵌入的\n额外占用。这种方法不仅有效解决了多视觉专家导致的超长序列问题，还减少了位置编码的冗余\n分配，从而优化了视觉标记的处理效率，为多模态模型的可扩展性提供了重要支持。\n7.3 多模态大语言模型训练策略\n深度神经网络缩放法则（Scaling Law）为多模态大语言模型的训练策略提供了重要参考。以\n往业界普遍采用增加计算资源和模型规模的方式来提升性能，然而，根据文献 [304] 的研究成果，\n优化数据处理环节亦可带来突破性进展。以 CLIP 为例，其采用 4 亿张图像进行训练，开源版本\nOpenCLIP[305] 则需数百卡 G...",
        "大语言模型：从理论到实践 233\n7.2 大语言模型与多模态融合架构\n近年来， 随着基于Transformer 架构的算法取得了显著进展， 视觉语言模型、 音频语言模型都\n有了很大的发展， 模型架构也多种多样， 包括双编码器架构、 融合架构和编码器-解码器架构等。 这\n些架构不断演化并结合新的技术，例如混合模态注意力机制、对比学习、强化学习等，进一步提\n升了模型的性能和适应能力。\n本节将围绕多模态大语言模型的架构展开介绍，分别探讨视觉语言模型架构、语音语言模型\n架构以及多模态大语言模型架构。\n7.2.1 视觉语言模型架构\n视觉语言模型（Vision-Language Models，VLM）是一类旨在结合计算机视觉与自然语言处理\n能力的模型， 近年来借助基于Transformer 的技术取得了显著进展。 这些模型的训练方法可以分为\n四种主要范式： 对比学习、 掩码预测、 生成式学习以及映射学习。 对比学习通过正负样本对的表示\n相似性与差异性训练模型；掩码预测则通过遮掩图像或文本的部分信息，训练模型进行重建；生\n成式视觉语言模型则专注于生成图像或文本，但因其复杂性通常需要更高的计算资源；映..."
      ],
      "response_time": 4.7618772983551025,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.09577079136415172,
        "semantic_similarity": 0.7493255734443665
      }
    },
    {
      "sample_id": "0123",
      "question": "最终构建了多少条轨迹？",
      "ground_truth": "1,866 条轨迹。",
      "predicted_answer": "最终构建了 1,866 条轨迹。[1]",
      "ground_context": [
        "最终构建了 1,866 条轨迹。"
      ],
      "retrieved_contexts": [
        "格的过滤。每条交互轨迹都会获得一个奖励值，基于此奖励值，可以自动筛选出高质量的轨迹数\n据。最终构建了 1,866 条轨迹。\n采用 AgentTuning 方法对 Llama 2 模型进行微调，并构建了开源的 AgentLM 模型。AgentLM\n在未知智能任务中展现了很好的性能， 同时在MMLU、GSM8K、HumanEval 和 MT-Bench 等通用\n任务上依然保持了优异的表现。开源的 AgentLM-70B 在智能体任务表现上可与 GPT-3.5-turbo 相\n媲美。\n8.3.3 长期记忆\n大模型智能体的记忆模型由长期记忆和短期记忆构成。短期记忆可以通过将记忆内容以提示\n语句嵌入大模型输入上下文，依靠大模型的上下文理解能力实现存储和使用。长期记忆则通过构\n建记忆库来管理和检索， 以实现知识的持久化存储与高效调用。 在长期任务中， 智能体将经验、 知\n识等存储到记忆库，需要时检索记忆内容，与当前任务需求结合，提升问题解决效率。\n大模型智能体实现长期记忆的常见方法之一是引入外部记忆库。长期记忆可存储为灵活的形\n式，例如文本文件或结构化数据库，并通过检索机制与反思机制进行访问与更新...",
        "大语言模型：从理论到实践 281\n轨迹交互任务。在评估方法上，采用了 1-shot 评估策略，主要是为了满足智能体任务中对输出格\n式精确性的严格要求。对于每个任务，均提供来自训练集的完整交互过程作为示例。轨迹交互过\n程主要包括两个阶段。首先，向模型提供任务描述及一个成功的 1-shot 示例，以帮助其理解任务\n要求。随后进入正式交互阶段，向模型输入当前指令和必要的上下文信息。模型基于这些信息及\n此前的反馈内容，生成“思考” （Thought）并采取相应的行动。环境则根据模型的操作提供反馈，\n反馈内容可能包括状态变化或新的信息。 上述过程循环进行， 直至模型完成任务目标或达到Token\n限制。 若模型连续三次生成相同的输出， 则被视为重复性失败。 若模型输出的格式不符合要求， 则\n通过 BLEU 指标将其与所有可能的操作选项进行比较，并选择最接近的选项作为该步骤的操作。\n在涉及真实场景的智能体任务中，由于任务复杂性较高，即便是 GPT-4 在此类任务上的表现\n也未能达到预期。为了确保数据质量， AgentInstruct 数据集构造过程中还对其交互轨迹进行了严\n格的过滤。每条交互轨迹都会..."
      ],
      "response_time": 3.4133856296539307,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.8,
        "rouge2": 0.6666666666666666,
        "rougeL": 0.8,
        "bleu": 0.4172261448611506,
        "semantic_similarity": 0.8891462087631226
      }
    },
    {
      "sample_id": "0124",
      "question": "MemoryBank 的核心组件之一是什么？",
      "ground_truth": "记忆存储（Memory Stroage）是 MemoryBank 的核心组件之一，存储了丰富的信息，包含日常对话记录、过去的事件总结和用户个性评估的演变， 从而构建了一个动态的多层次记忆全景图。",
      "predicted_answer": "MemoryBank 的核心组件之一是记忆存储（Memory Stroage）[1]。",
      "ground_context": [
        "记忆存储（Memory Stroage）是 MemoryBank 的核心组件之一，存储了丰富的信息，包含日常对话记录、 过去的事件总结和用户个性评估的演变， 从而构建了一个动态的多层次记忆全景图。"
      ],
      "retrieved_contexts": [
        "记忆增强提示词\n记忆检索\nSiliconFriend事件摘要用户画像相关记忆\n元提示词\n历史\n检索词\nTomorrow is my GF s’birthdayYou should prepare gifts......SendDo you remember the gifts she like? \n图 8.7 MemoryBank 方法框架 [407]\n记忆存储（Memory Stroage）是 MemoryBank 的核心组件之一，存储了丰富的信息，包含日\n常对话记录、 过去的事件总结和用户个性评估的演变， 从而构建了一个动态的多层次记忆全景图。\n通过按时间顺序记录多轮对话并添加时间戳，构建了有序的交互历史。这种细致的记录不仅支持\n精确的记忆检索，还为后续记忆更新提供了详细索引。\nMemoryBank 借鉴了人类记忆的复杂性， 不仅简单存储， 还对对话进行提炼， 生成每日事件总\n结，并进一步凝练为全局总结，形成层次化的记忆结构，为用户交互和重要事件提供鸟瞰式视角。\n具体来说，以之前的每日对话或每日事件为输入，要求大语言模型使用提示“ଽಸ [߅ؓ/൙\nࡱ]ྐ༏” 来总结每日事件或全局事...",
        "式，例如文本文件或结构化数据库，并通过检索机制与反思机制进行访问与更新。外部记忆库可\n以采用向量数据库或可读写的神经网络记忆库等模式，模型能够动态地获取或更新所需知识。其\n中，检索增强生成（Retrieval-Augmented Generation, RAG ）是一种典型方法，将检索与生成有机\n结合，适用于知识动态变化的场景。然而，该方法在应用中仍面临检索效率和记忆库质量的挑战，\n这对系统性能具有重要影响。\n文献 [407] 提出了 MemoryBank 方法， 允许模型调用相关记忆， 通过持续的记忆更新不断进化，\n通过综合之前交互的信息，随着时间的推移理解和适应用户个性。MemoryBank 框架如图8.7所示，\n它由记忆存储、记忆检索以及记忆更新模块组成，每次用户输入的提示词会与记忆模块检索结果\n一起构成记忆增强的提示词。记忆存储作为主要的数据存储库，保存了对话的详细记录、事件总\n结和用户个性评估。记忆检索允许根据上下文进行记忆回忆。记忆更新受到艾宾浩斯遗忘曲线理\n论（Ebbinghaus Forgetting Curve Theory ）的启发，改理论认为遗忘在学习之后立即开始..."
      ],
      "response_time": 2.827484369277954,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.8571428571428571,
        "rouge2": 0.4,
        "rougeL": 0.5714285714285715,
        "bleu": 0.09033042855250471,
        "semantic_similarity": 0.875525176525116
      }
    },
    {
      "sample_id": "0125",
      "question": "大模型智能体是如何解决早期基于强化学习的智能体面临的哪些问题的？",
      "ground_truth": "大模型智能体通过融合大模型的强大多模态理解与生成能力，突破了早期智能体在知识迁移、计算成本高和大量数据训练等方面的限制，同时也解决了大模型本身无法感知外部环境和调用外部工具的问题。",
      "predicted_answer": "大模型智能体解决了早期基于强化学习的智能体面临的一些关键问题，这些问题包括高昂的计算成本、大量数据的需求以及难以实现的知识迁移[2]。大模型智能体通过集成大模型的强大功能，克服了这些挑战，特别是通过利用大模型的多模态理解和生成能力，实现了更快的信息处理和更高效的决策制定[2][3]。此外，大模型智能体还能够更好地与外部环境互动，从而扩展了智能体的功能范围[2]。",
      "ground_context": [
        "大模型智能体具有感知、决策、行动和记忆的能力，...以支持持续学习和适应动态环境。",
        "随着大模型的发展，其在诸多领域展现出惊人的语义处理能力，能够快速生成文本、回答问题，甚至完成一些复杂的知识推理任务。"
      ],
      "retrieved_contexts": [
        "度学习的兴起，深度强化学习应运而生，它融合了强化学习与深度神经网络技术 [358, 359]，促使智\n能体具备从高维输入数据中学习复杂策略的能力，得以在未知环境里自如探索、自主学习，进而\n在从电子游戏竞技到机器人操控等诸多领域广泛渗透， 产生了AlphaGo[360]、DQN[361] 等一系列重\n要成果。但是，强化学习智能体依旧面临着训练周期冗长、采样效率欠佳以及稳定性不足等棘手\n难题，尤其在错综复杂的现实世界场景应用中，这些短板更加凸显[355]。\n2023 以来，大模型异军突起，其所展现出的惊人的能力引发广泛瞩目，基于大模型构建的智\n能体也日益备受瞩目[362–365]。 大模型智能体具有感知、 决策、 行动和记忆的能力， 通过感知模块捕\n获周围环境的信息，利用大模型进行推理和决策，通过执行器实施具体行动，同时还能存储和管\n理记忆， 以支持持续学习和适应动态环境[366]。 大模型智能体将大模型作为智能体的核心中枢， 即\n大脑或控制器的关键构成要素，同时借助多模态感知、工具运用等策略，全方位拓展智能体的感\n知范畴与行动边界。凭借思维链、问题分解等技术手段，大模型智能体得以彰显出可...",
        "8. 大模型智能体\n一直以来， 实现通用类人智能都是人类不懈追求的目标，智能体， 也是在该背景下提出的。 早\n期的智能体主要是基于强化学习实现的，不仅计算成本高，需要用大量的数据训练，而且难以实\n现知识迁移。随着大模型的发展，其在诸多领域展现出惊人的语义处理能力，能够快速生成文本、\n回答问题， 甚至完成一些复杂的知识推理任务。 研究人员开始思考如何将大模型与智能体结合， 从\n而突破大模型本身不具备与外部世界联系， 无法感知外部环境以及调用外部工具的问题。 同时， 智\n能体借助大模型强大的多模态理解与生成优势，可以快速处理信息、规划行动。智能体与大模型\n结合展现出了强大的能力，因此近年来大模型智能体受到了越来越多的关注并在很多应用领域取\n得了很好的实践结果。\n本章将重点介绍智能体的发展、大语言模型智能体架构，最后以 LangChain 为例介绍大语言\n模型智能体实践。\n8.1 智能体基础\n“智能体” （Agent）也称为智能代理，这一概念源远流长，其历史渊源可上溯至亚里士多德、\n休谟等先哲的相关论述。 从哲学维度剖析， “智能体”意指具备行动潜能的实体， 而“代理”一词，\n则侧重于对这种...",
        "大规模语言模型：从理论到实践 191\n智能体和环境。 在强化学习过程中， 智能体与环境不断交互。 智能体在环境中获取某个状态后， 会\n根据该状态输出一个动作， 也称为决策。 动作会在环境中执行， 环境会根据智能体采取的动作， 给\n出下一个状态及当前动作带来的奖励。智能体的目标就是尽可能多地从环境中获取奖励。本节将\n介绍强化学习的基本概念、强化学习与有监督学习的区别，以及在大语言模型中基于人类反馈的\n强化学习流程。\n图 6.1 强化学习基本框架\n在现实生活中，经常会遇到需要通过探索和试错来学习的情境。例如，孩子学会骑自行车的\n过程或是教宠物狗如何玩飞盘。宠物狗一开始对如何抓飞盘一无所知，但每当它成功抓住飞盘时，\n都可以给予它一定的奖励。这种通过与环境交互，根据反馈来学习最佳行为的过程正是强化学习\n的核心思想。通过宠物狗学习抓飞盘的例子，可以引出一些强化学习中的基本概念。\n（1）智能体与环境：在宠物狗学习抓飞盘的场景中，宠物狗就是一个智能体（Agent），它做\n出决策（Decision）并执行动作。它所在的场景， 包括飞盘的飞行轨迹和速度， 以及其他可能的因\n素，构成了环境（Environ...",
        "知环境中的适应能力；而教程学习与示范学习的结合，则能够为模型理解复杂工具操作提供双重\n支持。 这种多策略融合不仅提升了模型的学习效率， 还为处理更复杂的多工具任务开辟了新路径。\n8.3 大模型智能体训练\n大模型智能体的核心能力涵盖了感知、规划、记忆以及工具使用，这些能力使其能够弥补传\n统大模型无法与外部世界交互的局限性。然而，大语言模型在最初的设计中并不具备这些核心能\n力。大语言模型主要依赖于大规模的文本数据训练，擅长语言生成和理解，但无法直接使用外部"
      ],
      "response_time": 5.353971719741821,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.09470703009420087,
        "semantic_similarity": 0.9079307317733765
      }
    },
    {
      "sample_id": "0126",
      "question": "如何结合视觉信息和思维链提示来提升大语言模型在多模态感知中的表现？",
      "ground_truth": "结合视觉信息和思维链提示可以显著提升大语言模型在多模态感知中的表现。通过将视觉输入转化为文本描述，能够为智能体提供更全面的环境理解，而思维链提示则能提升推理过程的准确性和可解释性。具体来说，研究人员通过将视觉编码与语言理解对齐的方法增强了模型对视觉信息的感知能力，同时利用思维链提示降低推理难度，提高最终结果的准确率。",
      "predicted_answer": "为了提升大语言模型在多模态感知中的表现，可以通过以下几种方式结合视觉信息和思维链提示：\n\n1. 将视觉信息转化为文本描述，使用思维链提示逐步展示模型的推理过程。这样不仅能帮助模型更好地理解复杂的视觉信息，还能提高其推理能力[255][395]。\n2. 结合视觉编码器与大语言模型，通过增加一个可学习的接口层来对齐视觉编码与大语言理解能力，同时使用思维链提示指导模型如何处理视觉信息[387]。\n3. 使用基于边界框标注的方法来增强视觉语义关联能力，在视觉与文本之间建立精确的对应关系。这种方法有助于模型更准确地理解视觉信息并与相应的文本描述关联起来[315]。\n\n通过以上方法，可以有效地结合视觉信息和思维链提示，提升大语言模型在多模态感知中的表现。",
      "ground_context": [
        "将视觉信息与其他模态数据相结合，能够使智能体对外部环境的理解更加全面且精准。",
        "通过实验发现，使用由不同人员编写的符号推理示例在准确率上存在高达 28.2% 的差异，而改变范例的顺序在大多数任务中则只产生了不到 2% 的变化。"
      ],
      "retrieved_contexts": [
        "对环境和任务有深刻理解， 从而生成并优化任务执行计划， 制定合理的行动步骤以实现既定目标。\n然而，仅仅通过扩大语言模型的规模，并不能显著提升推理（Reasoning）能力，如常识推理、逻\n辑推理、数学推理等。通过示例（Demonstration）或者明确指导模型在面对问题时如何逐步思考，\n促使模型在得出最终答案之前生成中间的推理步骤，可以显著提升其在推理任务上的表现。这种\n方法被称为思维链提示（Chain-of-Thought Prompting）[395]。同样地，面对复杂任务或问题时，大\n语言模型可以展现出良好的规划 （Planning） 能力。 通过引导模型首先将复杂的问题分解为多个较\n为简单的子问题，然后逐一解决这些子问题，可使模型得出最终答案，这种策略被称为由少至多\n提示[396]。 本节将重点介绍如何利用思维链提示和由少至多提示这两种方式， 提升大语言模型的推\n理规划能力。\n1. 思维链提示\n语言模型在推理能力方面的表现一直未能令人满意，一些研究人员认为这可能是因为此前的\n模式是直接让模型输出结果，而忽略了其中的思考过程。人类在解决包括数学应用题在内的、涉\n及多步推理的问题...",
        "大语言模型：从理论到实践 267\n陌生场景下，提升智能体的文本感知能力显得尤为重要。\n在视觉感知领域，尽管大语言模型在理解和处理多轮对话方面展现了卓越的性能[65]，但仍然\n无法处理视觉模态信息。 视觉输入通常包含丰富的环境信息， 例如物体的属性、 空间关系以及场景\n布局。将视觉信息与其他模态数据相结合，能够使智能体对外部环境的理解更加全面且精准 [255]。\n为了赋予智能体理解视觉信息的能力，一种直接的方法是将视觉输入通过图像描述生成技术转换\n为对应的文本描述[386]。 这种方法的优点在于其高度的可解释性， 并且无需为生成描述进行额外训\n练，从而显著节约计算资源。然而，此方法在转换过程中可能会丢失大量潜在信息，导致视觉信\n息的表达不完整。为解决上述问题，研究人员尝试将大语言模型与视觉编码器相结合，并通过增\n加一个可学习的接口层来对齐视觉编码与大模型的语言理解能力，从而增强大模型对视觉信息的\n感知能力[387]。 这一方法有效降低了大模型在学习视觉语言对齐任务中的负担， 并显著提升了其在\n视觉感知方面的性能。\n在音频感知方面，声音信息是外界环境中不可或缺的重要组成部分，为大模型智能体...",
        "7.3.2 视觉语义关联\n视觉语义关联是多模态大语言模型和生成模型研究中的一项核心挑战。其主要目标是解决模\n型对文本提示理解不充分的问题，这种不足可能导致模型忽略提示中的某些关键信息，或错误生\n成不存在的内容。模型在处理视觉与文本的关联时，需要克服诸多复杂性，例如物体的空间位置\n关系（如左右位置） 、否定表达、计数能力，以及属性理解（如颜色和纹理） 。虽然目前尚无单一\n的方法能够完全解决这些问题，但研究者提出了一些行之有效的策略来提升模型的视觉语义关联\n能力。本节将重点介绍两种常用的改进方法：基于边界框标注和负样本生成方法。\n1. 基于边界框标注\n基于边界框标注是一种直接且高效的方式，用于增强视觉语义关联能力。例如， X-VLM[315]\n模型通过结合边界框回归与交并比 (IoU) 损失， 成功实现了视觉概念的精确定位， 并将这些概念与\n对应的文本描述对齐。通过明确标注图像中物体的位置及其相关描述，该模型能够更精准地将文\n本提示与正确的视觉线索关联，从而显著提升语义理解能力。这种方法的核心在于细粒度的视觉",
        "大语言模型：从理论到实践 233\n7.2 大语言模型与多模态融合架构\n近年来， 随着基于Transformer 架构的算法取得了显著进展， 视觉语言模型、 音频语言模型都\n有了很大的发展， 模型架构也多种多样， 包括双编码器架构、 融合架构和编码器-解码器架构等。 这\n些架构不断演化并结合新的技术，例如混合模态注意力机制、对比学习、强化学习等，进一步提\n升了模型的性能和适应能力。\n本节将围绕多模态大语言模型的架构展开介绍，分别探讨视觉语言模型架构、语音语言模型\n架构以及多模态大语言模型架构。\n7.2.1 视觉语言模型架构\n视觉语言模型（Vision-Language Models，VLM）是一类旨在结合计算机视觉与自然语言处理\n能力的模型， 近年来借助基于Transformer 的技术取得了显著进展。 这些模型的训练方法可以分为\n四种主要范式： 对比学习、 掩码预测、 生成式学习以及映射学习。 对比学习通过正负样本对的表示\n相似性与差异性训练模型；掩码预测则通过遮掩图像或文本的部分信息，训练模型进行重建；生\n成式视觉语言模型则专注于生成图像或文本，但因其复杂性通常需要更高的计算资源；映..."
      ],
      "response_time": 7.512453556060791,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.1533301357890283,
        "semantic_similarity": 0.9362828731536865
      }
    },
    {
      "sample_id": "0127",
      "question": "在智能体如何结合长期记忆和视觉感知能力来提升其任务执行效率与环境理解能力上，有哪些关键点？",
      "ground_truth": "智能体通过长期记忆构建记忆库实现知识的持久化存储与高效调用，并在执行任务中系统化地存储经验和知识。当需要处理陌生场景时，结合视觉信息能够提升智能体的文本感知能力，而将视觉输入转换为文本描述的技术可以帮助智能体理解环境信息。通过将长期记忆与视觉感知结合，智能体能够更全面地理解外部环境，从而提升任务的执行效率和准确性。",
      "predicted_answer": "根据提供的文档，我无法直接找到关于智能体如何结合长期记忆和视觉感知能力来提升其任务执行效率与环境理解能力的具体关键点描述。文档提到了智能体的多种能力，包括记忆模块、感知模块和工具使用模块，但未详细说明两者如何具体结合。因此，无法直接得出结论。[1][2][3][4][5]",
      "ground_context": [
        "长期记忆通过构建记忆库来实现管理和检索，支持知识的持久化存储与高效调用。",
        "在陌生场景下，提升智能体的文本感知能力显得尤为重要。"
      ],
      "retrieved_contexts": [
        "长期记忆通过构建记忆库来实现管理和检索，支持知识的持久化存储与高效调用。包括构建\n和检索两个部分： （1）构建：在长期任务的执行过程中，智能体会将累积的经验、知识以及数据\n系统化地存储至记忆库中。记忆库的形式可以包括向量数据库、知识图谱等。 （2）检索：智能体\n需要获取以往的经验或相关知识时，可以通过查询记忆库进行检索，并将检索到的记忆内容作为\n大模型的输入，与当前任务需求结合后进行处理。例如，在面对类似问题时，智能体能够检索到\n此前解决相似问题的经验，从而显著提升问题解决的效率与准确性。\n2. 记忆操作\n智能体的记忆操作则包括写入、读取和反思等多个环节，这些操作旨在确保智能体能够高效\n地管理和利用其记忆资源，从而提升任务执行能力与智能化水平。\n记忆写入指将新的信息或经验存储到记忆模块中。在短期记忆中，写入的方式通常是将新的\n文本信息直接插入到上下文中，而在长期记忆中，则需要将信息存储到记忆库中，并对其进行索\n引与标记，以便后续检索使用。例如，当智能体完成某项任务后，可以将任务的执行过程及其结\n果记录为参考数据，供未来使用。通过不断积累经验，智能体能够逐步优化其能力，实现更高水\n平的智...",
        "提升整体的工作效率和用户体验。\n在人机平等协作范式中，强调智能体与人类作为平等的合作伙伴，共同参与任务的规划与执\n行。这种模式注重智能体的适应性与自主性，通过协同合作实现任务的高效完成。例如，在任务\n执行过程中， 智能体能够主动寻求人类的反馈， 并根据反馈动态调整其行为策略[384]。 与人类主导\n范式不同，这种模式不仅要求智能体具备执行能力，还需要其通过自主学习与优化不断提升自身\n能力。\n随着智能体在环境感知、 推理与决策能力方面的进步， 人机交互的效率与深度也将不断提高。\n通过持续的优化与协同，人类与智能体之间可以实现真正的无缝合作，使智能体成为人类创新和\n效率提升的重要伙伴。\n8.2 大语言模型智能体架构\n智能体可以被视为独立的个体，能够接收并处理外部信息，进而给出响应。大模型智能体基\n本组成如图8.2 所示，主要包含以下几个核心模块：感知模块、规划模块、记忆模块、工具使用模\n块。对于外界输入，智能体借助多模态能力将文字、音频、图像等多种形式的信息转换为机器能\n够理解的表现形式；进而由规划模块对这些信息进行处理，结合记忆模块完成推理、规划等复杂\n任务；智能体可能会利用工具使用模块...",
        "大语言模型：从理论到实践 269\n8.2.3 记忆模块\n记忆模块是在大模型智能体中承担着管理与操作智能体记忆的核心功能，包括对长短期记忆\n的存储、读取、处理以及反思等任务。该模块不仅负责存储历史数据与经验，还能够高效提取和\n更新信息，从而实现长期记忆与短期记忆之间的有机交互。通过记忆模块的支持，智能体在处理\n连续性任务时能够保持上下文的连贯性，并基于以往经验做出更加准确的判断与决策。\n1. 记忆模型\n大模型智能体所采用的记忆模型包括长期记忆和短期记忆两部分。它们各自有不同的功能和\n实现方式，但都依赖于大模型的强大计算和理解能力。\n短期记忆通常通过将记忆内容以提示语句的形式嵌入大模型输入的上下文中，借助大模型的\n上下文理解能力来实现。 包括存储和使用两个部分： （1） 存储： 在任务执行过程中， 关键的上下文\n信息与事件会被实时记录，形成短期记忆内容； （2）使用：在后续任务中，这些记忆内容会作为\n提示语句输入至大模型的上下文中，帮助模型基于提示进行推理与决策。例如，将前几步的操作\n结果及重要的环境信息作为输入内容，支持模型在接下来的步骤中做出更加合理的判断与选择。\n长期记忆通过构建记忆...",
        "平的智能表现。\n记忆读取是指从记忆模块中提取与当前任务相关的信息，以支持任务的完成。在短期记忆中，\n读取操作通常是直接从上下文中提取提示信息并加以使用；而在长期记忆中，则通过检索记忆库\n来获得相关内容，通常通过匹配任务需求与记忆信息的方式完成。例如，当处理一个复杂问题时，\n智能体可以从长期记忆库中提取具有参考价值的解决方案，从而提供精准的建议或策略，提升任\n务处理的效率与质量。\n记忆反思是智能体对已存储记忆进行回顾与分析的一种机制，旨在进一步优化其行为策略与",
        "度学习的兴起，深度强化学习应运而生，它融合了强化学习与深度神经网络技术 [358, 359]，促使智\n能体具备从高维输入数据中学习复杂策略的能力，得以在未知环境里自如探索、自主学习，进而\n在从电子游戏竞技到机器人操控等诸多领域广泛渗透， 产生了AlphaGo[360]、DQN[361] 等一系列重\n要成果。但是，强化学习智能体依旧面临着训练周期冗长、采样效率欠佳以及稳定性不足等棘手\n难题，尤其在错综复杂的现实世界场景应用中，这些短板更加凸显[355]。\n2023 以来，大模型异军突起，其所展现出的惊人的能力引发广泛瞩目，基于大模型构建的智\n能体也日益备受瞩目[362–365]。 大模型智能体具有感知、 决策、 行动和记忆的能力， 通过感知模块捕\n获周围环境的信息，利用大模型进行推理和决策，通过执行器实施具体行动，同时还能存储和管\n理记忆， 以支持持续学习和适应动态环境[366]。 大模型智能体将大模型作为智能体的核心中枢， 即\n大脑或控制器的关键构成要素，同时借助多模态感知、工具运用等策略，全方位拓展智能体的感\n知范畴与行动边界。凭借思维链、问题分解等技术手段，大模型智能体得以彰显出可..."
      ],
      "response_time": 4.984045028686523,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.03512086092126586,
        "semantic_similarity": 0.8427127003669739
      }
    },
    {
      "sample_id": "0128",
      "question": "如何完成角色扮演的交互过程？",
      "ground_truth": "可以直接调用 step() 函数完成角色扮演的交互过程，获取两个智能体的新一轮输出。",
      "predicted_answer": "要完成角色扮演的交互过程，可以按照以下步骤操作：\n\n1. 设定角色扮演的上下文和任务需求。\n2. 使用 `role_play_session.step()` 函数让智能体依次发言，直到达到设定的轮数上限或某一方终止对话。\n3. 在每轮交互中，`step()` 函数返回当前轮次的助手响应和用户响应。\n4. 检查是否有任一响应标记为终止条件，如果是，则打印终止原因并退出循环；否则继续下一轮交互。\n\n具体代码实现如下：\n```python\nwhile n < chat_turn_limit: #usi\n    assistant_response, user_response = role_play_session.step(input_assistant_msg) #usi\n    if assistant_response.terminated:\n        print(f\"AI Assistant terminated. Reason: {assistant_response.info['termination_reasons']}\") #usi\n        break\n    if user_response.terminated:\n        print(f\"AI User terminated. Reas... #usi\n```\n[1]",
      "ground_context": [
        "基于 RolePlaying 的结构，可以直接调用 step() 函数完成角色扮演的交互过程，获取两个智能体的新一轮输出："
      ],
      "retrieved_contexts": [
        "大语言模型：从理论到实践 289\n基于 RolePlaying 的结构，可以直接调用 step() 函数完成角色扮演的交互过程，获取两个智能\n体的新一轮输出：\nassistant_response, user_response = role_play_session.step(input_assistant_msg)\n角色扮演的完整对话实现如下：\nwhile n < chat_turn_limit: #ս੽Ցཋᇅ\n#ྍ၂੽ൻԛ\nassistant_response, user_response = role_play_session.step(input_assistant_msg)\n#߅ؓ\nif assistant_response.terminated:\nprint(\"AI Assistant terminated. \"\nf\"Reason: {assistant_response.info['termination_reasons']}.\")\nbreak\nif user_response.terminated:\nprint(\"AI User terminated. \"\nf\"Reas...",
        "times 2 = $16.\nAdding $2 to this, we get that Jimmy has $16 + $2 = $\n boxed{18}$.\n2. 角色扮演\n角色扮演（Role-Playing）是指在事先设计的情景中自然地扮演某个角色。通过构造特定的提\n示，大语言模型有能力扮演不同的角色——无论是一个五年级的小学生，还是一个计算机领域的\n专家。令人意想不到的是，扮演特定角色的大语言模型能够激发其内部独特的领域知识，产生比\n没有指定角色时更好的答案。角色扮演在赋予智能体个体优势和专业技能的同时，更在多个智能\n体的协作交流中体现出了极大的价值，大大提高了多智能体系统的问题解决效率。\nCAMEL 是角色扮演的经典应用实例，该框架实现了两个智能体的交互，其中一个智能体作\n为用户，另一个智能体作为助手。此外， CAMEL 中还允许用户自由选择是否需要设置任务明确\n智能体与评论智能体，任务明确智能体专门负责将人类给出的初始任务提示细致化，评论智能体\n则负责评价交互的内容， 一方面引导交互向正确的方向进行， 另一方面判定任务目标是否已达成。\nCAMEL 中定义了一个 RoleP...",
        "为例，最简单的交互实现可大致分为以下步骤。\n（1）对于每个任务，用户首先描述任务的基本需求：\nquestion = \"Jimmy has $2 more than twice the money Ethel has.\nIf Ethal has $8, how much money is Jimmy having?\" #ิԛ໙ี\nagent_contexts = [[{\"role\": \"user\", \"content\": \"\"\"Can you solve the following math\nproblem? {} Explain your reasoning.\nYour final answer should be a single\nnumerical number, in the form\nboxed{{answer}}, at the end of your\nresponse.\"\"\".format(question)}]\nfor agent in range(agents)] #ᄯൻೆิൕ\n（2）每个智能体按一定顺序依次发言：\nfor i, agent_context in enu..."
      ],
      "response_time": 7.620502233505249,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.034482758620689655,
        "rouge2": 0.0,
        "rougeL": 0.034482758620689655,
        "bleu": 0.03079738361509587,
        "semantic_similarity": 0.7623436450958252
      }
    },
    {
      "sample_id": "0129",
      "question": "如何构建消息？",
      "ground_truth": "message = construct_message(agent_contexts_other, question, 2*round - 1 )",
      "predicted_answer": "根据提供的文档，我无法回答这个问题。",
      "ground_context": [
        "message = construct_message(agent_contexts_other, question, 2*round - 1 )"
      ],
      "retrieved_contexts": [],
      "response_time": 2.6599342823028564,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.0,
        "semantic_similarity": 0.43772733211517334
      }
    },
    {
      "sample_id": "0130",
      "question": "LangChain 中的记忆模块提供了哪些基本操作？",
      "ground_truth": "记忆系统需要支持两个基本操作：读取和写入。",
      "predicted_answer": "LangChain 中的记忆模块需要支持两个基本操作：读取和写入。这两个操作使得每个链可以在接收初始用户输入并执行核心逻辑之前从记忆系统中读取内容，在完成一轮交互后也会将这一轮的输入和输出保存到记忆系统中以便未来使用。[1][2]",
      "ground_context": [
        "记忆系统需要支持两个基本操作：读取和写入。"
      ],
      "retrieved_contexts": [
        "overall_chain = SimpleSequentialChain(chains=[synopsis_chain, review_chain], verbose =True)\n5. 记忆\n大多数大语言模型应用都使用对话方式与用户交互。对话中的一个关键环节是能够引用和参\n考之前对话中的信息。对于对话系统来说，最基础的要求是能够直接访问一些过去的消息。在更\n复杂的系统中还需要一个能够不断更新的事件模型，其能够维护有关实体及其关系的信息。在\nLangChain 中，这种能存储过去交互信息的能力被称为“记忆”。 LangChain 中提供了许多用于\n向系统添加记忆的方法，可以单独使用，也可以无缝整合到链中使用。\nLangChain 记忆模块的基本框架如图 8.10 所示。记忆系统需要支持两个基本操作：读取和写\n入。 每个链都根据输入定义了核心执行逻辑， 其中一些输入直接来自用户， 但有些输入可以来源于\n记忆。在接收到初始用户输入，但执行核心逻辑之前，链将从记忆系统中读取内容并增强用户输",
        "304 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n入。在核心逻辑执行完毕并返回答复之前，链会将这一轮的输入和输出都保存到记忆系统中，以\n便在将来使用它们。\n图 8.10 LangChain 记忆模块的基本框架\nLangChain 中提供了多种对记忆方式的支持，ConversationBufferMemory 是记忆中一种非常简\n单的形式，它将聊天消息列表保存到缓冲区中，并将其传递到提示模板中，代码示例如下：\nfrom langchain.memory import ConversationBufferMemory\nmemory = ConversationBufferMemory()\nmemory.chat_memory.add_user_message(\"hi!\")\nmemory.chat_memory.add_ai_message(\"whats up?\" )\n这种记忆系统非常简单，因为它只记住了先前的对话，并没有建立更高级的事件模型，也没有在\n多个对话之间共享信息，其可用于简单的对话系统，例如问答系统或聊天机器人。对于更复杂的\n对话系统，需要更高级的记忆系统来支持更...",
        "接口；链（Chain） ， 用于复杂应用的调用序列；记忆（Memory） ， 用于在链的多次运行之间持久化\n应用程序状态；智能体（Agent） ，语言模型作为推理器决定要执行的动作序列；回调（Callback） ，\n用于记录和流式传输任何链式组装的中间步骤。下文中的介绍和代码基于 LangChain V0.0.248 版\n本（2023 年 7 月 31 日发布） 。\n2. 模型输入/输出\nLangChain 中的模型输入/输出模块是与各种大语言模型进行交互的基本组件，是大语言模型\n应用的核心元素。 该模块的基本流程如图8.8 所示， 主要包含以下部分：Prompts、Language Models\n及 Output Parsers。将用户的原始输入与模型和示例进行组合输入大语言模型，再根据大语言模型\n的返回结果进行输出或者结构化处理。",
        "308 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\nLangChain 在 langchain/callbacks 模块中提供了一些内置的处理程序，其中最基本的处理程序\n是 StdOutCallbackHandler，它将所有事件记录到 stdout 中，代码示例如下：\nfrom langchain.callbacks import StdOutCallbackHandler\nfrom langchain.chains import LLMChain\nfrom langchain.llms import OpenAI\nfrom langchain.prompts import PromptTemplate\nhandler = StdOutCallbackHandler()\nllm = OpenAI()\nprompt = PromptTemplate.from_template(\"1 + {number} = \" )\n#ט߭\n#৽ൈཁൔഡᇂStdOutCallbackHandler\nchain = LLMChain(llm=llm, prompt =prompt, call...",
        "大语言模型：从理论到实践 269\n8.2.3 记忆模块\n记忆模块是在大模型智能体中承担着管理与操作智能体记忆的核心功能，包括对长短期记忆\n的存储、读取、处理以及反思等任务。该模块不仅负责存储历史数据与经验，还能够高效提取和\n更新信息，从而实现长期记忆与短期记忆之间的有机交互。通过记忆模块的支持，智能体在处理\n连续性任务时能够保持上下文的连贯性，并基于以往经验做出更加准确的判断与决策。\n1. 记忆模型\n大模型智能体所采用的记忆模型包括长期记忆和短期记忆两部分。它们各自有不同的功能和\n实现方式，但都依赖于大模型的强大计算和理解能力。\n短期记忆通常通过将记忆内容以提示语句的形式嵌入大模型输入的上下文中，借助大模型的\n上下文理解能力来实现。 包括存储和使用两个部分： （1） 存储： 在任务执行过程中， 关键的上下文\n信息与事件会被实时记录，形成短期记忆内容； （2）使用：在后续任务中，这些记忆内容会作为\n提示语句输入至大模型的上下文中，帮助模型基于提示进行推理与决策。例如，将前几步的操作\n结果及重要的环境信息作为输入内容，支持模型在接下来的步骤中做出更加合理的判断与选择。\n长期记忆通过构建记忆..."
      ],
      "response_time": 3.906799077987671,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.11991635715604551,
        "semantic_similarity": 0.8153607845306396
      }
    },
    {
      "sample_id": "0131",
      "question": "Jimmy has how much money?",
      "ground_truth": "$18",
      "predicted_answer": "Jimmy 有 $18。根据文档中的计算，Ethel 有 $8，而 Jimmy 拥有的钱是 Ethel 的两倍再多 $2，即 \\(8 \\times 2 + 2 = 18\\) 美元 [1][2]。",
      "ground_context": [
        "Jimmy has $16 + $2 = $ boxed{18}$."
      ],
      "retrieved_contexts": [
        "boxed{18}$.\n#੽ൻԛ\nBased on the solutions provided by other agents, I also arrive at the answer:\nJimmy has twice the money Ethel has, which is $8*2=$16,\nand he also has $2 more than that, which is $16+$2=$\n boxed{18}$.\nYes, based on the information provided and the solutions given by other agents, Jimmy has $18.\nAnswer: $\n boxed{18}$.\nGiven that Ethel has $8 and Jimmy has $2 more than twice Ethel's money,\nwe can calculate Jimmy's money as follows.\nTwice Ethel's money is $8\n times 2 = $16.\nAdding $...",
        "大语言模型：从理论到实践 287\n#၂੽ൻԛ\nWe know that Jimmy has $2 more than twice the money Ethel has.\nTwice the money Ethel has is $8 x 2 = $16.\nTwo more than $16 is $16 + $2 = $18.\nTherefore, Jimmy has $18.\nAnswer: $\n boxed{18}$.\nWe know that Jimmy has $2 more than twice the money Ethel has.\nTwice the money Ethel has is $8*2=<<8*2=16>>16.\nAdding $2 to this, we get that Jimmy has $16+$2=$\n boxed{18}$.\nTwice the money Ethel has is $8\n cdot 2=16$.\nJimmy has $2$ more than that, so his total is $16+2=\n boxed{18}$.\n...",
        "times 2 = $16.\nAdding $2 to this, we get that Jimmy has $16 + $2 = $\n boxed{18}$.\n2. 角色扮演\n角色扮演（Role-Playing）是指在事先设计的情景中自然地扮演某个角色。通过构造特定的提\n示，大语言模型有能力扮演不同的角色——无论是一个五年级的小学生，还是一个计算机领域的\n专家。令人意想不到的是，扮演特定角色的大语言模型能够激发其内部独特的领域知识，产生比\n没有指定角色时更好的答案。角色扮演在赋予智能体个体优势和专业技能的同时，更在多个智能\n体的协作交流中体现出了极大的价值，大大提高了多智能体系统的问题解决效率。\nCAMEL 是角色扮演的经典应用实例，该框架实现了两个智能体的交互，其中一个智能体作\n为用户，另一个智能体作为助手。此外， CAMEL 中还允许用户自由选择是否需要设置任务明确\n智能体与评论智能体，任务明确智能体专门负责将人类给出的初始任务提示细致化，评论智能体\n则负责评价交互的内容， 一方面引导交互向正确的方向进行， 另一方面判定任务目标是否已达成。\nCAMEL 中定义了一个 RoleP...",
        "286 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\nagents = 3 #ඔ\nrounds = 2 #ս੽Ցഈཋ\nquestion = \"Jimmy has $2 more than twice the money Ethel has.\nIf Ethal has $8, how much money is Jimmy having?\" #ิԛ໙ี\nagent_contexts = [[{\"role\": \"user\", \"content\": \"\"\"Can you solve the following math\nproblem? {} Explain your reasoning.\nYour final answer should be a single\nnumerical number, in the form\nboxed{{answer}}, at the end of your\nresponse.\"\"\".format(question)}]\nfor agent in range(agents)] #ᄯൻೆิൕ\nfor round in range(rounds): ...",
        "为例，最简单的交互实现可大致分为以下步骤。\n（1）对于每个任务，用户首先描述任务的基本需求：\nquestion = \"Jimmy has $2 more than twice the money Ethel has.\nIf Ethal has $8, how much money is Jimmy having?\" #ิԛ໙ี\nagent_contexts = [[{\"role\": \"user\", \"content\": \"\"\"Can you solve the following math\nproblem? {} Explain your reasoning.\nYour final answer should be a single\nnumerical number, in the form\nboxed{{answer}}, at the end of your\nresponse.\"\"\".format(question)}]\nfor agent in range(agents)] #ᄯൻೆิൕ\n（2）每个智能体按一定顺序依次发言：\nfor i, agent_context in enu..."
      ],
      "response_time": 4.435457229614258,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.13333333333333333,
        "rouge2": 0.0,
        "rougeL": 0.13333333333333333,
        "bleu": 0.012670762492928651,
        "semantic_similarity": 0.6454181671142578
      }
    },
    {
      "sample_id": "0132",
      "question": "LangChain 中的链有哪些类型？",
      "ground_truth": "除了上例中的 LLMChain，LangChain 中的链还包含 RouterChain、SimpleSequentialChain、SequentialChain、TransformChain 等。",
      "predicted_answer": "LangChain 中的链包括 LLMChain、RouterChain、SimpleSequentialChain、SequentialChain 和 TransformChain 等类型[1]。其中，LLMChain 是一种基本的链类型，而 RouterChain 可以根据输入数据的属性选择调用不同的子链。SimpleSequentialChain 表示一系列简单的顺序链，每个步骤只有一个输入和输出。SequentialChain 是更一般的连续链形式，允许有多个输入和输出。TransformChain 则可以在输入输出之间加入自定义转换函数[1][2][3]。",
      "ground_context": [
        "除了上例中的 LLMChain，LangChain 中的链还包含 RouterChain、SimpleSequentialChain、SequentialChain、TransformChain 等。"
      ],
      "retrieved_contexts": [
        "chain = LLMChain(llm=chat, prompt =chat_prompt_template)\nprint(chain.run(\"colorful socks\" ))\n除了上例中的 LLMChain，LangChain 中的链还包含 RouterChain、SimpleSequentialChain、Se-\nquentialChain、TransformChain 等。RouterChain 可以根据输入数据的某些属性/特征值，选择调用\n哪个子链（Subchain） 。SimpleSequentialChain 是最简单的序列链形式，其中的每个步骤具有单一\n的输入/输出， 上一个步骤的输出是下一个步骤的输入。SequentialChain 是连续链的更一般的形式，\n允许多个输入/输出。TransformChain 可以引入自定义转换函数，对输入进行处理后再输出。以下\n是使用 SimpleSequentialChain 的代码示例：",
        "db = FAISS.from_documents(texts, embeddings)\nretriever = db.as_retriever()\ndocs = retriever.get_relevant_documents(\"what did he say about ketanji brown jackson\" )\n4. 链\n虽然独立使用大语言模型能够应对一些简单任务，但对于更加复杂的需求，可能需要将多个\n大语言模型进行链式组合，或与其他组件进行链式调用。 LangChain 为这种“链式”应用提供了\nChain 接口， 并将该接口定义得非常通用。 作为一个调用组件的序列， 其中还可以包含其他链。 基\n本接口实现非常简单，代码示例如下：\nclass Chain(BaseModel, ABC):\n\"\"\"१\"\"\"\nmemory: BaseMemory\ncallbacks: Callbacks\ndef __call__(\nself,\ninputs: Any,\nreturn_only_outputs: bool = False,\ncallbacks: Callbacks = None,...",
        "间进行接口处理。大语言模型和组件通过“链（Chain） ”连接，使得开发人员可以快速开发原型\n系统和应用程序。LangChain 的主要价值体现在以下几个方面。\n（1） 组件化：LangChain 框架提供了用于处理大语言模型的抽象组件， 以及每个抽象组件的一\n系列实现。这些组件具有模块化设计，易于使用，无论是否使用 LangChain 框架的其他部分，都\n可以方便地使用这些组件。\n（2） 现成的链式组装：LangChain 框架提供了一些现成的链式组装， 用于完成特定的高级任务。\n这些现成的链式组装使得入门变得更加容易。对于更复杂的应用程序， LangChain 框架也支持自\n定义现有链式组装或构建新的链式组装。\n（3） 简化开发难度： 通过提供组件化和现成的链式组装，LangChain 框架可以大大简化大语言\n模型应用的开发难度。开发人员可以更专注于业务逻辑，而无须花费大量时间和精力处理底层技\n术细节。\nLangChain 提供了以下 6 种标准化、 可扩展的接口， 并且可以外部集成：模型输入/输出（Model\nI/O） ， 与大语言模型交互的接口；数据连接（Data Connec...",
        "overall_chain = SimpleSequentialChain(chains=[synopsis_chain, review_chain], verbose =True)\n5. 记忆\n大多数大语言模型应用都使用对话方式与用户交互。对话中的一个关键环节是能够引用和参\n考之前对话中的信息。对于对话系统来说，最基础的要求是能够直接访问一些过去的消息。在更\n复杂的系统中还需要一个能够不断更新的事件模型，其能够维护有关实体及其关系的信息。在\nLangChain 中，这种能存储过去交互信息的能力被称为“记忆”。 LangChain 中提供了许多用于\n向系统添加记忆的方法，可以单独使用，也可以无缝整合到链中使用。\nLangChain 记忆模块的基本框架如图 8.10 所示。记忆系统需要支持两个基本操作：读取和写\n入。 每个链都根据输入定义了核心执行逻辑， 其中一些输入直接来自用户， 但有些输入可以来源于\n记忆。在接收到初始用户输入，但执行核心逻辑之前，链将从记忆系统中读取内容并增强用户输",
        "接口；链（Chain） ， 用于复杂应用的调用序列；记忆（Memory） ， 用于在链的多次运行之间持久化\n应用程序状态；智能体（Agent） ，语言模型作为推理器决定要执行的动作序列；回调（Callback） ，\n用于记录和流式传输任何链式组装的中间步骤。下文中的介绍和代码基于 LangChain V0.0.248 版\n本（2023 年 7 月 31 日发布） 。\n2. 模型输入/输出\nLangChain 中的模型输入/输出模块是与各种大语言模型进行交互的基本组件，是大语言模型\n应用的核心元素。 该模块的基本流程如图8.8 所示， 主要包含以下部分：Prompts、Language Models\n及 Output Parsers。将用户的原始输入与模型和示例进行组合输入大语言模型，再根据大语言模型\n的返回结果进行输出或者结构化处理。"
      ],
      "response_time": 4.956136226654053,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.5714285714285715,
        "rouge2": 0.3157894736842105,
        "rougeL": 0.47619047619047616,
        "bleu": 0.06931890609950131,
        "semantic_similarity": 0.852009654045105
      }
    },
    {
      "sample_id": "0133",
      "question": "Jimmy has how much money?",
      "ground_truth": "$18",
      "predicted_answer": "Jimmy 有 $18。因为 Ethel 有 $8，而 Jimmy 拥有的钱是 Ethel 的两倍再多 $2，即 $8 \\times 2 + $2 = $18。[1][2]",
      "ground_context": [
        "Yes, based on the information provided and the solutions given by other agents, Jimmy has $18."
      ],
      "retrieved_contexts": [
        "boxed{18}$.\n#੽ൻԛ\nBased on the solutions provided by other agents, I also arrive at the answer:\nJimmy has twice the money Ethel has, which is $8*2=$16,\nand he also has $2 more than that, which is $16+$2=$\n boxed{18}$.\nYes, based on the information provided and the solutions given by other agents, Jimmy has $18.\nAnswer: $\n boxed{18}$.\nGiven that Ethel has $8 and Jimmy has $2 more than twice Ethel's money,\nwe can calculate Jimmy's money as follows.\nTwice Ethel's money is $8\n times 2 = $16.\nAdding $...",
        "大语言模型：从理论到实践 287\n#၂੽ൻԛ\nWe know that Jimmy has $2 more than twice the money Ethel has.\nTwice the money Ethel has is $8 x 2 = $16.\nTwo more than $16 is $16 + $2 = $18.\nTherefore, Jimmy has $18.\nAnswer: $\n boxed{18}$.\nWe know that Jimmy has $2 more than twice the money Ethel has.\nTwice the money Ethel has is $8*2=<<8*2=16>>16.\nAdding $2 to this, we get that Jimmy has $16+$2=$\n boxed{18}$.\nTwice the money Ethel has is $8\n cdot 2=16$.\nJimmy has $2$ more than that, so his total is $16+2=\n boxed{18}$.\n...",
        "times 2 = $16.\nAdding $2 to this, we get that Jimmy has $16 + $2 = $\n boxed{18}$.\n2. 角色扮演\n角色扮演（Role-Playing）是指在事先设计的情景中自然地扮演某个角色。通过构造特定的提\n示，大语言模型有能力扮演不同的角色——无论是一个五年级的小学生，还是一个计算机领域的\n专家。令人意想不到的是，扮演特定角色的大语言模型能够激发其内部独特的领域知识，产生比\n没有指定角色时更好的答案。角色扮演在赋予智能体个体优势和专业技能的同时，更在多个智能\n体的协作交流中体现出了极大的价值，大大提高了多智能体系统的问题解决效率。\nCAMEL 是角色扮演的经典应用实例，该框架实现了两个智能体的交互，其中一个智能体作\n为用户，另一个智能体作为助手。此外， CAMEL 中还允许用户自由选择是否需要设置任务明确\n智能体与评论智能体，任务明确智能体专门负责将人类给出的初始任务提示细致化，评论智能体\n则负责评价交互的内容， 一方面引导交互向正确的方向进行， 另一方面判定任务目标是否已达成。\nCAMEL 中定义了一个 RoleP...",
        "286 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\nagents = 3 #ඔ\nrounds = 2 #ս੽Ցഈཋ\nquestion = \"Jimmy has $2 more than twice the money Ethel has.\nIf Ethal has $8, how much money is Jimmy having?\" #ิԛ໙ี\nagent_contexts = [[{\"role\": \"user\", \"content\": \"\"\"Can you solve the following math\nproblem? {} Explain your reasoning.\nYour final answer should be a single\nnumerical number, in the form\nboxed{{answer}}, at the end of your\nresponse.\"\"\".format(question)}]\nfor agent in range(agents)] #ᄯൻೆิൕ\nfor round in range(rounds): ...",
        "为例，最简单的交互实现可大致分为以下步骤。\n（1）对于每个任务，用户首先描述任务的基本需求：\nquestion = \"Jimmy has $2 more than twice the money Ethel has.\nIf Ethal has $8, how much money is Jimmy having?\" #ิԛ໙ี\nagent_contexts = [[{\"role\": \"user\", \"content\": \"\"\"Can you solve the following math\nproblem? {} Explain your reasoning.\nYour final answer should be a single\nnumerical number, in the form\nboxed{{answer}}, at the end of your\nresponse.\"\"\".format(question)}]\nfor agent in range(agents)] #ᄯൻೆิൕ\n（2）每个智能体按一定顺序依次发言：\nfor i, agent_context in enu..."
      ],
      "response_time": 4.375841379165649,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.13333333333333333,
        "rouge2": 0.0,
        "rougeL": 0.13333333333333333,
        "bleu": 0.01447931032215957,
        "semantic_similarity": 0.6306257843971252
      }
    },
    {
      "sample_id": "0134",
      "question": "What is the function of a retriever?",
      "ground_truth": "检索器是一个接口，其功能是基于非结构化查询返回相应的文档。",
      "predicted_answer": "Retrievers 的功能是基于非结构化查询返回相应的文档[1]。它们不需要存储文档，而是根据查询要求返回结果。在 LangChain 中，BaseRetriever 类定义了这一接口[1]。",
      "ground_context": [
        "Retrievers（检索器） 是一个接口， 其功能是基于非结构化查询返回相应的文档。"
      ],
      "retrieved_contexts": [
        "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap =0)\ndocuments = text_splitter.split_documents(raw_documents)\ndb = FAISS.from_documents(documents, OpenAIEmbeddings())\n#ྛཌྷරྟෆ෬\nquery = \"What did the president say about Ketanji Brown Jackson\"\ndocs = db.similarity_search(query)\nprint(docs[0].page_content)\nRetrievers（检索器） 是一个接口， 其功能是基于非结构化查询返回相应的文档。 检索器不需要\n存储文档，只需要能根据查询要求返回结果即可。检索器可以使用向量存储的方式执行操作，也\n可以使用其他方式执行操作。LangChain 中的 BaseRetriever 类定义如下："
      ],
      "response_time": 3.358643054962158,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.21176535098798327,
        "semantic_similarity": 0.801341712474823
      }
    },
    {
      "sample_id": "0135",
      "question": "类公开了多少个方法？",
      "ground_truth": "两个",
      "predicted_answer": "类公开了两个方法：一个是用于文档嵌入表示的方法，另一个是用于查询嵌入表示的方法。[1]",
      "ground_context": [
        "类公开了两个方法：一个用于文档嵌入表示，另一个用于查询嵌入表示。"
      ],
      "retrieved_contexts": [
        "类公开了两个方法：一个用于文档嵌入表示，另一个用于查询嵌入表示。前者输入多个文本，后\n者输入单个文本。之所以将它们作为两个单独的方法，是因为某些嵌入模型为文档和查询采用了\n不同的嵌入策略。以下是使用 OpenAI 的 API 接口完成文本嵌入的代码示例：\nfrom langchain.embeddings import OpenAIEmbeddings\nembeddings_model = OpenAIEmbeddings(openai_api_key=\"...\")\nembeddings = embeddings_model.embed_documents(\n[\n\"Hi there!\" ,\n\"Oh, hello!\" ,\n\"What's your name?\" ,\n\"My friends call me World\" ,\n\"Hello World!\"\n]\n)\nlen(embeddings), len(embeddings[0])\nembedded_query = embeddings_model.embed_query(\"What was the name mentioned in th..."
      ],
      "response_time": 3.471874475479126,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.009849574084214482,
        "semantic_similarity": 0.5418955087661743
      }
    },
    {
      "sample_id": "0136",
      "question": "如何在使用 ChatOpenAI 进行对话时获取 assistant 的回复？",
      "ground_truth": "通过执行代码 chat_llm_chain.predict(human_input='Hi there my friend') 可以得到 assistant 的回复，然后可以继续通过 chat_llm_chain.predict(human_input='Not too bad - how are you?') 来获取进一步的对话。",
      "predicted_answer": "在使用 `ChatOpenAI` 进行对话时，可以通过 `predict` 方法来获取 `assistant` 的回复。具体来说，可以参照以下代码片段：\n\n```python\nchat_llm_chain = LLMChain(\n    llm=llm,\n    prompt=prompt,\n    verbose=True,\n    memory=memory,\n)\nresponse = chat_llm_chain.predict(human_input=\"Hi there my friend\")\n```\n\n这里的 `response` 就是 `assistant` 的回复内容。[2][5]",
      "ground_context": [
        "chat_llm_chain.predict(human_input='Hi there my friend') 可以得到如下输出结果：'Hello! How can I assist you today, my friend?'",
        "content = completion['choices'][0]['message']['content']"
      ],
      "retrieved_contexts": [
        "大语言模型：从理论到实践 289\n基于 RolePlaying 的结构，可以直接调用 step() 函数完成角色扮演的交互过程，获取两个智能\n体的新一轮输出：\nassistant_response, user_response = role_play_session.step(input_assistant_msg)\n角色扮演的完整对话实现如下：\nwhile n < chat_turn_limit: #ս੽Ցཋᇅ\n#ྍ၂੽ൻԛ\nassistant_response, user_response = role_play_session.step(input_assistant_msg)\n#߅ؓ\nif assistant_response.terminated:\nprint(\"AI Assistant terminated. \"\nf\"Reason: {assistant_response.info['termination_reasons']}.\")\nbreak\nif user_response.terminated:\nprint(\"AI User terminated. \"\nf\"Reas...",
        "),\n]\nmemory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages =True)\nllm = ChatOpenAI(openai_api_key=OPENAI_API_KEY, temperature =0)\nagent_chain = initialize_agent(\ntools,\nllm,\nagent=AgentType.CHAT_CONVERSATIONAL_REACT_DESCRIPTION,\nverbose=True,\nmemory=memory\n)\n注意， 此处在agent 类型选择时使用了 “CHA T_CONVERSA TIONAL_REACT_DESCRIPTION” ， 模\n型将使用 ReAct 逻辑生成。根据上面定义的智能体，使用如下调用方式：\nagent_chain.run(input=\"what's my name?\" )\n给出如下回复：",
        "大语言模型：从理论到实践 293\n全模型， 接收字符串提示词作为输入， 并输出字符串。OpenAI 的 GPT-3 是 LLM 实现的一个实例。\nChat Model 专为会话交互设计，与传统的纯文本补全模型相比，这一模型的 API 采用了不同的接\n口方式：它需要一个标有说话者身份的聊天消息列表作为输入，如“系统” 、 “AI”或“人类” 。作\n为输出，Chat Model 会返回一个标为“AI”的聊天消息。GPT-4 和 Anthropic 的 Claude 都可以通\n过 Chat Model 调用。以下是利用 LangChain 调用 OpenAI API 的代码示例：\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.schema import (AIMessage, HumanMessage, SystemMessage)\nchat = ChatOpenAI(\nopenai_api_key=\"...\",\ntemperature=0,\nmodel='gpt-3.5-turbo'\n)\nmessages = [\nS...",
        "completion = openai.ChatCompletion.create( #࿽\nmodel=\"gpt-3.5-turbo-0301\", # ࿊ᄴଆ྘\nmessages=agent_context, #ൻೆ\nn=1)\ncontent = completion[\"choices\"][0][\"message\"][\"content\"] #໓Чଽಸ\nassistant_message = {\"role\": \"assistant\", \"content\": content} #೤ູᇆିุ\nagent_context.append(assistant_message) #ᇀਙі",
        "])\nmemory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages =True)\nllm = ChatOpenAI()\nchat_llm_chain = LLMChain(\nllm=llm,\nprompt=prompt,\nverbose=True,\nmemory=memory,\n)\nchat_llm_chain.predict(human_input=\"Hi there my friend\" )\n执行上述代码可以得到如下输出结果：\n> Entering new LLMChain chain...\nPrompt after formatting:\nSystem: You are a chatbot having a conversation with a human.\nHuman: Hi there my friend\n> Finished chain.\n'Hello! How can I assist you today, my friend?'\n在此基础上继续执行如下语句：\nchat_llm_..."
      ],
      "response_time": 4.242672443389893,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 1.0,
        "mrr": 0.2
      },
      "answer_quality_metrics": {
        "rouge1": 0.5555555555555556,
        "rouge2": 0.423076923076923,
        "rougeL": 0.40740740740740744,
        "bleu": 0.19072256742561,
        "semantic_similarity": 0.8289359211921692
      }
    },
    {
      "sample_id": "0137",
      "question": "如何使用向量存储的检索器来获取与用户输入相关的文档，并同时使用大语言模型来处理用户的对话？",
      "ground_truth": "可以通过get_relevant_documents方法或异步调用aget_relevant_documents方法获得与查询文档最相关的文档，同时使用大语言模型结合ChatPromptTemplate来处理用户的对话。",
      "predicted_answer": "首先，使用向量存储（如FAISS）作为检索器，可以将用户输入转化为向量表示，并与存储的文档嵌入向量进行比较，找到最相关的文档[1]。接着，大语言模型可以根据检索得到的相关文档进一步理解和生成回复，提高对话质量[2]。具体步骤包括加载文本数据、提取嵌入向量以及构建向量数据库。当用户提出查询时，系统会将其转化为向量并与数据库中的向量对比，挑选出最相关的文档[1][2]。",
      "ground_context": [
        "它的使用非常简单，可以通过get_relevant_documents方法或通过异步调用aget_relevant_documents方法获得与查询文档最相关的文档。",
        "prompt = ChatPromptTemplate.from_messages([SystemMessage(content=\"You are a chatbot having a conversation with a human.\"), MessagesPlaceholder(variable_name=\"chat_history\"), HumanMessagePromptTemplate.from_template(\"{human_input}\")])"
      ],
      "retrieved_contexts": [
        "大语言模型：从理论到实践 299\nV ector Stores（向量存储）是存储和检索非结构化数据的主要方式之一。它首先将数据转化为\n嵌入表示，然后存储生成的嵌入向量。在查询阶段，系统会利用这些嵌入向量来检索与查询内容\n“最相似”的文档。向量存储的主要任务是保存这些嵌入向量并执行基于向量的搜索。 LangChain\n能够与多种向量数据库集成， 如Chroma、FAISS 和 Lance 等。以下为使用 FAISS 向量数据库的代\n码示例：\nfrom langchain.document_loaders import TextLoader\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom langchain.vectorstores import FAISS\n#ཟਈթԥᇏ\nraw_documents = TextLoader('../../../state_of_the_union.txt').loa...",
        "大语言模型：从理论到实践 317\n点， 首先通过检索模块 （Retriever） 根据查询内容定位并查找相关数据源， 然后筛选出与查询高度\n相关的信息作为检索结果。这些检索结果随后与生成模块（Generator）协作，以增强生成过程的\n质量和效果。\n用户输入各种模态数据\n检索模块生成模块结果\n文本代码图像音频视频知识科学数据……3D\n数据\n编码索引\n稀疏检索稠密检索其他\nTr ansfor merLSTMDiffusionGAN……\n文本\n代码\n图片\n视频\n知识\n科学数据\n3D\n音频\n图 9.1 典型大模型检索增强生成过程 [413]\n检索模块负责从外部知识库或数据源中定位与用户查询相关的信息。检索器通常基于向量检\n索技术或其他高效的检索算法，将输入的自然语言查询转换为向量表示，并与外部数据源中的内\n容进行匹配。外部数据源可以是文档数据库、知识图谱、 API 接口或实时搜索引擎等。检索模块\n不仅需要快速准确地定位相关内容，还需对检索结果进行筛选和排序，以确保返回的内容与用户\n查询具有高度相关性。这一模块的性能直接影响生成器后续处理的质量和效率。\n生成模块则是基于检索器提供的相关内容生成...",
        "大语言模型：从理论到实践 331\n信息。例如，对于“汽车”和“车辆”这样的同义词，稀疏检索器通常无法感知两者的语义相似\n性，从而可能导致检索结果的相关性下降。稀疏向量的低语义表达能力限制了其在语言理解任务\n中的适用性。\n2. 稠密检索\n稠密检索（Dense Retrieval）是一种通过深度学习模型将查询和文档编码为稠密向量（ Dense\nV ectors） 的检索方法。 与稀疏向量不同， 稠密向量的每个维度都可能有值， 从而能够捕捉更丰富的\n语义信息。 这种方法依赖预训练语言模型 （如BERT、RoBERTa） 或特定的双塔模型 （Dual Encoder）\n来生成语义嵌入，使得查询和文档在语义空间中更接近，从而更好地匹配用户意图。在语义搜索、\n问答系统和对复杂查询的处理任务中， 稠密检索表现出了显著的优势。 稠密检索架构如图9.8所示。\n查询词\n查询词向量\n相似度 数据集\n图 9.8 稠密检索架构图\n稠密检索的核心优势在于其强大的语义表达能力。由于深度模型能够理解上下文信息和复杂\n的语义关系，稠密向量不仅可以捕捉显性特征，还能处理同义词、上下文依赖和多层次语义。例\n如，对于“汽车”..."
      ],
      "response_time": 5.847339391708374,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0.0,
        "bleu": 0.03584115632052194,
        "semantic_similarity": 0.7052662372589111
      }
    },
    {
      "sample_id": "0138",
      "question": "如何使用基于向量存储的检索器来检索与查询文档最相关的文档，以及向量存储的主要功能是什么？",
      "ground_truth": "基于向量存储的检索器通过get_relevant_documents方法或异步调用aget_relevant_documents方法获得与查询文档最相关的文档。向量存储的主要功能是将数据转化为嵌入表示并存储生成的嵌入向量，利用这些嵌入向量在查询阶段检索与查询内容最相似的文档。",
      "predicted_answer": "要使用基于向量存储的检索器来检索与查询文档最相关的文档，可以通过`get_relevant_documents`方法或其异步版本`aget_relevant_documents`方法来获取相关文档。向量存储的主要功能包括将非结构化数据转换为嵌入表示，存储生成的嵌入向量，并在查询阶段利用这些向量来检索与查询内容最相似的文档。[1][2]",
      "ground_context": [
        "它的使用非常简单， 可以通过get_relevant_documents 方法或通过异步调用 aget_relevant_documents 方法获得与查询文档最相关的文档。",
        "V ector Stores（向量存储）是存储和检索非结构化数据的主要方式之一。它首先将数据转化为嵌入表示，然后存储生成的嵌入向量。在查询阶段，系统会利用这些嵌入向量来检索与查询内容“最相似”的文档。"
      ],
      "retrieved_contexts": [
        ") -> List[Document]:\n\"\"\"֖\nArgs:\nquery:Ա\ncallbacks:ਙі\nReturns:\nਙі\n\"\"\"\n...\n它的使用非常简单， 可以通过get_relevant_documents 方法或通过异步调用 aget_relevant_ documents\n方法获得与查询文档最相关的文档。 基于向量存储的检索器 （V ector store-backed retriever） 是使用\n向量存储检索文档的检索器。它是向量存储类的轻量级包装器，与检索器接口契合，使用向量存\n储实现的搜索方法 （如相似性搜索和MMR） 来查询使用向量存储的文本。 以下是一个基于向量存\n储的检索器的代码示例：",
        "大语言模型：从理论到实践 299\nV ector Stores（向量存储）是存储和检索非结构化数据的主要方式之一。它首先将数据转化为\n嵌入表示，然后存储生成的嵌入向量。在查询阶段，系统会利用这些嵌入向量来检索与查询内容\n“最相似”的文档。向量存储的主要任务是保存这些嵌入向量并执行基于向量的搜索。 LangChain\n能够与多种向量数据库集成， 如Chroma、FAISS 和 Lance 等。以下为使用 FAISS 向量数据库的代\n码示例：\nfrom langchain.document_loaders import TextLoader\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom langchain.vectorstores import FAISS\n#ཟਈթԥᇏ\nraw_documents = TextLoader('../../../state_of_the_union.txt').loa..."
      ],
      "response_time": 5.159667015075684,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 0.5
      },
      "answer_quality_metrics": {
        "rouge1": 0.8571428571428571,
        "rouge2": 0.8333333333333333,
        "rougeL": 0.8571428571428571,
        "bleu": 0.4291850194292662,
        "semantic_similarity": 0.9613039493560791
      }
    },
    {
      "sample_id": "0139",
      "question": "Coze是一个什么类型的平台？",
      "ground_truth": "Coze（扣子）是一个大模型智能体开发平台。",
      "predicted_answer": "Coze是一个大模型智能体开发平台[1]，它帮助开发者以低门槛的方式快速构建、测试和部署智能体。该平台集成了丰富的功能，如插件、长短期记忆、工作流和卡片等，以便于开发者实现复杂任务的自动化并提供强大的扩展和定制能力。",
      "ground_context": [
        "Coze（扣子）是一个大模型智能体开发平台，整合了插件、长短期记忆、工作流、卡片等丰富功能，能够以低门槛、快速搭建个性化或具备商业价值的智能体，并发布到豆包、飞书、网页等多种平台，实现全场景覆盖。"
      ],
      "retrieved_contexts": [
        "312 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n模板，用户无需编写代码，甚至完全不需要编程能力，即可完成智能体的设计、开发和部署。这\n类平台通常集成了预训练的大语言模型，并提供强大的工具支持，如知识库管理、对话流程设计\n和外部 API 集成，极大地简化了开发流程。需要注意的是，这种方式在定制化能力、性能优化和\n扩展性上存在一定局限，复杂场景或高性能需求场景下适应程度需要详细评估。\nCoze（扣子）是一个大模型智能体开发平台，整合了插件、长短期记忆、工作流、卡片等丰\n富功能，能够以低门槛、快速搭建个性化或具备商业价值的智能体，并发布到豆包、飞书、网页\n等多种平台，实现全场景覆盖。通过模块化与高效的工具支持，Coze 帮助开发者快速构建、测试\n和部署智能体，实现复杂任务的自动化，同时提供强大的扩展和定制能力。其插件系统支持智能\n体与外部工具无缝对接，如数据库查询、第三方 API 调用、任务管理工具等，在多种环境中执行\n精准任务；长短期记忆功能让智能体在短期对话中保持上下文一致，并通过长期记忆存储重要信\n息，实现自然、智能的交互体验；工作流功能允许用户通过拖拽式界面快速设计...",
        "\"action_input\": \"Cloudy with showers. Low around 55F. Winds S at 5 to 10 mph.\nChance of rain 60%. Humidity76%.\"\n}\n> Finished chain.\n'Cloudy with showers. Low around 55F. Winds S at 5 to 10 mph. Chance of rain 60%. Humidity76%.'\n可以看到，模型采用 ReAct 的提示模式生成内容。通过上述两种不同的用户输入及相应的系统回\n复，可以看到智能体自动根据用户输入选择是否使用搜索工具。\n8.4.3 智能体平台 Coze 实践\n使用零代码或低代码平台构建大模型智能体是一种高效便捷的开发方式，适合缺乏编程经验\n的用户或需要快速验证概念（Proof of Concept，PoC）的场景。 通过可视化界面、 拖拽组件和预设",
        "调用插件或执行复杂任务；卡片功能则为智能体提供了信息展示和互动的新形式，让用户在网页\n或移动端直观查看数据、流程和结果。\n使用 Coze 平台可以通过以下简单的五个步骤就可以构造快速搭建一个“夸夸机器人” ，并在\n多个平台提供对外服务。\n步骤 1： 创建一个智能体。 在扣子平台创建智能体非常简单： 登录后， 点击页面左上角的 “L” ，\n输入智能体名称和功能介绍，并通过生成图标自动生成头像，或使用“AI 创建”功能，通过自然\n语言描述需求， 由平台自动生成智能体。 点击确认后， 进入智能体编排页面。 在这里， 可以通过左\n侧人设与回复逻辑面板描述智能体的身份和任务；利用中间技能面板为智能体配置扩展能力；在\n右侧预览与调试面板中实时测试智能体，确保其功能和交互效果符合预期。\n步骤 2：编写提示词。配置智能体的第一步是编写提示词，即定义智能体的人设与回复逻辑。\n这部分内容决定了智能体的基本人设，并持续影响其在所有会话中的回复效果。在设计提示词时，\n建议明确模型的角色、设计特定的语言风格，并限制回答范围，以确保对话内容符合用户的预期。\n例如，对于一个“夸夸机器人” ，提示词可以设置为："
      ],
      "response_time": 4.484152793884277,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.6666666666666666,
        "rouge2": 0.0,
        "rougeL": 0.6666666666666666,
        "bleu": 0.10496883320657069,
        "semantic_similarity": 0.8577353954315186
      }
    },
    {
      "sample_id": "0140",
      "question": "索引模块在 RAG 系统中的核心任务是什么？",
      "ground_truth": "其核心任务是将文档划分为可管理的片段（Chunk），也成为“块”，为后续的检索和生成提供组织良好的内容基础。",
      "predicted_answer": "索引模块在 RAG 系统中的核心任务是将文档划分为可管理的片段（Chunk），也为后续的检索和生成提供组织良好的内容基础。[1]",
      "ground_context": [
        "索引（Index）是 RAG 系统中至关重要的过程，其核心任务是将文档划分为可管理的片段（Chunk），也成为“块”，为后续的检索和生成提供组织良好的内容基础。"
      ],
      "retrieved_contexts": [
        "324 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n其架构分为三个层级： 顶层聚焦于RAG 的关键阶段， 将每个阶段视为独立模块， 同时引入一个编\n排模块来协调 RAG 流程； 中层由每个模块内的子模块组成， 进一步细化和优化各项功能； 底层由\n操作的基本单元（即操作符）构成。在模块化 RAG 框架中，RAG 系统可以通过计算图的形式表\n示，其中节点代表具体的操作符。\n本章将重点介绍 Modular RAG 框架下的各模块，包括：索引、检索前优化、检索、检索后优\n化、生成以及编排。\n9.2.1 索引模块\n索引（Index）是 RAG 系统中至关重要的过程，其核心任务是将文档划分为可管理的 片段\n（Chunk），也成为“块” ，为后续的检索和生成提供组织良好的内容基础。片段切分是将文档拆分\n为更小的、可管理的、语义完整的信息单元的过程，其构建需要综合考虑内容的语义特性、上下\n文完整性以及检索和生成的实际需求。在构建片段时， 首先需要确定片段的大小（长度） 。 片段的\n大小通常用字符数、单词数或句子数来衡量，具体取决于任务要求和模型的能力。\n较大的片段在构建时能够捕获更多上下...",
        "容缺乏事实依据的问题。然而，其复杂的“检索-生成”架构使得评估变得尤为重要，不仅需要考\n察检索和生成组件的独立性能，还需关注两者的协同作用，以及系统在动态知识更新中的适应性\n与泛化能力。此外，RAG 系统在不同任务场景中的表现差异、多样化的应用需求以及对用户体验\n的影响（如响应速度、生成内容的准确性和可读性）都凸显了构建全面评估框架的必要性，以及\nRAG 评估所面临的挑战。\n检索模块是 RAG 系统的核心部分，负责从庞大的外部知识库中提取与用户查询相关的信息。\n检索模块的评估面临多重挑战。首先，知识库的广度与动态性使得评估复杂化。RAG 系统通常依\n赖多样化的知识来源，包括结构化数据库、维基百科页面甚至整个互联网，这些知识库的内容会\n随着时间和领域的变化而更新。因此，需要设计评估指标，以衡量系统在不同时间点和知识领域\n中检索有效性的稳定性和准确性。其次，检索内容的质量直接关系到生成组件的表现。除了评估\n相关性外，还需考察检索结果的准确性和可靠性。低质量或误导性的检索内容可能对生成结果产\n生负面影响，因此对检索内容的筛选和质量控制至关重要。此外，现有的检索指标（如精确率和\n召回率） 无法...",
        "等） ， 评估数据集能够更全面地展现系统在不同领域中的适应性和泛化能力。 这种多样化设计确保\n了 RAG 系统在广泛应用场景中的可靠性和实用性。\n9.5.4 评估指标\n评估指标是衡量 RAG 系统性能的核心工具，直接影响评估结果的可信度和系统优化的方向。\n在评估 RAG 系统时，需要对各种评估指标有深入的理解，以便准确衡量评估目标。由于 RAG 系\n统由检索组件和生成组件构成，以下从检索模块、生成模块和整体系统三个层面展开详细讨论。",
        "在不同任务场景中的表现。根据 RAG 系统的“检索-生成”结构，评估目标可分为针对检索的评\n估、针对生成的评估，以及面向整体系统的协同能力评估。\n1. 检索模块的评估目标\n检索模块是 RAG 系统的基础， 其主要任务是从知识库中提取与用户查询相关的信息， 为生成\n模块提供支持。在评估检索组件时，需要明确以下几个关键目标：\n（1） 相关性 （Relevance） ：检索组件的首要目标是确保其返回的文档与用户查询高度相关。 相\n关性评估旨在衡量检索出的文档是否能够准确反映用户查询所需要的信息。例如，在问答任务中，\n检索出的文档是否包含回答问题所需的事实或背景知识。相关性通常通过计算检索结果与查询之\n间的匹配程度来评估，可以使用指标如精确率（Precision）和召回率（Recall） 。\n（2）准确性（Accuracy） ：除了相关性，检索结果的准确性也至关重要。准确性评估需要考察\n检索到的文档是否在信息上是可靠的，是否包含错误、误导性内容或低质量信息。检索组件返回\n的错误信息可能直接导致生成组件生成不真实的回答，因此检索结果的准确性对整体系统的性能\n至关重要。\n（3）覆盖率与多样性（Co...",
        "330 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n查询结构化不仅仅是将自然语言转换为结构化查询语言，还需要结合语义信息和元数据，以\n构建更复杂和准确的查询。 通过将用户意图与数据结构相结合， 系统能够生成更强大的查询语句。\n例如，Text-to-SQL 技术能够将自然语言问题转换为 SQL 语句， 从关系型数据库中提取答案；Text-\nto-Cypher 则用于处理图数据查询，基于图结构返回更精确的结果。这种方式使 RAG 系统能够在\n融合多种数据类型的同时，确保查询的精准性和多样性，从而提供更全面的答案和更优质的用户\n体验。\n9.2.3 检索\n检索模块在 RAG 系统中扮演着至关重要的角色。在 RAG 系统中，检索模块需要能够高效地\n处理大量的文本数据，并且需要能够准确地识别和匹配查询和文档之间的语义相似性。因此，检\n索模型的选择和优化对于 RAG 系统的性能至关重要，因为它们直接影响到检索的准确性和效率。\n检索模型还需要能够适应不同的数据类型和查询类型，以确保在各种场景下都能够提供准确的检\n索结果。目前的检索主要分为：稀疏检索、稠密检索和混合检索。本节将分别介绍上述..."
      ],
      "response_time": 3.5731921195983887,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.5,
        "rouge2": 0.0,
        "rougeL": 0.5,
        "bleu": 0.6910656309607446,
        "semantic_similarity": 0.797759473323822
      }
    },
    {
      "sample_id": "0141",
      "question": "OpenAI 于哪一年推出了 SearchGPT？",
      "ground_truth": "2024",
      "predicted_answer": "OpenAI 于 2024 年推出了 SearchGPT[1]。",
      "ground_context": [
        "OpenAI 于 2024 年推出了 SearchGPT，进一步推动了 AI 搜索技术的发展"
      ],
      "retrieved_contexts": [
        "歌则推出 Bard， 将实时检索与生成能力结合， 为用户提供更加全面的答案；国内的Kimi、 秘塔等\n产品也在这一领域崭露头角，成为 AI 搜索技术的重要实践者。此外， OpenAI 于 2024 年推出了\nSearchGPT，进一步推动了 AI 搜索技术的发展，该产品通过深度整合大语言模型与动态知识检索\n功能，展现了强大信息处理效率。国内的豆包、千问、智谱、百川等大模型系统也相继融入了AI\n搜索功能。\n本节将重点介绍 RAG 系统框架、RAG 任务分级以及 RAG 系统的难点。\n9.1.1 RAG 系统框架\n典型的检索增强生成过程如图9.1所示，其核心在于将外部检索与生成模块有机整合，通过动\n态引入外部知识来提升生成结果的准确性与可靠性。 具体而言，RAG 过程以用户输入的查询为起",
        "得分高于 88% 的应试者，包括美国律师资格考试（ Uniform Bar Exam ） 、法学院入学考试（Law\nSchool Admission Test） 、学术能力评估（Scholastic Assessment Test，SA T）等。GPT-4o 是 OpenAI\n于 2024 年 5 月发布的多模态大模型，其中“o”代表“omni”即“全能” 。它能接受文本、音频和\n图像组合输入并生成文本、 音频和图像的任意组合输出， 可处理50 种语言， 在232 毫秒内对音频",
        "GPT-o3 2024 年 12 月 OpenAI - 推理模型\n豆包 1.5Pro 2025 年 1 月 字节跳动 - 对话模型\nGrok-3 2025 年 2 月 XAI - 对话推理模\n型\n1 模型参数量根据微软公司发表的文献 [38] 获取，数字并未得到 OpenAI 官方证实",
        "大规模语言模型：从理论到实践 5\nGPT-1[2] 模型， 开启了预训练语言模型时代。BERT-Base版本的参数量为 1.1 亿个，BERT-Large版\n本的参数量为 3.4 亿个，GPT-1 的参数量为 1.17 亿个。这在当时，比其他深度神经网络的参数量，\n已经有了数量级上的提升。2019 年 OpenAI 发布了 GPT-2[11]， 其参数量达到15 亿个。 此后，Google\n也发布了参数规模为 110 亿个的 T5[19] 模型。2020 年，OpenAI 进一步将语言模型的参数量扩展\n到 1750 亿个， 发布了GPT-3[13]。 此后， 国内也相继推出了一系列的大语言模型， 包括清华大学的\nERNIE[20]、 百度的ERNIE[21]、 华为的PanGU-α[22] 等。 此阶段的研究主要集中在语言模型本身， 对\n仅编码器（Encoder Only） 、编码器-解码器（Encoder-Decoder） 、仅解码器（Decoder Only）等各种\n类型的模型结构都有相应的研究。模型大小与 BERT 类似，通常采用预训练微调范式，针对不同\n下游任务进行微调。 这些...",
        "arXiv:2502.07864, 2025.\n[65] OpenAI. Gpt-4 technical report[J]. arXiv preprint arXiv:2303.08774, 2023."
      ],
      "response_time": 2.497783660888672,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.4,
        "rouge2": 0.0,
        "rougeL": 0.4,
        "bleu": 0.021598265052211615,
        "semantic_similarity": 0.4629324972629547
      }
    },
    {
      "sample_id": "0142",
      "question": "大语言模型在本科低年级知识点记忆能力测试中的表现是多少？",
      "ground_truth": "73.6%",
      "predicted_answer": "GPT-4 Turbo 在本科低年级知识点记忆能力测试中的表现为 73.6%[1]。",
      "ground_context": [
        "根据 LLMEV AL-3[411] 评测结果，GPT-4 Turbo 在本科低年级知识点记忆能力测试中的表现仅为 73.6%。"
      ],
      "retrieved_contexts": [
        "能得到了快速提升。这些模型在自然语言处理、推理和生成任务中展现出了前所未有的表现。然\n而，尽管如此，大语言模型对知识的记忆能力仍然受到其模型架构和训练范式的限制。根据文献\n[410] 的研究，模型在预训练数据中需要对同一知识点进行多达 1000 次的曝光，才能较为准确地\n记忆该知识点。根据 LLMEV AL-3[411] 评测结果，GPT-4 Turbo 在本科低年级知识点记忆能力测试\n中的表现仅为 73.6%。 这表明， 即便是参数量巨大的模型， 其知识记忆效率依然较低， 且难以完全\n覆盖所有领域的知识点。\n此外，大语言模型的性能很大程度上依赖于训练期间所接触到的静态数据。这种依赖性导致\n模型在面对实时更新的信息、长尾知识（即训练数据中罕见或未出现的知识）以及动态变化的内\n容时，往往表现出明显的局限性。例如，当模型需要处理最新的科技进展、时事新闻或特定领域\n的专业知识时，其生成结果可能出现错误、不完整甚至虚构的现象。这种现象被称为大语言模型\n的“幻觉”问题，是当前大语言模型研究领域的一大挑战。",
        "82 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n被重复多次，并使用这个数据集训练了一组模型。研究发现了一个强烈的双峰下降现象，即重复\n数据可能会导致训练损失在中间阶段增加。 例如， 通过将0.1% 的数据重复 100 次， 即使其余90%\n的训练数据保持不变，一个参数量为 800M 的模型的性能也可能降低到与参数量为 400M 的模型\n相同。此外，研究人员还设计了一个简单的复制评估，即将《哈利·波特》 （Harry Potter）的文字\n复制 11 次，计算模型在该段上的损失。在仅有 3% 的重复数据的情况下，训练过程中性能最差的\n轮次仅能达到参数量为其 1/3 的模型的效果。\n文献 [14] 对大语言模型的记忆能力进行分析，根据训练样例在训练数据中出现的次数，显示\n了记忆率的变化情况， 如图3.10 所示。 可以看到， 对于在训练中只见过一次的样例，PaLM 模型的\n记忆率为 0.75%，而其对见过 500 次以上的样例的记忆率超过 40%。这也在一定程度上说明重复\n数据对于语言模型建模具有重要影响。这也可能进一步影响使用上下文学习的大语言模型的泛化\n能力。 由于Pa...",
        "能力。 由于PaLM 模型仅使用了文档级别过滤， 因此片段级别 （100 个以上词元） 可能出现非常高\n的重复次数。\n图 3.10 大语言模型记忆能力评测 [14]\n3.3.3 数据多样性\n来自不同领域、使用不同语言、应用于不同场景的训练数据具有不同的语言特征，包含不同\n语义知识。通过使用不同来源的数据进行训练，大语言模型可以获得广泛的知识。表 3.2 给出了\nLLaMA 模型训练所使用的数据集。可以看到，LLaMA 模型训练混合了大量不同来源的数据，包\n括网页、代码、论文、图书等。针对不同的文本质量，LLaMA 模型训练针对不同质量和重要性的\n数据集设定了不同的采样概率，表中给出了不同数据集在完成 1.4 万亿个词元训练时的采样轮数。",
        "据集，会对大语言模型在问答任务上的表现产生怎样的影响？ （3）不同的大语言模型在指令微调\n阶段，对于数据的需求方面存在着怎样的差异呢？\n针对上述问题， 文献[224] 给出了详细的分析。 研究人员使用了ENTITYQUESTIONS[225]， 这\n是一个包含维基百科上 24 个不同话题知识的问答数据集。选择了其中 12 个与地点相关的原始训\n练集作为训练数据， 将它们对应的测试集作为测试集， 并将剩余12 个话题的测试集作为领域外测\n试集。通过设计的多模板补全机制，能够可靠地评估大语言模型对不同知识的记忆程度。利用该\n机制，根据其知识记忆水平将训练和测试集均进行了 5 个级别的划分。\n文献 [224] 中将训练数据划分为六个不同的数据量级别，从 60 个样本到完整数据集不等，并\n通过从 12 个话题中均匀抽样来构建训练集。实验结果表明，仅需 60 个训练样本的指令微调，就\n足以使大语言模型高效执行问答任务，并展现出强大的泛化能力。如图5.4所示。无论基础模型或\n记忆水平如何，大语言模型在使用较少训练样本时的表现优于使用 960 个或全部样本。增加训练"
      ],
      "response_time": 3.6809051036834717,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.5,
        "rouge2": 0.33333333333333337,
        "rougeL": 0.5,
        "bleu": 0.013038525962482981,
        "semantic_similarity": 0.5040380954742432
      }
    },
    {
      "sample_id": "0143",
      "question": "检索增强生成的映射关系是基于什么建立的？",
      "ground_truth": "基于 D 建立从 Q 到 A 的映射关系。",
      "predicted_answer": "检索增强生成的映射关系是基于给定的数据D建立的[1]。",
      "ground_context": [
        "应用f 的任务是基于 D 建立从 Q 到 A 的映射关系。"
      ],
      "retrieved_contexts": [
        "检索增强生成整个过程也可以形式化定义为：\nf : Q × D − → A (9.1)\n其中，Q、A 和 D 分别代表用户输入（查询） 、期望的响应（答案）以及给定的数据。应用f 的任\n务是基于 D 建立从 Q 到 A 的映射关系。\n检索增强生成因其强大的知识整合与生成能力，在智能问答、知识管理、内容生成、个性化\n推荐、辅助决策以及教育培训等领域得到了广泛应用。以 RAG 技术为核心的 AI 搜索自 2023 年\n以来呈现出爆发式增长，迅速受到广泛欢迎，正逐渐成为人们获取信息的重要工具。与传统搜索\n引擎相比，AI 搜索能够以更加智能化的方式精准理解用户需求，为用户提供个性化、上下文相关\n且高效的搜索体验。不再仅仅是一个“信息检索工具” ，AI 搜索正在被视为一种“答案引擎” ，能\n够直接生成具有深度分析和语义理解的精确答案，从而极大地提升了用户体验。\n2023 年，全球多家知名科技企业相继推出了基于大语言模型的 AI 搜索产品，为这一领域注\n入了强劲动力。例如，微软推出的Bing AI 在结合大语言模型和 RAG 技术的基础上，显著扩展了\n传统搜索的功能；Perplexity AI 借助..."
      ],
      "response_time": 3.4525158405303955,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.4,
        "rouge2": 0.0,
        "rougeL": 0.4,
        "bleu": 0.0953476696024777,
        "semantic_similarity": 0.7345629930496216
      }
    },
    {
      "sample_id": "0144",
      "question": "Modular RAG 系统由多少个模块组成？",
      "ground_truth": "多个独立但紧密协作的模块",
      "predicted_answer": "Modular RAG 系统由多个独立但紧密协作的模块组成[4]。具体来说，文档中提到该系统至少包含了索引、检索前优化、检索、检索后优化、生成以及编排六个主要模块[3][4]。因此，Modular RAG 系统由至少六个模块组成。",
      "ground_context": [
        "Modular RAG 系统由多个独立但紧密协作的模块组成， 每个模块负责处理特定的功能或任务。"
      ],
      "retrieved_contexts": [
        "检索器优化\nNegative PairRetrieverLM-SupervisedLLM\n编排\nGenerate\nKnowledge Graph\nRetrieve\nQuery\nScheduleReasoning Pathrr\n知识指南\n12 3QueryGenerateOutput\nRetrieve\nJudgeJudge调度\nNeed Retrieval ?Retrieval Again ?Only rely on retrieved chunksAllow LLM’s own knowledge\nSemantic Analysis\nHard PromptSoft Prompt\nQuery\nPipeline1Pipeline2Judge\n路由\n检索器源SentenceDocumentChunks\nChunks\nChunks\nCodeTable\nEntitySub-GraphTripletstructured\ngranularity\n图 9.3 模块化检索增强生成（Modular RAG）架构 [421]\nModular RAG 系统由多个独立但紧密协作的模块组成， 每个模块负责处理特定...",
        "336 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n时，显得尤为重要。相比传统的语言模型， RAG 系统通过检索阶段获取最新或特定领域的信息，\n克服了模型内生记忆的局限性，尤其是在处理长尾问题或细分领域的专业知识时，可以表现更加\n出色。\n9.2.6 编排\n编排模块是 RAG 系统中的核心控制单元，它负责在关键节点进行决策并动态选择后续步骤。\n与传统固定流程的僵化方法不同，编排模块引入了灵活的适应能力，可以根据先前结果实时调整\n流程。 这种模块化、 动态化的特性是Modular RAG 的标志性特点， 展现出更高的智能化和灵活性。\n本节将分别介绍编排模块的主要模块， 包含路由 （Routing） 、 调度 （Scheduling） 以及融合 （Fusion） 。\n1. 路由\n在响应多样化查询的过程中，RAG 系统可以通过路由机制将查询分配到针对不同场景设计的\n特定管道中。这种机制是一个通用性较强的 RAG 架构的重要特性，能够处理各种复杂的情境需\n求。路由模式可以分为三种主要类型：元数据路由、语义路由以及混合路由。\n元数据路由（Metadata Routing）基于查询中提...",
        "324 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n其架构分为三个层级： 顶层聚焦于RAG 的关键阶段， 将每个阶段视为独立模块， 同时引入一个编\n排模块来协调 RAG 流程； 中层由每个模块内的子模块组成， 进一步细化和优化各项功能； 底层由\n操作的基本单元（即操作符）构成。在模块化 RAG 框架中，RAG 系统可以通过计算图的形式表\n示，其中节点代表具体的操作符。\n本章将重点介绍 Modular RAG 框架下的各模块，包括：索引、检索前优化、检索、检索后优\n化、生成以及编排。\n9.2.1 索引模块\n索引（Index）是 RAG 系统中至关重要的过程，其核心任务是将文档划分为可管理的 片段\n（Chunk），也成为“块” ，为后续的检索和生成提供组织良好的内容基础。片段切分是将文档拆分\n为更小的、可管理的、语义完整的信息单元的过程，其构建需要综合考虑内容的语义特性、上下\n文完整性以及检索和生成的实际需求。在构建片段时， 首先需要确定片段的大小（长度） 。 片段的\n大小通常用字符数、单词数或句子数来衡量，具体取决于任务要求和模型的能力。\n较大的片段在构建时能够捕获更多上下...",
        "大语言模型：从理论到实践 339\n模型或来源异构的场景。在这些复杂情况下，RRF 能够显著提升预测的准确性和整合效果，成为\n多分支融合的重要工具。\n9.3 RAG 系统设计模式\n基于 Modular RAG 的设计，各种模式通过模块化操作符之间的协作形成了模块的工作流，称\n为 RAG 流 （RAG flow） 。RAG 流可以被分解为由子函数组成的图结构， 通过控制逻辑， 这些操作\n符可以按照预定的管道线执行，同时在必要时支持条件判断、分支或循环操作。通过深入分析现\n有的 RAG 方法，这些模式的模块化特性使其能够灵活适应多样化的场景需求，同时提高了 RAG\n系统的设计效率和扩展性。\n本章将介绍典型的 RAG 系统模式，包括线性模式、条件模式、分支模式、循环模式等。\n9.3.1 线性模式\n在 RAG 系统中，线性模式是最简单且最常用的工作流模式，其流程可以分为几个核心模块，\n包括预检索（Pre-Retrieval） 、检索、后检索（Post-Retrieval）以及生成模块，如图9.10所示。当预\n检索和检索后处理模块缺失时， 线性模式会简化为朴素检索增强生成 （Naive RAG） ..."
      ],
      "response_time": 4.259544372558594,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.08899971683562802,
        "semantic_similarity": 0.650266170501709
      }
    },
    {
      "sample_id": "0145",
      "question": "显性事实查询的答案通常存在于什么形式的文档中？",
      "ground_truth": "显性事实查询的答案通常直接存在于特定领域的文档或文档片段中，以明文形式呈现。",
      "predicted_answer": "显性事实查询的答案通常直接存在于特定领域的文档或文档片段中[1]。",
      "ground_context": [
        "显性事实查询的答案通常直接存在于特定领域的文档或文档片段中，以明文形式呈现。"
      ],
      "retrieved_contexts": [
        "大语言模型：从理论到实践 319\n1. 显性事实查询\n显性事实查询是检索增强查询中最简单的一类。这类查询的答案通常直接存在于特定领域的\n文档或文档片段中， 以明文形式呈现， 无需复杂的推理或逻辑分析即可解答。例如， “֊ն࿐Ⴕ\n཮౵”这样的问题，模型仅需从外部数据中找到答案并返回。对于这一层级的查询，模型的\n主要任务是准确地定位和提取相关信息，从而生成准确的响应。这种查询形式对数据的检索效率\n和精度有较高要求，但生成过程本身相对简单，更多依赖于数据的可用性和检索机制的有效性。\n显性事实问题也是 RAG 系统中占比最大的问题， 有大量用户查询词都属于此类型， 例如： “ᇏ\n൞ଧ่Ĥ” 、 “ഒĤ” 、 “৘čNyquist’s TheoremĎ\n൞൉હĤ” 、 “ഒĤ”等等。\n显性事实查询主要依赖于正确的数据检索，以便大语言模型能够生成准确的响应。由于其高\n效性、灵活性和相对较低的成本，检索增强生成技术成为处理此类查询的最常用解决方案。然而，\n即使采用 RAG 技术， 构建一个稳健且高质量的系统仍面临诸多挑战， 包括：1） 数据处理， 例如外\n部数据通常高度非结构化，包含表格、图像、视频..."
      ],
      "response_time": 3.461188793182373,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.7323567981495755,
        "semantic_similarity": 0.9551881551742554
      }
    },
    {
      "sample_id": "0146",
      "question": "如何通过查询改写和技能添加来提升大语言模型在复杂问题求解中的表现？",
      "ground_truth": "查询改写可以通过对用户的原始查询进行语义优化和结构调整，增强检索效率，并确保生成内容与用户需求的高度匹配。与此同时，为智能体添加技能，比如多模态插件，可以扩展模型的功能，使其能够处理多样化的信息，从而提升整体表现。",
      "predicted_answer": "通过查询改写和技能添加，可以显著提升大语言模型在复杂问题求解中的表现。具体来说：\n\n1. **查询改写**：通过对用户的原始查询进行优化，比如语义优化、语言简化或结构调整，可以使查询更加明确和精确，便于模型识别核心意图并生成相关答案[2][3]。这有助于减少语言歧义，增强检索效率，并确保生成的答案更加可靠。\n\n2. **技能添加**：虽然文档中没有直接提到技能添加的具体方法，但可以类比工具学习的概念[4]，即通过训练大语言模型学会使用各种工具来辅助解决问题。这样可以在面对复杂问题时，模型不仅能提供文本回复，还能通过调用外部API等手段获取更多相关信息，从而给出更为全面和准确的回答。",
      "ground_context": [
        "查询转换（Query Transformation）又称查询改写（Query Rewrite），是指通过对用户的原始查询进行改写或重构，将其转换为更适合检索和生成的形式，从而提升系统的理解能力和检索效果。",
        "如果模型能力能覆盖智能体功能，则仅需编写提示词；否则需添加技能拓展能力。例如，文本类模型无法处理多模态内容，可绑定多模态插件理解PPT、图片等。"
      ],
      "retrieved_contexts": [
        "大语言模型：从理论到实践 329\n选择并行执行以提高效率，或按顺序逐步解决以保持逻辑一致性。\n在子查询生成后， 为确保结果的准确性， 可以引入验证机制， 例如 “验证链” （Chain-of-V erification,\nCoV e）[424]。通过让大语言模型对扩展生成的子查询及其结果进行逐步验证，能够有效减少生成\n内容与真实情况不符的问题。这种方法确保了子查询的输出质量，使得最终的答案不仅与用户需\n求高度相关，而且更加可靠和可信，从而显著提升模型在复杂问题求解中的表现。\n2. 查询转换\n查询转换（Query Transformation）又称查询改写（Query Rewrite） ，是指通过对用户的原始查\n询进行改写或重构， 将其转换为更适合检索和生成的形式， 从而提升系统的理解能力和检索效果。\n这种方法通常对用户输入的查询进行语义优化、语言简化或结构调整，使其更加明确和精确，便\n于模型识别核心意图并生成相关答案。例如，将模糊或冗长的查询改写为短小精炼的关键词形式，\n或者将复杂的问题分解为更易处理的结构化查询。 通过这种方式， 查询变形能够减少语言歧义， 增\n强检索效率，并确保生成...",
        "328 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n9.2.2 检索前优化\n为了解决 RAG 系统直接依赖用户原始查询进行检索所带来的问题， 检索前优化 （Pre-retrieval\nProcessing）模块被设计用于优化查询输入，从而提高检索的有效性。用户查询往往存在两个主要\n挑战：查询措辞不当，问题可能过于复杂或语言组织不清晰，导致检索效果不佳；语言复杂性和\n歧义性， 尤其是在包含专业术语或多义缩写的情况下， 语言模型难以准确理解查询意图。 例如， 对\n于缩写“LLM” ，系统可能无法区分其是指“大语言模型” （Large Language Model）还是法律领域\n的“法学硕士” （Master of Laws） 。预检索模块通过对用户查询进行重构、扩展或语义优化，能够\n减少语言歧义和表述模糊， 从而为下游检索任务提供更精准的输入， 显著提升RAG 系统在复杂查\n询场景中的性能。\n本节将重点介绍预检索的核心模块，包括：查询扩展、查询转换以及查询组织。\n1. 查询扩展\n查询扩展（Query Expansion）是一种通过将单一查询扩展为多个查询的方法，用以丰富查询\n...",
        "大规模语言模型：从理论到实践 161\n图 5.3 LLaMA 65B 模型使用 LIMA [42] 训练效果对比\n文献 [190] 研究也表明， 在模型构建过程中， 数据工程起着至关重要的作用， 可以通过提升数\n据集的多样性，显著增强模型的泛化能力。训练数据多样性的提升，可以从多个方面着手，例如\n使用来自不同源头、具备不同特征且呈现不同分布的数据。此外，实验结果也说明，在数据选择\n环节，多样性有着不可忽视的作用。对比随机选择、均匀选择这两种常见方式，具备多样性的数\n据选择策略展现出明显优势。此外，相较于单纯聚焦于挑选高质量数据，若能将数据质量与多样\n性标准有机结合，模型也可以达到更好的效果[223]。\n在问答任务方面，大语言模型的预训练依托于多样化的语料库来开展，这些语料库包含了多\n种类型的内容，并且涵盖了丰富的世界知识。大语言模型在预训练完成后，大量的知识被编码进\n了模型的参数之中。而通过监督微调的方式，就能够把这些已经编码进参数的知识有效地应用于\n问答任务里。然而，针对大语言模型的问答任务能力提升，存在着三个亟待解决的关键问题： （1）\n指令微调阶段，究竟需要多少数据量，才能使大语...",
        "272 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n工具，也不能很好对任务进行多步骤的规划。同时大语言模型构建之初也没有考虑记忆和使用用\n户全部对话历史。为了弥补这些不足，研究者们开始系统地研究如何提升大语言模型解决上述问\n题的能力。本节将重点介绍大语言模型工具使用能力提升、推理规划能力提升以及长期记忆构建\n与应用的策略方法。\n8.3.1 工具学习\n大模型工具学习 （Tool Learning） 是指通过让大语言模型学会使用各种工具的调用方式， 进而\n利用合适的工具去实现特定的功能需求。 例如， 用户输入“请告诉我上海今天的天气。 ”具备工具\n使用能力的大语言模型会给出如下响应：\n1. ്љ಩ༀো྘ğ ฿గҰ࿘಩ༀb\n2.Ⴈ฿గ APIğႨ฿గ APIༀ APIčೂ WeatherMapෂҰ࿘ҕඔĎ b\nresponse = requests.get(\"https://api.weathermap/data/2.5/weather\",\nparams={\n\"q\": \"Shanghai\",\n\"date\": \"2025-1-6\",\n\"appid\": \"your_api_key\"..."
      ],
      "response_time": 6.783246994018555,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.10185618710001206,
        "semantic_similarity": 0.8429417610168457
      }
    },
    {
      "sample_id": "0147",
      "question": "在处理复杂查询时，RAG系统如何提高检索效果，并同时应对多模态数据的挑战？",
      "ground_truth": "RAG系统通过检索前优化模块改善检索效果，具体方法包括查询扩展，以丰富用户查询的内容并减少语言歧义。此外，随着系统支持多模态数据，跨模态对齐和生成能力的提升成为新的挑战，要求系统能够有效处理不同类型的数据并确保输出连贯且具有上下文相关性。",
      "predicted_answer": "在处理复杂查询时，RAG系统可以通过以下步骤提高检索效果：首先将复杂查询拆解为可解答的子查询，分别检索与其相关的信息，并整合子查询的结果，生成对原始查询的完整回答[3]。这样不仅可以提升检索效率，还能增强模型在复杂场景中的适应能力和表现。\n\n面对多模态数据的挑战，RAG系统需要克服跨模态对齐和生成连贯且有意义的多模态输出两大难题。跨模态对齐需要构建统一的表示空间，确保检索结果能准确捕捉不同模态间的语义关联；生成方面则需要生成模型具备跨模态推理能力，以整合多模态信息，确保输出内容具有上下文相关性及视觉和语义一致性[4]。",
      "ground_context": [
        "预检索模块通过对用户查询进行重构、扩展或语义优化，能够减少语言歧义和表述模糊，从而为下游检索任务提供更精准的输入。",
        "多样化的数据类型需要统一的检索框架，而目前的跨模态检索策略尚不足以同时有效处理文本、图像以及潜在的视频或音频数据。"
      ],
      "retrieved_contexts": [
        "许多应用强大的能力。然而，RAG 系统在检索质量、系统效率与任务优化、多模态扩展等方面仍\n面临诸多挑战。解决这些问题对于推动 RAG 系统的发展、释放其全部潜力至关重要。\n1. 检索质量的挑战\n检索质量是 RAG 系统的核心， 因为它直接影响生成结果的相关性和连贯性。 然而， 现有检索\n技术在处理噪声时仍存在不足。RAG 系统经常会引入无关或误导性的文档， 这些噪声会干扰生成\n过程，导致虚假或不可靠的内容输出。源数据的质量问题对检索增强系统的性能也会产生重要影\n响。低质量数据中可能存在噪声、无关信息、错误、重复或矛盾内容，严重干扰知识提取的准确\n性和输出质量。此外，知识数据的整理过程也极具复杂性，需要处理复杂文件格式（如 PDF）的\n解析，探索合理的知识切分方式以避免主题内容被割裂，同时还需完成知识共享和问答对的生成\n等工作，以充分提高数据的利用效率和系统的响应能力。\n此外， 当检索阶段未能找到相关文档时， 生成模型往往仍尝试生成输出， 这可能导致错误或无\n意义的内容。特别是在查询模糊或表述不清时，这一问题尤为突出。为解决此问题， 像HyDE[36]\n这样的技术通过生成伪文档来更好地表...",
        "一定进展。然而，随着数据集规模的扩大和查询复杂性的提升，扩展多模态检索和生成能力仍然\n是一个重大挑战。 未来研究可以集中于支持更多样化的媒体类型 （如视频和语音） ， 同时优化系统\n以提升其在大规模复杂场景中的性能，为 RAG 系统的进一步发展提供新的方向。\n9.2 模块化检索增强生成架构\n随着检索增强生成技术的发展， 系统功能日益复杂， 面临的挑战也愈加突出， 包括复杂数据源\n的整合、 系统的可解释性与可控性需求、 组件的选择与优化以及工作流的编排与调度。 这些问题不",
        "9.4.2 查询优化\n如前所述，RAG 系统在处理用户查询时，需要对查询优化进行深入改进，以应对多种复杂挑\n战。对于简单查询，例如日常问候等无需上下文支持的情况，模型应避免执行不必要的信息检索，\n直接生成答案， 从而减少无关上下文对响应质量的影响。 对于复杂查询， 直接使用原始查询进行检\n索通常难以获取足够的相关信息。模型需要首先将复杂查询拆解为可解答的子查询，分别检索与\n其相关的信息，并整合子查询的结果，生成对原始查询的完整回答。而对于多义性较强的模糊查\n询，直接检索原始查询往往无法提供全面的答案。模型需通过识别用户意图来澄清查询内容，并\n构建精准的检索请求，获取相关信息后生成细致且全面的响应。通过优化查询流程，RAG 系统不\n仅能够提升检索效率，还能显著增强模型在复杂场景中的适应能力和表现。\n针对上述问题，文献 [440] 提出了 RQ-RAG 算法，旨在通过动态优化查询以提升检索增强生\n成的效果。该方法基于 7B 规模的 Llama2 模型，采用端到端训练，使其能够通过重写、分解和消\n除歧义来动态优化搜索查询。为了训练模型具备上述功能，核心是构建与推理过程相匹配的训练\n数据。为了...",
        "或列表） ， 并且生成内容中可能遗漏必要的细节， 导致输出的完整性和规范性不足， 从而影响系统\n的实用性和用户体验。\n3. 多模态扩展性的挑战\n随着 RAG 系统扩展到支持多模态数据（如文本、图像和音频） ，其在多模态检索、对齐和生\n成方面面临新的挑战。首先，跨模态对齐是一个核心难题。多样化的数据类型需要统一的检索框\n架，而目前的跨模态检索策略尚不足以同时有效处理文本、图像以及潜在的视频或音频数据。这\n种对齐过程不仅需要构建统一的表示空间，还需确保检索结果能够准确捕捉不同模态之间的语义\n关联。\n在生成方面，如何生成连贯且有意义的多模态输出是另一大挑战。生成模型需要具备跨模态\n推理能力，以整合多模态信息，确保输出内容既具有上下文相关性，又在视觉和语义上保持一致。\n这种能力在多模态生成任务中尤为关键，如视觉问答和图像描述生成。然而，现有模型在处理复\n杂、多模态的上下文时仍存在局限性，难以生成自然且连贯的跨模态响应。\n目前的研究， 如MuRAG[418]、REVEAL[419] 和 Re-ViLM[420]， 在多模态检索与生成方面取得了\n一定进展。然而，随着数据集规模的扩大和查询复杂性的提...",
        "效减少冗余并增强结果的多样性。 例如， 在选择关键短语时，MMR 会优先考虑与查询高度相关且\n不重复的短语，从而平衡结果的相关性和信息量。这种基于规则的方式简单高效，适用于许多具\n有固定规则需求的场景。\n基于模型的重排序 （Model-based Rerank） 则利用语言模型对数据块进行排序， 通常通过计算数\n据块与查询之间的相关性来完成。这种方法能够动态地根据查询上下文判断数据块的重要性，从\n而生成更精准的排序结果。重排序模型的技术持续迭代，已经从文本数据扩展到多模态数据（如\n表格和图像） ， 实现了更广泛的应用场景。 相比于规则方法， 基于模型的重排序能够捕捉更复杂的\n语义关系， 特别是在需要理解深层次上下文的任务中表现突出。 因此， 重排序在RAG 系统中不仅\n是提升检索质量的重要工具，也为多模态数据处理提供了强有力的支持。\n2. 内容压缩\n将大量相关文档段拼接为冗长的上下文通常会引入噪声，削弱大语言模型对关键信息的感知\n能力。为解决上述问题，压缩（Compression）方法核心目标是通过内容压缩减少噪声，同时保留\n信息完整性，以提高语言模型的推理效率。\n内容压缩的一种方法是..."
      ],
      "response_time": 6.892499208450317,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.4,
        "rouge2": 0.0,
        "rougeL": 0.4,
        "bleu": 0.06169859991994311,
        "semantic_similarity": 0.8900266289710999
      }
    },
    {
      "sample_id": "0148",
      "question": "如何在Coze平台上构建一个夸夸机器人，并确保其能够自动选择最佳的动作序列？",
      "ground_truth": "在Coze平台构建夸夸机器人可以通过简单的五个步骤完成。首先，创建一个智能体，输入名称和功能介绍，并编写提示词以定义人设与回复逻辑。这些提示词将影响智能体在会话中的回复效果。同时，智能体的核心思想是使用大语言模型作为推理引擎，以确定要采取的动作序列。通过将大语言模型与动作列表结合，智能体可以自动选择最佳的动作序列，从而实现自动化决策和行动。",
      "predicted_answer": "在Coze平台上构建夸夸机器人并确保其能够自动选择最佳动作序列主要涉及以下几个步骤：\n\n1. **创建智能体**：登录Coze平台，点击左上角的“L”，输入智能体名称和功能介绍，配置头像，然后进入智能体编排页面。在这里，你需要描述智能体的身份和任务，在技能面板配置相关能力，并在预览与调试面板中测试智能体的功能[1]。\n\n2. **编写提示词**：这是定义智能体基本人设的关键步骤。你可以设定模型的角色，比如一个“夸夸机器人”，并通过指定特定的语言风格来引导智能体的回复方式。这有助于确保对话内容符合用户的期望[1]。\n\n需要注意的是，文档中并没有提及具体的技术细节关于如何实现自动选择最佳动作序列，因此这可能依赖于具体的编程逻辑或者预先设定的规则来决定智能体的行为[1]。",
      "ground_context": [
        "使用 Coze 平台可以通过以下简单的五个步骤就可以构造快速搭建一个“夸夸机器人”，并在多个平台提供对外服务。",
        "智能体的核心思想是使用大语言模型来选择要执行的一系列动作。在链中，操作序列是硬编码在代码中的。"
      ],
      "retrieved_contexts": [
        "调用插件或执行复杂任务；卡片功能则为智能体提供了信息展示和互动的新形式，让用户在网页\n或移动端直观查看数据、流程和结果。\n使用 Coze 平台可以通过以下简单的五个步骤就可以构造快速搭建一个“夸夸机器人” ，并在\n多个平台提供对外服务。\n步骤 1： 创建一个智能体。 在扣子平台创建智能体非常简单： 登录后， 点击页面左上角的 “L” ，\n输入智能体名称和功能介绍，并通过生成图标自动生成头像，或使用“AI 创建”功能，通过自然\n语言描述需求， 由平台自动生成智能体。 点击确认后， 进入智能体编排页面。 在这里， 可以通过左\n侧人设与回复逻辑面板描述智能体的身份和任务；利用中间技能面板为智能体配置扩展能力；在\n右侧预览与调试面板中实时测试智能体，确保其功能和交互效果符合预期。\n步骤 2：编写提示词。配置智能体的第一步是编写提示词，即定义智能体的人设与回复逻辑。\n这部分内容决定了智能体的基本人设，并持续影响其在所有会话中的回复效果。在设计提示词时，\n建议明确模型的角色、设计特定的语言风格，并限制回答范围，以确保对话内容符合用户的预期。\n例如，对于一个“夸夸机器人” ，提示词可以设置为："
      ],
      "response_time": 7.17509913444519,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.2222222222222222,
        "rouge2": 0.0,
        "rougeL": 0.2222222222222222,
        "bleu": 0.06032054032972033,
        "semantic_similarity": 0.9122966527938843
      }
    },
    {
      "sample_id": "0149",
      "question": "RRR 方法在预检索阶段引入了什么模块？",
      "ground_truth": "查询重写模块",
      "predicted_answer": "RRR 方法在预检索阶段引入了查询重写模块，该模块是基于 T5-large 模型微调的小型可训练语言模型。[425]",
      "ground_context": [
        "文献 [425] 提出的“重写-检索-阅读” （Rewrite-Retrieve-Read，RRR）方法就是一个典型的线性 RAG 流模式。在预检索阶段，RRR 方法引入了查询重写模块，该模块是基于 T5-large 模型微调的小型可训练语言模型。"
      ],
      "retrieved_contexts": [
        "基本的检索和生成过程。 常见的线性RAG 流通过在预检索阶段引入查询变换模块 （比如重写或隐\n式文档扩展（HyDE）操作符） ，以及在检索后阶段使用排序模块来优化检索结果，从而提升最终\n生成的质量。\n查询词预检索\n检索\n后检索\n生成\n回答\n图 9.10 RAG flow 的线性模式 [421]\n文献 [425] 提出的“重写-检索-阅读” （Rewrite-Retrieve-Read，RRR）方法就是一个典型的线\n性 RAG 流模式。在预检索阶段，RRR 方法引入了查询重写模块，该模块是基于 T5-large 模型微\n调的小型可训练语言模型。该模块通过强化学习框架进行优化，将查询重写过程建模为一个马尔\n可夫决策过程（Markov Decision Process, MDP ） 。 在这一过程中， 查询重写模块以大语言模型的最\n终输出质量作为奖励信号，以此调整和优化生成的查询。具体而言，强化学习通过策略梯度方法\n对重写模块进行训练，使其生成的查询更符合检索任务的需求，提高检索和生成的整体效率和效\n果。在检索阶段，RRR 方法使用稀疏编码模型（如 BM25）作为检索工具，从外部知识库中获取...",
        "328 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n9.2.2 检索前优化\n为了解决 RAG 系统直接依赖用户原始查询进行检索所带来的问题， 检索前优化 （Pre-retrieval\nProcessing）模块被设计用于优化查询输入，从而提高检索的有效性。用户查询往往存在两个主要\n挑战：查询措辞不当，问题可能过于复杂或语言组织不清晰，导致检索效果不佳；语言复杂性和\n歧义性， 尤其是在包含专业术语或多义缩写的情况下， 语言模型难以准确理解查询意图。 例如， 对\n于缩写“LLM” ，系统可能无法区分其是指“大语言模型” （Large Language Model）还是法律领域\n的“法学硕士” （Master of Laws） 。预检索模块通过对用户查询进行重构、扩展或语义优化，能够\n减少语言歧义和表述模糊， 从而为下游检索任务提供更精准的输入， 显著提升RAG 系统在复杂查\n询场景中的性能。\n本节将重点介绍预检索的核心模块，包括：查询扩展、查询转换以及查询组织。\n1. 查询扩展\n查询扩展（Query Expansion）是一种通过将单一查询扩展为多个查询的方法，用以丰富查询\n..."
      ],
      "response_time": 3.7359752655029297,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.0406422493756618,
        "semantic_similarity": 0.6916326284408569
      }
    },
    {
      "sample_id": "0150",
      "question": "后检索分支模式的特点是什么？",
      "ground_truth": "后检索分支的特点在于单一查询驱动的检索过程，而并行生成则聚焦于对不同文档块的独立处理。",
      "predicted_answer": "后检索分支模式的特点是从单一查询开始，通过检索模块获取多个文档块。每个文档块被独立送入生成模块进行处理，生成对应的结果集合。随后，这些生成的结果通过合并模块进行整合，形成最终结果。这一模式适合需要从同一查询结果中挖掘多角度信息的场景，能够充分利用检索到的内容，提高生成结果的多样性和质量。[1]",
      "ground_context": [
        "后检索分支的特点在于单一查询驱动的检索过程，而并行生成则聚焦于对不同文档块的独立处理。"
      ],
      "retrieved_contexts": [
        "后检索分支（Post-Retrieval Branching）模式则从单一查询开始，通过检索模块获取多个文档\n块。每个文档块被独立送入生成模块进行处理，生成对应的结果集合。随后，这些生成的结果通\n过合并模块进行整合，形成最终结果。与预检索分支不同，后检索分支的特点在于单一查询驱动\n的检索过程，而并行生成则聚焦于对不同文档块的独立处理。该模式适合需要从同一查询结果中\n挖掘多角度信息的场景，能够充分利用检索到的内容，提高生成结果的多样性和质量。",
        "大语言模型：从理论到实践 341\n分支间执行不同的流程， 以应对复杂场景的多样化需求， 如图9.12所示； 后检索分支模式是分支间\n执行相同的 RAG 流程，用于生成多样化的结果，如图9.13所示。通过这样的结构，分支模式能够\n从多个角度生成和整合信息，从而提升系统的生成能力与结果质量，对多任务处理和复杂场景具\n有显著优势。\n预检索分支 （Pre-Retrieval Branching） 是一种通过生成多个子查询并并行检索的模式， 用于提\n高检索的全面性和生成结果的多样性。具体而言，该模式从一个初始查询开始，通过查询扩展模\n块将其扩展为多个子查询。每个子查询随后通过检索模块检索相关文档，形成文档集合。这些文\n档集合连同对应的子查询一起送入生成模块，生成答案集合。最终，这些生成的答案通过融合模\n块进行整合，形成最终结果。这种模式通过并行检索与生成，能够从多个角度充分挖掘潜在信息，\n从而提升生成结果的覆盖度和准确性。\n查询词\n检索\n生成\n回答查询扩展 融合检索\n生成\n检索\n生成\n生成\n图 9.12 RAG flow 的预检索分支模式 [421]\n后检索分支（Post-Retrieval B...",
        "任务需求调整 RAG 的各个组件， 确保生成的回答既符合场景需求， 又具有高相关性和准确性。 这\n种灵活性使得条件模式在处理多样化、复杂性高的任务时具有显著优势。\n9.3.3 分支模式\n分支模式通过并行运行多个分支的方式增加结果的多样性和鲁棒性。具体来说，分支模式在\n某个模块中生成多个并行分支， 每个分支可以独立执行相同或不同的RAG 流程。 这些流程由多个\n处理模块组成，生成各自的分支输出结果。随后，所有分支的结果通过聚合函数合并为中间输出\n结果。重要的是，聚合后的结果并不一定标志着流程的结束，还可以继续传递到后续模块（如验\n证模块）进行进一步处理。因此，分支模式的整体流程可以表示为从分支生成、独立处理、结果\n聚合到后续处理的完整流水线。\n与条件模式不同，分支模式的特点在于同时运行多个并行分支，而非从多个选项中选择一个\n分支。分支模式可以根据不同任务需求设计为多种结构类型，通常分为两类：预检索分支模式是",
        "342 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n查询词\n生成\n回答融合检索\n生成\n生成\n生成\n查明变换\n图 9.13 RAG flow 的后检索分支模式 [421]\n9.3.4 循环模式\n循环模式的核心是检索与生成步骤之间的相互依赖性。 循环模式通过引入调度模块进行控制，\n确保系统可以根据需要在特定模块之间重复执行某些操作。 这一模式可以被抽象为一个有向图， 其\n中节点代表系统的各个模块，边表示模块之间的控制流或数据流。当一个模块能够返回到之前的\n模块时，该系统就形成了一个循环结构。这种循环设计允许系统在流程中对某些步骤进行重复优\n化，从而提升任务的完成效果。\n循环模式的关键在于判断模块（Judge Module） ，用于决定流程是否需要返回到之前的模块或\n继续向下执行。例如，当一个模块完成后，判断模块可以决定是进入下一个模块还是返回到前置\n模块。如果系统决定返回，则执行循环操作；如果系统决定不返回，则流程继续向前。这种灵活\n的控制机制使得循环模式能够动态调整整个流程，从而提高系统的适应性、灵活性以及对复杂任\n务的处理能力。\n循环模式可以进一步细分为三种类型：迭代型、..."
      ],
      "response_time": 5.038950204849243,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.029690516179988503,
        "semantic_similarity": 0.8336231708526611
      }
    },
    {
      "sample_id": "0151",
      "question": "权重 λ(d, q) 是由什么确定的？",
      "ground_truth": "权重 λ(d, q) 由文档 d 和输入查询 q 之间的相似度得分确定。",
      "predicted_answer": "权重 λ(d, q) 由文档 d 和输入查询 q 之间的相似度得分确定 [2]。该权重使用 softmax 函数来计算，以确保权重经过归一化且总和为 1。公式表示为：\n\\[ \\lambda(d, q) = \\frac{e^{s(d,q)}}{\\sum_{d' \\in D_q} e^{s(d',q)}} \\] [2]",
      "ground_context": [
        "权重 λ(d, q) 由文档 d 和输入查询 q 之间的相似度得分确定。"
      ],
      "retrieved_contexts": [
        "全面丰富，又能准确反映问题的多维特性。融合模块的引入，使系统在应对复杂查询时能够提供\n更加综合且连贯的回答，大幅提升了整体的适应能力与输出质量。融合模块主要包含大语言模型\n融合、加权继承以及倒数排名融合等方法。\n大语言模型融合是多分支信息整合的直接方法之一， 利用大语言模型强大的分析与整合能力，\n将不同分支的信息进行统一处理。然而，这种方法面临一些挑战，特别是在处理超出大语言模型\n上下文窗口限制的长答案时。为了缓解这一问题，通常会先对每个分支的答案进行摘要提取，提\n炼关键内容后再输入 LLM， 从而在长度限制内保留最重要的信息。 这种方法确保了答案的完整性\n与精确性，即使在处理复杂的多分支生成时也能提供高质量的整合结果。\n加权集成是一种基于多分支生成结果的加权选择方法，通过不同分支生成的词元（token）的\n加权值来综合选择最终输出。具体而言，权重是通过文档与输入查询的相似度得分计算的，使用\nSoftmax 函数对权重进行归一化，确保所有权重之和为 1。该方法可按如下公式计算：\np(y|q, Dq) =\nX\nd∈Dq\np(y|d, q) · λ(d, q) (9.8)\n权重 λ(d, ...",
        "权重 λ(d, q) 由文档 d 和输入查询 q 之间的相似度得分确定。该权重使用 softmax 函数来计算，以\n确保权重经过归一化且总和为 1。\nλ(d, q) = es(d,q)\nP\nd∈Dq es(d,q) (9.9)\n倒数排名融合 （Reciprocal Rank Fusion，RRF） 是一种集成技术， 专门用于将多个检索结果的\n排名整合为统一的列表。它通过一种定制的加权平均方法，增强了整体预测性能与排名精度 [433]。\nRRF 的核心优势在于其动态的权重分配机制，基于分支之间的相互作用进行调整，特别适合处理"
      ],
      "response_time": 4.588907957077026,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 0.5
      },
      "answer_quality_metrics": {
        "rouge1": 0.2758620689655173,
        "rouge2": 0.2222222222222222,
        "rougeL": 0.2758620689655173,
        "bleu": 0.1897865055465156,
        "semantic_similarity": 0.8477749824523926
      }
    },
    {
      "sample_id": "0152",
      "question": "文献[440] 采用了与哪些方法类似的方式来生成高质量的大规模数据？",
      "ground_truth": "与 Self-RAG[431] 和 SAIL[441] 类似的方法",
      "predicted_answer": "文献[440] 采用了与 Self-RAG[431] 和 SAIL[441] 类似的方法来生成高质量的大规模数据。[1]",
      "ground_context": [
        "文献[440] 采用了与 Self-RAG[431] 和 SAIL[441] 类似的方法，设计了一套自动化的数据生成流程，以优化查询、检索信息并生成精确的响应，同时减少人工干预所需的资源和时间成本。"
      ],
      "retrieved_contexts": [
        "数据。为了生成高质量的大规模数据，文献[440] 采用了与 Self-RAG[431] 和 SAIL[441] 类似的方法，\n设计了一套自动化的数据生成流程，以优化查询、检索信息并生成精确的响应，同时减少人工干\n预所需的资源和时间成本。"
      ],
      "response_time": 3.871243715286255,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.8333333333333333,
        "rouge2": 0.8,
        "rougeL": 0.8333333333333333,
        "bleu": 0.45637960572361486,
        "semantic_similarity": 0.7966176271438599
      }
    },
    {
      "sample_id": "0153",
      "question": "RAG 系统可以通过哪种方式直接进行判断？",
      "ground_truth": "RAG 系统还可以通过大语言模型直接进行判断（LLM Judge）。",
      "predicted_answer": "RAG 系统可以通过大语言模型直接进行判断（LLM Judge），这种方式包括利用 LLM 的上下文学习能力通过精心设计的提示来进行决策[1][2]。",
      "ground_context": [
        "RAG 系统还可以通过大语言模型直接进行判断（LLM Judge） 。"
      ],
      "retrieved_contexts": [
        "(9.7)\n其中，ˆst 表示临时答案，st 是语言模型的输出。 接受ˆst 的条件是其内部的所有词元都必须具有大\n于或等于阈值 τ 的关联概率。如果不满足这一条件，系统就会转而生成新的答案。\nRAG 系统还可以通过大语言模型直接进行判断（LLM Judge） 。这一方式包括两种主要方法：\n第一种方法利用 LLM 的上下文学习能力，通过精心设计的提示来进行决策。这种方法的优势在\n于无需对模型进行额外的微调，但其判断结果的准确性通常依赖于 LLM 对提示的理解程度。第\n二种方法通过对 LLM 进行微调， 使其生成特定的触发标记， 来直接控制模型的行为。 例如， 借助",
        "大语言模型：从理论到实践 357\n使得评估结果容易受到主观因素的影响。此外，生成内容的质量评估需要覆盖多个维度，包括准\n确性、连贯性、流畅性和可读性等。这些维度的多样性要求设计更加细致和全面的评测指标，以\n全面反映生成组件的表现。\nRAG 系统的整体性能评估需要超越对检索和生成组件的单独考察， 重点关注两者之间的协同\n作用。检索结果的质量会直接影响生成结果，而生成组件的表现也取决于其对检索内容的有效利\n用。 因此， 评估需要量化检索对生成的实际贡献， 并分析两者在不同任务中的交互效果。 此外， 在\n实际应用中，RAG 系统的响应能力同样重要。 例如， 用户通常关注系统处理模糊查询的能力、 响应\n速度以及在多轮对话中的表现。这些实际应用场景中的关键因素往往被传统评测框架所忽略，因\n此需要在整体评估中引入新的指标和方法，以全面衡量 RAG 系统在真实场景中的实际效用和用\n户体验。\n9.5.2 评估目标\n评估目标是 RAG 系统性能评估的核心， 直接决定了评估框架的设计方向与具体实施方式。 评\n估目标需要清晰地定义检索组件、生成组件及整体系统的性能衡量标准，同时能够全面覆盖系统\n在不同任务场...",
        "重要作用，它能够识别关键节点，负责管理和协调系统的各个流程。包括何时需要进行外部数据\n检索、如何评估生成结果的充分性，以及在必要时决定是否启动进一步的检索。这一模块特别适\n用于递归、迭代和自适应检索的场景，确保系统能够根据当前任务的需求动态调整流程，从而在\n适当的时机停止生成或启动新的检索循环。 这种智能调度机制使RAG 系统更高效、 更精准地处理\n复杂任务。调度模型主要三种实现方式，包括规则判断、大语言模型判断以及知识引导调度。\n规则判定（Rule Judge）是一个重要的机制，用于评估生成答案的质量并决定进一步的操作。\n系统通过评分机制对生成的答案进行质量评估， 并根据预设的阈值判断是否继续或终止生成过程。\n具体来说， 系统会检查生成答案中每个词的概率是否高于设定的阈值τ， 若满足条件， 则接受当前\n答案；否则，系统会重新生成新答案。这种方法确保了生成内容的可靠性和准确性，同时为系统\n的迭代改进提供了依据。规则调度可以如下形式化定义：\nyt =\n8\n<\n:\nˆst 如果 ˆst 的所有词元的概率都 ⩾ τ\nst = LM([Dqt, x, y<t]) 其他情况\n(9.7)\n其中，ˆ...",
        "等） ， 评估数据集能够更全面地展现系统在不同领域中的适应性和泛化能力。 这种多样化设计确保\n了 RAG 系统在广泛应用场景中的可靠性和实用性。\n9.5.4 评估指标\n评估指标是衡量 RAG 系统性能的核心工具，直接影响评估结果的可信度和系统优化的方向。\n在评估 RAG 系统时，需要对各种评估指标有深入的理解，以便准确衡量评估目标。由于 RAG 系\n统由检索组件和生成组件构成，以下从检索模块、生成模块和整体系统三个层面展开详细讨论。",
        "大语言模型：从理论到实践 367\n9.6.2 查询分解与检索结果融合 RAG 系统\n针对复杂问题，RAG 系统在处理查询之前的优化阶段，通常需要引入查询分解等技术。这是\n因为复杂查询往往包含多个子问题或逻辑层次，直接检索可能难以获得高质量的结果。通过查询\n分解，可以将复杂查询拆分成更小、更易处理的子查询，从而提高检索的准确性和生成回答的质\n量。如图9.20所示，查询分解作为预处理步骤加入基础 RAG 系统中。\n问题\nQ1\nQ2\nQ3\n向量存储文档\n文档\n文档\n回答\n向量存储\n向量存储\n图 9.20 包含查询分解与检索结果融合的 RAG 系统\n使用 LangChain 可以快速构建一个包含查询分解与检索结果融合的 RAG 系统:"
      ],
      "response_time": 3.5043535232543945,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.6666666666666666,
        "rouge2": 0.5714285714285715,
        "rougeL": 0.6666666666666666,
        "bleu": 0.30923581500792785,
        "semantic_similarity": 0.921093225479126
      }
    },
    {
      "sample_id": "0154",
      "question": "RQ-RAG 方法提出了哪三种选择方式？",
      "ground_truth": "基于PPL的选择基于置信度的选择基于集成的选择集成",
      "predicted_answer": "RQ-RAG 方法提出了三种选择方式：基于PPL的选择、基于置信度的选择和基于集成的选择。[440]",
      "ground_context": [
        "基于PPL的选择基于置信度的选择集成"
      ],
      "retrieved_contexts": [
        "的空间，为最终的响应提供更丰富的支持。\nPPL：3.2置信度：0.7\nPPL：2.1(MIN)置信度：0.7\nPPL：4.3置信度：0.8（MAX）\n指令\n模型生成的查询词\n检索得到的上下文\n最终结果\n基于PPL的选择基于置信度的选择\n基于集成的选择集成\n图 9.18 RQ-RAG 解码策略流程 [440]\n如何从这些轨迹中选取最合适的路径是 RQ-RAG 系统中的关键问题之一。令 pM 表示一个\n参数为 M 的大语言模型，[R1, R2, . . . , Rn] 表示 n 条轨迹，其中每条轨迹都包含一个序列，记为\n[X, Y ]。 其中，X 是输入提示，Y 是由 Z1, Z2, . . . , Zi（每个 Zi 都是查询和检索到的上下文的组合）\n组成的 i 个中间步骤，以及最终答案 Yf inal 的拼接结果。针对这一问题，RQ-RAG 方法提出了三",
        "9.4.2 查询优化\n如前所述，RAG 系统在处理用户查询时，需要对查询优化进行深入改进，以应对多种复杂挑\n战。对于简单查询，例如日常问候等无需上下文支持的情况，模型应避免执行不必要的信息检索，\n直接生成答案， 从而减少无关上下文对响应质量的影响。 对于复杂查询， 直接使用原始查询进行检\n索通常难以获取足够的相关信息。模型需要首先将复杂查询拆解为可解答的子查询，分别检索与\n其相关的信息，并整合子查询的结果，生成对原始查询的完整回答。而对于多义性较强的模糊查\n询，直接检索原始查询往往无法提供全面的答案。模型需通过识别用户意图来澄清查询内容，并\n构建精准的检索请求，获取相关信息后生成细致且全面的响应。通过优化查询流程，RAG 系统不\n仅能够提升检索效率，还能显著增强模型在复杂场景中的适应能力和表现。\n针对上述问题，文献 [440] 提出了 RQ-RAG 算法，旨在通过动态优化查询以提升检索增强生\n成的效果。该方法基于 7B 规模的 Llama2 模型，采用端到端训练，使其能够通过重写、分解和消\n除歧义来动态优化搜索查询。为了训练模型具备上述功能，核心是构建与推理过程相匹配的训练\n数据。为了...",
        "大语言模型：从理论到实践 349\n所示：\nL = max\nM\nE(x,y)∼D[log pM (y|q1, d1, d2, . . . , qi, di, x) (9.15)\n其中，L 代表试图最大化的概率值，M 表示模型参数， 期望E(x,y)∼D 是对数据集 D 求平均，pM (y|q1, d1, q2, d2, . . . , qi, di, x)\n表示在给定输入 x、第 i 步经过优化的查询 qi 以及检索到的文档 di 的情况下，模型 M 生成回复\ny 的概率。\nRQ-RAG 在推理过程中采用了一种树形解码策略，其具体流程如图9.18所示。在每个时间步，\n模型可以根据需要对查询进行重写、 分解、 消除歧义， 或直接生成回答。 通过特殊标记的引导， 该\n策略能够控制解码路径的扩展，并以“生成 → 检索 → 生成 → 检索 →⋯⋯→ 答案”的循环过程\n逐步展开。在每次迭代中，模型会根据任务需求生成不同类型的搜索查询，例如重写、分解或消\n歧查询。这些查询将被用于检索与其对应的上下文信息，从而形成不同的解码路径。基于设定的\n探索宽度和深度范围，RQ-RAG 能够生成多条候选轨迹，通过...",
        "（3） 使用ChatGPT， 根据优化后的查询及其对应的检索上下文生成响应。 在这一阶段，ChatGPT\n被提示根据上下文信息生成与查询高度相关的回答， 同时避免冗余和噪声信息对响应质量的干扰。\n整个流程通过不断重复，最终生成了约 40,000 条数据实例。\nRQ-RAG 所使用的任务池涵盖了多种代表性任务，确保模型能够适应不同场景需求。这些任\n务包括单跳问答任务（如 Arc-Easy/Arc-Challenge[442] 和 OpenbookQA[443]） ，用于测试模型的基础\n推理能力；多跳问答任务（如HotpotQA[444] 和 Musique[445]） ，要求模型整合多步信息以推导答案；\n以及歧义问答任务（如 ASQA） ，评估模型处理多义性问题的能力。此外，为了提升模型的通用能\n力， 还引入了指令跟随任务， 包括LIMA[446]、WizardLM[447]、Open-Orca[448]、OpenAssistant[200] 和\nGPT4-Alpaca[35]， 这些任务通过多样化的场景训练模型理解和执行自然语言指令的能力。 最终， 任\n务池共收集了 42810 个实例..."
      ],
      "response_time": 3.5048372745513916,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.4,
        "rouge2": 0.0,
        "rougeL": 0.4,
        "bleu": 0.25079292295301137,
        "semantic_similarity": 0.7857524752616882
      }
    },
    {
      "sample_id": "0155",
      "question": "RQ-RAG 在推理过程中采用了什么策略？",
      "ground_truth": "RQ-RAG 在推理过程中采用了一种树形解码策略。",
      "predicted_answer": "RQ-RAG 在推理过程中采用了树形解码策略，该策略允许模型在每个时间步根据需要对查询进行重写、分解、消除歧义或直接生成回答。通过特殊标记的引导，此策略能够控制解码路径的扩展，并以“生成 → 检索 → 生成 → 检索 →⋯⋯→ 答案”的循环过程逐步展开。[440]",
      "ground_context": [
        "RQ-RAG 在推理过程中采用了一种树形解码策略，其具体流程如图9.18所示。"
      ],
      "retrieved_contexts": [
        "大语言模型：从理论到实践 349\n所示：\nL = max\nM\nE(x,y)∼D[log pM (y|q1, d1, d2, . . . , qi, di, x) (9.15)\n其中，L 代表试图最大化的概率值，M 表示模型参数， 期望E(x,y)∼D 是对数据集 D 求平均，pM (y|q1, d1, q2, d2, . . . , qi, di, x)\n表示在给定输入 x、第 i 步经过优化的查询 qi 以及检索到的文档 di 的情况下，模型 M 生成回复\ny 的概率。\nRQ-RAG 在推理过程中采用了一种树形解码策略，其具体流程如图9.18所示。在每个时间步，\n模型可以根据需要对查询进行重写、 分解、 消除歧义， 或直接生成回答。 通过特殊标记的引导， 该\n策略能够控制解码路径的扩展，并以“生成 → 检索 → 生成 → 检索 →⋯⋯→ 答案”的循环过程\n逐步展开。在每次迭代中，模型会根据任务需求生成不同类型的搜索查询，例如重写、分解或消\n歧查询。这些查询将被用于检索与其对应的上下文信息，从而形成不同的解码路径。基于设定的\n探索宽度和深度范围，RQ-RAG 能够生成多条候选轨迹，通过...",
        "9.4.2 查询优化\n如前所述，RAG 系统在处理用户查询时，需要对查询优化进行深入改进，以应对多种复杂挑\n战。对于简单查询，例如日常问候等无需上下文支持的情况，模型应避免执行不必要的信息检索，\n直接生成答案， 从而减少无关上下文对响应质量的影响。 对于复杂查询， 直接使用原始查询进行检\n索通常难以获取足够的相关信息。模型需要首先将复杂查询拆解为可解答的子查询，分别检索与\n其相关的信息，并整合子查询的结果，生成对原始查询的完整回答。而对于多义性较强的模糊查\n询，直接检索原始查询往往无法提供全面的答案。模型需通过识别用户意图来澄清查询内容，并\n构建精准的检索请求，获取相关信息后生成细致且全面的响应。通过优化查询流程，RAG 系统不\n仅能够提升检索效率，还能显著增强模型在复杂场景中的适应能力和表现。\n针对上述问题，文献 [440] 提出了 RQ-RAG 算法，旨在通过动态优化查询以提升检索增强生\n成的效果。该方法基于 7B 规模的 Llama2 模型，采用端到端训练，使其能够通过重写、分解和消\n除歧义来动态优化搜索查询。为了训练模型具备上述功能，核心是构建与推理过程相匹配的训练\n数据。为了...",
        "量与准确性。\n在检测模型的训练过程中，RAG-HA T采用了两阶段策略。 第一阶段专注于训练模型输出幻觉\n的预测标签，完成基础的幻觉检测任务；第二阶段通过使用 LoRA 微调，使模型能够基于预测标\n签生成幻觉的详细解释，包括幻觉描述以及防御性建议。在推理时，两阶段模型以级联方式应用，\n先检测幻觉，再生成解释性描述。这种训练策略不仅显著提升了幻觉检测的精度，还增强了模型",
        "的空间，为最终的响应提供更丰富的支持。\nPPL：3.2置信度：0.7\nPPL：2.1(MIN)置信度：0.7\nPPL：4.3置信度：0.8（MAX）\n指令\n模型生成的查询词\n检索得到的上下文\n最终结果\n基于PPL的选择基于置信度的选择\n基于集成的选择集成\n图 9.18 RQ-RAG 解码策略流程 [440]\n如何从这些轨迹中选取最合适的路径是 RQ-RAG 系统中的关键问题之一。令 pM 表示一个\n参数为 M 的大语言模型，[R1, R2, . . . , Rn] 表示 n 条轨迹，其中每条轨迹都包含一个序列，记为\n[X, Y ]。 其中，X 是输入提示，Y 是由 Z1, Z2, . . . , Zi（每个 Zi 都是查询和检索到的上下文的组合）\n组成的 i 个中间步骤，以及最终答案 Yf inal 的拼接结果。针对这一问题，RQ-RAG 方法提出了三",
        "大语言模型：从理论到实践 355\n9.4.5 检索与生成联合优化\n文献 [456] 提出了 RankRAG 方法，利用单个大语言模型完成重排序和答案生成。 RankRAG\n通过两阶段微调策略：通用指令微调以及排序与生成指令调优。不仅优化了语言模型的生成能力，\n还赋予其上下文排序能力。RankRAG 方法的训练和推理流程如图9.19所示。\n训练推理查询语料集\n检索器(DPR、Dragon等)\n阶段1:SFT\n阶段2:RankRAG指令微调\n重排序生成答案\n对话数据集阅读理解数据集上下文排序数据集检索增强QA和排序数据集\n新增加的任务\n对话数据集思维链数据集长文本QA合成指令数据集\n图 9.19 RankRAG 方法流程 [456]\n在第一阶段，RankRAG 通过有监督微调提升语言模型的基本指令遵循能力。 使用的数据包括\n高质量的指令遵循数据集， 例如OpenAssistant、Dolly、SODA 以及长文本问答数据集 ELI5 等， 总\n计 128,000 样本。 微调过程中， 模型采用多轮对话格式， 将用户与助手的历史对话作为上下文， 仅\n对助手的最后一个响应计算损失。 这一阶段为..."
      ],
      "response_time": 4.824697017669678,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.8,
        "rouge2": 0.6666666666666666,
        "rougeL": 0.8,
        "bleu": 0.11773262364378798,
        "semantic_similarity": 0.9105801582336426
      }
    },
    {
      "sample_id": "0156",
      "question": "GTE 模型在预训练阶段使用了多少对无标注的文本对？",
      "ground_truth": "约8 亿对无标注的文本对",
      "predicted_answer": "GTE 模型在预训练阶段使用了约8亿对无标注的文本对。[1]",
      "ground_context": [
        "GTE 模型在预训练阶段， 使用了约8 亿对无标注的文本对， 数据来源多样， 包括网页数据"
      ],
      "retrieved_contexts": [
        "i=1 es(q,d−)/τ (9.10)\n其中，s(q, d) 通过文本 q 和 d 的嵌入向量（q = E(q) 以及 d = E(d)）之间的向量距离来估计这两\n段文本之间的相似度。\nGTE 模型中， 给定一批正例文本对样本{(q1, d1), (q2, d2), . . . ,(qn, dn)}， 作者提出一种改进的\n对比损失，如下所示：\nLicl = − 1\nn\nnX\ni=1\nlog es(qi,di)/τ\nZ (9.11)\nZ =\nX\nj\nes(qi,dj )/τ +\nX\nj̸=i\nes(qi,qj )/τ +\nX\nj\nes(qj ,di)/τ +\nX\nj̸=i\nes(dj ,di)/τ (9.12)\n其中，Z 采用余弦相似度作为相似度度量 s(q, d)。GTE 使用 BERT 等预训练语言模型进行初始化，\n通过对语言模型生成的上下文词元表示进行平均池化来获取文本嵌入向量。\nGTE 模型在预训练阶段， 使用了约8 亿对无标注的文本对， 数据来源多样， 包括网页数据 （如"
      ],
      "response_time": 2.9545748233795166,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.5,
        "rouge2": 0.0,
        "rougeL": 0.5,
        "bleu": 0.346697783111003,
        "semantic_similarity": 0.7666517496109009
      }
    },
    {
      "sample_id": "0157",
      "question": "在医疗信息检索中，如何通过动态权重分配机制提高检索系统的性能，同时结合MMD和MPD数据集的特性进行优化？",
      "ground_truth": "通过倒数排名融合（RRF）技术，可以有效整合多个检索结果的排名，从而增强整体预测性能与排名精度。同时，MMD数据集提供了可靠的医学信息检索评估基准，而MPD数据集则确保了分析的准确性和可靠性，经过预处理和清洗的文献增强了数据的质量。因此，结合这些数据集的特性，利用RRF的动态权重分配机制，可以针对特定领域进行微调，以应对复杂的查询挑战。",
      "predicted_answer": "根据提供的文档，我无法回答这个问题。文档中主要讨论了数据集（MMD和MPD）及其用于优化医疗信息检索的方法，但并未提及使用动态权重分配机制来提高检索系统性能的具体方法。因此，无法直接回答如何通过动态权重分配机制来优化这些数据集的应用。",
      "ground_context": [
        "MMD 是一个综合且可靠的医学信息检索评估基准，专注于医疗领域的检索系统性能测试。",
        "倒数排名融合 （Reciprocal Rank Fusion，RRF） 是一种集成技术， 专门用于将多个检索结果的排名整合为统一的列表。"
      ],
      "retrieved_contexts": [
        "大语言模型：从理论到实践 347\n和 Medical Paper Dataset (MPD) 。MMD 是一个综合且可靠的医学信息检索评估基准，专注于医疗\n领域的检索系统性能测试。该数据集的数据来源于权威的“WHO Medicine”数据库以及“国家药\n典” 中所有药物信息， 包含超过20 万条记录。MPD 是一个从美国国家生物技术信息中心 （NCBI）\n采样 1,000 篇医学论文构建而成的数据集。为确保分析的准确性和可靠性，MPD 经过了一系列预\n处理和清洗操作， 排除了不符合研究标准的文献 （如非正式会议演讲和非同行评审的报告） ， 并移\n除了表格数据和不规范的数学公式。清洗后的文档被分割为固定长度的文本段（最大序列长度为\n768） ， 以适应嵌入模型的输入要求， 同时保留足够的上下文信息。 最终MPD 包含 886 篇论文， 共\n79,966 条数据。 实验结果证明，EM-FT 方法在 MMD 上的召回率和精度分别提高了 3.2%-6.0%， 在\nMPD 上的召回率和精度分别提高了 14.4%-42.6%。在一定程度上也说明，针对特定领域对文本嵌\n入模型进行微调很有必要。\n9....",
        "这种多样化且精心设计的数据分布为模型提供了广泛的语义理解能力，同时通过微调使其能够在\n特定任务中表现出色。\n虽然通用文本嵌入已经有非常的好效果，但是针对特定领域的微调对于提升检索质量依然有\n非常重要的影响。通过微调，模型能够更准确地理解查询的语境和细微差异，从而提高检索阶段\n的效果。具体而言，微调能够增强模型的语义匹配能力，使其生成更具语境感知的嵌入，这不仅\n能更有效地匹配查询与潜在文档，还能显著提升检索内容的相关性。对于特定领域的数据进行微\n调，可以使模型更好地掌握领域专有的术语、风格和知识，生成更加精准和专业的内容。特别是\n在处理稀有查询时，微调可以充分利用领域知识，有效应对罕见或特殊表述的查询，这对于医疗、\n法律和教育等专业领域尤为重要。\n文献 [438] 提出了专门针对医学文档检索的框架 REMED，其中 EM-FT 模型通过高效的嵌入\n式微调方法，对预训练模型中的医学句子表示进行端到端微调，从而提高医学检索性能。作者选\n用 m3e-base[439] 和 e5-base-v2[434] 作为嵌入模型的基线。EM-FT 方法结合了对比学习作为损失函\n数，以优化模型性能并准确捕捉..."
      ],
      "response_time": 4.532445430755615,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.6666666666666666,
        "rouge2": 0.5,
        "rougeL": 0.6666666666666666,
        "bleu": 0.05518705101707244,
        "semantic_similarity": 0.7183072566986084
      }
    },
    {
      "sample_id": "0158",
      "question": "在使用检索增强生成（RAG）方法时，如何优化大语言模型以减少生成中的幻觉问题，同时又能确保生成结果的多样性和准确性？",
      "ground_truth": "通过实施 Hallucination Aware Tuning（RAG-HA T）方法来训练幻觉检测模型，从而识别和解释幻觉，并提供防御性建议。此外，利用预检索分支模式，通过生成多个子查询并并行检索，能够提升检索的全面性与生成结果的多样性，最终通过融合模块整合生成的答案，以确保结果的质量和准确性。",
      "predicted_answer": "在使用检索增强生成（RAG）方法时，可以通过以下方式优化大语言模型以减少生成中的幻觉问题，同时确保生成结果的多样性和准确性：\n\n1. 使用直接偏好优化（DPO）方法对模型进行训练，构建成对的偏好数据集，指导模型生成更少幻觉内容的回答[1][3]。\n2. 构建幻觉检测模型，识别并解释幻觉产生的位置和原因，结合生成的幻觉解释内容对包含幻觉的原始回答进行重写，去除幻觉[2][3]。\n3. 引入防御性建议（Defensive Advice），明确指出可能导致幻觉的模糊表述，并提供改进建议，以减少分类边界的不确定性[2]。\n4. 借助GPT-4 Turbo等工具对包含幻觉的输出进行重写，生成“优选”样本；对于优质回答，仅针对特定句子进行优化，避免引入新幻觉[3]。\n5. 引入“过于谨慎惩罚”（OCP）策略，随机删除“优选”样本中的句子以生成“拒绝”样本，鼓励模型保持回答的内容完整性[3]。\n6. 将生成的高质量样本通过幻觉检测模型验证，确保其准确性，必要时重复重写过程[3]。\n7. 在训练过程中结合选择性采样和自举式训练策略，提高数据质量和准确性[2]。\n8. 调整检索增强生成的过程，确保在利用检索信息与语言模型自身能力之间取得平衡，避免生成结果出现不一致或相互矛盾的内容[5]。",
      "ground_context": [
        "大模型幻觉指的是大语言模型生成的内容中出现与事实不符、缺乏依据或与输入信息相矛盾的表述。在实际应用中，即使采用检索增强生成（RAG）方法，大语言模型仍然可能出现幻觉问题，例如对检索到的内容进行错误或扭曲的解释。",
        "预检索分支是一种通过生成多个子查询并并行检索的模式，用于提高检索的全面性和生成结果的多样性。最终，这些生成的答案通过融合模块进行整合，形成最终结果。"
      ],
      "retrieved_contexts": [
        "于构建偏好数据集， 通过直接偏好优化 （Direct Preference Optimization，DPO） 方法对大语言模型\n进行训练，从而有效降低模型生成幻觉的概率，同时提升回答质量。\nRAG-HA T在构造幻觉检测方法时， 采用了基于选择性采样的训练数据构建策略。 在RAGTruth[450]\n数据集的基础上， 虽然该数据集标注了幻觉文本的具体片段， 但缺乏对幻觉的详细描述， 因此RAG-\nHA T借助 GPT-4 Turbo 自动生成幻觉描述， 以支持检测模型的训练。 这些描述包括三部分内容： 幻\n觉的二元标签 （标识句子是否包含幻觉） 、 幻觉发生的位置和原因的详细解释， 以及防御性建议 （De-\nfensive Advice） 。 防御性建议明确指出文本中可能导致幻觉的模糊表述， 并提供改进建议， 从而帮助\n减少分类边界的不确定性， 降低幻觉的发生率。 此外，RAG-HA T借鉴了自举式训练 （Bootstrapping-\nstyle Training） 和拒绝采样的策略， 对GPT-4 的输出进行多轮评估与再生成， 以确保生成数据的质\n量与准确性。\n在检测模型的训练过程...",
        "（3）基于集成的选择：选择累积置信度得分最高的结果作为最终输出，可以表示为： Yfinal =\narg maxy\nP\ni:Yi=y Conf(Yi)。其中，最终结果 Yfinal 是所有候选结果中置信度分数累积最大的一项，\n通过对所有候选结果 Yi，取值等于 y 的置信度分数 Conf(Yi) 进行累加求和，确定最佳答案。\n9.4.3 幻觉感知的生成模型优化\n大模型幻觉指的是大语言模型生成的内容中出现与事实不符、缺乏依据或与输入信息相矛盾\n的表述。在实际应用中，即使采用检索增强生成（RAG）方法，大语言模型仍然可能出现幻觉问\n题，例如对检索到的内容进行错误或扭曲的解释，这在高信任场景中带来了显著风险。\n文献 [449] 提出了一种专门针对检索增强生成中幻觉问题的方法，Hallucination Aware Tuning\n（简称 RAG-HA T） 。该方法通过训练幻觉检测模型，识别出幻觉并给出易于理解的解释，说明幻\n觉产生的位置和原因，以及提供防御性建议。利用这些检测结果，特别是幻觉描述，借助 GPT-4\nTurbo 对包含幻觉的 RAG 输出进行重写，以去除幻觉内容。随后，原始输出和...",
        "大语言模型：从理论到实践 351\n在处理边界案例时的解释能力和鲁棒性。\nRAG-HA T采用 DPO 方法进行模型训练，通过构建成对的偏好数据集，指导大语言模型生成\n更少幻觉内容的回答。 在回答重写阶段， 针对包含幻觉的原始回答， 结合生成的幻觉解释内容， 利\n用 GPT-4 Turbo 对其进行重写，去除幻觉并生成“优选” （Chosen）样本。而对于被判定为优质的\n回答， 则通过防御性建议限定重写范围， 仅针对特定句子进行优化， 以避免引入新的幻觉内容。 此\n外，重写后的回答通过幻觉检测模型进行验证，确保其准确性，如发现仍存在幻觉，则重复重写\n过程，直至生成高质量的样本，保证数据集的完整性和可靠性。\n为进一步提升模型的回答质量，RAG-HA T还在偏好数据集中引入了 “过于谨慎惩罚” （Overly\nCautious Penalization, OCP） 策略。 由于模型在训练后可能倾向于通过缩短回答来降低幻觉率， 从而\n影响回答的内容丰富性，OCP 随机从 “优选” 样本中删除一个句子以生成 “拒绝” （Rejected） 样本，\n鼓励模型在减少幻觉的同时保持回答的内容完整性。 ...",
        "316 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n检索增强生成（Retrieval-Augmented Generation, RAG）自 2020 年首次在文献 [412] 中提出以\n来， 引起了广泛关注。 为了弥补大语言模型在知识覆盖、 实时性以及准确性方面的不足， 自2022 年\nChatGPT 发布以来，RAG 技术得到了迅猛发展。RAG 通过引入外部知识库或实时搜索工具， 使模\n型在推理和生成过程中能够动态检索相关信息，而不再仅依赖预训练阶段固化的参数化知识。例\n如， 当用户提出 “复旦大学在哪里？ ” 这一问题时， 采用RAG 技术的系统会首先检索复旦大学官网、\n百科介绍等相关页面，并将全部或部分内容与用户问题合并，作为提示输入大语言模型。这种方\n法将基于大语言模型的问题解答从依赖模型记忆的知识的问答任务（闭卷问题回答，Closed-book\nQA）转变为“阅读理解”的任务，即从“闭卷考试”转变为“开卷考试” 。这一技术有效弥补了\n大语言模型在知识记忆和动态信息处理方面的不足，为解决长尾知识的获取以及减少幻觉现象提\n供了切实可行的解决方案。\n检索增强生成整个...",
        "322 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n2. 系统效率与任务优化的挑战\nRAG 系统的复杂工作流程，包括查询分类、检索、重新排序和生成等多个步骤，使其在效率\n上面临诸多挑战。随着文档集合规模的增长，检索和重新排序过程的延迟问题愈发严重。深度学\n习驱动的重新排序模型（如 RankLLaMA[417]）尽管在性能上表现优异，但其计算开销非常高，尤\n其是在需要多轮推理的复杂场景中。RAG 系统组件之间的相互依赖性也增加了优化难度， 例如分\n块策略、嵌入模型和重新排序算法等。模块化设计可以通过实现各组件的独立优化，同时考虑跨\n组件的交互影响，从而提升整体效率。\n检索增强系统在生成过程中需要在利用检索信息与语言模型自身能力之间寻求平衡，但这一\n平衡较难实现，直接影响生成结果的质量和可靠性。同时，当检索到的多个文档内容相互冲突时，\n系统缺乏有效的冲突解决策略，容易导致生成结果出现不一致或相互矛盾的内容，从而进一步降\n低系统的准确性和可信度。\n在引入外部知识进行检索增强的过程中，模型的某些通用能力可能受到影响，使其在特定领\n域的表现更加局限。此外，大语言模型在生成输出时可能..."
      ],
      "response_time": 11.055783748626709,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0625,
        "rouge2": 0.0,
        "rougeL": 0.0625,
        "bleu": 0.031315288829726964,
        "semantic_similarity": 0.7548373341560364
      }
    },
    {
      "sample_id": "0159",
      "question": "混合检索和递归型循环检索在信息检索中的作用是什么？",
      "ground_truth": "混合检索结合了稀疏检索和稠密检索的优势，能够提高检索系统的效率和效果，尤其在处理同义词和复杂语义关系时表现出色。而递归型循环检索则通过每一步依赖于前一步的输出，逐层深入挖掘信息，适合需要分步推理的任务场景。两者共同促进了现代信息检索的精确性和深度。",
      "predicted_answer": "混合检索和递归型循环检索在信息检索中有不同的作用：\n\n- 混合检索的作用是结合稀疏检索和稠密检索的优势来提升检索系统的效率和效果。它能够在保持高计算效率的同时，增强系统的语义理解能力[3][4]。\n\n- 递归型循环检索则侧重于通过多次迭代和逐步深化检索过程来获取更深层次的信息。这种方法适用于需要多步推理或解决复杂问题的情景，能逐步精炼检索目标并生成高质量的答案[2]。",
      "ground_context": [
        "混合检索（Hybrid Retrieval）是结合稀疏检索和稠密检索优势的检索方法，用于提升检索系统的效率和效果。",
        "递归型检索的显著特点在于每一步都依赖于前一步的输出，并通过不断加深检索过程，逐步挖掘更深层次的信息。"
      ],
      "retrieved_contexts": [
        "应用于广告推荐、问答系统等场景，表现出了良好的效果。\n尽管混合检索方法在许多场景中表现优异，但其设计和实现也存在一定的技术挑战。首先是\n如何有效融合稀疏向量和稠密向量的得分，因为两者的分布和尺度不同，需要设计合理的归一化\n或加权策略。 其次， 混合检索的计算开销依然较高， 尤其是在需要实时处理大规模用户查询时， 如\n何进一步优化效率是一个重要问题。随着硬件性能的提升和检索算法的优化，混合检索有望在未\n来的信息检索系统中占据更加重要的地位，为用户提供更高效、更精准的检索服务。\n9.2.4 检索后优化\n检索后优化（Post-retrieval processing）是优化大语言模型生成效果的重要步骤。直接将检索\n到的文本块输入大语言模型并不能得到最好的结果，存在诸多挑战。首先，大语言模型与人类类\n似， 对长文本往往只能记住开头和结尾部分， 而容易遗忘中间内容， 这被称为 “中间遗忘” （lost in\nthe middle）问题。其次，检索到的文本中可能包含噪声信息或与事实相悖的内容，这些“噪声/反",
        "大语言模型：从理论到实践 343\n查询词检索\n生成\n判断\n回答\n迭代N次\n图 9.14 RAG flow 的迭代型循环模式 [421]\n（2） 递归型循环检索模式是一种具有明显依赖性和层次性的检索方式。 如图9.15所示， 递归型\n检索的显著特点在于每一步都依赖于前一步的输出，并通过不断加深检索过程，逐步挖掘更深层\n次的信息。通常，递归型检索遵循类似树状的结构，每次检索都会基于一个重新改写的查询展开，\n从而精确地针对当前需要获取的知识进行检索。递归型检索还包含明确的退出机制，用以确保在\n满足终止条件时流程终止，避免无限递归。这种机制能够有效控制流程的深度和复杂性。在RAG\n系统中，递归型循环模式通过查询转换模块生成新的查询，以推动检索逐层深入。这种方式特别\n适合需要分步推理或分解复杂问题的任务场景，能够逐步定位相关信息并生成高质量的回答。\n查询词检索\n生成\n判断\n回答\n可选查询词变换\n图 9.15 RAG flow 的递归型循环模式 [421]\n（3）自适应型（主动型）模式是一种超越传统被动检索模式的新兴模式，得益于大语言模型\n的强大能力。 如图9.16所示， 这种模式的核心思想类似于...",
        "332 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n者结合，既保留了稀疏检索的高效性，又增强了系统在语义理解上的能力。\n混合检索的核心思想是将稀疏向量和稠密向量的得分进行融合，或者在检索流程中分阶段使\n用两者。例如，在第一阶段，使用稀疏检索从大规模文档库中快速筛选出一个候选集合（通常称\n为“粗排” ） ；在第二阶段，对候选文档进行稠密检索或语义重排序，以提升结果的相关性。这种\n分阶段策略既能降低稠密检索的计算成本，又能显著提高检索质量。混合检索在效率和效果之间\n达到了良好的平衡。混合检索架构如图9.9所示。\n查询词\n稠密查询词向量\n相似度\n数据集\n稀疏查询词向量相似度 前k个相关文档\n前k个相关文档\n重排序\n相关文档\n图 9.9 混合检索架构图\n混合检索的优势在于其灵活性和适应性。对于需要显性词项匹配的查询（如“精确匹配”类\n问题） ， 稀疏检索能够快速捕捉关键词； 而对于需要语义理解的复杂查询 （如自然语言表达的长尾\n问题） ， 稠密检索能够提供更相关的结果。 此外， 混合检索可以根据不同的应用场景调整稀疏和稠\n密部分的权重，从而实现个性化的优化。例如，在工业界的大规模...",
        "如，对于“汽车”和“车辆”这样的同义词，稠密检索器可以识别它们在语义上的相近性，从而\n提高检索结果的相关性。稠密检索在捕捉细粒度语义关联方面优于传统的稀疏检索方法。\n然而，稠密检索也面临一些挑战，特别是在计算成本和存储要求方面。由于稠密向量通常是\n高维向量（例如 768 维或更高） ，因此处理和存储大规模文档库的稠密向量需要更高的计算资源。\n此外， 稠密检索依赖于深度学习模型的训练， 模型的质量和训练数据的规模直接影响检索效果， 这\n可能增加系统的开发复杂性和维护成本。稠密检索的高计算需求限制了其在资源受限场景中的应\n用。 尽管如此， 稠密检索已经成为RAG 系统和现代信息检索中的重要方法， 尤其是在需要高语义\n理解能力的任务中。\n3. 混合检索\n混合检索 （Hybrid Retrieval） 是一种结合稀疏检索和稠密检索优势的检索方法， 用于提升检索\n系统的效率和效果。 稀疏检索 （如TF-IDF 和 BM25） 擅长处理显性特征， 能够快速匹配高频词项，\n同时在大规模文档库中表现出极高的计算效率；而稠密检索（如基于深度学习模型生成的语义向\n量）能够捕捉复杂的语义关系，对理解同义词、..."
      ],
      "response_time": 5.5793328285217285,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.14012791512573355,
        "semantic_similarity": 0.928403377532959
      }
    },
    {
      "sample_id": "0160",
      "question": "状态空间模型中，A 矩阵代表什么？",
      "ground_truth": "状态转移矩阵",
      "predicted_answer": "在状态空间模型中，A 矩阵代表状态转移矩阵，表示内部状态之间的关联，体现了系统的动态特性 [1]。",
      "ground_context": [
        "其中，A 是状态转移矩阵、B 表示控制量对状态量的影响、C 表示当前状态量对输出影响和 D 表示当前控制量对输出影响，"
      ],
      "retrieved_contexts": [
        "为，并利用当前状态对未来的状态或输出进行预测。两个方程可以如下形式化表示：\nh′(t) = Ah(t) + Bx(t) (10.1)\ny(t) = Ch(t) + Dx(t) (10.2)\n其中，A 是状态转移矩阵、B 表示控制量对状态量的影响、C 表示当前状态量对输出影响和 D 表\n示当前控制量对输出影响， 上述四个矩阵都是可学习的， 也称为模型参数，h 表示中间状态，x 表\n示输入序列。\n状态空间模型的基本过程如图10.5所示。输入信号 x 与矩阵 B 相乘，生成一个向量，用于表\n示输入 x 对系统状态的影响。状态表示（State Representation）h 是一个隐向量，包含了系统的核\n心“知识” 。通过与矩阵A 相乘，状态表示描述了内部状态之间的关联，从而体现系统的动态特\n性。在预测输出之前，需要根据当前状态和输入信号更新状态。最后，通过矩阵C 将状态映射到\n输出空间，利用矩阵 D 提供从输入到输出的直接信号（通常被称为跳跃连接（Skip Connection） ） ，\n生成最终的输出。矩阵 C 描述了状态与输出之间的关系，即如何将状态转换为输出结果。\n跳跃连接\n图 10...",
        "192 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n6.1.1 强化学习基础概念\n智能体与环境的不断交互过程中，会获得很多观测 oi。针对每一个观测，智能体会采取一个\n动作 ai，也会得到一个奖励 ri。可以定义历史 Ht 是观测、动作、奖励的序列：\nHt = o1, a1, r1, o2, a2, r2, · · · , ot, at, rt (6.1)\n由于智能体在采取当前动作时会依赖它之前得到的历史， 因此可以把环境整体状态St 看作关于历\n史的函数：\nSt = f (Ht) (6.2)\n当智能体能够观察到环境的所有状态时，称环境是完全可观测的（ Fully Observed） ，这时观测ot\n等于 St。当智能体只能看到部分观测时，称环境是部分可观测的（Partially Observed） ，这时观测\n是对状态的部分描述。整个状态空间使用 S 表示。\n在给定的环境中， 有效动作的集合经常被称为动作空间（Action Space）， 使用A 表示。例如\n围棋（Go）这样的环境具有离散动作空间（Discrete Action Space），智能体的动作数量在这个空\n..."
      ],
      "response_time": 3.6159026622772217,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.04196966566468063,
        "semantic_similarity": 0.6977354288101196
      }
    },
    {
      "sample_id": "0161",
      "question": "final_rag_chain 是什么？",
      "ground_truth": "final_rag_chain 是一个包含 retrieval_chain_rag_fusion 和 question 的组合。",
      "predicted_answer": "`final_rag_chain` 是一个处理流程，用于生成响应给定问题的答案。具体步骤包括从模板创建提示、使用检索链和问题调用检索器、融合结果以及通过语言模型生成最终答案[1]。[1]",
      "ground_context": [
        "final_rag_chain = (\"context\": retrieval_chain_rag_fusion, \"question\": itemgetter( \"question\") | prompt | llm | StrOutputParser())"
      ],
      "retrieved_contexts": [
        "Question: {question}\n\"\"\"\nprompt = ChatPromptTemplate.from_template(template)\nfinal_rag_chain = (\n\"context\": retrieval_chain_rag_fusion, \"question\": itemgetter( \"question\")\n| prompt\n| llm\n| StrOutputParser()\n)\nprint(final_rag_chain.invoke(\"question\":question))",
        "fused_scores[doc_str] += 1 / (rank + k)\n#ݔࢲ\nreranked_results = [\n(loads(doc), score)\nfor doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n]\n#ْ߭\nreturn reranked_results\nquestion =\"཮౵?\"\n#৽\nretrieval_chain_rag_fusion = generate_queries | retriever.map() | reciprocal_rank_fusion\ndocs = retrieval_chain_rag_fusion.invoke(\"question\": question)\nprint(len(docs))\n#֥ࢳٳRAG ৽\ntemplate = \"\"\"Answer the following question based on this context:\n{context}\nQuestion: {question}\n\"\"\"\nprom..."
      ],
      "response_time": 3.2592406272888184,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.4615384615384615,
        "rouge2": 0.36363636363636365,
        "rougeL": 0.4615384615384615,
        "bleu": 0.08243191945517342,
        "semantic_similarity": 0.7546088695526123
      }
    },
    {
      "sample_id": "0162",
      "question": "ROUGE 主要用于评估什么任务？",
      "ground_truth": "生成摘要任务",
      "predicted_answer": "ROUGE 主要用于评估生成摘要任务，衡量生成内容与参考答案的文本片段重叠程度。[1][2]",
      "ground_context": [
        "ROUGE（Recall-Oriented Understudy for Gisting Evaluation ）ROUGE 主要用于评估生成摘要任务，衡量生成内容与参考答案的文本片段重叠程度。"
      ],
      "retrieved_contexts": [
        "文本中 n-gram 的匹配概率。BLEU 适合评估结构化任务（如机器翻译） ，但对开放性任务的灵活\n性有限。\nROUGE（Recall-Oriented Understudy for Gisting Evaluation ）ROUGE 主要用于评估生成摘要\n任务，衡量生成内容与参考答案的文本片段重叠程度。常用的 ROUGE 指标有 ROUGE-N（基于\nn-gram） 、ROUGE-L（基于最长公共子序列，LCS） 。计算公式如下：\nROUGE-N =\nP(Overlapping N-grams)P(Reference N-grams) (9.21)\nROUGE-L = F1(LCS) = (1 + β2) × PrecisionLCS × RecallLCS\n(β2 × PrecisionLCS + RecallLCS) (9.22)\n其中，PrecisionLCS 表示生成文本中最长公共子序列的精确率，RecallLCS 表示参考文本中最长公共\n子序列的召回率。\nExact Match（EM） 用于评估生成回答与参考答案的完全一致性， 常用于问答任务， 公式如下\n所示：\nEM =...",
        "在参考译文 Sj 中出现的次数。\n文本摘要采用 ROUGE[597]（Recall-Oriented Understudy for Gisting Evaluation ） 评估方法， 该方\n法也称为面向召回率的要点评估，是文本摘要中最常用的自动评估指标之一。 ROUGE 与机器翻\n译的评估指标 BLEU 类似，能根据机器生成的候选摘要和标准摘要（参考答案）之间词级别的匹\n配程度来自动为候选摘要评分。ROUGE 包含一系列变种， 其中应用最广泛的是ROUGE-N， 它统"
      ],
      "response_time": 3.0193238258361816,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.05022135506220879,
        "semantic_similarity": 0.5982474088668823
      }
    },
    {
      "sample_id": "0163",
      "question": "使用的嵌入模型是什么？",
      "ground_truth": "HuggingFaceEmbedding(model_name= \"BAAI/bge-large-zh-v1.5\")",
      "predicted_answer": "文本嵌入模型旨在将非结构化文本转换为嵌入表示，以进行语义搜索[2][3][4]。常用的嵌入模型包括词袋模型（BoW）、TF-IDF、Word2Vec、GloVe、FastText以及基于Transformer的模型如BERT、Sentence-BERT和OpenAI的text-embedding-ada等[1][3][4]。最新的通用文本嵌入模型则致力于构建能适应多任务、多领域、多语言的统一模型[4]。",
      "ground_context": [
        "embed_model = HuggingFaceEmbedding(model_name= \"BAAI/bge-large-zh-v1.5\")"
      ],
      "retrieved_contexts": [
        "大语言模型：从理论到实践 345\n（Retrieval-Augmented Generation）系统中，文本嵌入表示是实现向量搜索的核心技术。\n文本嵌入技术有很长的研究历史，大体上可以分为四个阶段：计数式嵌入（Count-based Em-\nbeddings） ： 这一阶段的方法包括词袋模型 （Bag of Words, BoW） 和TF-IDF， 用词频和逆文档频率\n来表示文本， 但忽略了词语的语义和上下文信息， 仅能反映基本的词汇相关性； 静态词嵌入 （Static\nDense Word Embeddings） ： 代表性模型如Word2V ec、GloV e和 FastText， 通过上下文生成固定的词\n向量。这一阶段捕捉了词语的语法和语义相似性，但每个词的向量是静态的，无法反映词义在不\n同上下文中的变化；上下文嵌入（Contextualized Embeddings） ：这一阶段引入了上下文敏感的动\n态嵌入模型， 如GPT 和 BERT 等。 这些模型通过双向或单向Transformer 结构， 生成能够根据上下\n文调整的词或句子向量，实现了对多义词和复杂语境的更深层次理解；...",
        "298 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\npage_content='Computer science is the study of computation, information, and automation.\nMembers of Congress and' metadata={'start_index': 0}\npage_content='and automation.\nComputer science spans theoretical disciplines (such as algorithms,\ntheory of computation, and information theory)'\nmetadata={'start_index': 60}\nText embedding models（文本嵌入模型） 旨在将非结构化文本转换为嵌入表示。 基于文本的嵌入\n表示可以进行语义搜索， 查找最相似的文本片段。Embeddings 类则用于与文本嵌入模型进行交互，\n并为不同的嵌入模型提供统一的标准接口， 包括OpenAI、Cohere 等。LangC...",
        "能力的依赖程度也各不相同。例如，向量化模块需要借助预训练模型生成高质量的嵌入向量，以\n捕捉文本的深层语义特征；查询转换模块可能需要结合提示工程、或模型微调的方式，生成更精\n准的检索查询；而重排序模型则需要在结合用户输入和上下文的基础上，优化排序策略以提高最\n终输出的质量。 因此， 如何高效地选择、 训练和集成这些模型， 成为构建高性能RAG 系统的关键。\n本节将按照 Modular RAG 架构中模块划分，介绍典型的 RAG 系统中各模块所采用算法和优\n化方法。\n9.4.1 文本嵌入模型微调\n文本嵌入（Text Embedding）是一种将文本转换为固定维度向量（通常是高维浮点数组）的\n技术，旨在以数学形式捕捉语言的语义信息，并将其映射到向量空间中。通过深度学习模型（如\nWord2V ec、GloV e、FastText，以及基于 Transformer 的模型如 BERT、Sentence-BERT 和 OpenAI\n的 text-embedding-ada 等） ，文本的语义、语法及上下文特征能够被有效编码为向量表示。在RAG",
        "Text Embeddings） ：最新阶段致力于构建能适配多任务、多领域、多语言的统一模型。通过利用\n大规模多样化数据、合成数据生成以及大语言模型（LLMs）作为骨干网络，如 E5[434]、BGE[435]、\nGecko[436] 等，通用文本嵌入模型可以在分类、检索、聚类等任务中表现出色，显著提升了跨任务\n和跨领域的泛化能力。\n通用文本嵌入模型目标应对众多下游任务， 文献[437] 提出的 GTE 模型 （General-purpose Text\nEmbedding） 引入了多阶段对比学习策略， 并采用多样化的训练数据混合方式： 在预训练阶段， 使\n用未经任何筛选或清理的大量开源数据，通过无监督对比学习来学习基本的语言模式；在第二阶\n段，利用有监督微调，通过对比学习使用规模更小、质量更高的数据集对嵌入向量进行优化。\n对于查询语句 q 所对应的一个相关（正例）文档 d+ 以及一组不相关（负例）文档 D− =\n{d−\n1 , d−\n2 , . . . , d−\nn }，InfoNCE 损失[53] 的定义如下所示：\nLcl = − log es(q,d+)/τ\nes(q,d+)/τ ..."
      ],
      "response_time": 5.618654251098633,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0.0,
        "bleu": 0.004367174244469995,
        "semantic_similarity": 0.5003199577331543
      }
    },
    {
      "sample_id": "0164",
      "question": "OmniEval包含多少个自动生成的测试示例？",
      "ground_truth": "11400",
      "predicted_answer": "OmniEval 包含 11400 个自动生成的测试示例和 1700 个人工标注的测试示例[1]。根据问题的要求，这里提供了自动生成测试示例的数量。",
      "ground_context": [
        "OmniEval包含 11400 个自动生成的测试示例和 1700 个人工标注的测试示例。"
      ],
      "retrieved_contexts": [
        "场景的结构化评估。OmniEval 包含 11400 个自动生成的测试示例和 1700 个人工标注的测试示例。\n3. 数据集的构建策略\n在应用场景中，评估数据集的构建需要结合任务特点与评估目标，以全面衡量 RAG 系统的\n性能。为了测试 RAG 系统在动态真实场景中的表现，部分评估基准（如 RGB、MultiHop-RAG、\nCRUD-RAG 和 DomainRAG）通过爬取新闻、年度变化数据或实时信息生成评估数据集。这些动\n态数据集能够有效检验系统在面对训练数据未覆盖的最新信息时的适应能力和处理效率，从而评\n估其动态性和时效性。\n针对特定评估目标，定制化数据集的构建能够更好地模拟复杂任务场景。例如，DomainRAG\n设计了结合单轮与多轮对话的任务，测试系统在复杂用户交互中的表现；CDQA 则通过多文档生\n成任务，评估系统在整合和分析多源信息时的能力。通过任务定制化，评估数据集能够更精确地\n反映系统在特定场景下的实际性能。\n数据集的多样性和覆盖率是评估 RAG 系统全面性能的关键指标。 通过结合多种任务类型 （如\n开放域问答、多轮对话、事实验证等）和多样化的数据来源（如新闻、百科全书...",
        "360 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n法的灵活性使得数据集能够更好地适应实际需求，同时对评估 RAG 系统的动态知识处理能力提\n出了更高要求。\nRGB[463]、MultiHop-RAG[464] 和 CRUD-RAG[465] 是基于自动生成数据集的典型案例。这些基\n准通过在线新闻文章生成数据集，用于测试 RAG 系统在处理真实世界信息时的表现。数据集内\n容超越了训练数据的覆盖范围， 评估系统对于动态、 实时信息的适应能力。CDQA[466] 则结合新闻\n来源的数据生成评估集，并引入标签器辅助构建更复杂的评价任务。DomainRAG[467] 结合了单文\n档、多文档、单轮对话和多轮对话等多种任务类型，数据集内容基于高校招生和注册信息的年度\n变化生成。 该数据集通过提供更新后的信息， 强迫系统利用动态的知识库来完成任务， 对RAG 系\n统的时效性和适应性进行了全面评估。OmniEval[468] 提出一个专门针对金融领域的自动和全方位\n的 RAG 评估基准， 将查询分为抽取式问答、 多跳推理、 对比、 对话和长文本问答5 个任务类别以\n及 16 个金融主题，..."
      ],
      "response_time": 4.182584762573242,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.4,
        "rouge2": 0.0,
        "rougeL": 0.4,
        "bleu": 0.007986728673290205,
        "semantic_similarity": 0.35761702060699463
      }
    },
    {
      "sample_id": "0165",
      "question": "大语言模型推理遵循什么模式？",
      "ground_truth": "自回归模式",
      "predicted_answer": "大语言模型推理遵循自回归模式[1]。在这种模式下，模型在每次生成词元时都需要依赖于之前生成的词元的键和值。为了减少计算开销，通常会使用键值缓存技术来避免每次都重新计算键和值。",
      "ground_context": [
        "图 10.1 大语言模型推理遵循自回归模式"
      ],
      "retrieved_contexts": [
        "大语言模型：从理论到实践 371\n图 10.1 大语言模型推理遵循自回归模式\n在每次 GPT 推理中， 对每个词元的自注意力操作需要其前面词元的键和值。 最简单且无状态\n的实现需要在每次迭代中重新计算所有的键和值，这会导致大量额外的计算开销。为了避免这种\n重新计算的开销，FAIRSEQ[470] 提出了键值缓存（Key-V alue Cache） ，即在迭代中保存键和值，以\n便重复使用。 根据上述方法和技术， 大语言模型的推理过程可以分为预填充阶段 （Prefilling Stage）\n和解码阶段（Decoding Stage）两个阶段，如图10.2所示。在预填充阶段，模型会计算并存储初始\n输入词元的键-值 （KV） 缓存， 同时生成第一个输出词元。 随后进入解码阶段， 模型逐个生成后续\n输出词元，并在每一步更新 KV 缓存，直至完成整个推理过程。",
        "6 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n输入做出反应，性能较 GPT-4 有显著提升。2024 年 9 月 OpenAI 又推出的全新推理模型 GPT-o1，\n在复杂推理任务上表现卓越，能通过内部思维链模拟人类思考，在数学、科学等领域超越人类专\n家及 GPT-4o。国内外各大公司和研究机构相继发布了此类系统，包括复旦大学的MOSS、阿里巴\n巴的 Qwen、 深度求索的DeepSeek、Google 的 Gemini、XAI 的 Grok、 科大讯飞的星火大模型、 智\n谱的 ChatGLM 等。\n表1.1 和表1.2 分别给出了截至 2025 年 2 月典型开源和闭源大语言模型的基本情况。可以看\n到， 从2022 年开始， 大语言模型的数量呈爆发式的增长， 各大公司和研究机构都在发布不同类型\n的大语言模型。模型类型中，基础模型是指仅经过预训练的模型；对话模型是指在预训练模型基\n础上经过有监督微调和强化学习训练的模型，具备对话和完成任务的能力；推理模型是指专注于\n逻辑推理增强的大语言模型。",
        "大语言模型：从理论到实践 349\n所示：\nL = max\nM\nE(x,y)∼D[log pM (y|q1, d1, d2, . . . , qi, di, x) (9.15)\n其中，L 代表试图最大化的概率值，M 表示模型参数， 期望E(x,y)∼D 是对数据集 D 求平均，pM (y|q1, d1, q2, d2, . . . , qi, di, x)\n表示在给定输入 x、第 i 步经过优化的查询 qi 以及检索到的文档 di 的情况下，模型 M 生成回复\ny 的概率。\nRQ-RAG 在推理过程中采用了一种树形解码策略，其具体流程如图9.18所示。在每个时间步，\n模型可以根据需要对查询进行重写、 分解、 消除歧义， 或直接生成回答。 通过特殊标记的引导， 该\n策略能够控制解码路径的扩展，并以“生成 → 检索 → 生成 → 检索 →⋯⋯→ 答案”的循环过程\n逐步展开。在每次迭代中，模型会根据任务需求生成不同类型的搜索查询，例如重写、分解或消\n歧查询。这些查询将被用于检索与其对应的上下文信息，从而形成不同的解码路径。基于设定的\n探索宽度和深度范围，RQ-RAG 能够生成多条候选轨迹，通过...",
        "附加到原始输入中，并将“复旦大学位于”作为一个新的整体输入模型以生成下一个词元。这个\n生成过程持续进行，直到生成表示序列结束的 <eos> 标志或达到预定义的最大输出长度为止。大\n语言模型的推理过程与其他深度学习模型 （如BERT、ResNet 等） 非常不同，BERT 的执行时间通\n常是确定且高度可预测的。但是，在大语言模型的推理过程中，虽然每次迭代执行的时间仍然具\n有确定性，但迭代次数（输出长度）是未知的，这使得一个大语言模型推理任务的总执行时间是\n不可预测的。",
        "vi 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n\u001a\u000f\u0012\u000f\u0013ࠩٳ318\n\u001a\u000f\u0012\u000f\u0014ׄ321\n9.2 模块化检索增强生成架构 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 322\n\u001a\u000f\u0013\u000f\u0012෬ႄଆॶ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 324\n\u001a\u000f\u0013\u000f\u0013߄328\n\u001a\u000f\u0013\u000f\u0014෬ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ..."
      ],
      "response_time": 4.491971254348755,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.024128958361671914,
        "semantic_similarity": 0.6563403010368347
      }
    },
    {
      "sample_id": "0166",
      "question": "文档评分的合成方法包括哪几种版本？",
      "ground_truth": "离散版本、连续版本和混合版本。",
      "predicted_answer": "文档评分的合成方法包括三种版本：离散版本、连续版本和混合版本。[1][2][3]",
      "ground_context": [
        "合成方法旨在通过多种策略对文档进行重新排序，以提高检索结果的相关性。这些方法包括离散版本、连续版本和混合版本。"
      ],
      "retrieved_contexts": [
        "{query}\nHere is the analysis of the {query name}:\n{query analysis}\nHere is the {doc name}:\n{doc}\nHere is the analysis of the {doc name}:\n{doc analysis}\n在对文档相关性进行判断后， 文档评分的合成方法旨在通过多种策略对文档进行重新排序， 以\n提高检索结果的相关性。这些方法包括离散版本、连续版本和混合版本。在离散版本中，文档根\n据模型的判断被划分为“相关” （输出为“是” ）和“不相关” （输出为“否” ）两类。对于每一类\n文档，保留初始检索排名的相对顺序，即相关文档始终排在不相关文档之前。虽然这种方法简单\n直观，但其性能高度依赖于提示的设计和第一阶段检索的质量。\n为了克服离散方法的局限性，连续版本利用模型输出的“是”概率（py）和“否”概率（pn）\n对文档进行更细粒度的评分。具体来说，评分函数通过归一化 py 和 pn 的值来计算文档的相关性\n得分：S(d) = py\npy+pn\n，从而确保不同文档的评分具有可比性。根据这些得分，所有文档...",
        "py+pn\n，从而确保不同文档的评分具有可比性。根据这些得分，所有文档被重新排\n序，得分越高的文档排名越靠前。与离散版本相比，连续版本能够更精确地捕捉文档的相关性梯\n度，适用于需要更细腻排序的场景。\n混合版本进一步结合了连续版本的概率评分和第一阶段检索中的 BM25 分数，通过加权求和\n的方式生成综合评分。 具体地， 最终评分由概率得分Sprob 和 BM25 分数 SBM25 按照权重系数 α 进\n行加权：S = αSprob + SBM25， 综合了推理能力和表层匹配的优点。 混合版本通过模型集成的方式，\n兼顾深层语义推理和表层匹配效果，在实际应用中表现出更强的稳定性和适用性。"
      ],
      "response_time": 3.4157772064208984,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.29108736587772466,
        "semantic_similarity": 0.8224061727523804
      }
    },
    {
      "sample_id": "0167",
      "question": "在使用OllamaLLM生成搜索查询时，如何确保检索结果既全面又多样化？",
      "ground_truth": "在使用OllamaLLM生成搜索查询时，可以通过确保检索组件能够全面覆盖用户的不同信息需求，避免信息冗余，从而实现检索结果的全面性和多样性。同时，检索组件需要具备动态适应性，以快速适应知识库的变化，确保在检索过程中能够获取到最新的相关信息。",
      "predicted_answer": "根据提供的文档，我无法回答这个问题。",
      "ground_context": [
        "llm = OllamaLLM(model=\"qwen2.5\")",
        "检索组件需要确保其检索结果能够全面覆盖查询的不同维度，同时避免信息冗余。多样性评估旨在衡量检索结果是否包含多样化的视角或信息来源，尤其是在处理开放域问答或多轮对话时，这一点尤为重要。",
        "由于RAG 系统依赖动态更新的知识库（如互联网爬取的数据），检索组件需要能够快速适应知识库的变化。"
      ],
      "retrieved_contexts": [],
      "response_time": 2.8012802600860596,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.00015409948782012229,
        "semantic_similarity": 0.4176093339920044
      }
    },
    {
      "sample_id": "0168",
      "question": "大语言模型在提升效率方面有哪些研究方向，并且在自然语言处理任务中展现了哪些能力？",
      "ground_truth": "大语言模型在自然语言理解与生成等任务中展现了卓越的能力，并推动了人工智能技术的快速发展。为应对资源消耗的挑战，研究者们从模型、数据和计算框架等多个角度探索了提升大模型效率的优化方法，包括模型压缩、量化、数据选择和优化训练框架等技术。",
      "predicted_answer": "为了提升大语言模型的效率，研究者们主要关注以下几个方面：\n\n- **模型压缩与优化**：减少模型的大小和参数数量，从而降低计算和存储的需求。\n- **训练效率优化**：改进训练算法和技术，加快训练速度并节省资源。\n- **推理效率优化**：通过优化推理过程，提高模型在实际应用场景中的响应速度。\n- **专用框架的设计与应用**：开发专门针对大语言模型的高效执行框架，进一步提高性能。\n\n此外，大语言模型在自然语言处理任务中展示了以下能力：\n\n- 自然语言理解和生成：能够准确理解复杂的语言结构和上下文，生成流畅且符合语法规则的文本。\n- 文化背景和隐喻理解：对于涉及特定文化背景或隐喻的内容，能够做出恰当的理解和解释。\n- 专业知识处理：在医疗、科技等专业领域内，能够理解和生成高质量的专业文本。\n- 实体识别与信息抽取：从大量的文本资料中高效地提取关键信息，如人物、地点、组织等实体。\n- 跨语言处理：能够有效地进行多语言间的翻译工作，提升跨语言沟通的质量和效率。",
      "ground_context": [
        "大语言模型在自然语言理解与生成等任务中展现了卓越的能力，不仅推动了人工智能技术的快速发展，也为社会各领域的应用带来了深远的影响。",
        "因此，如何在保持模型性能的同时提高其效率，已成为当前大模型研究中的重要议题。为应对这一问题，研究者们从模型、数据和计算框架等多个角度探索了提升大模型效率的优化方法。"
      ],
      "retrieved_contexts": [
        "10. 大语言模型效率优化\n大语言模型在自然语言理解与生成等任务中展现了卓越的能力，不仅推动了人工智能技术的\n快速发展，也为社会各领域的应用带来了深远的影响。然而，这些强大的能力背后伴随着巨大的\n资源消耗，包括计算、存储和能源需求，这对环境、经济以及技术可持续性带来了严峻挑战。因\n此，如何在保持模型性能的同时提高其效率，已成为当前大模型研究中的重要议题。为应对这一\n问题，研究者们从模型、数据和计算框架等多个角度探索了提升大模型效率的优化方法。通过模\n型压缩、量化、数据选择和优化训练框架等技术，显著降低了训练与推理成本，为实现更加可持\n续和普惠的人工智能提供了可能性。\n本章将重点从模型、训练和推理三个角度系统性探讨提升大语言模型效率的技术进展，分别\n涵盖模型压缩与优化、训练效率优化，以及推理效率优化和专用框架的设计与应用。\n10.1 效率优化基础\n大语言模型的推理过程遵循自回归模式 （Autoregressive Pattern） ， 如图10.1 所示。 例如， 针对\n输入 “复旦大学位” ， 模型预测 “于” 的概率比 “置” 的概率高。 因此， 在第一次迭代后， “于” 字被\n附加...",
        "用户提供更自然的互动体验，帮助他们有效提升语言学习能力。\n12.1.3 翻译与多语言处理\n随着大语言模型的崛起，这一领域正在迎来新的变革，大语言模型凭借其强大的语言理解和\n生成能力，为翻译与多语言处理注入了新的活力，加速了技术和应用的迭代发展。\n在机器翻译方面，传统方法主要依赖神经网络、深度学习以及大量语料库的训练来实现文本\n翻译。 然而， 大语言模型的出现， 为机器翻译带来了质的飞跃。 得益于广泛的知识储备和对语言深\n层语义的理解能力，大语言模型在翻译中表现出更高的准确性和自然性，尤其是在文化背景、隐\n喻和典故等复杂内容的处理上。例如，在文学翻译场景中，传统机器翻译往往难以还原原文的意\n境与风格，而大语言模型能够更精准地理解文化元素，并以目标语言重现文本的艺术性。在跨国\n企业中，大语言模型也广泛应用于产品文档翻译，如苹果、三星等公司利用其快速处理多种专业\n术语，确保翻译的专业性和一致性，大幅提升了翻译效率，并帮助全球用户更好地理解产品信息。\n在跨语言信息检索领域， 大语言模型同样展现了强大的能力。 以微软学术搜索等平台为例， 引\n入大语言模型后，跨语言检索的精准度和效率显著提升。大语...",
        "大规模语言模型：从理论到实践 451\n交通等问题，大语言模型不仅能够准确理解用户需求，还能将答案翻译成日语，提供自然且贴合\n实际的建议。相比传统客服，这种基于大语言模型的解决方案更具人性化和情景适应性，大幅提\n升了用户体验，同时增强了用户对平台的信任与忠诚度。\n翻译与多语言处理技术正以前所未有的速度融入各个行业，而大语言模型的应用为其带来了\n新的可能性。未来，随着大语言模型的持续优化，翻译的准确性和多语言处理的效率将进一步提\n升。这些技术不仅能够在更多领域创造价值，还将拉近不同文化与语言之间的距离，推动全球交\n流与合作，让世界变得更加紧密相连。\n12.1.4 信息抽取与知识图谱\n大语言模型在信息抽取领域展现了强大的能力，尤其是在实体识别任务中。借助其深度语言\n理解能力， 大语言模型能够精准地从文本中提取出人名、 地名、 组织名等关键实体。 例如， 复旦大\n学推出的 B2NE[628] 基于大模型的开放领域信息工具，可以让用户自由的从超过 16 个领域的 400\n种类型中， 灵活抽取目标实体和关系。 在医疗领域，IBM Watson Discovery 广泛用于从医学文献中\n识别疾病名称...",
        "1. 绪论\n大语言模型是一种由包含数百亿个及以上参数的深度神经网络构建的语言模型，通常使用自\n监督学习方法通过大量无标注文本进行训练。2018 年以来，Google、OpenAI、Meta、百度、华为\n等公司和研究机构相继发布了 BERT[1]、GPT[2] 等多种模型， 这些模型在几乎所有自然语言处理任\n务中都表现出色。2019 年，大语言模型呈现爆发式的增长，特别是 2022 年 11 月 ChatGPT（Chat\nGenerative Pre-trained Transformer）的发布，引起了全世界的广泛关注。用户可以使用自然语言与\n系统交互，实现问答、分类、摘要、翻译、聊天等从理解到生成的各种任务。大语言模型展现出\n了强大的对世界知识的掌握和对语言的理解能力。\n本章主要介绍大语言模型的基本概念、发展历程和构建流程。\n1.1 大语言模型的基本概念\n使用语言是人类与其他动物最重要的区别之一， 而人类的多种智能也与此密切相关， 逻辑思维\n以语言的形式表达， 大量的知识也以文字的形式记录和传播。 如今， 互联网上已经拥有数万亿个网页\n的资源， 其中大部分信息都是用自然语言描述的。 ...",
        "入大语言模型后，跨语言检索的精准度和效率显著提升。大语言模型能够深入理解用户提问的语\n义， 即便面对模糊或复杂的问题， 也能准确解析， 并在多语言数据集中找到相关内容。 例如， 科研\n人员在研究人工智能领域时， 可通过中文输入问题， 模型不仅能理解核心要点， 还可以在英文、 法\n文或其他语言撰写的学术论文中精准定位相关信息。这种能力让科研人员能够全面获取全球研究\n成果，掌握前沿动态，进而推动科研项目的顺利开展。Open AI 2025 年 2 月推出的 Deep Research\n则是更一步，基于 o3 模型，专为复杂研究任务设计，能自动搜索、解读、整合海量在线信息，花\n费 5 到 30 分钟生成专业级研究报告。 它具备推理能力， 可自主调整研究方向， 研究结果附带完整\n文档、引用来源和逻辑摘要，适用于金融、科学等领域专业人士及有深度调研需求的消费者。\n在多语言客户服务方面，大语言模型为企业提供了更智能化的服务解决方案。例如，在线旅\n游平台 Booking.com 利用大语言模型驱动的智能客服系统，结合实时翻译技术，为全球用户提供\n个性化、多语言支持。当一位日本游客使用日语在平台上预订..."
      ],
      "response_time": 8.627973318099976,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.03126032988358938,
        "semantic_similarity": 0.8825809955596924
      }
    },
    {
      "sample_id": "0169",
      "question": "如何评估生成内容的创造性和真实性？",
      "ground_truth": "评估生成内容的创造性可以通过语义嵌入的多样性指标和冗余度分析来实现。高语义相似性分值表明生成的内容更具创造性和多样性，而低冗余度意味着信息表达新颖且丰富。在真实性方面，真实性检测通过如FEVER评分等指标，评估生成内容与事实的匹配程度，以避免生成“幻觉”内容。",
      "predicted_answer": "评估生成内容的真实性主要关注内容是否基于准确的信息，避免事实错误或误导性内容[1][4]。至于创造性，则更多地体现在多样性上，即生成内容应在保持与输入主题一致的同时展现出显著的多样性和创新性[2]。",
      "ground_context": [
        "该指标通过评估生成内容中各部分的语义相似性来计算其多样性，分值越高，表明生成内容在语义表达上越具有差异性，从而更具创造性和多样性。",
        "真实性检测在避免生成“幻觉”内容方面发挥关键作用，FEVER评分是一种常用的方法，用于测试生成内容与事实的匹配程度。"
      ],
      "retrieved_contexts": [
        "cination） 。 因此， 生成组件需要确保其输出内容忠实于检索到的信息， 即生成的回答必须基于检索\n到的事实，而不是凭空捏造。真实性评估目标在于衡量生成内容是否准确反映了检索结果中的信\n息，避免出现事实错误或误导性内容。\n（3） 正确性 （Correctness） ： 在许多任务中， 生成的回答需要与给定的参考答案 （Ground Truth）\n保持一致。正确性评估目标旨在衡量生成内容与标准答案之间的一致性，特别是在有明确答案的\n任务（如问答或填空任务）中。这通常通过自动化指标（如 F1 分数或精确匹配率）来衡量。\n（4）连贯性与流畅性（Coherence and Fluency ） ：生成内容的连贯性和流畅性是评估生成组件\n的重要目标之一。连贯性指回答内容是否逻辑通顺，是否能够完整表达查询的意图；流畅性则关\n注语言表达是否符合自然语言的语法和用法。这些目标通常通过人工评估或语言模型的评分机制\n来实现。\n（5）生成内容的多维度要求（Multi-Dimensional Requirements） ：生成组件的评估需要覆盖多\n个维度，包括内容的可读性、丰富性和结构化程度。例如，在生成...",
        "务评估中，多样性是重要考量。此类任务要求生成内容在保持与输入主题一致的同时呈现出显著\n的多样性和创新性。评估生成内容的多样性需要从语义差异性和冗余度两个方面入手，以全面衡\n量生成内容的丰富程度和信息分布特性。\n语义多样性得分 （Semantic Diversity Score, SDS） 是衡量生成内容语义层面差异性的重要指标，\n其公式为：\nSDS = 1 − cos(Embedding1, Embedding2) (9.26)",
        "大规模语言模型：从理论到实践 425\n• 知识准确性：评估生成文本中所呈现的知识是否准确无误。 这涉及事实陈述、 概念解释、 历\n史事件描述等方面。 生成的文本应基于准确的知识和可靠的信息源， 避免错误、 虚假或误导\n性的内容。确保所提供的知识准确无误。\n• 知识丰富性： 评估生成文本所包含的知识是否丰富多样。 生成的文本应能够提供充分的信息，\n涵盖相关领域的不同方面。 这可以通过提供具体的例子、 详细的解释和相关的背景知识来实\n现。确保生成文本在知识上具有广度和深度，能够满足读者的需求。\n• 知识一致性： 评估生成文本中知识的一致性。 这包括确保文本中不出现相互矛盾的知识陈述，\n避免在不同部分或句子中提供相互冲突的信息。 生成的文本应该在整体上保持一致， 使读者\n能够得到一致的知识体系。\n11.3.2 评估方法\n评估方法的目标是解决如何对大语言模型生成结果进行评估的问题。有些指标可以通过比较\n正确答案或参考答案与系统生成结果直接计算得出， 例如准确率、 召回率等。 这种方法被称为自动\n评估（Automatic Evaluation） 。 然而， 有些指标并不是可以直接计算出来的， 而...",
        "关键作用。 这类内容对用户体验和系统可靠性具有严重影响， 因此需要通过科学的指标进行评估。\nFEVER 评分是一种常用的方法， 用于测试生成内容与事实的匹配程度。 其核心指标是证据支\n持率（Evidence Support Rate, ESR ） ，其公式为：\nESR = 正确的信息数\n全部信息数 (9.28)\n高 ESR 表明生成的文本有较高比例能够得到检索证据的支持，有助于评估生成内容的事实基础。\n误导率 （Misleading Rate, MR） 是衡量生成内容中包含误导性信息比例的关键指标， 其公式为：\nMR = 误导性信息数\n全部信息数 (9.29)\n误导性信息是用户最难以察觉的错误类型，因为它通常以真伪混杂的方式呈现。较低的 MR 值意\n味着生成模型更具可信度，能够生成更可靠的内容。在实际场景中，例如医疗或法律领域，误导\n性信息可能会导致严重的后果，因此通过 MR 指标能够有效衡量和优化生成模块在这些高敏感性\n领域的表现。\n错误检测率（Error Detection Rate, EDR ）则反映生成系统发现并标记错误信息的能力，其公\n式为：\nEDR = 检测出的错误数\n总错...",
        "所示：\nEM = 正确答案数量\n全部答案数量 (9.23)\nEM 适合有明确标准答案的任务，对开放性生成任务不适用。\n（2）连贯性与流畅性（Coherence and Fluency ） ：生成内容需要逻辑连贯、语法正确且自然流\n畅。这些评估通常通过人工评分或基于语言模型的自动评分完成。\n人工评分是常用方法之一， 通过评分标准量化生成内容的表现。 例如， 流畅性评分从0（完全"
      ],
      "response_time": 4.132611036300659,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 0.5
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0.0,
        "bleu": 0.05729374986058566,
        "semantic_similarity": 0.8251301646232605
      }
    },
    {
      "sample_id": "0170",
      "question": "降低训练精度被广泛认为是减少训练成本的什么方向之一？",
      "ground_truth": "降低训练精度被广泛认为是减少训练成本最具潜力的方向之一。",
      "predicted_answer": "降低训练精度被广泛认为是减少训练成本最具潜力的方向之一。[2]",
      "ground_context": [
        "降低训练精度被广泛认为是减少训练成本最具潜力的方向之一， 它可以提供更高的速度、 更小的内存占用以及更低的通信开销。"
      ],
      "retrieved_contexts": [
        "大语言模型：从理论到实践 387\n标签合理性的推理依据。 推理依据以自然语言解释的形式呈现， 用于支持模型预测的标签。 （2） 在\n训练较小的学生模型时，不仅使用任务标签，还借助这些推理依据进行学习。推理依据提供了更\n加丰富和详细的信息，解释了输入为何被映射到特定的输出标签，同时也包含了仅通过原始输入\n可能难以推断出的相关任务知识。\n10.3 低精度训练\n大语言模型的训练通常需要海量的计算资源，包括大量的 GPU 或 TPU，以及庞大的存储和\n内存空间。DeepSeek-V3 模型 [40]，即使采用了多种训练优化策略，训练一次仍然需要耗费 266.4\n万 H800 GPU 小时。在如此巨大的计算开销下，如何在有限资源内提升模型训练和推理效率已成\n为研究的核心热点。\n降低训练精度被广泛认为是减少训练成本最具潜力的方向之一， 它可以提供更高的速度、 更小\n的内存占用以及更低的通信开销。 目前主流训练框架 （例如Megatron-LM、MetaSeq 和 Colossal-AI）\n仍然采用 FP32 全精度或混合精度的 FP16/BF16 策略。 随着Nvidia H100 GPU 的推出..."
      ],
      "response_time": 3.412473201751709,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.8277932960330121,
        "semantic_similarity": 0.9623111486434937
      }
    },
    {
      "sample_id": "0171",
      "question": "ExpertSparsity 是什么？",
      "ground_truth": "ExpertSparsity 是一种专家稀疏化方法，用于 MoE 中的前馈神经网络专家的稀疏化。",
      "predicted_answer": "ExpertSparsity[514] 是一种专家稀疏化方法，用于 MoE 中的前馈神经网络专家的稀疏化。它通过计算原始输出和稀疏化后输出之间的 Frobenius 范数来量化被稀疏化的专家的损失。[514]",
      "ground_context": [
        "ExpertSparsity 是一种专家稀疏化方法，用于 MoE 中的前馈神经网络专家的稀疏化。"
      ],
      "retrieved_contexts": [
        "大语言模型：从理论到实践 383\n配器进行渐进式结构化剪枝，使模型的固有知识得以转移，更好地保留冗余结构中的信息。同时\n引入结构稀疏优化算法，利用 LoRA 模块的信息来更新权重，提高知识保存率。\n混合专家 （MoE） 技术在大语言模型领域备受关注。 近期一些研究开始探索针对基于MoE 的\n大语言模型的专家剪枝方法。ExpertSparsity[514] 是一种专家稀疏化方法，用于 MoE 中的前馈神经\n网络专家的稀疏化。它通过计算原始输出和稀疏化后输出之间的 Frobenius 范数来量化被稀疏化\n的专家的损失。对 MoE 模型中的专家进行分层评估和剪枝，根据专家对模型整体性能的贡献程\n度，去除那些对性能影响较小的专家，以达到压缩模型和提高计算效率的目的。采用渐进式剪枝\n（Progressive Pruning）方法，逐步地对专家进行剪枝操作，在每次剪枝后评估模型性能，确保剪枝\n过程不会导致模型性能大幅下降，通过这种渐进的方式找到最优的剪枝策略。在推理过程中，采\n用了动态跳过 （Dynamic Skipping） 方法， 根据输入数据的特点动态地决定是否跳过某些专家的计\n算，对于那些..."
      ],
      "response_time": 4.38924765586853,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.5714285714285715,
        "rouge2": 0.0,
        "rougeL": 0.5714285714285715,
        "bleu": 0.35894348974034107,
        "semantic_similarity": 0.9229313135147095
      }
    },
    {
      "sample_id": "0172",
      "question": "什么是知识蒸馏？",
      "ground_truth": "知识蒸馏（Knowledge Distillation, KD）是一种广泛应用的模型压缩技术，其核心思想是将大型模型 （称为教师模型，Teacher Model） 的知识迁移到较小的模型 （称为学生模型，Student Model） 中。",
      "predicted_answer": "知识蒸馏（Knowledge Distillation, KD）是一种模型压缩技术，其核心思想是将大型模型（称为教师模型，Teacher Model）的知识迁移到较小的模型（称为学生模型，Student Model）中[1][471]。根据是否可以访问教师模型的内部结构（如参数、梯度），知识蒸馏可以分为白盒知识蒸馏和黑盒知识蒸馏两种类型[1][471]。",
      "ground_context": [
        "知识蒸馏（Knowledge Distillation, KD）是一种广泛应用的模型压缩技术，其核心思想是将大型模型 （称为教师模型，Teacher Model） 的知识迁移到较小的模型 （称为学生模型，Student Model） 中。"
      ],
      "retrieved_contexts": [
        "算，对于那些对当前输入不太重要的专家，可以直接跳过，从而减少不必要的计算量，提高推理\n速度。\n10.2.4 知识蒸馏\n知识蒸馏（Knowledge Distillation, KD）是一种广泛应用的模型压缩技术，其核心思想是将大\n型模型 （称为教师模型，Teacher Model） 的知识迁移到较小的模型 （称为学生模型，Student Model）\n中。现有研究主要关注如何高效地将大语言模型的各种能力传递到学生模型中。根据是否可以访\n问大模型的内部结构 （如参数、 梯度） ， 知识蒸馏可以分为两大类： 白盒知识蒸馏和黑盒知识蒸馏，\n如图 10.9 所示。\n白盒知识蒸馏黑盒知识蒸馏\n教师模型学生模型基于API的教师模型\n图 10.9 白盒知识蒸馏（左）和黑盒知识蒸馏（右）示意图 [471]\n白盒知识蒸馏（White-box KD）指的是利用对教师模型结构和参数的访问权限来进行蒸馏的\n方法。这种方式使得知识蒸馏能够有效地利用教师模型的中间特征和输出分布，以提升学生模型\n的性能。黑盒知识蒸馏（Black-box KD）指的是在教师模型的结构和参数不可用的情况下进行知",
        "384 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n识蒸馏的方法。通常，黑盒知识蒸馏仅使用教师模型获得的最终结果来提炼学生模型。\n1. 白盒知识蒸馏\n白盒知识蒸馏能够获取教师模型的细节信息， 因而可以采用多种策略来提高学生模型的性能。\n给定教师分布 pT (y|x) 以及由参数 θ 确定的学生分布 pS\nθ (y|x)， 标准的知识蒸馏目标 （包括针对序列\n级模型的几种变体）[515, 516] 本质上是最小化教师分布和学生分布之间近似的正向 Kullback-Leibler\ndivergence （KLD） ，记为KL[pT ||pS\nθ ]，这会迫使 pS\nθ 覆盖 pT 的所有高概率区域（mode，也成模\n态） 。 对于文本分类任务， 这种方法表现良好， 因为输出空间通常由有限的类别组成， 使得pT (y|x)\n和 pS\nθ (y|x) 的高概率区域都很少。 然而， 对于开放式文本生成任务 （大语言模型应用通常属于这种\n情况） ，输出空间要复杂得多，并且由于模型容量有限，pT (y|x) 所包含的高概率区域数量可能远\n远超过 pS\nθ (y|x) 所能表达的数量。最...",
        "况，分别对应于散度 D 设为正向 KL 散度，且学生模型生成数据比例 λ 分别为 1 和 0 的情况。也\n就是说，广义知识蒸馏允许对比例 λ 和散度进行其他选择。\n此外，TED[523] 提出了一种任务感知的逐层知识蒸馏方法。 该方法在教师模型和学生模型的每\n一层后添加过滤器，首先训练这些特定于任务的过滤器，然后在训练学生模型的过滤器时冻结教\n师模型的过滤器， 以使学生模型的输出特征能够与对应的教师过滤器输出特征对齐。MiniMoE[524]\n则通过采用专家混合（Mixture-of-Experts, MoE）模型作为学生模型，来缩小学生模型与教师模型\n之间的能力差距。KPTD[525] 提出了一种通过知识蒸馏将实体定义中的知识转移到大语言模型参数\n中的方法。该方法基于实体定义生成一个转移集，并利用这些定义对学生模型进行蒸馏，使学生\n模型的输出分布与教师模型相匹配。\n2. 黑盒知识蒸馏\n黑盒蒸馏的核心目标是在无法访问大模型内部参数的情况下，通过其输出（如分类概率或生\n成的文本） 来指导学生模型的学习。 具体而言， 学生模型可以通过模仿大模型的输出分布 （如分类\n概率分布） 来接近其行为...",
        "θ 对 pT 的空白区域（void region）赋\n予不合理的高概率[517]， 在自由运行的生成过程中， 这种现象可能会导致学生模型生成在教师分布\npT 下几乎不可能出现的样本[518]。\n针对该问题，MiniLLM[519] 采用标准的白盒知识蒸馏方法，但将正向 KLD 替换为反向 KLD，\n即 KL[pS\nθ ||pT ]。 与最小化KL[pT ||pS\nθ ] 相比， 最小化KL[pS\nθ ||pT ] 会能够引导学生分布 pS\nθ 关注教师\n分布 p 的主要高概率区域， 同时对p 的空白区域赋予较低的概率[520]。 在大语言模型的文本生成任\n务中，这意味着学生模型可以避免学习教师分布中过多的长尾变体，而是更专注于生成内容的准\n确性。 这在需要真实性和可靠性的实际场景中至关重要。 为了优化minθ KL[pS\nθ ||pT ]，MiniLLM 使\n用策略梯度法（Policy Gradient）[521] 推导目标函数的梯度，并通过以下改进措施进一步稳定和加\n速训练：单步分解以降低方差，教师混合采样以缓解奖励操纵问题，以及长度归一化以消除长度\n偏差。\n文献 [522] 则将自...",
        "将同策略模仿扩展到知识蒸馏， 文献[522] 提出了 on-policy KD。 在知识蒸馏过程中使用同策略数\n据时，学生模型会根据教师模型的输出分布，针对其自生成输出序列中的错误词元获得词元特定\n的反馈。 这形成了一种类似于在强化学习中看到的反馈循环， 有助于最小化训练-推理分布不匹配\n的问题。此外，随着学生模型在训练过程中不断改进，其生成的数据质量也会提高。给定输入 x，\n学生模型生成输出序列 y，并在中间状态 y<n 上模仿教师模型的词元级分布 pT (yn|x)。具体而言，\n同策略损失 LOD 由下式给出：\nLOD(θ) = Ex∼X\n\u0002\nEy∼pS (·|x)\n\u0002\nDKL\n\u0000\npT ∥pS\nθ\n\u0001\n(y|x)\n\u0003\u0003\n(10.9)\n类似于同策略模仿，on-policy KD 不会通过学生模型的采样分布 pS(·|x) 进行反向传播。这种\n不依赖采样的方式使得训练更加稳定，同时计算效率更高。在 on-policy KD 中，训练是在学生模\n型可能生成的输出序列上进行的。训练过程中，通过设置温度参数 γ = 1 来鼓励学生生成具有多\n样性的序列。此外，针对无标签的输入提示，由于学生..."
      ],
      "response_time": 5.359394788742065,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.7777777777777778,
        "rouge2": 0.7499999999999999,
        "rougeL": 0.7777777777777778,
        "bleu": 0.495934950025059,
        "semantic_similarity": 0.9714325666427612
      }
    },
    {
      "sample_id": "0173",
      "question": "SpecInfer 采用了什么方法来构建词元树？",
      "ground_truth": "SpecInfer 采用了一种静态扩展策略，以预设的扩展配置表示为向量 < k 1, k2, ..., km >。",
      "predicted_answer": "SpecInfer 采用了基于合并的词元树构建方法，通过整合多个小推测模型（SSM）来协同预测大语言模型的输出。这种方法利用了无监督的方法，并通过自适应增强技术对多个SSM进行联合优化，使其输出更接近大语言模型的结果。[1][2][3]",
      "ground_context": [
        "SpecInfer 采用了一种静态扩展策略，以预设的扩展配置表示为向量 < k 1, k2, ..., km >。"
      ],
      "retrieved_contexts": [
        "396 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n构建词元树。 这一方法的核心在于观察到， 当SSM 与大语言模型）出现不一致时（即两者选择的\nTop-1 词元不同） ，LLM 选择的词元通常出现在 SSM 的 Top-K 词元中， 且K 值较小。 但是， 如果直\n接在每一步都选择 Top-K 词元会导致潜在词元序列数量呈指数增长，显著增加推理延迟和内存开\n销。因此，SpecInfer 采用了一种静态扩展策略，以预设的扩展配置表示为向量 < k 1, k2, ..., km >，\n其中 m 为最大推测解码步数，ki 表示第 i 步每个词元的扩展数量。例如，扩展配置 < 2, 2, 1 > 会\n生成 4 个词元序列。\n基于合并的词元树构建通过整合多个 SSM 来协同预测大语言模型的输出。 SpecInfer 采用无\n监督方法， 通过自适应提升 （Adaptive Boosting） 对多个SSM 进行联合优化， 使它们的输出与LLM\n的结果更为一致。在此过程中，SpecInfer 利用通用文本数据集（如 OpenWebText 语料库） ，将文\n本数据转换为一系列提示样本...",
        "SpecInfer 算法的核心在于利用小模型预测目标大语言模型的输出，并将这些预测组织为词元\n树结构。词元树的每个节点表示一个候选词元序列，通过基于树的并行解码机制，同时验证所有\n候选词元序列的正确性。为最大化推测性能，需要探索极其庞大的候选词元序列搜索空间。目前\n的大语言模型通常涉及非常大的词汇表，例如， Qwen 2.5 的词汇表大小达到了 15.16 万[136]，而\nSpecInfer 平均能够正确预测接下来的 4 个词元。因此，需要处理一个包含 1516434 ≈ 5.29 × 1020\n个可能词元组合的搜索空间。\n为了解决上述问题，首先需要使用大语言模型现有的提炼、量化和/或剪枝变体，构造小推测\n模型（Small Speculative model ，SSM） ，来指导推测。使用SSM 进行推测推理的一个关键挑战在\n于， 由于SSM 通常比大语言模型小 100 - 1000 倍，SSM 与大语言模型之间的一致性本质上受到模\n型能力差距的限制。SpecInfer 通过同时考虑针对给定输入提示以树结构组织的各种词元序列，来\n最大化推测性能。分别通过利用单个 SSM 内部以及多个...",
        "个 SSM 进行全面微调， 词元那些提示样本中SSM 与 LLM 生成词元完全一致的部分； 接着， 过滤\n已词元的提示样本， 利用剩下的样本对下一个SSM 进行微调。 通过重复这一流程，SpecInfer 生成\n了一组多样化的 SSM，它们的联合输出在训练数据上能够与 LLM 的结果实现高度一致性。\nSpecInfer 使用基于树的并行解码来计算其树注意力， 为了能够在词元树上进行并行化的验证，\nSpecInfer 提出了一种树形注意力（Tree Attention）计算方法，通过构造的掩码矩阵和基于深度优\n先的 KV-cache 更新机制， 验证器可以在不增加额外存储的同时， 尽可能并行化树中每一条路径的\n解码过程。相比于朴素的逐序列或逐词元的解码方式，树形解码可以同时在内存开销和计算效率\n上达到最优。对于给定的推测词元树 N ，SpecInfer 使用基于树的并行解码来计算其树注意力，并\n生成一个输出张量 O，该张量为 N 中的每个节点 u 都包含一个词元。SpecInfer 的词元树验证器\n对照大语言模型检查推测词元的正确性 SpecInfer 同时支持贪心解码和随机采样。\n一些...",
        "一些大语言模型使用贪心解码生成词元， 即在每个解码步骤中贪心选择可能性最高的词元。 针\n对此类模型，SpecInfer 从 N 的根节点开始， 迭代地对照大语言模型的原始输出检查节点的推测结\n果。对于 N 中的节点 u，如果 u 包含一个子节点 v（即 pv = u） ，且其词元与大语言模型的输出\n匹配（即 tv = O(u)） ，那么SpecInfer 就成功推测出其下一个词元。在这种情况下， SpecInfer 完\n成对节点 u 的验证，并继续检查其子节点 v。当节点 u 不包含与大语言模型输出匹配的子节点时，\nSpecInfer 将 O(u) 作为已验证节点添加到 N 中，并终止验证过程。最后，所有已验证节点追加到\n当前生成的词元序列 V 中。 词元树验证使SpecInfer 能够机会性地解码多个词元， 同时保持与增量\n解码相同的生成性能。\n为了提高生成词元的多样性，许多大语言模型采用随机解码，即从概率分布 P (ui|U ; ΘLLM )\n中采样一个词元，其中 U = u0, . . . , ui−1 是此前生成的词元，ui 是要生成的下一个词元，ΘLLM\n表示参数化的大语言模...",
        "表示参数化的大语言模型。为了使用随机解码验证推测词元树，SpecInfer 引入了一种多步推测采\n样（Multi-step Speculative Sampling，MSS）算法来进行验证。 对于词元树N 中的非叶子节点， 对\n比多个 SSM 输出与大语言模型输出的概率， P (xs|u,ΘLLM )\nP (xs|u,ΘSSM s ) ，在一定范围之内就可以通过验证。\n在推测解码方法中，草稿词元的接受率在很大程度上取决于草稿模型与目标大语言模型输出"
      ],
      "response_time": 4.228026628494263,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.3636363636363636,
        "rouge2": 0.0,
        "rougeL": 0.3636363636363636,
        "bleu": 0.022801219815770213,
        "semantic_similarity": 0.6729699373245239
      }
    },
    {
      "sample_id": "0174",
      "question": "整个流程从什么开始？",
      "ground_truth": "整个流程从初始化一个预训练的学生模型开始。",
      "predicted_answer": "根据提供的文档，我无法回答这个问题。",
      "ground_context": [
        "整个流程从初始化一个预训练的学生模型开始，依次通过以下步骤进行："
      ],
      "retrieved_contexts": [],
      "response_time": 2.5984697341918945,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.022839165849062423,
        "semantic_similarity": 0.43486762046813965
      }
    },
    {
      "sample_id": "0175",
      "question": "FP8 的无穷大表示是什么？",
      "ground_truth": "N/A",
      "predicted_answer": "FP8 的无穷大表示为 S.11111.002 [1][2]。",
      "ground_context": [
        "无穷大（Infinity） N/A S.11111.002"
      ],
      "retrieved_contexts": [
        "388 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n表 10.1 FP8 二进制形式 [529]\n指标名 E4M3 E5M2\n指数偏置（Exponent bias） 7 15\n无穷大（Infinity） N/A S.11111.002\n非数值（NaN） S.1111.1112 S.11111.{01, 10, 11}2\n负零（Negative Zero） S.1000.0002 S.10000.002\n最大正规数（Max normal） S.1111.1102 = 1.75 × 28 = 448 S.11110.112 = 1.75 ×215 = 57,344\n最小正规数（Min normal） S.0001.0002 = 26 S.00001.002 = 214\n最大次正规数（Max subnorm） S.0000.1112 = 0.875 × 26 S.00000.112 = 0.75 × 214\n最小次正规数（Min subnorm） S.0000.0012 = 29 S.00000.012 = 216\n随着浮点数位数的降低， 舍入误差 （如 “大数吃小数” ） 变...",
        "成为下一代低精度数据表示的主流格式。相较于现有的 16 位和 32 位混合精度方案，FP8 不仅能\n够将训练速度提升一倍， 还能实现50% 到 75% 的内存和通信开销优化， 这一突破性进展为构建下\n一代大规模基础模型开辟了广阔前景。\n在本节中将首先介绍 FP8 编码方式，并在此基础上介绍基于 FP8 的大模型训练方法。\n10.3.1 FP8 编码\nFP8 是一种低精度浮点数格式，专为提高计算效率和降低存储需求而设计，广泛应用于深度\n学习模型的训练和推理中。FP8 编码采用 IEEE 浮点表示的变体，包括符号位（S，sign） 、指数位\n（E，exponent）和尾数位（M，mantissa） 。指数位决定了动态范围，而尾数位决定了表示精度。其\n关键特征是通过减少位数来降低计算复杂度和内存占用。FP8 的常见表示方式有以下几种：E5M2\n（5 位指数和 2 位尾数） 、E4M3（4 位指数和 3 位尾数） 、E3M4（3 位指数和 4 位尾数） 以及E2M5（2\n位指数和 5 位尾数） 。 通过调整指数位的数量，FP8 可以适应不同动态范围的计算需求。 由于E3M4\n和以及 E2M5 ..."
      ],
      "response_time": 2.9744856357574463,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0.0,
        "bleu": 0.0,
        "semantic_similarity": 0.4011036157608032
      }
    },
    {
      "sample_id": "0176",
      "question": "稀疏化的目标是什么？",
      "ground_truth": "其目标是通过增加模型参数或激活中零值元素的比例，降低计算复杂度和内存使用。",
      "predicted_answer": "稀疏化的目标是通过增加模型参数或激活中零值元素的比例，降低计算复杂度和内存使用。[1]",
      "ground_context": [
        "稀疏化 （Sparsification） 是一种模型压缩技术， 其目标是通过增加模型参数或激活中零值元素的比例，降低计算复杂度和内存使用。"
      ],
      "retrieved_contexts": [
        "大语言模型：从理论到实践 381\n阵。通过迭代应用量化和 SVD，LoftQ 实现了对原始权重更准确的近似，从而进一步提升模型的\n性能和适配能力。\n10.2.3 模型稀疏化\n稀疏化 （Sparsification） 是一种模型压缩技术， 其目标是通过增加模型参数或激活中零值元素\n的比例，降低计算复杂度和内存使用。稀疏化利用计算过程中对零元素的高效忽略，实现了资源\n的节约和性能的优化。在大语言模型中，稀疏化通常应用于权重参数和注意力激活。稀疏化的主\n要策略包括权重剪枝和稀疏注意力机制。稀疏注意力机制已在本书第 1 节进行了详细讨论，本节\n将重点探讨权重剪枝机制。\n权重剪枝（Weight Pruning）是一种系统地从模型中移除不那么关键的权重和结构的方法，旨\n在在预填充阶段和解码阶段减少计算和内存成本，同时不显著牺牲性能。权重剪枝方法根据剪枝\n过程的粒度可以分为两类：无结构剪枝和结构化剪枝，如图10.8所示。\n无结构剪枝粒度：权重结构化剪枝粒度：通道/组/层\n图 10.8 无结构剪枝和结构化剪枝示意图 [471]\n1. 无结构剪枝\n无结构剪枝（Unstructured pruning） ..."
      ],
      "response_time": 3.5818827152252197,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.754506780835705,
        "semantic_similarity": 0.8291143178939819
      }
    },
    {
      "sample_id": "0177",
      "question": "MSS算法是什么的验证方法？",
      "ground_truth": "MSS算法用于验证推测词元树。",
      "predicted_answer": "MSS算法是用于验证推测词元树的验证方法[1]。",
      "ground_context": [
        "为了使用随机解码验证推测词元树，SpecInfer 引入了一种多步推测采样（Multi-step Speculative Sampling，MSS）算法来进行验证。"
      ],
      "retrieved_contexts": [
        "表示参数化的大语言模型。为了使用随机解码验证推测词元树，SpecInfer 引入了一种多步推测采\n样（Multi-step Speculative Sampling，MSS）算法来进行验证。 对于词元树N 中的非叶子节点， 对\n比多个 SSM 输出与大语言模型输出的概率， P (xs|u,ΘLLM )\nP (xs|u,ΘSSM s ) ，在一定范围之内就可以通过验证。\n在推测解码方法中，草稿词元的接受率在很大程度上取决于草稿模型与目标大语言模型输出"
      ],
      "response_time": 3.3194077014923096,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.6666666666666666,
        "rouge2": 0.0,
        "rougeL": 0.6666666666666666,
        "bleu": 0.21972813874997157,
        "semantic_similarity": 0.9570242762565613
      }
    },
    {
      "sample_id": "0178",
      "question": "FP8-LM 框架是由哪些组织的研究人员开源的？",
      "ground_truth": "Microsoft Azure 和 Microsoft Research",
      "predicted_answer": "FP8-LM 框架是由 Microsoft Azure 和 Microsoft Research 的研究人员开源的。[1]",
      "ground_context": [
        "为了解决这一问题，Microsoft Azure 和 Microsoft Research 的研究人员开源了 FP8-LM 框架。"
      ],
      "retrieved_contexts": [
        "通信成本优化方面效果有限，未能充分挖掘 FP8 的潜力。为了解决这一问题，Microsoft Azure 和\nMicrosoft Research 的研究人员开源了 FP8-LM 框架 [530]。该框架提出了一种高度优化的 FP8 混合\n精度训练方法，专为大语言模型设计。其核心思想是将 FP8 的计算、存储和通信贯穿于大型模型\n训练的全过程，使前向传播和反向传播全程基于低精度 FP8，从而显著降低系统工作负载，并实\n现更高效的训练过程。2025 年 1 月，文献 [531] 提出的方法，更是将精度进一步降低到 FP4。\n使用 FP8 进行大语言模型的训练并非易事，主要面临数据下溢或上溢问题，以及因 FP8 数据\n格式动态范围较窄和精度较低而引发的量化误差，这些问题可能导致数值不稳定性，甚至在训练\n过程中出现不可逆的发散现象。 为了解决这些挑战， 文献[530] 指出， 在大语言模型的训练中， 大\n部分变量（如梯度、优化器状态）可以采用低精度数据格式，而不会影响模型的准确性，也无需\n调整超参数。 具体而言，FP8-LM 提出了三个优化级别， 通过逐步引入FP8 通信、FP8 优化器以及..."
      ],
      "response_time": 2.984455108642578,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.7272727272727273,
        "rouge2": 0.6666666666666666,
        "rougeL": 0.7272727272727273,
        "bleu": 0.17923344640485428,
        "semantic_similarity": 0.6195186376571655
      }
    },
    {
      "sample_id": "0179",
      "question": "在使用 FP8 进行大语言模型训练时，如何解决低精度全局归约操作引发的数值不稳定性问题？",
      "ground_truth": "为了解决使用 FP8 进行大语言模型训练时出现的数值不稳定性问题，可以采用后缩放方法。在全局归约过程中，先对梯度进行求和，然后在梯度收集的过程中进行除法缩放。这种方法使梯度值接近 FP8 数据类型的最大值，有效缓解了下溢问题，尽管可能引发上溢问题。",
      "predicted_answer": "为了解决低精度全局归约操作（如 FP8）引发的数据下溢和上溢问题，文献 [530] 提出了自动缩放技术 [3]。该技术通过引入一个动态变化的自动缩放因子 µ，在训练过程中对梯度值进行适应性调整，从而减少梯度中的上溢和下溢情况。具体来说，如果在梯度聚合时数值比例超过指定阈值，则缩放因子 µ 减半；相反，若该比例始终低于阈值，则逐步将 µ 按指数规律增加到原值的 2。这样可以根据实际梯度分布灵活调整 µ，确保数值稳定性。[3]",
      "ground_context": [
        "使用 FP8 进行大语言模型的训练并非易事，主要面临数据下溢或上溢问题，以及因 FP8 数据格式动态范围较窄和精度较低而引发的量化误差，这些问题可能导致数值不稳定性，甚至在训练过程中出现不可逆的发散现象。",
        "具体而言，在全局归约过程中，跨 GPU 聚合梯度通常有两种标准方法：预缩放和后缩放。后缩放方法使梯度值接近 FP8 数据类型的最大值，有效缓解了下溢问题。"
      ],
      "retrieved_contexts": [
        "大语言模型：从理论到实践 389\n持 FP8 的低位并行化，包括张量并行、流水线并行和序列并行。\n1. FP8 梯度和 AllReduce 通信\n现有的混合精度训练方法通常采用 16 位或 32 位数据类型来计算和存储梯度 [532]，这导致整\n个训练过程中集体通信对带宽的需求非常高。然而，直接将 FP8 应用于梯度会引发精度下降的问\n题，主要原因在于低精度全局归约（Low-bit All-Reduce）操作中容易出现下溢和上溢问题。\n具体而言，在全局归约过程中，跨 GPU 聚合梯度通常有两种标准方法：预缩放（Pre-scaling）\n和后缩放 （Post-scaling） 。 预缩放方法是在求和之前， 将第i 个 GPU 计算出的梯度 gi 除以 GPU 总\n数 N ，其公式如下：\ng = g1/N + g2/N + + gN /N (10.12)\n当 N 较大时， 这种除法可能导致数据下溢， 尤其是在使用FP8 低精度表示梯度时。 后缩放方法则\n先对梯度进行求和，然后在梯度收集的过程中进行除法缩放，公式为：\ng = (g1 + g2 + + gN )/N (10.13)\n后缩放方法...",
        "通信成本优化方面效果有限，未能充分挖掘 FP8 的潜力。为了解决这一问题，Microsoft Azure 和\nMicrosoft Research 的研究人员开源了 FP8-LM 框架 [530]。该框架提出了一种高度优化的 FP8 混合\n精度训练方法，专为大语言模型设计。其核心思想是将 FP8 的计算、存储和通信贯穿于大型模型\n训练的全过程，使前向传播和反向传播全程基于低精度 FP8，从而显著降低系统工作负载，并实\n现更高效的训练过程。2025 年 1 月，文献 [531] 提出的方法，更是将精度进一步降低到 FP4。\n使用 FP8 进行大语言模型的训练并非易事，主要面临数据下溢或上溢问题，以及因 FP8 数据\n格式动态范围较窄和精度较低而引发的量化误差，这些问题可能导致数值不稳定性，甚至在训练\n过程中出现不可逆的发散现象。 为了解决这些挑战， 文献[530] 指出， 在大语言模型的训练中， 大\n部分变量（如梯度、优化器状态）可以采用低精度数据格式，而不会影响模型的准确性，也无需\n调整超参数。 具体而言，FP8-LM 提出了三个优化级别， 通过逐步引入FP8 通信、FP8 优化器以及...",
        "g = (g1 + g2 + + gN )/N (10.13)\n后缩放方法使梯度值接近 FP8 数据类型的最大值，有效缓解了下溢问题。但与此同时，这种方法\n在梯度聚合时容易引发上溢问题。\n针对上述问题，FP8-LM[530] 提出了一种自动缩放 （Automatic Scaling） 技术， 以同时解决预缩\n放和后缩放方法中的下溢和上溢问题。该方法通过引入一个动态变化的自动缩放因子 µ，在训练\n过程中对梯度值进行适应性调整，从而减少梯度中上溢和下溢的情况。其核心公式为：\ng′\ni = µ · gi (10.14)\n对 g′\ni 的梯度值进行统计分析， 旨在量化在FP8 表示范围内达到最大可行值的数值比例。 如果该比\n例超过指定阈值（例如 0.001%） ，则在后续训练步骤中将缩放因子µ 减半（设置为 µ/2） ，以降低\n上溢的风险。 相反， 如果该比例始终低于阈值， 则在1000 个训练步骤的时间跨度内逐步将 µ 按指\n数规律增加到原值的 2，从而有效降低下溢风险。这种动态调整机制能够根据实际梯度分布灵活\n调整 µ，在缓解上溢和下溢问题的同时，保证 FP8 精度下的数值稳定性。\nFP8...",
        "大语言模型：从理论到实践 391\n的精度。研究提出了一个指导原则：梯度统计量可以使用较低的精度，而主权重则需要分配较高\n的精度。具体而言，一阶梯度矩能够容忍较大的量化误差，因此可以使用低精度的 FP8 格式，而\n二阶梯度矩则需要更高的精度。这是因为在 Adam 的模型更新过程中，梯度的方向比其大小更为\n关键。尽管带有张量缩放的 FP8 格式在一定程度上会引入精度损失，但它能够有效保持一阶矩的\n分布，与高精度张量几乎一致。此外，由于梯度值通常较小，在计算二阶梯度矩时对梯度进行平\n方运算可能导致数据下溢。 为了避免数值不稳定性并保持精度， 二阶梯度矩需要分配16 位的较高\n精度存储。\n另一方面，FP8-LM 的研究团队发现保持主权重使用高精度存储至关重要。其主要原因在于\n训练过程中，权重更新的幅度可能会变得极小或极大，为主权重分配更高的精度能够有效防止信\n息丢失，从而确保训练的稳定性和准确性。在实现中，主权重有两种可行的存储方案：使用FP32\n全精度或带有张量缩放的 FP16。相比之下，带有张量缩放的 FP16 在不显著降低精度的同时，可\n以显著节省内存。因此，FM8-LM 默认选择在优化...",
        "通过这一设计，FM8-LM 的 FP8 混合精度优化器在训练过程中，每个参数仅消耗 6 字节的内存：\n2|{z}\n主权重\n+ 1|{z}\n梯度\n+ 1 + 2|{z}\nAdam 状态\n= 6字节 (10.20)\n3. FP8 分布式并行训练\n训练大语言模型需要分布式学习策略， 以实现跨多GPU 的并行化。 常用的策略包括数据并行\n（Data Parallelism） 、张量并行（Tensor Parallelism） 、流水线并行（Pipeline Parallelism）以及序列并\n行（Sequence Parallelism） 。每种并行策略都有其优点，并在现有系统中以互补的方式使用。对于\n这些策略的 FP8 支持而言，数据并行和流水线并行无需进行任何特定的修改，因为在将数据批次\n或模型层拆分到不同设备时，这两种策略并不涉及额外的 FP8 计算和通信。\n张量并行将模型的单个层划分到多个设备上，使得权重、梯度和激活张量的分片分布在不同\n的 GPU 上， 而不是集中在单个GPU 上。 为了在张量并行中支持FP8，FP8-LM 将分片的权重和激\n活张量转换为 FP8 格式，用于线性层的计..."
      ],
      "response_time": 6.0393385887146,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.28571428571428575,
        "rouge2": 0.0,
        "rougeL": 0.28571428571428575,
        "bleu": 0.019404459070494776,
        "semantic_similarity": 0.8195010423660278
      }
    },
    {
      "sample_id": "0180",
      "question": "在大语言模型中，如何通过稀疏化技术和推测采样技术来优化模型的性能和生成多样化的输出？",
      "ground_truth": "稀疏化技术通过增加模型参数或激活中零值元素的比例，降低计算复杂度和内存使用，从而优化模型性能。推测采样技术则在保持输出分布等效性的同时，与核采样的概率特性一致，生成多样化的词元序列，适配解码框架。",
      "predicted_answer": "在大语言模型中，可以通过稀疏化技术和推测采样技术来优化模型的性能和生成多样化的输出。稀疏化技术主要包括权重剪枝和稀疏注意力机制，权重剪枝可以减少计算和内存成本而不显著牺牲性能[1]。推测采样技术可以在推理过程中根据输入数据的特点动态地决定是否跳过某些专家的计算，从而提高计算效率[2]。这两种方法共同作用，有助于实现模型性能的提升和输出的多样性。",
      "ground_context": [
        "稀疏化利用计算过程中对零元素的高效忽略，实现了资源的节约和性能的优化。",
        "推测采样在保持输出分布等效性的同时，与核采样的概率特性一致，从而能够生成多样化的词元序列。"
      ],
      "retrieved_contexts": [
        "大语言模型：从理论到实践 381\n阵。通过迭代应用量化和 SVD，LoftQ 实现了对原始权重更准确的近似，从而进一步提升模型的\n性能和适配能力。\n10.2.3 模型稀疏化\n稀疏化 （Sparsification） 是一种模型压缩技术， 其目标是通过增加模型参数或激活中零值元素\n的比例，降低计算复杂度和内存使用。稀疏化利用计算过程中对零元素的高效忽略，实现了资源\n的节约和性能的优化。在大语言模型中，稀疏化通常应用于权重参数和注意力激活。稀疏化的主\n要策略包括权重剪枝和稀疏注意力机制。稀疏注意力机制已在本书第 1 节进行了详细讨论，本节\n将重点探讨权重剪枝机制。\n权重剪枝（Weight Pruning）是一种系统地从模型中移除不那么关键的权重和结构的方法，旨\n在在预填充阶段和解码阶段减少计算和内存成本，同时不显著牺牲性能。权重剪枝方法根据剪枝\n过程的粒度可以分为两类：无结构剪枝和结构化剪枝，如图10.8所示。\n无结构剪枝粒度：权重结构化剪枝粒度：通道/组/层\n图 10.8 无结构剪枝和结构化剪枝示意图 [471]\n1. 无结构剪枝\n无结构剪枝（Unstructured pruning） ...",
        "大语言模型：从理论到实践 383\n配器进行渐进式结构化剪枝，使模型的固有知识得以转移，更好地保留冗余结构中的信息。同时\n引入结构稀疏优化算法，利用 LoRA 模块的信息来更新权重，提高知识保存率。\n混合专家 （MoE） 技术在大语言模型领域备受关注。 近期一些研究开始探索针对基于MoE 的\n大语言模型的专家剪枝方法。ExpertSparsity[514] 是一种专家稀疏化方法，用于 MoE 中的前馈神经\n网络专家的稀疏化。它通过计算原始输出和稀疏化后输出之间的 Frobenius 范数来量化被稀疏化\n的专家的损失。对 MoE 模型中的专家进行分层评估和剪枝，根据专家对模型整体性能的贡献程\n度，去除那些对性能影响较小的专家，以达到压缩模型和提高计算效率的目的。采用渐进式剪枝\n（Progressive Pruning）方法，逐步地对专家进行剪枝操作，在每次剪枝后评估模型性能，确保剪枝\n过程不会导致模型性能大幅下降，通过这种渐进的方式找到最优的剪枝策略。在推理过程中，采\n用了动态跳过 （Dynamic Skipping） 方法， 根据输入数据的特点动态地决定是否跳过某些专家的计\n算，对于那些...",
        "大语言模型：从理论到实践 247\n更专注于学习高级视觉语义， 而非过度依赖OCR 任务， 从而提升模型在对象和场景相关零样本任\n务中的性能。\n基于预训练 VLM 的剪枝方法是目前提升数据质量与模型训练效率的最有效策略之一。这些\n方法通过计算图文对的嵌入相似性来评估对齐程度。其中，CLIPScore [310] 依托预训练的 CLIP 模\n型，计算图像和文本嵌入之间的余弦相似度，并据此对图文对进行排序；LAION 的筛选策略基于\n由 4 亿对图文对训练的 OpenAI CLIP 模型，对大规模网络数据集进行对齐评估，并过滤得分最低\n的样本；T-MARS 方法[311] 在计算 CLIPScore 前， 通过检测并遮盖图像中的文本区域， 从而提升对\n齐分数的准确性； 而Sieve [312] 则利用在小而精的数据集上预训练的生成式图像描述模型， 有效减\n少了 CLIPScore 排序中的误判（如高分或低分错误） 。 通过这些优化策略， 图文对的筛选变得更加\n精准，显著提高了数据质量和模型性能。\n多样化且平衡的数据集是提升多模态大语言模型泛化能力的核心因素 [313]。为构建这样的数\n据集，...",
        "批量解码或并行解码）以降低内存访问成本。这些优化策略将在后续章节中进一步探讨。\n10.2 模型优化\n模型优化是提升大语言模型推理效率的重要手段，主要集中在优化模型结构和模型压缩两方\n面。 模型结构优化通过设计高效的模型结构直接提升效率， 包括高效FFN 设计、 注意力机制优化、\nMoE 架构设计、Transformer 代替架构设计等， 这些内容大部分都在本书第二章大语言模型基础部\n分进行了介绍。模型压缩则涵盖了多种技术， 旨在通过修改模型的数据表示（例如量化） 、 改变其\n架构（例如稀疏化、结构优化等）或者知识蒸馏来提高预训练模型的推理效率。"
      ],
      "response_time": 5.512752056121826,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.0593886960928275,
        "semantic_similarity": 0.871578574180603
      }
    },
    {
      "sample_id": "0181",
      "question": "在训练大型语言模型时，如何通过知识蒸馏和并行计算来提高学生模型的生成准确性和训练效率？",
      "ground_truth": "通过采用反向 KLD 的知识蒸馏方法，MiniLLM 引导学生模型更加关注教师分布的高概率区域，从而提升生成内容的准确性。同时，使用 FP8 格式进行前向计算和反向梯度通信，以及序列并行和张量并行技术，可以有效节省内存和提高训练效率。",
      "predicted_answer": "在训练大型语言模型时，可以通过知识蒸馏来提高学生模型的生成准确性和训练效率。具体来说，白盒知识蒸馏能够获取教师模型的详细信息，因此可以采用多种策略来提升学生模型的表现[3][4]。例如，在白盒知识蒸馏中，标准的目标函数是通过最小化教师分布和学生分布之间的KL散度来实现的，这种方法可以使学生模型覆盖教师模型的所有高概率区域[1]。\n\n另外，通过使用同策略模仿扩展知识蒸馏（on-policy KD），可以在知识蒸馏过程中提供词元级别的反馈，帮助学生模型更精确地模仿教师模型的行为，进一步提高生成准确性[2]。这种反馈循环也有助于解决训练-推理分布不匹配的问题，同时提高了训练过程的稳定性[2]。\n\n尽管这些方法能够显著提升学生模型的性能，但在实际操作中，还需要考虑计算资源的限制。为此，可以结合并行计算技术来加速训练过程。例如，MiniMoE[4] 使用专家混合模型作为学生模型，能够在保持较高性能的同时降低计算成本，从而提高训练效率[4]。",
      "ground_context": [
        "θ 对 pT 的空白区域（void region）赋予不合理的高概率， 在自由运行的生成过程中， 这种现象可能会导致学生模型生成在教师分布 pT 下几乎不可能出现的样本。",
        "序列并行通过将输入序列拆分为多个子序列，并将这些子序列分配到不同的设备上， 从而有效节省激活内存。"
      ],
      "retrieved_contexts": [
        "384 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n识蒸馏的方法。通常，黑盒知识蒸馏仅使用教师模型获得的最终结果来提炼学生模型。\n1. 白盒知识蒸馏\n白盒知识蒸馏能够获取教师模型的细节信息， 因而可以采用多种策略来提高学生模型的性能。\n给定教师分布 pT (y|x) 以及由参数 θ 确定的学生分布 pS\nθ (y|x)， 标准的知识蒸馏目标 （包括针对序列\n级模型的几种变体）[515, 516] 本质上是最小化教师分布和学生分布之间近似的正向 Kullback-Leibler\ndivergence （KLD） ，记为KL[pT ||pS\nθ ]，这会迫使 pS\nθ 覆盖 pT 的所有高概率区域（mode，也成模\n态） 。 对于文本分类任务， 这种方法表现良好， 因为输出空间通常由有限的类别组成， 使得pT (y|x)\n和 pS\nθ (y|x) 的高概率区域都很少。 然而， 对于开放式文本生成任务 （大语言模型应用通常属于这种\n情况） ，输出空间要复杂得多，并且由于模型容量有限，pT (y|x) 所包含的高概率区域数量可能远\n远超过 pS\nθ (y|x) 所能表达的数量。最...",
        "将同策略模仿扩展到知识蒸馏， 文献[522] 提出了 on-policy KD。 在知识蒸馏过程中使用同策略数\n据时，学生模型会根据教师模型的输出分布，针对其自生成输出序列中的错误词元获得词元特定\n的反馈。 这形成了一种类似于在强化学习中看到的反馈循环， 有助于最小化训练-推理分布不匹配\n的问题。此外，随着学生模型在训练过程中不断改进，其生成的数据质量也会提高。给定输入 x，\n学生模型生成输出序列 y，并在中间状态 y<n 上模仿教师模型的词元级分布 pT (yn|x)。具体而言，\n同策略损失 LOD 由下式给出：\nLOD(θ) = Ex∼X\n\u0002\nEy∼pS (·|x)\n\u0002\nDKL\n\u0000\npT ∥pS\nθ\n\u0001\n(y|x)\n\u0003\u0003\n(10.9)\n类似于同策略模仿，on-policy KD 不会通过学生模型的采样分布 pS(·|x) 进行反向传播。这种\n不依赖采样的方式使得训练更加稳定，同时计算效率更高。在 on-policy KD 中，训练是在学生模\n型可能生成的输出序列上进行的。训练过程中，通过设置温度参数 γ = 1 来鼓励学生生成具有多\n样性的序列。此外，针对无标签的输入提示，由于学生...",
        "算，对于那些对当前输入不太重要的专家，可以直接跳过，从而减少不必要的计算量，提高推理\n速度。\n10.2.4 知识蒸馏\n知识蒸馏（Knowledge Distillation, KD）是一种广泛应用的模型压缩技术，其核心思想是将大\n型模型 （称为教师模型，Teacher Model） 的知识迁移到较小的模型 （称为学生模型，Student Model）\n中。现有研究主要关注如何高效地将大语言模型的各种能力传递到学生模型中。根据是否可以访\n问大模型的内部结构 （如参数、 梯度） ， 知识蒸馏可以分为两大类： 白盒知识蒸馏和黑盒知识蒸馏，\n如图 10.9 所示。\n白盒知识蒸馏黑盒知识蒸馏\n教师模型学生模型基于API的教师模型\n图 10.9 白盒知识蒸馏（左）和黑盒知识蒸馏（右）示意图 [471]\n白盒知识蒸馏（White-box KD）指的是利用对教师模型结构和参数的访问权限来进行蒸馏的\n方法。这种方式使得知识蒸馏能够有效地利用教师模型的中间特征和输出分布，以提升学生模型\n的性能。黑盒知识蒸馏（Black-box KD）指的是在教师模型的结构和参数不可用的情况下进行知",
        "况，分别对应于散度 D 设为正向 KL 散度，且学生模型生成数据比例 λ 分别为 1 和 0 的情况。也\n就是说，广义知识蒸馏允许对比例 λ 和散度进行其他选择。\n此外，TED[523] 提出了一种任务感知的逐层知识蒸馏方法。 该方法在教师模型和学生模型的每\n一层后添加过滤器，首先训练这些特定于任务的过滤器，然后在训练学生模型的过滤器时冻结教\n师模型的过滤器， 以使学生模型的输出特征能够与对应的教师过滤器输出特征对齐。MiniMoE[524]\n则通过采用专家混合（Mixture-of-Experts, MoE）模型作为学生模型，来缩小学生模型与教师模型\n之间的能力差距。KPTD[525] 提出了一种通过知识蒸馏将实体定义中的知识转移到大语言模型参数\n中的方法。该方法基于实体定义生成一个转移集，并利用这些定义对学生模型进行蒸馏，使学生\n模型的输出分布与教师模型相匹配。\n2. 黑盒知识蒸馏\n黑盒蒸馏的核心目标是在无法访问大模型内部参数的情况下，通过其输出（如分类概率或生\n成的文本） 来指导学生模型的学习。 具体而言， 学生模型可以通过模仿大模型的输出分布 （如分类\n概率分布） 来接近其行为...",
        "概率分布） 来接近其行为， 从而实现性能的压缩与迁移。 此外， 学生模型还可以在大模型的指导下\n学习特定任务能力或者大语言模型的泛化能力，包括上下文学习（ICL）能力[526]、思维链（CoT）\n推理能力[395] 以及指令跟随（IF）能力[24] 等。\nTAPIR[527]（Task-Aware Curriculum Planning for Instruction Refinement） 框架通过多任务课程规\n划，蒸馏黑盒大语言模型的指令回答能力。它利用教师大模型挑选学生模型难以遵循的指令，进\n行难度重采样，从而提升学生模型的学习效果。同时，为了平衡学生模型的多任务技能， TAPIR\n对训练集中的任务配比进行调整，重新分配任务多样性分布，并根据多任务特点自动优化教师模\n型的回答风格。此外，通过引入课程规划机制，TAPIR 框架系统地提高任务难度级别，逐步增强\n学生大语言模型的能力。TAPIR 框架整体结构如图10.10所示。"
      ],
      "response_time": 8.445947885513306,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0.0,
        "bleu": 0.02070202960687307,
        "semantic_similarity": 0.7590945959091187
      }
    },
    {
      "sample_id": "0182",
      "question": "MedQA是什么？",
      "ground_truth": "MedQA是美国医学执业考试中的医学知识。",
      "predicted_answer": "MedQA是一种用于美国医学执业考试中的医学知识问答数据集，包含问题和4∼5个选项的答案[1][2]。",
      "ground_context": [
        "MedQA（USMLE）问题 + 答案（4 ∼ 5 个选项） 11450/1273 美国医学执业考试中的医学知识"
      ],
      "retrieved_contexts": [
        "MedQA\n（USMLE）\n问题 + 答案\n（4 ∼ 5 个选项） 11450/1273 美国医学执业考试中的医学知识\nMedMCQA\n（AIIMS/NEET）\n问题 + 答案\n（4 个选项和解释） 18.7 万/6100 印度医学入学考试中的医学知识\nPubMedQA\n问题 + 上下文 + 答案\n（Y es/No/Maybe）\n（长回答）\n500/500\n标注 QA 对 1000\n无标注数据 6.12 万\n生物医学科学文献\nMMLU 问题 + 答案\n（4 个选项） 123/1089 涵盖解剖学、 临床知识、 大学医学、 医\n学遗传学、专业医学和大学生物学\nLiveQA\nTREC-2017\n问题 + 长答案\n（参考标注答案） 634/104 用户经常询问的一般医学知识\nMedicationQA 问题 + 长答案 NA/674 用户经常询问的药物知识\nHealthSearchQA 问题 + 手册\n专业解释 3375 用户经常搜索的医学知识\n11.3 大语言模型评估方法\n在大语言模型评估体系和数据集构建的基础上，评估方法需要解决如何评估的问题，包括采\n用哪些评估指标，以及如何进行评估等。本...",
        "420 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n语言模型在医学问题回答上的能力[585]， 包括阅读理解能力、 准确回忆医学知识并使用专业知识的\n能力。目前已有一些医疗相关数据集，分别评估了不同方面，包括医学考试题评估集 MedQA[591]\n和 MedMCQA[592]，医学研究问题评估集 PubMedQA[593]，以及面向普通用户的医学信息需求评估\n集 LiveQA[594] 等。文献 [585] 提出了 MultiMedQA 数据集，集成了 6 种已有医疗问答数据集，题\n型涵盖多项选择、长篇问答等，包括 MedQA[591]、MedMCQA[592]、PubMedQA[593]、MMLU[573]、\nLiveQA[594] 和 MedicationQA[595]。在此基础上根据常见健康查询构建了 HealthSearchQA 数据集。\nMultiMedQA[585] 评估集中所包含的数据集、题目类型、数据量等信息如表11.2 所示。\n表 11.2 MultiMedQA [585] 评估集中所包含的数据集、题目类型、数据量等信息\n数据集 题目类型 数据量（开发/..."
      ],
      "response_time": 3.6653387546539307,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.33333333333333337,
        "rouge2": 0.0,
        "rougeL": 0.33333333333333337,
        "bleu": 0.20068932752887542,
        "semantic_similarity": 0.8794950246810913
      }
    },
    {
      "sample_id": "0183",
      "question": "vLLM 可以支持哪些常用模型？",
      "ground_truth": "vLLM 可以支持 Aquila、Baichuan、BLOOM、Falcon、GPT-2、InternLM、LLaMA、LLaMA-2 等常用模型。",
      "predicted_answer": "vLLM 可以支持 Aquila、Baichuan、BLOOM、Falcon、GPT-2、InternLM、LLaMA、LLaMA-2 等常用模型。[1]",
      "ground_context": [
        "vLLM 可以支持 Aquila、Baichuan、BLOOM、Falcon、GPT-2、InternLM、LLaMA、LLaMA-2等常用模型，使用方式也非常简单，不用对原始模型进行任何修改。"
      ],
      "retrieved_contexts": [
        "402 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n多模态环境中表现卓越。综合来看，vLLM V1 的优化涵盖了执行效率、缓存管理、推理架构和多\n模态支持，为复杂推理场景提供了更加高效、灵活和可扩展的解决方案。\nvLLM 可以支持 Aquila、Baichuan、BLOOM、Falcon、GPT-2、InternLM、LLaMA、LLaMA-2\n等常用模型，使用方式也非常简单，不用对原始模型进行任何修改。以 OPT-125M 模型为例，可\n以使用如下代码进行推理应用：\nfrom vllm import LLM, SamplingParams\n#ิൕဢ২\nprompts = [\n\"Hello, my name is\" ,\n\"The president of the United States is\" ,\n\"The capital of France is\" ,\n\"The future of AI is\" ,\n]\n#ࡹsamplingའ\nsampling_params = SamplingParams(temperature=0.8, top_p =0.95)\n#նე࿽ଆ྘\n...",
        "则是基于预训练的映射方法利用大语言模型与图像编码器之间的映射关系，降低了从零开始训练\n的计算成本。 值得注意的是， 这些训练范式并非相互排斥， 许多VLM 结合了对比、 掩码和生成等\n多种方法，以实现更强大的表现能力。本节将分别介绍上述四种模型架构进行介绍。\n1. 对比学习\n在机器学习领域中，对比学习框架应用于众多方面。在视觉语言模型的训练过程中，对比学\n习通过正例对和负例对来优化模型。其训练目标是使模型能够为正例对生成相似的表示，同时为\n负例对生成差异化的表示，如图7.2所示。\n图像编码器文本编码器\n推动相似推动远离\nA photo of a dogA photo of a birdA photo of a cat……\n图 7.2 视觉语言模型对比学习范式 [271]\n这一技术路线可以追溯到 LeCun 等学者于 2006 年提出的基于能量的模型 （Energy-Based Mod-\nels，EBM）研究[272]。该方法的核心思想是构建一个由参数 θ 定义的系统，该系统会对观测数据\n施加负向影响（低能量状态） ，同时对未观测的数据施加正向影响（即高能量状态） 。理想情况下，\n来自目...",
        "大语言模型：从理论到实践 401\n将大语言模型计算图的运算分割为多个阶段，并在不同设备上以流水线方式执行，因此 FastServe\n需要同时处理分布式引擎中的多个批次。 由于键值缓存占据了GPU 显存的很大一部分， 因此在分\n布式服务中，FastServe 的键值缓存也被分割到多块 GPU 上。 在大语言模型推理中， 每个键值张量\n都由大语言模型的同一阶段使用。因此，FastServe 按照张量并行的要求对键值张量进行分割，并\n将每个键值张量分配给相应的 GPU，以便 GPU 上的所有计算只使用本地的键值张量。\n10.5 vLLM 推理框架实践\nvLLM 是由加州大学伯克利分校开发，并在 Chatbot Arena 和 Vicuna Demo 上部署使用的大\n语言模型推理服务开源框架。 vLLM 利用 PagedAttention 注意力算法，有效地管理注意力的键和\n值。vLLM 的吞吐量是 HuggingFace transformers 的 24 倍，并且无须进行任何模型架构的更改。\nPagedAttention 注意力算法的主要目标是解决键值缓存的管理问题。PagedAttent..."
      ],
      "response_time": 3.655524492263794,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.9565217391304348,
        "rouge2": 0.9523809523809523,
        "rougeL": 0.9565217391304348,
        "bleu": 0.8907171682201394,
        "semantic_similarity": 0.9832810163497925
      }
    },
    {
      "sample_id": "0184",
      "question": "大语言模型评估分为哪三个大的方面？",
      "ground_truth": "知识与能力、伦理与安全，以及垂直领域评估。",
      "predicted_answer": "大语言模型评估分为三个大的方面：知识与能力、伦理与安全，以及垂直领域评估[1]。",
      "ground_context": [
        "从整体上可以将大语言模型评估分为三个大的方面：知识与能力、伦理与安全，以及垂直领域评估。"
      ],
      "retrieved_contexts": [
        "大规模语言模型：从理论到实践 407\n不同的评估体系和方法，并且对于不同阶段的模型应该独立进行评估。\n11.2 大语言模型评估体系\n传统的自然语言处理算法通常需要针对不同任务独立设计和训练。而大语言模型则不同，它\n采用单一模型，却能够执行多种复杂的自然语言处理任务。例如，同一个大语言模型可以用于机\n器翻译、文本摘要、情感分析、对话生成等多个任务。因此，在大语言模型评估中，首先需要解\n决的就是构建评估体系的问题。 从整体上可以将大语言模型评估分为三个大的方面：知识与能力、\n伦理与安全，以及垂直领域评估。\n11.2.1 知识与能力\n大语言模型具有丰富的知识和解决多种任务的能力，包括自然语言理解（例如文本分类、信\n息抽取、情感分析、语义匹配等） 、知识问答（例如阅读理解、开放领域问答等） 、自然语言生成\n（例如机器翻译、文本摘要、文本创作等） 、逻辑推理（例如数学解题、文本蕴含） 、代码生成等。\n知识与能力评估体系主要分为两大类：一类是以任务为核心的评估体系；一类是以人为核心的评\n估体系。\n1. 以任务为核心的评估体系\nHELM 评估[557] 构造了 42 类评估场景（Scenario） ...",
        "424 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n4. 大语言模型评估指标体系\n通过本节的前述内容，可以看到传统的自然语言处理评估大多针对单一任务设置不同的评估\n指标和方法。 大语言模型在经过指令微调和强化学习阶段后， 可以完成非常多不同种类的任务， 对\n于常见的自然语言理解或生成任务可以采用原有指标体系。虽然大语言模型在文本生成类任务上\n取得了突破性的进展，但是问题回答、文章生成、开放对话等文本生成类任务在此前并没有很好\n的评估指标， 因此， 针对大语言模型在文本生成方面的能力， 需要考虑建立新的评估指标体系。 为\n了更全面地评估大语言模型所生成的文本的质量，需要从三方面进行评估，包括语言层面、语义\n层面和知识层面。\n（1）语言层面的评估是评估大语言模型所生成文本质量的基础，要求生成的文本必须符合人\n类的语言习惯。这意味着生成的文本必须具有正确的词法、语法和篇章结构。具体如下：\n• 词法正确性： 评估生成文本中单词的拼写、 使用和形态变化是否正确。 确保单词拼写准确无\n误，不含有拼写错误。同时，评估单词的使用是否恰当，包括单词的含义、词性和用法等方\n面， 以确保单词在...",
        "11. 大语言模型评估\n大语言模型飞速发展，自 ChatGPT 于 2022 年 11 月底发布以来，国内外已相继发布了数百种\n开源和闭源的大语言模型。大语言模型在自然语言处理研究和人们的日常生活中扮演着越来越重\n要的角色。因此，如何评估大语言模型变得愈发关键。我们需要在技术和任务层面对大语言模型\n之间的优劣加以判断，也需要在社会层面对大语言模型可能带来的潜在风险进行评估。大语言模\n型与以往仅能完成单一任务的自然语言处理算法不同，它可以通过单一模型执行多种复杂的自然\n语言处理任务。因此，之前针对单一任务的自然语言处理算法评估方法并不适用于大语言模型的\n评估。如何构建大语言模型评估体系和评估方法是一个重要的研究问题。\n本章将首先介绍大语言模型评估的基本概念和难点， 并在此基础上从大语言模型评估体系、 大\n语言模型评估方法，以及大语言模型评估实践三个方面分别展开介绍。\n11.1 模型评估概述\n模型评估（Model Evaluation） ， 也称模型评价， 目标是评估模型在未见过的数据 （Unseen Data）\n上的泛化能力和预测准确性，以便更好地了解模型在真实场景中的表现。模型评估是在...",
        "大规模语言模型：从理论到实践 433\n11.4 大语言模型评估实践\n大语言模型的评估伴随着大语言模型研究同步飞速发展，大量针对不同任务、采用不同指标\n和方法的大语言模型评估不断涌现。本章前面几节分别针对大语言模型评估体系、评估指标和评\n估方法从不同方面介绍了当前大语言模型评估面临的问题，试图回答要从哪些方面评估大语言模\n型，以及如何评估大语言模型这两个核心问题。针对大语言模型构建不同阶段所产生的模型能力\n的不同，本节将分别介绍当前常见的针对基础模型、SFT 模型和 RL 模型的整体评估方案。\n11.4.1 基础模型评估\n大语言模型构建过程中产生的基础模型就是语言模型， 其目标就是建模自然语言的概率分布。\n语言模型构建了长文本的建模能力，使得模型可以根据输入的提示词生成文本补全句子。2020 年\nOpenAI 的研究人员在 1750 亿个参数的 GPT-3 模型上研究发现，在语境学习范式下，大语言模型\n可以根据少量给定的数据，在不调整模型参数的情况下，在很多自然语言处理任务上取得不错的\n效果[13]。图11.12 展示了不同参数量的大语言模型在简单任务中基于语境学习的表现。这个任务\n要求...",
        "158 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n三个维度对其进行分类：垃圾检测、指令遵循情况以及回答质量。回答质量评分又被细分为五个\n方面，包括质量、创造性、幽默性、礼貌性和无害性，并采用五点李克特量表进行打分。\n文献 [190] 对各类数据质量评价方法的影响模型训练的效果进行了评测。通过对比不同数据\n质量评价方法，使用包括 LLaMA-7B、LLaMA2-7B、LLaMA2-13B 以及 Mistral-7B 等在内的模型\n进行训练， 利用ARC、HellaSwag、MMLU、AlpacaEval 等评测集合进行评价。 从实验结果中可以\n看到，基于数据质量选择的方法即使在小规模数据情况下也能与使用全量训练的结果相匹配，并\n且优于从原始数据中随机选择子集的结果。比如在 Alpaca 数据集上，使用文献 [201] 提出的基于\n模型的 IFD 质量评价方法，仅选取 5% 的数据，就能够在 ARC、HellaSwag 以及 AlpacaEval 等评\n测集合上超过使用全量数据进行训练的结果。这可以反映出，指令数据的质量对于指令微调的效\n果有重要影响。\n2. 数据多样性\n数..."
      ],
      "response_time": 3.764779567718506,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.43519262616259946,
        "semantic_similarity": 0.8047323226928711
      }
    },
    {
      "sample_id": "0185",
      "question": "每年参与高考的人数是多少？",
      "ground_truth": "1200 万",
      "predicted_answer": "每年参与高考的人数是1200万[559]。",
      "ground_context": [
        "Gaokao（高考） 1200 万 中文"
      ],
      "retrieved_contexts": [
        "410 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n表 11.1 AGIEval 评估选择的任务和基本信息 [559]\n考试名称 每年参与人数 语言 任务名 评估条目（个）\nGaokao（高考） 1200 万 中文\nGK-geography 199\nGK-biology 210\nGK-history 243\nGK-chemistry 207\nGK-physics 200\nGK-En 306\nGK-Ch 246\nGK-Math-QA 351\nGK-Math-Cloze 118\nSA T 170 万 英语 SA T-En. 206\nSA T-Math 220\nLawyer Qualification Test\n（律师资格考试） 82 万 中文 JEC-QA-KD 1000\nJEC-QA-CA 1000\nLaw School Admission Test\n（LSA T） 17 万 英文\nLSA T-AR Law-Analytics 230\nLSA T-LR Law-Logic 510\nLSA T-RC Law-Reading 260\nCivil Service Examinat..."
      ],
      "response_time": 3.3262860774993896,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.6666666666666666,
        "rouge2": 0.0,
        "rougeL": 0.6666666666666666,
        "bleu": 0.04774753205784679,
        "semantic_similarity": 0.5590332746505737
      }
    },
    {
      "sample_id": "0186",
      "question": "评估数据集和训练数据集应该是什么关系？",
      "ground_truth": "相互独立的，以避免数据泄露的问题。",
      "predicted_answer": "评估数据集和训练数据集应该是相互独立的，以避免数据泄露的问题[1]。",
      "ground_context": [
        "评估数据集和训练数据集应该是相互独立的，以避免数据泄露的问题。"
      ],
      "retrieved_contexts": [
        "则用于最终评估模型的性能。评估数据集和训练数据集应该是相互独立的，以避免数据泄露的问\n题。此外，数据集选择还需要具有代表性，应该能够很好地代表模型在实际应用中可能遇到的数\n据。这意味着它应该涵盖各种情况和样本，以便模型在各种情况下都能表现良好。评估数据集的\n规模也应该足够大，以充分评估模型的性能。此外，评估数据集中应该包含一些特殊情况的样本，\n以确保模型在处理异常或边缘情况时仍具有良好的性能。\n大语言模型评估同样涉及数据集选择问题，但是大语言模型可以在单一模型中完成自然语言\n理解、逻辑推理、自然语言生成、多语言处理等任务。因此，如何构造大语言模型的评估数据集\n也是需要研究的问题。此外，由于大语言模型本身涉及语言模型训练、有监督微调、强化学习等\n多个阶段，每个阶段所产出的模型目标并不相同，因此，对于不同阶段的大语言模型也需要采用",
        "完成之后的一个必不可少的步骤。 目前， 针对单一任务的自然语言处理算法， 通常需要构造独立于\n训练数据的评估数据集，使用合适的评估函数对模型在实际应用中的效果进行预测。由于并不能\n完整了解数据的真实分布，因此简单地采用与训练数据独立同分布的方法构造的评估数据集，在\n很多情况下并不能完整地反映模型的真实情况。图11.1 为模型评估难点示意图，针对相同的训练\n数据，采用不同的算法或者超参数得到 4 个不同的分类器，可以看到，如果不能获取数据的真实\n分布，或者测试数据采样不够充分，分类器在真实使用中的效果就不能很好地通过上述方法进行\n评估。\n在模型评估的过程中，通常会使用一系列 评估指标（Evaluation Metrics）来衡量模型的表现，\n如准确率、精确率、召回率、F1 分数、ROC 曲线和 AUC 等。这些指标根据具体的任务和应用场\n景可能会有所不同。例如，在分类任务中，常用的评估指标包括准确率、精确率、召回率、F1 分\n数等；而在回归任务中，常用的评估指标包括均方误差和平均绝对误差等。但是对于文本生成类",
        "时的表现，以及系统在信息不足或不确定的情况下是否能够拒绝生成错误回答。\n9.5.3 评估数据集\n评估数据集是 RAG 系统性能评估中的关键组成部分，其质量和多样性直接影响评估结果的\n准确性和全面性。 在评估RAG 系统时， 数据集的选择与构建需要兼顾系统的检索能力、 生成能力\n以及整体协作表现。现有评估数据集的来源和构造方法多种多样，既包括基于已有资源的数据集，\n也包括为特定评估目标生成的全新数据集。不同的基准系统选择了不同的数据集策略，以适应各\n自的评估需求和应用场景。\n1. 基于现有资源的数据集\n许多评估框架依赖于已有的成熟数据集，如 KILT（Knowledge Intensive Language Tasks ）基\n准[457] 和 SuperGLUE[458] 数据集。 这些数据集涵盖了多种知识密集型任务。 例如：Natural Questions\n(NQ)[459] 提供开放域问答任务的数据， 测试系统对自然语言查询的回答能力。HotpotQA[444] 包含多\n跳问答任务， 要求RAG 系统能够从多个文档中综合信息来回答复杂问题。FEVER[460] 专注于事实验\n证任务..."
      ],
      "response_time": 3.5854103565216064,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.40188875403207,
        "semantic_similarity": 0.7718570232391357
      }
    },
    {
      "sample_id": "0187",
      "question": "CUAD 数据集中包含多少份合同？",
      "ground_truth": "500 多份合同",
      "predicted_answer": "CUAD 数据集中包含 510 份合同[3]。",
      "ground_context": [
        "CUAD 数据集中包括 500 多份合同， 每份合同都经过The Atticus Project 法律专家的精心标记， 以识别41 种不同类型的重要条款， 总共有超过13000 个标注。"
      ],
      "retrieved_contexts": [
        "少量重要内容，需要律师进行审查或分析，特别是要识别包含重要义务或警示条款的内容。对于\n法律专业人员来说，手动筛选长合同以找到这些少数关键条款可能既费时又昂贵，尤其是考虑到\n一份合同可能有数十页甚至超过 100 页。CUAD 数据集中包括 500 多份合同， 每份合同都经过The\nAtticus Project 法律专家的精心标记， 以识别41 种不同类型的重要条款， 总共有超过13000 个标注。\n判决预测是指根据事实描述预测法律判决结果，这也是法律人工智能（LegalAI）领域的关键\n应用之一。CAIL2018[588] 是针对该任务构建的大规模刑事判决预测数据集，包含 260 万个刑事案\n件， 涉及183 个刑法条文，202 个不同判决和监禁期限。 由于CAIL2018 数据集中的数据相对较短，\n并且只涉及刑事案件，文献 [586] 提出了 CAIL-Long 数据集，其中包含与现实世界中相同长度分\n布的民事和刑事案件。民事案件的平均长度达到了 1286.88 个汉字，刑事案件的平均长度也达到\n了 916.57 个汉字。整个数据集包括 1129053 个刑事案件和 1099605 ...",
        "大规模语言模型：从理论到实践 67\n疗对话数据。中文数据集包含 340 万条医生-患者对话，覆盖 172 个疾病领域，而英文数据集包含\n26 万条对话，覆盖 96 个疾病领域。\n法律领域也包含许多可用于模型训练的数据资源， 主要包括法律法规、 裁判文书等法律数据。\n这些数据通常可以从相关官方网站下载获得，且数据规模较大，能够为大模型提供大量的法律专\n业知识。 此外， 还还通过收集司法考试题目、 法律咨询、 法律问答等相关数据， 这类数据涉及了真\n实用户的法律需求与基于法律专业知识的解答。CUAD[114] 是一个包含 510 个商业法律合同、 超过\n1.3 万个标注的合同审查数据集， 由数十名法律专业人士和机器学习研究人员共同创建， 通过法律\n专业人士对这些合同数据进行扩充和详细标注。TigerBot-law[99] 则汇集了 11 类中国法律法规，以\n及一些多类别语料库，还纳入了从法律相关网站抓取的数据。\n3.2 数据处理\n大语言模型的相关研究表明，数据质量对于模型的影响非常大。因此，在收集了各种类型的\n数据之后， 需要对数据进行处理， 去除低质量数据、 重复数据、 有害信息、 个人...",
        "大规模语言模型：从理论到实践 419\n非常重要。 大语言模型在这些任务上的效果， 可以在一定程度上反映模型在工具使用方面的能力。\n除此之外，API-Bank[584] 针对 53 种常见的 API 工具， 标记了264 个对话， 共包含568 个 API 调用。\n针对模型使用外部工具的能力直接进行评估。\n3. 特定领域\n目前大语言模型研究除在通用领域之外， 也针对特定领域开展工作， 例如医疗[585]、 法律[429, 586]、\n财经[587] 等。如何针对特定领域的大语言模型进行评估也是重要的课题。针对特定领域，通常利\n用大语言模型完成有针对性的任务。 例如， 在法律人工智能 （Legal Artificial Intelligence，LegalAI）\n领域， 完成合同审查、 判决预测、 案例检索、 法律文书阅读理解等任务。 针对不同的领域任务， 需\n要构建不同的评估数据集和方法。\nContract Understanding Atticus Dataset（CUAD）[114] 是用于合同审查的数据集。 合同通常包含\n少量重要内容，需要律师进行审查或分析，特别是要识别包含重要义..."
      ],
      "response_time": 3.4143760204315186,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0.0,
        "bleu": 0.021598265052211615,
        "semantic_similarity": 0.7568454742431641
      }
    },
    {
      "sample_id": "0188",
      "question": "在评估文本生成类任务时，如何选择合适的数据集以确保评估的有效性和代表性？",
      "ground_truth": "评估数据集和训练数据集应该是相互独立的，以避免数据泄露的问题。此外，数据集选择还需要具有代表性，应该能够很好地代表模型在实际应用中可能遇到的情况，这意味着它应该涵盖各种情况和样本，以便模型在各种情况下都能表现良好。评估数据集的规模也应该足够大，以充分评估模型的性能，并且应该包含一些特殊情况的样本，以确保模型在处理异常或边缘情况时仍具有良好的性能。",
      "predicted_answer": "在评估文本生成类任务时，为了确保评估的有效性和代表性，应当选择相互独立且具有代表性的评估数据集[2]。这意呸着评估数据集应涵盖各种情况和样本，以便模型能够在各种条件下表现出色[2]。同时，评估数据集的规模应该足够大，以充分评估模型的性能[2]。此外，数据集中还应包含一些特殊的情况样本，以确保模型在处理异常或边缘情况时仍能保持良好的性能[2]。",
      "ground_context": [
        "评估数据集和训练数据集应该是相互独立的，以避免数据泄露的问题。此外，数据集选择还需要具有代表性，应该能够很好地代表模型在实际应用中可能遇到的情况。",
        "大语言模型评估同样涉及数据集选择问题，但是大语言模型可以在单一模型中完成自然语言理解、逻辑推理、自然语言生成、多语言处理等任务。"
      ],
      "retrieved_contexts": [
        "406 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n任务（例如机器翻译、文本摘要等） ，自动评估仍然是亟待解决的问题。\n图 11.1 模型评估难点示意图 [555]\n文本生成类任务的评估难点主要源于语言的灵活性和多样性， 同样一句话可以有非常多种表述方\n法。 对文本生成类任务进行评估可以采用人工评估和半自动评估方法。 以机器翻译评估为例， 人工评\n估虽然是相对准确的一种方式， 但是其成本高昂， 根据艾伦人工智能研究院 （AI2）GENIE 人工评估\n榜单给出的数据，针对800 条机器翻译结果进行评估需要花费约80 美元[556]。如果采用半自动评估\n方法， 利用人工给定的标准翻译结果和评估函数可以快速高效地给出评估结果， 但是目前半自动评估\n结果与人工评估结果的一致性还亟待提升。 对于用词差别很大， 但是语义相同的句子的判断本身也是\n自然语言处理领域的难题。如何有效地评估文本生成类任务的结果仍面临着极大的挑战。\n模型评估还涉及选择合适的评估数据集，针对单一任务，可以将数据集划分为训练集、验证\n集和测试集。训练集用于模型的训练，验证集用于调整模型的超参数及进行模型选择，而测...",
        "则用于最终评估模型的性能。评估数据集和训练数据集应该是相互独立的，以避免数据泄露的问\n题。此外，数据集选择还需要具有代表性，应该能够很好地代表模型在实际应用中可能遇到的数\n据。这意味着它应该涵盖各种情况和样本，以便模型在各种情况下都能表现良好。评估数据集的\n规模也应该足够大，以充分评估模型的性能。此外，评估数据集中应该包含一些特殊情况的样本，\n以确保模型在处理异常或边缘情况时仍具有良好的性能。\n大语言模型评估同样涉及数据集选择问题，但是大语言模型可以在单一模型中完成自然语言\n理解、逻辑推理、自然语言生成、多语言处理等任务。因此，如何构造大语言模型的评估数据集\n也是需要研究的问题。此外，由于大语言模型本身涉及语言模型训练、有监督微调、强化学习等\n多个阶段，每个阶段所产出的模型目标并不相同，因此，对于不同阶段的大语言模型也需要采用",
        "完成之后的一个必不可少的步骤。 目前， 针对单一任务的自然语言处理算法， 通常需要构造独立于\n训练数据的评估数据集，使用合适的评估函数对模型在实际应用中的效果进行预测。由于并不能\n完整了解数据的真实分布，因此简单地采用与训练数据独立同分布的方法构造的评估数据集，在\n很多情况下并不能完整地反映模型的真实情况。图11.1 为模型评估难点示意图，针对相同的训练\n数据，采用不同的算法或者超参数得到 4 个不同的分类器，可以看到，如果不能获取数据的真实\n分布，或者测试数据采样不够充分，分类器在真实使用中的效果就不能很好地通过上述方法进行\n评估。\n在模型评估的过程中，通常会使用一系列 评估指标（Evaluation Metrics）来衡量模型的表现，\n如准确率、精确率、召回率、F1 分数、ROC 曲线和 AUC 等。这些指标根据具体的任务和应用场\n景可能会有所不同。例如，在分类任务中，常用的评估指标包括准确率、精确率、召回率、F1 分\n数等；而在回归任务中，常用的评估指标包括均方误差和平均绝对误差等。但是对于文本生成类",
        "时的表现，以及系统在信息不足或不确定的情况下是否能够拒绝生成错误回答。\n9.5.3 评估数据集\n评估数据集是 RAG 系统性能评估中的关键组成部分，其质量和多样性直接影响评估结果的\n准确性和全面性。 在评估RAG 系统时， 数据集的选择与构建需要兼顾系统的检索能力、 生成能力\n以及整体协作表现。现有评估数据集的来源和构造方法多种多样，既包括基于已有资源的数据集，\n也包括为特定评估目标生成的全新数据集。不同的基准系统选择了不同的数据集策略，以适应各\n自的评估需求和应用场景。\n1. 基于现有资源的数据集\n许多评估框架依赖于已有的成熟数据集，如 KILT（Knowledge Intensive Language Tasks ）基\n准[457] 和 SuperGLUE[458] 数据集。 这些数据集涵盖了多种知识密集型任务。 例如：Natural Questions\n(NQ)[459] 提供开放域问答任务的数据， 测试系统对自然语言查询的回答能力。HotpotQA[444] 包含多\n跳问答任务， 要求RAG 系统能够从多个文档中综合信息来回答复杂问题。FEVER[460] 专注于事实验\n证任务..."
      ],
      "response_time": 5.502829313278198,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 0.5
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.37068448669423815,
        "semantic_similarity": 0.876824676990509
      }
    },
    {
      "sample_id": "0189",
      "question": "在评估大语言模型的能力时，如何结合工具使用和特定任务场景的选择来提升模型的表现？",
      "ground_truth": "评估大语言模型的能力时，需要结合工具使用的能力和特定任务场景的选择。片段A提到，Web浏览器插件使ChatGPT能够访问最新的信息，并且通过复杂的推理任务评估模型的工具使用能力，如数学问题求解。而片段B则强调了评估任务的多样性，具体包括问答和摘要，并通过选择覆盖率高、与用户任务对应的场景来进行高效评估。因此，通过有效利用工具和选择恰当的评估任务场景，可以增强大语言模型在特定技能上的表现。",
      "predicted_answer": "在评估大语言模型的能力时，可以通过让模型学习使用各种工具，并针对特定领域的任务进行评估来提升模型表现。例如，通过Tool Learning让模型掌握如何调用工具[2]，并在实际任务中应用这些技能。此外，还可以构建针对特定领域（如法律、医疗等）的评估数据集和方法，从而更全面地考察模型在该领域的专业性和实用性[1]。",
      "ground_context": [
        "Web 浏览器插件使 ChatGPT 能够访问最新的信息。",
        "评估的功能包括任务（如问答、摘要）和领域（如维基百科 2018 年的数据集）。"
      ],
      "retrieved_contexts": [
        "大规模语言模型：从理论到实践 419\n非常重要。 大语言模型在这些任务上的效果， 可以在一定程度上反映模型在工具使用方面的能力。\n除此之外，API-Bank[584] 针对 53 种常见的 API 工具， 标记了264 个对话， 共包含568 个 API 调用。\n针对模型使用外部工具的能力直接进行评估。\n3. 特定领域\n目前大语言模型研究除在通用领域之外， 也针对特定领域开展工作， 例如医疗[585]、 法律[429, 586]、\n财经[587] 等。如何针对特定领域的大语言模型进行评估也是重要的课题。针对特定领域，通常利\n用大语言模型完成有针对性的任务。 例如， 在法律人工智能 （Legal Artificial Intelligence，LegalAI）\n领域， 完成合同审查、 判决预测、 案例检索、 法律文书阅读理解等任务。 针对不同的领域任务， 需\n要构建不同的评估数据集和方法。\nContract Understanding Atticus Dataset（CUAD）[114] 是用于合同审查的数据集。 合同通常包含\n少量重要内容，需要律师进行审查或分析，特别是要识别包含重要义...",
        "272 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n工具，也不能很好对任务进行多步骤的规划。同时大语言模型构建之初也没有考虑记忆和使用用\n户全部对话历史。为了弥补这些不足，研究者们开始系统地研究如何提升大语言模型解决上述问\n题的能力。本节将重点介绍大语言模型工具使用能力提升、推理规划能力提升以及长期记忆构建\n与应用的策略方法。\n8.3.1 工具学习\n大模型工具学习 （Tool Learning） 是指通过让大语言模型学会使用各种工具的调用方式， 进而\n利用合适的工具去实现特定的功能需求。 例如， 用户输入“请告诉我上海今天的天气。 ”具备工具\n使用能力的大语言模型会给出如下响应：\n1. ്љ಩ༀো྘ğ ฿గҰ࿘಩ༀb\n2.Ⴈ฿గ APIğႨ฿గ APIༀ APIčೂ WeatherMapෂҰ࿘ҕඔĎ b\nresponse = requests.get(\"https://api.weathermap/data/2.5/weather\",\nparams={\n\"q\": \"Shanghai\",\n\"date\": \"2025-1-6\",\n\"appid\": \"your_api_key\"...",
        "要求模型从一个单词中去除随机符号，包括使用和不使用自然语言提示词的情况。可以看到，大\n语言模型具有更好的从上下文信息中学习任务的能力。在此之后，大语言模型评估也不再局限于\n困惑度、交叉熵等传统评估指标，而更多采用综合自然语言处理任务集合的方式进行评估。\n图 11.12 不同参数量的大语言模型在简单任务中基于语境学习的表现 [13]\n1. GPT-3 评估\nOpenAI 的研究人员针对 GPT-3[13] 的评估主要包含两个部分：传统语言模型评估及综合任务\n评估。在传统语言模型评估方面，采用了基于 Penn Tree Bank （PTB）[605] 数据集的困惑度评估；\nLambada[142] 数据集用于评估长距离语言建模能力，补全句子的最后一个单词； HellaSwag[606] 数\n据集要求模型根据故事内容或一系列说明选择最佳结局；StoryCloze[607] 数据集也用于评估模型根",
        "大规模语言模型：从理论到实践 433\n11.4 大语言模型评估实践\n大语言模型的评估伴随着大语言模型研究同步飞速发展，大量针对不同任务、采用不同指标\n和方法的大语言模型评估不断涌现。本章前面几节分别针对大语言模型评估体系、评估指标和评\n估方法从不同方面介绍了当前大语言模型评估面临的问题，试图回答要从哪些方面评估大语言模\n型，以及如何评估大语言模型这两个核心问题。针对大语言模型构建不同阶段所产生的模型能力\n的不同，本节将分别介绍当前常见的针对基础模型、SFT 模型和 RL 模型的整体评估方案。\n11.4.1 基础模型评估\n大语言模型构建过程中产生的基础模型就是语言模型， 其目标就是建模自然语言的概率分布。\n语言模型构建了长文本的建模能力，使得模型可以根据输入的提示词生成文本补全句子。2020 年\nOpenAI 的研究人员在 1750 亿个参数的 GPT-3 模型上研究发现，在语境学习范式下，大语言模型\n可以根据少量给定的数据，在不调整模型参数的情况下，在很多自然语言处理任务上取得不错的\n效果[13]。图11.12 展示了不同参数量的大语言模型在简单任务中基于语境学习的表现。这个任务\n要求...",
        "下游任务进行微调。 这些模型参数量大都在10 亿个以上， 由于微调的计算量很大， 这类模型的影\n响力在当时相较 BERT 类模型有不小的差距。\n能力探索阶段集中于 2019 年至 2022 年，由于大语言模型很难针对特定任务进行微调，研究\n人员开始探索在不针对单一任务进行微调的情况下如何发挥大语言模型的能力。2019 年，Radford\n等人在文献 [11] 中使用 GPT-2 模型研究了大语言模型在零样本情况下的任务处理能力。 在此基础\n上，Brown 等人在 GPT-3[13] 模型上研究了通过语境学习进行少样本学习的方法，将不同任务的少\n量有标注的实例拼接到待分析的样本之前输入语言模型，语言模型根据实例理解任务并给出正确\n的结果。基于 GPT-3 的语境学习在 TriviaQA、WebQS、CoQA 等评测集合中都展示出了非常强的\n能力，在有些任务中甚至超过了此前的有监督方法。上述方法不需要修改语言模型的参数，模型\n在处理不同任务时无须花费大量计算资源进行模型微调。仅依赖语言模型本身，其性能在很多任\n务上仍然很难达到有监督学习 （Supervised Learning） 的效果，..."
      ],
      "response_time": 5.130933046340942,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0.0,
        "bleu": 0.09945352458958724,
        "semantic_similarity": 0.8117570877075195
      }
    },
    {
      "sample_id": "0190",
      "question": "如何利用大语言模型生成的自然语言指令在多模态环境中执行家庭任务？",
      "ground_truth": "可以通过大语言模型生成详细的自然语言指令，然后在虚拟环境中执行这些指令。比如在VirtualHome模拟器中，研究人员通过众包收集了大量家庭任务的描述，并将其转换为简单的代码，从而实现智能体执行这些任务。同时，vLLM的优化支持了多模态环境，为复杂推理场景提供了高效的解决方案，能够与多种常用模型兼容。",
      "predicted_answer": "为了利用大语言模型生成的自然语言指令在多模态环境中执行家庭任务，可以采用VirtualHome环境[1]。在这个环境中，首先通过众包方式收集一个大型的家庭任务知识库，每个任务包括一个名称和自然语言指令。然后，标注者将这些指令转化为简单的代码形式。最后，在三维模拟器VirtualHome中实现最频繁的交互动作，使智能体程序能够执行由程序定义的任务。这样就可以利用大语言模型生成的自然语言指令来指导智能体完成具体的家庭任务。",
      "ground_context": [
        "大语言模型还具有从外部环境接收反馈并根据行为指令执行操作的能力，例如生成用自然语言描述的详细且高度逼真的行动计划，并用来操作智能体。",
        "vLLM V1 的优化涵盖了执行效率、缓存管理、推理架构和多模态支持，为复杂推理场景提供了更加高效、灵活和可扩展的解决方案。"
      ],
      "retrieved_contexts": [
        "大规模语言模型：从理论到实践 417\n2. 环境交互\n大语言模型还具有从外部环境接收反馈并根据行为指令执行操作的能力，例如生成用自然语\n言描述的详细且高度逼真的行动计划，并用来操作智能体 [577, 578]。为了测试这种能力，研究人员\n提出了多个具身智能 （Embodied AI） 环境和标准评估数据集， 包括VirtualHome[579]、ALFRED[580]、\nBEHA VIOR[581]、V oyager[372]、GITM[582] 等。\nVirtualHome[579] 构建了一个三维模拟器，用于家庭任务（如清洁、烹饪等） ，智能体程序可\n以执行由大语言模型生成的自然语言动作。VirtualHome 评估数据收集过程如图11.7 所示， 首先通\n过众包方式收集一个大型的家庭任务知识库。每个任务都有一个名称和一个自然语言指令。然后\n为这些任务收集“程序” ，其中标注者将指令“翻译”成简单的代码。在三维模拟器VirtualHome\n中实现了最频繁的（交互）动作，使智能体程序执行由程序定义的任务。此外，VirtualHome 还提\n出了一些方法， 可以从文本和视频中自动生成程...",
        "大规模语言模型：从理论到实践 153\n图 5.2 Self-Instruct 数据生成过程 [189]\n步骤 1：生成任务指令\n手动构建一个包含 175 个任务的小型指令数据集，称为种子指令集，用于初始化指令池。然\n后让模型以自举（Bootstrapping）的方式，利用指令池生成新任务的指令：每次从指令池中采样8\n条任务指令（其中 6 条来自人工编写的种子指令，2 条是模型迭代生成的） ，将其拼接为上下文示\n例，引导预训练语言模型 GPT-3 生成更多的新任务的指令，直到模型自己停止生成，或达到模型\n长度限制， 或是在单步中生成了过多示例 （例如当出现了“Task 16”时） 。 本步骤所使用的提示如\n下所示：\nCome up with a series of tasks:\nTask 1: {instruction for existing task 1}\nTask 2: {instruction for existing task 2}\nTask 3: {instruction for existing task 3}\nTask 4: {instruction for existi...",
        "下游任务进行微调。 这些模型参数量大都在10 亿个以上， 由于微调的计算量很大， 这类模型的影\n响力在当时相较 BERT 类模型有不小的差距。\n能力探索阶段集中于 2019 年至 2022 年，由于大语言模型很难针对特定任务进行微调，研究\n人员开始探索在不针对单一任务进行微调的情况下如何发挥大语言模型的能力。2019 年，Radford\n等人在文献 [11] 中使用 GPT-2 模型研究了大语言模型在零样本情况下的任务处理能力。 在此基础\n上，Brown 等人在 GPT-3[13] 模型上研究了通过语境学习进行少样本学习的方法，将不同任务的少\n量有标注的实例拼接到待分析的样本之前输入语言模型，语言模型根据实例理解任务并给出正确\n的结果。基于 GPT-3 的语境学习在 TriviaQA、WebQS、CoQA 等评测集合中都展示出了非常强的\n能力，在有些任务中甚至超过了此前的有监督方法。上述方法不需要修改语言模型的参数，模型\n在处理不同任务时无须花费大量计算资源进行模型微调。仅依赖语言模型本身，其性能在很多任\n务上仍然很难达到有监督学习 （Supervised Learning） 的效果，..."
      ],
      "response_time": 5.636893272399902,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.4,
        "rouge2": 0.0,
        "rougeL": 0.4,
        "bleu": 0.09226051886923509,
        "semantic_similarity": 0.8536975383758545
      }
    },
    {
      "sample_id": "0191",
      "question": "如何避免大语言模型训练中的数据泄露问题？",
      "ground_truth": "OpenAI 的研究人员对于每个基准测试， 会生成一个 “干净” 版本， 该版本会移除所有可能泄露的样本。",
      "predicted_answer": "为了避免大语言模型训练中的数据泄露问题，研究者们采取了几种措施。首先，对于基准测试，研究人员会生成一个“干净”版本的数据集，移除所有可能泄露的样本，这些样本与预训练集中任何13-gram重叠[1]。其次，为了避免使用易抓取的数据源，如国家考试试题，评估工具C-EV AL会选择模拟考试或小规模地方考试的数据[2]。此外，他们还会通过解析和注释互联网上的PDF或Microsoft Word文档来获取数据，减少直接使用纯文本带来的风险[2]。最后，从预训练语料库中删除包含个人身份信息的内容也是必要的步骤，可以通过基于规则的算法实现这一目标[3]。",
      "ground_context": [
        "为了避免这个因素的干扰，OpenAI 的研究人员对于每个基准测试， 会生成一个 “干净” 版本， 该版本会移除所有可能泄露的样本。"
      ],
      "retrieved_contexts": [
        "由于大语言模型在训练阶段需要使用大量种类繁杂且来源多样的训练数据，因此不可避免地\n存在数据泄露的问题， 即测试数据出现在语言模型训练数据中。 为了避免这个因素的干扰，OpenAI\n的研究人员对于每个基准测试， 会生成一个 “干净” 版本， 该版本会移除所有可能泄露的样本。 泄\n露样本的定义大致为与预训练集中任何 13-gram 重叠的样本 （或者当样本长度小于13-gram 时， 与\n整个样本重叠） 。 目标是非常保守地标记任何可能存在污染的内容， 以便生成一个高度可信且无污\n染的干净子集。 之后， 使用干净子集对GPT-3 进行评估， 并将其与原始得分进行比较。 如果干净子\n集上的得分与整个数据集上的得分相似，则表明即使存在污染也不会对结果产生显著影响。如果\n干净子集上的得分较低，则表明污染可能会提升评估结果。GPT-3 数据泄露的影响评估如图11.13\n所示。x 轴表示数据集中有多少数据可以被高度自信地认为是干净的，而 y 轴显示了在干净子集\n上进行评估时性能的差异。可以看到，虽然污染水平通常很高，有四分之一的基准测试超过50%，\n但在大多数情况下，性能变化很小。\n图 11.13 ...",
        "436 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n为了减小数据污染的风险，C-EV AL在创建过程中采取了一系列策略。 首先， 避免使用来自国\n家考试（例如高考和国家专业考试）的试题。这些试题大量出现在网络上，容易被抓取并出现在\n训练数据中，从而导致潜在的数据泄露问题。 C-EV AL的研究人员从模拟考试或小规模地方考试\n中收集数据，以避免数据污染。其次， C-EV AL中的大多数样本并非直接来自纯文本或结构化问\n题，而是来源于互联网上的 PDF 或 Microsoft Word 文档。为了将这些样本转化为结构化格式，研\n究人员进行了解析和仔细注释。在这个过程中，一些题目可能涉及复杂的 LaTeX 方程式转换，这\n进一步减小了数据污染的风险。通过对原始文档的解析和注释，能够获得可用于评估的最终结构\n化样本。减小数据污染的风险，可确保评估工具的可靠性和准确性。\n11.4.2 SFT 模型和 RL 模型评估\n经过训练的 SFT 模型及 RL 模型具备指令理解能力和上下文理解能力，能够完成开放领域任\n务，具备阅读理解、翻译、生成代码等能力，也具备了一定的对未知任务的泛化能力。...",
        "70 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n3.2.3 隐私消除\n由于绝大多数预训练数据源于互联网， 因此不可避免地会包含涉及敏感或个人信息 （Personally\nIdentifiable Information，PII）的用户生成内容，这可能会增加隐私泄露的风险[127]。如图3.3 所示，\n输入前缀词“East Stroudsburg Stroudsburg” ，语言模型在此基础上补全了姓名、电子邮件地址、电\n话号码、传真号码及实际地址。这些信息都是模型从预训练数据中学习得到的。因此，非常有必\n要从预训练语料库中删除包含个人身份信息的内容。\n图 3.3 从大语言模型中获得隐私数据的例子 [127]\n删除隐私数据最直接的方法是采用基于规则的算法，BigScience ROOTS Corpus [128] 在构建过\n程中就采用了基于命名实体识别的方法，利用命名实体识别算法检测姓名、地址、电话号码等个\n人信息内容并进行删除或者替换。该方法使用了基于 Transformer 的模型，并结合机器翻译技术，\n可以处理超过 100 种语言的文本，消除其中的隐私信息。该方法被..."
      ],
      "response_time": 6.371446847915649,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0.0,
        "bleu": 0.0793087976529888,
        "semantic_similarity": 0.6667523384094238
      }
    },
    {
      "sample_id": "0192",
      "question": "当 B 和 C 的值大于多少时，麦克尼马尔检验可以相对准确地近似计算 p 值？",
      "ground_truth": "50",
      "predicted_answer": "当 B 和 C 的值大于 50 时，麦克尼马尔检验可以相对准确地近似计算 p 值 [1][2]。",
      "ground_context": [
        "当 B 和 C 的值大于 50 时，麦克尼马尔检验可以相对准确地近似计算p 值"
      ],
      "retrieved_contexts": [
        "大规模语言模型：从理论到实践 431\n文献 [604] 在上述公式的基础上，提出了一个连续性修正版本，这也是目前更常用的变体：\nχ2 = (|B − C| − 1)2\nB + C (11.21)\n当 B 和 C 的值大于 50 时， 麦克尼马尔检验可以相对准确地近似计算p 值， 如果B 和 C 的值\n相对较小（B + C < 25） ，则建议使用以下二项式检验公式计算p 值：\np = 2\nnX\ni=B\n \nn\ni\n!\n0.5i(1 − 0.5)n−i (11.22)\n其中 n = B + C，因子 2 用于计算双侧 p 值（Two-sided p-value） 。\n针对图11.11 中的两种情况，可以使用 mlxtend[555] 来计算 p 值和 χ2：\nfrom mlxtend.evaluate import mcnemar\nimport numpy as np\ntb_a = np.array([[9959, 11],\n[1, 29]])\nchi2, p = mcnemar(ary=tb_a, exact =True)\nprint('chi-squared-a:', chi2)\np...",
        "430 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n图 11.10 用于麦克尼马尔检验的混淆矩阵 [603]\n图11.11 给出了两个样例，根据图11.11(a) 和图11.11(b)，可以计算得到模型 1 和模型 2 在两种\n情况下的准确率分别为 99.7% 和 99.6%。根据图11.11(a)，可以看到模型 1 回答正确且模型 2 回答\n错误的数量为 11，但是反过来模型 2 回答正确且模型 1 回答错误的数量仅为 1。在图11.11(b) 中，\n这两个数字变成了 25 和 15。 显然， 图11.11(b) 中的模型 1 与模型 2 之间的差异更大， 图11.11(a) 中\n的模型 1 与模型 2 之间的差异则没有这么明显。\n图 11.11 麦克尼马尔检验样例 [603]\n为了量化表示上述情况，麦克尼马尔检验中提出的零假设是概率 p(B) 与 p(C) 相等，即两个\n模型都没有表现得比另一个好。麦克尼马尔检验的统计量（ “卡方值” ）计算公式如下：\nχ2 = (B − C)2\nB + C (11.20)\n设定显著性水平阈值 （例如α = 0.05） 之后， 可以计..."
      ],
      "response_time": 3.4090094566345215,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.2857142857142857,
        "rouge2": 0.0,
        "rougeL": 0.2857142857142857,
        "bleu": 0.01015265650850611,
        "semantic_similarity": 0.4798768162727356
      }
    },
    {
      "sample_id": "0193",
      "question": "LLMEV AL第二期的目标是什么？",
      "ground_truth": "LLMEV AL第二期（LLMEV AL-2）的目标是以用户日常使用为主线，重点考查大语言模型解决不同专业本科生和研究生在日常学习中所遇到的问题的能力。",
      "predicted_answer": "LLMEV AL第二期（LLMEV AL-2）的目标是以用户日常使用为主线，重点考查大语言模型解决不同专业本科生和研究生在日常学习中所遇到的问题的能力。[1]",
      "ground_context": [
        "LLMEV AL第二期（LLMEV AL-2）的目标是以用户日常使用为主线， 重点考查大语言模型解决不同专业本科生和研究生在日常学习中所遇到的问题的能力。"
      ],
      "retrieved_contexts": [
        "444 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n排名选手和低排名选手的比赛中，如果高排名选手获胜，那么只会从低排名选手处获得很少的排\n名分。然而，如果低排名选手爆冷获胜，则可以获得更多排名分。虽然这种评分系统非常适合竞\n技比赛，但是与顺序有关，并且对噪声非常敏感。积分制得分也是一种常见的比赛评分系统，用\n于在竞技活动中确定选手或团队的排名。该制度根据比赛中获得的积分数量，决定参与者在比赛\n中的表现和成绩。在 LLMEV AL评估中，根据用户给出的“A 系统好” 、 “B 系统好” 、 “两者一样\n好”及“两者都不好”的选择，分别给A 系统 +1 分，B 系统 +1 分，A 和 B 系统各 +0.5 分。该评\n分系统与顺序无关，并且对噪声的敏感程度相较 Elo 评分系统低。\nLLMEV AL第二期（LLMEV AL-2）的目标是以用户日常使用为主线， 重点考查大语言模型解\n决不同专业本科生和研究生在日常学习中所遇到的问题的能力。涵盖的学科非常广泛，包括计算\n机、 法学、 经济学、 医学、 化学、 物理学等12 个领域。 评估数据集包含两种题型： 客观题和主观题。\n通过这...",
        "大规模语言模型：从理论到实践 441\n图 11.15 Chatbot Arena 给出的系统之间的胜率矩阵 [196]\n6. LLMEVAL 评估\nLLMEV AL[411] 中文大语言模型评估先后进行了二期，LLMEV AL-1评估涵盖了 17 个大类、453\n个问题，包括事实性问答、阅读理解、框架生成、段落重写、摘要、数学解题、推理、诗歌生成、\n编程等各个领域。 针对生成内容的质量， 细化为5 个评分项， 分别是正确性、 流畅性、 信息量、 逻\n辑性和无害性，具体如下。\n• 正确性： 评估回答是否正确， 即所提供的信息是否正确无误。 一个高质量的回答应当在事实\n上是可靠的。\n• 流畅性：评估回答是否贴近人类语言习惯， 即语句是否通顺、 表达是否清晰。 一个高质量的\n回答应当易于理解，不含烦琐或难以解读的句子。\n• 信息量：评估回答是否提供了足够的有效信息，即回答中的内容是否具有实际意义和价值。"
      ],
      "response_time": 3.5303375720977783,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.9090909090909091,
        "rouge2": 0.888888888888889,
        "rougeL": 0.9090909090909091,
        "bleu": 0.9293609988725866,
        "semantic_similarity": 0.976615846157074
      }
    },
    {
      "sample_id": "0194",
      "question": "SWE-bench Verified 是什么的改进版本？",
      "ground_truth": "它是原版SWE-bench 的改进版本",
      "predicted_answer": "SWE-bench Verified 是 SWE-bench 的改进版本[2]。",
      "ground_context": [
        "它是原版SWE-bench 的改进版本，旨在解决原版在实际评估中暴露的多个问题， 例如单元测试过于严格、 问题描述不明确以及环境配置难度较高等。"
      ],
      "retrieved_contexts": [
        "438 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n集则涵盖了八种主流编程语言（Python、Java、C++、C#、JavaScript、TypeScript、PHP 和 Bash） 。\nHumanEval 系列评测为研究者提供了一个标准化的数据集和工具，用于量化模型在代码生成任务\n中的能力。\nLiveCodeBench[625] 是一个动态且全面的基准测试集， 专为评估大语言模型的代码生成能力设\n计。该测试集从 LeetCode、AtCoder、CodeForces 等竞赛平台持续收集新问题，截至 2025 年 1 月\n已包含 880 道高质量编码挑战，覆盖代码生成、自修复、代码执行和测试输出预测等多种能力场\n景。通过仅选用新发布的问题，避免训练数据与测试数据重叠，确保评估无污染且客观公正。它\n支持用户自定义模型风格和评估流程，提供直观的命令行接口及详尽文档，方便新手和专家快速\n上手。 此外， 公开的Leaderboard 增强透明度， 鼓励社区互动与模型性能的持续提升， 使其成为目\n前评估大语言模型编码能力的重要工具。\nSWE-bench V erified是 Op...",
        "能。 它是原版SWE-bench 的改进版本[626]， 旨在解决原版在实际评估中暴露的多个问题， 例如单元\n测试过于严格、 问题描述不明确以及环境配置难度较高等。 通过这些改进，SWE-bench V erified提\n供了更准确的评估方法， 能够更真实地反映AI 模型在软件工程任务中的能力。SWE-bench V erified\n基于原始 SWE-bench 测试集， 筛选出500 个由专业软件开发人员彻底审查和验证的样本。这些样\n本经过人工标注，确保问题描述清晰、单元测试适当，并剔除质量较差的样本，从而提高了基准\n测试的可靠性。 此外， 开发团队引入了基于容器化Docker 环境的新评估框架， 使测试过程更加一\n致和可靠，同时显著降低了因开发环境配置导致问题的可能性。每个样本都附带详细的人工注释，\n帮助研究人员和开发者更好地理解问题描述的清晰度和评估标准的有效性。这一改进为 AI 模型\n在软件工程领域的性能评估提供了更可靠的依据，推动了 AI 在该领域的发展和应用。\n3. 数学评测数据集\nGSM8K[227] 是一个包含 8500 个样本的小学数学问题数据集，其中训练集包含 75..."
      ],
      "response_time": 2.680504322052002,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 0.5
      },
      "answer_quality_metrics": {
        "rouge1": 0.5,
        "rouge2": 0.33333333333333337,
        "rougeL": 0.5,
        "bleu": 0.33649324423301513,
        "semantic_similarity": 0.7811435461044312
      }
    },
    {
      "sample_id": "0195",
      "question": "使用大语言模型进行评估的过程比较简单吗？",
      "ground_truth": "是的，使用大语言模型进行评估的过程比较简单。",
      "predicted_answer": "是的，使用大语言模型进行评估的过程比较简单。例如，对于文本质量判断问题，可以通过构造任务说明、待评估样本及对大语言模型的指令，将这些内容输入大语言模型，然后解析模型的输出以获取评分来进行评估[1]。",
      "ground_context": [
        "使用大语言模型进行评估的过程比较简单， 例如针对文本质量判断问题， 要构造任务说明、 待评估样本及对大语言模型的指令，将上述内容输入大语言模型，对给定的待评估样本质量进行评估，图 8.11 给出的指令要求大语言模型采用 5 级李克特量表法。"
      ],
      "retrieved_contexts": [
        "以应用于缺乏参考文本的任务。使用大语言模型进行结果评估的过程如图11.9 所示。\n使用大语言模型进行评估的过程比较简单， 例如针对文本质量判断问题， 要构造任务说明、 待\n评估样本及对大语言模型的指令，将上述内容输入大语言模型，对给定的待评估样本质量进行评\n估，图 8.11 给出的指令要求大语言模型采用 5 级李克特量表法。给定这些输入，大语言模型将通\n过生成一些输出句子来回答问题。通过解析输出句子以获取评分。不同的任务使用不同的任务说\n明集合， 并且每个任务使用不同的问题来评估样本的质量。 在文献[600] 中， 针对故事生成任务的\n文本质量又细分为 4 个属性。",
        "406 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n任务（例如机器翻译、文本摘要等） ，自动评估仍然是亟待解决的问题。\n图 11.1 模型评估难点示意图 [555]\n文本生成类任务的评估难点主要源于语言的灵活性和多样性， 同样一句话可以有非常多种表述方\n法。 对文本生成类任务进行评估可以采用人工评估和半自动评估方法。 以机器翻译评估为例， 人工评\n估虽然是相对准确的一种方式， 但是其成本高昂， 根据艾伦人工智能研究院 （AI2）GENIE 人工评估\n榜单给出的数据，针对800 条机器翻译结果进行评估需要花费约80 美元[556]。如果采用半自动评估\n方法， 利用人工给定的标准翻译结果和评估函数可以快速高效地给出评估结果， 但是目前半自动评估\n结果与人工评估结果的一致性还亟待提升。 对于用词差别很大， 但是语义相同的句子的判断本身也是\n自然语言处理领域的难题。如何有效地评估文本生成类任务的结果仍面临着极大的挑战。\n模型评估还涉及选择合适的评估数据集，针对单一任务，可以将数据集划分为训练集、验证\n集和测试集。训练集用于模型的训练，验证集用于调整模型的超参数及进行模型选择，而测...",
        "要求模型从一个单词中去除随机符号，包括使用和不使用自然语言提示词的情况。可以看到，大\n语言模型具有更好的从上下文信息中学习任务的能力。在此之后，大语言模型评估也不再局限于\n困惑度、交叉熵等传统评估指标，而更多采用综合自然语言处理任务集合的方式进行评估。\n图 11.12 不同参数量的大语言模型在简单任务中基于语境学习的表现 [13]\n1. GPT-3 评估\nOpenAI 的研究人员针对 GPT-3[13] 的评估主要包含两个部分：传统语言模型评估及综合任务\n评估。在传统语言模型评估方面，采用了基于 Penn Tree Bank （PTB）[605] 数据集的困惑度评估；\nLambada[142] 数据集用于评估长距离语言建模能力，补全句子的最后一个单词； HellaSwag[606] 数\n据集要求模型根据故事内容或一系列说明选择最佳结局；StoryCloze[607] 数据集也用于评估模型根",
        "大规模语言模型：从理论到实践 433\n11.4 大语言模型评估实践\n大语言模型的评估伴随着大语言模型研究同步飞速发展，大量针对不同任务、采用不同指标\n和方法的大语言模型评估不断涌现。本章前面几节分别针对大语言模型评估体系、评估指标和评\n估方法从不同方面介绍了当前大语言模型评估面临的问题，试图回答要从哪些方面评估大语言模\n型，以及如何评估大语言模型这两个核心问题。针对大语言模型构建不同阶段所产生的模型能力\n的不同，本节将分别介绍当前常见的针对基础模型、SFT 模型和 RL 模型的整体评估方案。\n11.4.1 基础模型评估\n大语言模型构建过程中产生的基础模型就是语言模型， 其目标就是建模自然语言的概率分布。\n语言模型构建了长文本的建模能力，使得模型可以根据输入的提示词生成文本补全句子。2020 年\nOpenAI 的研究人员在 1750 亿个参数的 GPT-3 模型上研究发现，在语境学习范式下，大语言模型\n可以根据少量给定的数据，在不调整模型参数的情况下，在很多自然语言处理任务上取得不错的\n效果[13]。图11.12 展示了不同参数量的大语言模型在简单任务中基于语境学习的表现。这个任务\n要求...",
        "11. 大语言模型评估\n大语言模型飞速发展，自 ChatGPT 于 2022 年 11 月底发布以来，国内外已相继发布了数百种\n开源和闭源的大语言模型。大语言模型在自然语言处理研究和人们的日常生活中扮演着越来越重\n要的角色。因此，如何评估大语言模型变得愈发关键。我们需要在技术和任务层面对大语言模型\n之间的优劣加以判断，也需要在社会层面对大语言模型可能带来的潜在风险进行评估。大语言模\n型与以往仅能完成单一任务的自然语言处理算法不同，它可以通过单一模型执行多种复杂的自然\n语言处理任务。因此，之前针对单一任务的自然语言处理算法评估方法并不适用于大语言模型的\n评估。如何构建大语言模型评估体系和评估方法是一个重要的研究问题。\n本章将首先介绍大语言模型评估的基本概念和难点， 并在此基础上从大语言模型评估体系、 大\n语言模型评估方法，以及大语言模型评估实践三个方面分别展开介绍。\n11.1 模型评估概述\n模型评估（Model Evaluation） ， 也称模型评价， 目标是评估模型在未见过的数据 （Unseen Data）\n上的泛化能力和预测准确性，以便更好地了解模型在真实场景中的表现。模型评估是在..."
      ],
      "response_time": 4.331592798233032,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.19568286265820947,
        "semantic_similarity": 0.8652329444885254
      }
    },
    {
      "sample_id": "0196",
      "question": "LLMEval-3 考虑的重要因素是什么？",
      "ground_truth": "防止作弊是 LLMEval-3 考虑的重要因素。",
      "predicted_answer": "防止作弊是 LLMEval-3 考虑的重要因素。具体来说，LLMEval-3 采取了“题库考试”模式来避免测试题库泄露导致的不公平现象，确保每套系统都需要完成全部题库以进行评测[1]。",
      "ground_context": [
        "防止作弊是 LLMEval-3 考虑的重要因素。"
      ],
      "retrieved_contexts": [
        "型语言能力。\n防止作弊是 LLMEval-3 考虑的重要因素。现有公开评测基准存在测试题库泄露的问题，因此\n可能出现 “刷榜” 、 “刷分” 等不公平现象， 在LLMEval-3 中， 每个参与评测的系统需要完成从总题",
        "同模型的知识问答效果。LLMEval-3 评测采用了一种新颖的评测模式，即“题库考试”模式，既\n可以满足模型随时测试的需求，又尽最大可能防止刷榜现象的发生。LLMEval-3 聚焦于专业知识\n能力评测，涵盖哲学、经济学、法学、教育学、文学、历史学、理学、工学、农学、医学、军事\n学、 管理学、 艺术学等教育部划定的13 个学科门类、50 余个二级学科， 共计约100 万道标准生成\n式问答题目。题目来源主要包括大学本科课后作业、大学本科期中期末考试、研究生入学考试等。\n为了尽可能的防止参与评测的大模型在预训练阶段引入大比例原始评测数据，LLMEval-3 评测题\n目来源尽可能为非互联网公开渠道， 数据格式为PDF 和 Word 文件， 经过一定的OCR 识别与数据\n清洗之后，将题目进行格式化处理。针对于不同的题型，提供给待测试模型标准接口，实现全流\n程自动化。与其他知识评测所采用的选择题模式不同，LLMEval-3 中所有问题将统一处理为生成\n式知识问答形式，并尽可能包含多种题型，包括简答，计算、判断、辨析、写作等。相较于具有\n标准格式的选择题，LLMEval-3 所采用的生成式知识问答，...",
        "通过这两种题型的有机组合，评估旨在全面考查模型在不同学科领域中解决问题的能力。每个学\n科都设计了 25∼30 道客观题和 10∼15 道主观题， 共计480 道题目。 评估采用了人工评分和GPT-4\n自动评分两种方法。对于客观题，答对即可获得满分，而对于答错的情况，根据回答是否输出了\n中间过程或解释，对解释的正确性进行评分。主观题方面，依据问答题的准确性、信息量、流畅\n性和逻辑性这四个维度评分，准确性（5 分） ：评估回答的内容是否有错误；信息量（3 分） ：评估\n回答提供的信息是否充足；流畅性（3 分） ：评估回答的格式和语法是否正确；逻辑性（3 分） ：评\n估回答的逻辑是否严谨。为了避免与网上已有的试题重复， LLMEV AL-2在题目的构建过程中力\n求独立思考，旨在更准确、更全面地反映大语言模型的能力和在真实场景中的实际表现。\nLLMEV AL第三期（LLMEV AL-3）基准测试提供了更加全面且更具挑战性的问题。其目标是\n评估模型在中文知识问答任务上的表现，并提供一个公平的比较平台，以便研究人员可以评估不\n同模型的知识问答效果。LLMEval-3 评测采用了一种新颖的评测模式，..."
      ],
      "response_time": 3.6413662433624268,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.5714285714285715,
        "rouge2": 0.4,
        "rougeL": 0.5714285714285715,
        "bleu": 0.1986589078880532,
        "semantic_similarity": 0.8229078054428101
      }
    },
    {
      "sample_id": "0197",
      "question": "评估生成文本中知识准确性涉及哪些方面？",
      "ground_truth": "这涉及事实陈述、概念解释、历史事件描述等方面。",
      "predicted_answer": "评估生成文本中知识准确性涉及以下几个方面：首先，知识准确性要求生成的文本基于准确的知识和可靠的信息源，避免错误、虚假或误导性的内容[1]。其次，知识准确性还涉及到确保文本中的知识丰富多样，能够提供充分的信息并涵盖相关领域的不同方面[2]。最后，知识准确性还包括保证文本中知识的一致性，即避免出现相互矛盾的知识陈述，使文本在整体上保持一致[1][2]。",
      "ground_context": [
        "知识准确性：评估生成文本中所呈现的知识是否准确无误。 这涉及事实陈述、概念解释、历史事件描述等方面。"
      ],
      "retrieved_contexts": [
        "大规模语言模型：从理论到实践 425\n• 知识准确性：评估生成文本中所呈现的知识是否准确无误。 这涉及事实陈述、 概念解释、 历\n史事件描述等方面。 生成的文本应基于准确的知识和可靠的信息源， 避免错误、 虚假或误导\n性的内容。确保所提供的知识准确无误。\n• 知识丰富性： 评估生成文本所包含的知识是否丰富多样。 生成的文本应能够提供充分的信息，\n涵盖相关领域的不同方面。 这可以通过提供具体的例子、 详细的解释和相关的背景知识来实\n现。确保生成文本在知识上具有广度和深度，能够满足读者的需求。\n• 知识一致性： 评估生成文本中知识的一致性。 这包括确保文本中不出现相互矛盾的知识陈述，\n避免在不同部分或句子中提供相互冲突的信息。 生成的文本应该在整体上保持一致， 使读者\n能够得到一致的知识体系。\n11.3.2 评估方法\n评估方法的目标是解决如何对大语言模型生成结果进行评估的问题。有些指标可以通过比较\n正确答案或参考答案与系统生成结果直接计算得出， 例如准确率、 召回率等。 这种方法被称为自动\n评估（Automatic Evaluation） 。 然而， 有些指标并不是可以直接计算出来的， 而...",
        "成有条理的论述，文本中的论证、推理、归纳、演绎等逻辑关系应该正确。句子的顺序应符\n合常规的时间、空间或因果关系，以便读者能够理解句子之间的联系。\n• 风格一致性：评估文本在整体风格上是否保持一致。 包括词汇选择、 句子结构、 表达方式等\n方面。文本应该在整体上保持一种风格或口吻。例如，正式文本应使用正式的语言和术语，\n而故事性的文本可以使用生动的描写和故事情节。\n（3）知识层面的评估主要关注知识准确性、知识丰富性和知识一致性。要求生成文本所涉及\n的知识准确无误、丰富全面，确保文本的可信度。具体如下：",
        "则，包括时态、数和派生等方面。\n• 语法正确性： 评估生成文本的句子结构和语法规则是否正确。 确保句子的构造完整， 各个语\n法成分之间的关系符合语法规则， 包括主谓关系、 动宾关系、 定状补关系等方面的准确应用。\n此外，还需要评估动词的时态是否使用正确，包括时态的一致性和选择是否符合语境。\n• 篇章结构正确性： 评估生成文本的整体结构是否合理。 确保文本段落之间连贯， 文本信息流\n畅自然， 包括使用恰当的主题句、 过渡句和连接词等。同时， 需要评估文本整体结构的合理\n性，包括标题、段落、章节等结构的使用是否恰当，以及文本整体框架是否清晰明了。\n（2）语义层面的评估主要关注文本的语义准确性、逻辑连贯性和风格一致性。要求生成的文\n本不出现语义错误或误导性描述，并且具有清晰的逻辑结构，能够按照一定的顺序和方式呈现出\n来。具体如下：\n• 语义准确性：评估文本是否传达了准确的语义信息。包括词语的确切含义和用法是否正确，\n以及句子表达的意思是否与作者的意图相符。 确保文本中使用的术语、 概念和描述准确无误，\n能够准确传达信息给读者。\n• 逻辑连贯性： 评估文本的逻辑结构是否连贯一致。 句子之间应该...",
        "362 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n其中 RD 是指真正相关的文档集合，T opkd 是指检索到的前 k 个文档。\n2. 生成模块的评估指标\n生成组件负责利用检索结果，根据用户查询生成连贯、准确的回答。其性能评估需要全面衡\n量生成内容的质量、真实性以及与检索内容的一致性。以下从准确性与忠实度、连贯性与流畅性、\n生成内容的多维度质量、开放性任务的多样性以及真实性检测五个方面详细说明生成组件的评估\n指标。\n（1）准确性与忠实度（Accuracy and Faithfulness ） ：生成内容必须忠实于检索结果，并准确回\n答用户的查询。以下是常用的评估指标：\nBLEU（Bilingual Evaluation Understudy）通过计算生成内容与参考答案之间的 n-gram 重叠程\n度来评估生成的准确性。计算公式如下：\nBLEU = BP × exp\n\u0010X\n(wn × log(pn))\n\u0011\n(9.20)\n其中，BP 为长度惩罚因子， 防止生成内容过短，wn 表示 n-gram 的权重，pn 表示生成文本与参考\n文本中 n-gram 的匹配概率。BLEU 适...",
        "cination） 。 因此， 生成组件需要确保其输出内容忠实于检索到的信息， 即生成的回答必须基于检索\n到的事实，而不是凭空捏造。真实性评估目标在于衡量生成内容是否准确反映了检索结果中的信\n息，避免出现事实错误或误导性内容。\n（3） 正确性 （Correctness） ： 在许多任务中， 生成的回答需要与给定的参考答案 （Ground Truth）\n保持一致。正确性评估目标旨在衡量生成内容与标准答案之间的一致性，特别是在有明确答案的\n任务（如问答或填空任务）中。这通常通过自动化指标（如 F1 分数或精确匹配率）来衡量。\n（4）连贯性与流畅性（Coherence and Fluency ） ：生成内容的连贯性和流畅性是评估生成组件\n的重要目标之一。连贯性指回答内容是否逻辑通顺，是否能够完整表达查询的意图；流畅性则关\n注语言表达是否符合自然语言的语法和用法。这些目标通常通过人工评估或语言模型的评分机制\n来实现。\n（5）生成内容的多维度要求（Multi-Dimensional Requirements） ：生成组件的评估需要覆盖多\n个维度，包括内容的可读性、丰富性和结构化程度。例如，在生成..."
      ],
      "response_time": 5.424364805221558,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 0.5
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.0045781972650348855,
        "semantic_similarity": 0.6107859015464783
      }
    },
    {
      "sample_id": "0198",
      "question": "司南平台支持多少种开源模型的评测？",
      "ground_truth": "超过100种",
      "predicted_answer": "司南平台支持超过100种开源模型的评测。[1]",
      "ground_context": [
        "此外，平台支持超过100种开源模型的评测，并预留接口供开发者接入自定义模型或 API 模型，如 OpenAI 接口。"
      ],
      "retrieved_contexts": [
        "司南平台具有多项显著特点，其开源可复现的设计让评测过程公开透明，确保结果的准确性\n和可信度。评测维度涵盖基础能力和综合能力两个层级，包括语言、知识、代码、长文本处理等\n12 个一级能力维度和 50 余个二级能力维度， 全面反映模型的实际性能。 此外， 平台支持超过100\n种开源模型的评测，并预留接口供开发者接入自定义模型或 API 模型，如 OpenAI 接口。司南平\n台还提供分布式高效评测方案，能够在本地或集群中并行分发任务，优化时间和资源分配。同时，\n它灵活支持用户自定义数据集和评测策略，提供零样本、小样本和思维链式评测方式，满足多样\n化的评测需求。\n5. Chatbot Arena 评估\nChatbot Arena 是一个以众包方式进行匿名对比评估的大语言模型基准评估平台 [196]。研究人\n员构造了多模型服务系统 FastChat。当用户进入评估平台后可以输入问题，同时得到两个匿名模\n型的回答， 如图11.14 所示。 在从两个模型中获得回复后， 用户可以继续对话或投票选择他们认为\n更好的模型。一旦提交了投票，系统会将模型名称告知用户。用户可以继续对话或重新开始与两\n个新选择的..."
      ],
      "response_time": 3.469649314880371,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.6666666666666666,
        "rouge2": 0.0,
        "rougeL": 0.6666666666666666,
        "bleu": 0.09010288336595924,
        "semantic_similarity": 0.5967910289764404
      }
    },
    {
      "sample_id": "0199",
      "question": "p 值的意义是什么？",
      "ground_truth": "p 值是观察这个经验 （或更大的） 卡方值的概率。",
      "predicted_answer": "p 值表示的是观察到的经验卡方值（或更大值）出现的概率。当 p 值低于设定的显著性水平阈值时，我们可以拒绝两个模型性能相同这一零假设，即认为这两个模型的性能存在差异。[1]",
      "ground_context": [
        "p 值是观察这个经验 （或更大的） 卡方值的概率。"
      ],
      "retrieved_contexts": [
        "p 值是观察这个经验 （或更大的） 卡方值的概率。 如果p 值小于预先设置的显著性水平阈值， 则可\n以拒绝两个模型性能相等的零假设。换句话说，如果 p 值小于显著性水平阈值，则可以认为两个\n模型的性能不同。"
      ],
      "response_time": 3.4147608280181885,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.5,
        "rouge2": 0.0,
        "rougeL": 0.5,
        "bleu": 0.05550448556499293,
        "semantic_similarity": 0.7967923879623413
      }
    },
    {
      "sample_id": "0200",
      "question": "在LLMEval基准测试中，如何确保评估模型在知识问答任务上的表现既准确又具有挑战性？",
      "ground_truth": "LLMEval-3通过采用新颖的评测模式，即“题库考试”模式，旨在评估模型在中文知识问答任务上的表现，并提供一个公平的比较平台。同时，评估涉及知识准确性、丰富性和一致性，以确保生成文本的可信度和逻辑关系的正确性。",
      "predicted_answer": "LLMEval-3 基准测试通过采用“题库考试”模式来确保评估模型在知识问答任务上的表现既准确又具有挑战性。这种模式既可以满足模型随时测试的需求，又能最大限度地防止刷榜现象的发生。此外，它还涵盖了广泛的学科领域，共计约100万道标准生成式问答题目，这些题目来源于大学本科课后作业、考试等非互联网公开渠道，以减少模型在预训练阶段引入大量原始评测数据的可能性[2]。",
      "ground_context": [
        "LLMEval-3基准测试提供了更加全面且更具挑战性的问题。其目标是评估模型在中文知识问答任务上的表现，并提供一个公平的比较平台，以便研究人员可以评估不同模型的知识问答效果。",
        "知识层面的评估主要关注知识准确性、知识丰富性和知识一致性。要求生成文本所涉及的知识准确无误、丰富全面，确保文本的可信度。"
      ],
      "retrieved_contexts": [
        "通过这两种题型的有机组合，评估旨在全面考查模型在不同学科领域中解决问题的能力。每个学\n科都设计了 25∼30 道客观题和 10∼15 道主观题， 共计480 道题目。 评估采用了人工评分和GPT-4\n自动评分两种方法。对于客观题，答对即可获得满分，而对于答错的情况，根据回答是否输出了\n中间过程或解释，对解释的正确性进行评分。主观题方面，依据问答题的准确性、信息量、流畅\n性和逻辑性这四个维度评分，准确性（5 分） ：评估回答的内容是否有错误；信息量（3 分） ：评估\n回答提供的信息是否充足；流畅性（3 分） ：评估回答的格式和语法是否正确；逻辑性（3 分） ：评\n估回答的逻辑是否严谨。为了避免与网上已有的试题重复， LLMEV AL-2在题目的构建过程中力\n求独立思考，旨在更准确、更全面地反映大语言模型的能力和在真实场景中的实际表现。\nLLMEV AL第三期（LLMEV AL-3）基准测试提供了更加全面且更具挑战性的问题。其目标是\n评估模型在中文知识问答任务上的表现，并提供一个公平的比较平台，以便研究人员可以评估不\n同模型的知识问答效果。LLMEval-3 评测采用了一种新颖的评测模式，...",
        "同模型的知识问答效果。LLMEval-3 评测采用了一种新颖的评测模式，即“题库考试”模式，既\n可以满足模型随时测试的需求，又尽最大可能防止刷榜现象的发生。LLMEval-3 聚焦于专业知识\n能力评测，涵盖哲学、经济学、法学、教育学、文学、历史学、理学、工学、农学、医学、军事\n学、 管理学、 艺术学等教育部划定的13 个学科门类、50 余个二级学科， 共计约100 万道标准生成\n式问答题目。题目来源主要包括大学本科课后作业、大学本科期中期末考试、研究生入学考试等。\n为了尽可能的防止参与评测的大模型在预训练阶段引入大比例原始评测数据，LLMEval-3 评测题\n目来源尽可能为非互联网公开渠道， 数据格式为PDF 和 Word 文件， 经过一定的OCR 识别与数据\n清洗之后，将题目进行格式化处理。针对于不同的题型，提供给待测试模型标准接口，实现全流\n程自动化。与其他知识评测所采用的选择题模式不同，LLMEval-3 中所有问题将统一处理为生成\n式知识问答形式，并尽可能包含多种题型，包括简答，计算、判断、辨析、写作等。相较于具有\n标准格式的选择题，LLMEval-3 所采用的生成式知识问答，...",
        "大规模语言模型：从理论到实践 441\n图 11.15 Chatbot Arena 给出的系统之间的胜率矩阵 [196]\n6. LLMEVAL 评估\nLLMEV AL[411] 中文大语言模型评估先后进行了二期，LLMEV AL-1评估涵盖了 17 个大类、453\n个问题，包括事实性问答、阅读理解、框架生成、段落重写、摘要、数学解题、推理、诗歌生成、\n编程等各个领域。 针对生成内容的质量， 细化为5 个评分项， 分别是正确性、 流畅性、 信息量、 逻\n辑性和无害性，具体如下。\n• 正确性： 评估回答是否正确， 即所提供的信息是否正确无误。 一个高质量的回答应当在事实\n上是可靠的。\n• 流畅性：评估回答是否贴近人类语言习惯， 即语句是否通顺、 表达是否清晰。 一个高质量的\n回答应当易于理解，不含烦琐或难以解读的句子。\n• 信息量：评估回答是否提供了足够的有效信息，即回答中的内容是否具有实际意义和价值。",
        "442 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n一个高质量的回答应当能够为提问者提供有用的、相关的信息。\n• 逻辑性：评估回答是否在逻辑上严密、正确，即所陈述的观点、论据是否合理。一个高质量\n的回答应当遵循逻辑原则，展示出清晰的思路和推理过程。\n• 无害性： 评估回答是否涉及违反伦理道德的信息， 即内容是否合乎道德规范。 一个高质量的\n回答应当遵循道德原则，避免传播有害、不道德的信息。\n这些评分项能够更全面地考量和评估大语言模型的表现。\n在构造评估目标的基础上，有多种方法可以对模型进行评估。包括分项评估、众包对比评估、\n公众对比评估、GPT-4 自动分项评估、GPT-4 对比评估等。那么，哪种方法更适合评估大语言模\n型， 这些方法各自的优缺点又是什么呢？ 为了研究这些问题，LLMEV AL-1对上述五种方式进行了\n效果对比。\n• 分项评估： 根据分项评估目标制定具体的评估标准， 并构造定标集合。 在此基础上对人员进\n行培训， 并进行试标和矫正。 再进行小批量标注， 在对齐标准后完成大批量标注。LLMEV AL\n分项评估界面如图11.16 所示。\n图 11.16 LLM..."
      ],
      "response_time": 5.653634786605835,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.6666666666666666,
        "rouge2": 0.5,
        "rougeL": 0.6666666666666666,
        "bleu": 0.11919695856325101,
        "semantic_similarity": 0.8379858732223511
      }
    },
    {
      "sample_id": "0201",
      "question": "在评测编程模型时，如何考虑统计检验的准确性？",
      "ground_truth": "在评测编程模型时，使用'PASS@K'指标评估模型表现，同时在进行统计检验时，如果B和C的值大于50，可以使用麦克尼马尔检验来近似计算p值；而当B和C的值较小（B + C < 25）时，则建议使用二项式检验公式来计算p值。",
      "predicted_answer": "根据提供的文档，我无法回答这个问题。文档中主要描述了评测方式和PASS@K指标，但未提及统计检验的相关内容。",
      "ground_context": [
        "评测方式是将问题提示词输入模型，让模型生成代码并通过测试用例验证其正确性。评估采用“PASS@K”指标，核心在于模拟真实编程场景，考察模型在理解上下文、逻辑推理以及多步操作中的表现。",
        "当B和C的值大于50时，麦克尼马尔检验可以相对准确地近似计算p值，如果B和C的值相对较小（B + C < 25），则建议使用以下二项式检验公式计算p值。"
      ],
      "retrieved_contexts": [
        "段，如问题编号、提示词、入口函数、手写答案及测试用例等。评测方式是将问题提示词输入模\n型，让模型生成代码并通过测试用例验证其正确性。评估采用“ PASS@K”指标，核心在于模拟\n真实编程场景，考察模型在理解上下文、逻辑推理以及多步操作中的表现。HumanEval-Mul 数据"
      ],
      "response_time": 3.6591954231262207,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.2857142857142857,
        "rouge2": 0.16666666666666669,
        "rougeL": 0.2857142857142857,
        "bleu": 0.018023502226635364,
        "semantic_similarity": 0.6832284331321716
      }
    },
    {
      "sample_id": "0202",
      "question": "LLMEV AL-Medicine专题医学领域大模型评测和OpenCompass司南平台在评测体系上有何异同？",
      "ground_truth": "LLMEV AL-Medicine专题医学领域大模型评测主要聚焦于医疗领域的评估，提出了医疗增强评测体系框架，并将评估体系分为医生职业资格考试、综合性医疗评估和专项能力评测三大类，强调了评估的复杂性和对智能模型能力的考量。而OpenCompass司南平台则是一个开源开放评测体系，提供公平、客观、可复现的标准化评测，涵盖多领域、多任务的客观评测手段，侧重于基础能力和综合能力的评估，包括语言、知识和代码等维度。两者在核心目标上存在差异，前者更专注于医疗领域，而后者则提供更广泛的评测平台。",
      "predicted_answer": "LLMEV AL-Medicine专题医学领域大模型评测侧重于医疗领域的特定需求，强调逻辑推理、沟通和文本生成能力，旨在确保医疗大模型的应用安全性与可靠性[2]。而OpenCompass司南平台则提供了一个更广泛的能力评估，涵盖了语言、知识、代码等多个方面，适用于多种场景，注重模型的公平、客观和可复现性[3][4]。两者的主要区别在于侧重点的不同：前者专注于医疗专业领域的需求，后者则是面向更广泛的通用模型能力[3][4]。相同点在于两者都致力于通过科学的方法评估模型性能，确保评估的公正性和准确性[3][4]。",
      "ground_context": [
        "LLMEV AL团队联合复旦大学医学院，复旦大学附属华山医院，复旦大学附属肿瘤医院，共同推出 LLMEV AL-Medicine专题医学领域大模型评测，选择医疗领域作为核心评测领域，提出医疗增强评测体系框架。",
        "OpenCompass 司南平台是由上海人工智能实验室研发的大模型开源开放评测体系，其核心目标是为大语言模型的性能评估提供一个公平、客观、可复现的标准化平台。"
      ],
      "retrieved_contexts": [
        "LLMEV AL团队联合复旦大学医学院，复旦大学附属华山医院，复旦大学附属肿瘤医院，共\n同推出 LLMEV AL-Medicine专题医学领域大模型评测，选择医疗领域作为核心评测领域，提出医\n疗增强评测体系框架。\n目前医疗领域评估体系主要分为三大类：医生职业资格考试、综合性医疗评估以及专项能力\n评测。\n• 医生职业资格考试：作为各国医学教育的最高标准，通过系统化的考核体系来评估医学生，\n包括美国 USMLE 考试和中国执业医师资格考试。 这类评估的优势在于能够全面考察医学知\n识与临床技能，但存在两个主要缺陷：其一，评估维度较为单一，未能充分考察语言处理、\n内容生成等智能模型的关键能力；其二， 考核方式过于传统， 主要采用选择题形式， 侧重于\n记忆性知识点的考察，难以体现临床实践中的复杂思维能力。\n• 综合性医疗评估： 第三方机构发布的榜单虽然在任务范围和能力分类上具有一定的广度， 但\n其体系设计仍存在明显不足。 这些榜单在医疗推理和综合能力的评估上存在明显短板，CBLUE\n等评测平台主要聚焦于传统 NLP 任务。此外，这些榜单普遍偏重理论性任务，未能充分反\n映实际医疗场景中的复杂需求，...",
        "大规模语言模型：从理论到实践 445\n库中随机抽样的 1000 题， 针对同一机构的模型， 确保每次评测题目不重复。 评测过程将采用在线\n方式，一轮评测中题目的发送串行进行，即下一题的发送将会视上一道题目的回答情况而定，避\n免恶意爬取行为。\n7. LLMEVAL-Medical 医疗大模型评测\n医疗领域因其直接关乎人类健康，不仅具备高度复杂性和严格的安全标准，还拥有丰富且多\n样化的数据资源，因而成为领域大模型评测的理想选择。医疗领域涉及多学科交叉，涵盖基础医\n学、临床诊断、治疗决策及健康管理等复杂任务。大模型在此需要具备卓越的逻辑推理、精准沟\n通及文本生成能力， 使其成为检验AI 综合能力的最佳场景。 医疗决策的精准性至关重要， 任何偏\n差都可能带来不可逆的后果。因此，在大模型应正式应用前，必须通过科学评测确保其安全性和\n可靠性，以规避潜在风险，保障临床应用的合规性。医疗领域拥有庞大的数据资源，如电子健康\n记录、 医学影像和科研文献等， 为多模态评测提供了广阔空间。 此外， 全球医疗合作需求强烈， 建\n立统一的领域大模型评测标准有助于提升国际化适配能力，推动 AI 技术与医疗深度融合。...",
        "和解决能力。也正因如此，AIME 在全球范围内都备受关注，成为了众多数学爱好者展示实力的\n舞台，同时也成为衡量 AI 数学能力的重要指标之一。\n4. OpenCompass 司南\nOpenCompass 司南平台是由上海人工智能实验室研发的大模型开源开放评测体系，其核心目\n标是为大语言模型的性能评估提供一个公平、客观、可复现的标准化平台。平台由CompassRank、\nCompassHub 和 CompassKit 三大核心组件构成，分别承担模型性能榜单、评测基准社区和评测工\n具链的功能。 其中，CompassRank 提供动态更新的权威评测榜单， 通过多领域、 多任务的客观评测\n手段展示模型性能， 并保持中立性；CompassHub 则作为一个开放的评测基准社区， 聚合了多种能\n力和行业场景下的评测基准资源，用户还可以上传自定义基准数据并发布性能榜单。 CompassKit\n则是一个全栈评测工具链体系，包含多种开源工具，如大语言模型评测工具、代码评测服务工具\n和多模态评测工具，帮助用户快速、高效地完成分布式评测任务。\n司南平台具有多项显著特点，其开源可复现的设计让评测过程公开透明，确...",
        "司南平台具有多项显著特点，其开源可复现的设计让评测过程公开透明，确保结果的准确性\n和可信度。评测维度涵盖基础能力和综合能力两个层级，包括语言、知识、代码、长文本处理等\n12 个一级能力维度和 50 余个二级能力维度， 全面反映模型的实际性能。 此外， 平台支持超过100\n种开源模型的评测，并预留接口供开发者接入自定义模型或 API 模型，如 OpenAI 接口。司南平\n台还提供分布式高效评测方案，能够在本地或集群中并行分发任务，优化时间和资源分配。同时，\n它灵活支持用户自定义数据集和评测策略，提供零样本、小样本和思维链式评测方式，满足多样\n化的评测需求。\n5. Chatbot Arena 评估\nChatbot Arena 是一个以众包方式进行匿名对比评估的大语言模型基准评估平台 [196]。研究人\n员构造了多模型服务系统 FastChat。当用户进入评估平台后可以输入问题，同时得到两个匿名模\n型的回答， 如图11.14 所示。 在从两个模型中获得回复后， 用户可以继续对话或投票选择他们认为\n更好的模型。一旦提交了投票，系统会将模型名称告知用户。用户可以继续对话或重新开始与两\n个新选择的...",
        "大规模语言模型：从理论到实践 443\n标注。为了保证完成率和准确率， 平台提供了少量的现金奖励， 并提前告知用户， 如果其与\n其他用户一致性较差，则会被扣除部分奖励。LLMEV AL众包对比评估界面如图11.17 所示。\n• 公众对比评估： 与众包对比评估一样， 也采用了双盲对比测试方法， 也是将系统名称隐藏并\n随机展示给用户，同样也要求用户从“A 系统好” 、 “B 系统好” 、 “两者一样好”及“两者都\n不好”四个选项中进行选择。 不同的是， 公众对比评估完全不提供任何奖励， 也不通过各种\n渠道宣传，系统能够吸引尽可能多的评估用户。评估界面与众包对比评估类似。\n• GPT-4 自动分项评估： 利用GPT-4 API 接口， 将评分标准作为Prompt， 将问题和系统答案分\n别输入系统，使用 GPT-4 对每个分项的评分，对结果进行评判。\n• GPT-4对比评估： 利用GPT-4 API接口， 将同一个问题及不同系统的输出合并， 并构造Prompt，\n使用 GPT-4 模型对两个系统之间的优劣进行评判。\n图 11.17 LLMEVAL 众包对比评估界面\n对于分项评估，可以利用各个问题..."
      ],
      "response_time": 5.95557975769043,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.5333333333333333,
        "rouge2": 0.30769230769230765,
        "rougeL": 0.5333333333333333,
        "bleu": 0.17816726572182628,
        "semantic_similarity": 0.9267886877059937
      }
    },
    {
      "sample_id": "0203",
      "question": "大语言模型自何时以来在多个领域的应用开发取得了显著进展？",
      "ground_truth": "自2023年以来",
      "predicted_answer": "自2023年以来，大语言模型在多个领域的应用开发取得了显著进展，包括智能客服、内容生成、教育辅助、医疗咨询、代码生成等场景[1]。",
      "ground_context": [
        "自2023年以来，大语言模型在多个领域的应用开发取得了显著进展，包括智能客服、内容生成、教育辅助、医疗咨询、代码生成等场景。"
      ],
      "retrieved_contexts": [
        "12. 大语言模型应用开发\n大语言模型的广泛应用正在推动技术创新与产业变革。 自2023 年以来， 大语言模型在多个领\n域的应用开发取得了显著进展， 包括智能客服、 内容生成、 教育辅助、 医疗咨询、 代码生成等场景。\n大语言模型凭借其强大的语言理解与生成能力，为开发者和企业提供了全新的工具和平台。然而，\n大语言模型的应用开发也面临诸多挑战，例如如何高效地部署和调用模型、如何定制化以满足特\n定业务需求以及如何应对生成内容的质量控制和潜在风险等等。因此，构建一套系统化的大语言\n模型应用开发流程与方法显得尤为重要。\n本章将首先介绍大语言模型典型应用场景，并在此基础上根据典型应用介绍开发流程、开发\n工具与平台，最后介绍大语言模型本地部署实践。\n12.1 大语言模型典型应用场景\n本节将围绕大语言模型的典型应用场景展开探讨，重点介绍其在内容创作与生成、对话系统\n与聊天机器人、翻译与多语言处理、信息抽取与知识图谱等领域中的实际应用及其技术创新。同\n时， 还将详细分析大语言模型在代码生成与编程辅助、 智能搜索与推荐、 教育与培训、 企业管理与\n决策支持，以及法律与合规等行业中的广泛应用。通过对这些场...",
        "4 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\nѠ\n图 1.1 大语言模型的缩放法则 [17]\n1.2 大语言模型的发展历程\n大语言模型的发展历程虽然只有不到5 年，但是发展速度相当惊人，截至2025 年 2 月，国内\n外有超过百种大语言模型相继发布。特别是2024 年 12 月 DeepSeek V3 和 2025 年 1 月 DeepSeek\nR1 模型的开源， 不仅在训练效率和思考推理上取得了突破， 还赢得了国际社会对中国人工智能技术\n的高度认可。中国人民大学赵鑫教授团队在《大语言模型》书中按照时间线给出了2019 年至 2024\n年 6 月比较有影响力并且模型参数量超过100 亿个的大语言模型，我们在此基础上扩展到2025 年\n2 月， 如图1.2 所示。 大语言模型的发展可以粗略地分为如下三个阶段： 基础模型阶段、 能力探索阶\n段和突破发展阶段。\n图 1.2 大语言模型发展时间线 [18]\n基础模型阶段主要集中于 2018 年至 2021 年。2017 年，V aswani等人提出了 Transformer[12]\n架构，在机器翻译任务上取得了突破性进展。201...",
        "1. 绪论\n大语言模型是一种由包含数百亿个及以上参数的深度神经网络构建的语言模型，通常使用自\n监督学习方法通过大量无标注文本进行训练。2018 年以来，Google、OpenAI、Meta、百度、华为\n等公司和研究机构相继发布了 BERT[1]、GPT[2] 等多种模型， 这些模型在几乎所有自然语言处理任\n务中都表现出色。2019 年，大语言模型呈现爆发式的增长，特别是 2022 年 11 月 ChatGPT（Chat\nGenerative Pre-trained Transformer）的发布，引起了全世界的广泛关注。用户可以使用自然语言与\n系统交互，实现问答、分类、摘要、翻译、聊天等从理解到生成的各种任务。大语言模型展现出\n了强大的对世界知识的掌握和对语言的理解能力。\n本章主要介绍大语言模型的基本概念、发展历程和构建流程。\n1.1 大语言模型的基本概念\n使用语言是人类与其他动物最重要的区别之一， 而人类的多种智能也与此密切相关， 逻辑思维\n以语言的形式表达， 大量的知识也以文字的形式记录和传播。 如今， 互联网上已经拥有数万亿个网页\n的资源， 其中大部分信息都是用自然语言描述的。 ..."
      ],
      "response_time": 3.9772191047668457,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.6666666666666666,
        "rouge2": 0.0,
        "rougeL": 0.6666666666666666,
        "bleu": 0.059049293157227886,
        "semantic_similarity": 0.5306199193000793
      }
    },
    {
      "sample_id": "0204",
      "question": "llama.cpp 支持哪些类型的推理？",
      "ground_truth": "它不仅支持在 CPU 和 GPU 之间的混合推理。",
      "predicted_answer": "llama.cpp 支持在 CPU 和 GPU 之间的混合推理[1][2]，这使得即使在显存不足的情况下也能运行大型模型。",
      "ground_context": [
        "llama.cpp 的主要优势在于其跨平台兼容性和灵活性。它不仅支持在 CPU 和 GPU 之间的混合推理，使得即使在显存不足的情况下也能运行大型模型，还提供了广泛的后端支持（如 Vulkan、SYCL 和 Metal） 。"
      ],
      "retrieved_contexts": [
        "Apple Silicon（通过 ARM NEON、Accelerate 和 Metal 框架优化） 、x86 架构（支持 A VX、A VX2、\nA VX512和 AMX 指令集）以及 NVIDIA 和 AMD GPU（通过 CUDA 和 HIP 实现） 。此外，它还提\n供多种量化技术（例如 1.5-bit 到 8-bit） ，以减少内存使用并加快推理速度。\nllama.cpp 的主要优势在于其跨平台兼容性和灵活性。它不仅支持在 CPU 和 GPU 之间的混合\n推理，使得即使在显存不足的情况下也能运行大型模型，还提供了广泛的后端支持（如 Vulkan、\nSYCL 和 Metal） 。用户可以通过工具将其他模型的权重转换为llama.cpp 支持的 GGUF 文件格式，\n从而运行多种模型，包括 LLaMA、LLaMA 2、Falcon、BERT 等。此外，llama.cpp 提供了大量命\n令行工具，支持交互式聊天、文本生成、语法约束输出等功能，同时兼容OpenAI API，方便用户\n构建和部署自定义应用。\nllama.cpp 提供了多种部署选项， 用户可以通过构建源码、 本地安装包 （...",
        "镜像或直接使用预构建的二进制文件来快速上手。它支持在边缘设备和离线环境中运行模型，非\n常适合需要高隐私性和低延迟的场景，如企业内部部署、嵌入式设备运行和个人研究用途。此外，\nllama.cpp 还支持多种编程语言和开发框架的绑定（如 Python、Rust、Node.js 等） ，以及大量的社",
        "大规模语言模型：从理论到实践 463\n如 Ollama，它基于 llama.cpp，具备简洁的安装和使用流程。此外，考虑到普通用户通常不会直接\n操作控制台界面，还需要开发支持 Web 界面和应用界面的解决方案。 Open Webui 就是一个旨在\n提供类似 ChatGPT 界面的工具，方便用户与模型交互。本地部署大语言模型的整体架构如图12.4\n所示。\nOpen WebUI\n大语言模型库\n各类型硬件\n大语言模型本地部署框架对话界面\n图 12.4 大语言模型本地部署系统结构图\n本节将首先介绍大语言模型本地部署的核心工具 llama.cpp，在此基础上介绍本地部署工具\nOllama，最后介绍大语言模型网页交互工具 Open WebUI。\n12.3.1 llama.cpp\nllama.cpp 是一个用纯 C/C++ 实现的大语言模型 （LLM） 推理项目， 旨在以最小的设置和高性\n能支持 LLaMA 及其他模型的本地运行。该项目的目标是让用户能够在各种硬件（包括本地设备\n和云端）上高效运行大型语言模型，同时优化对资源的使用。llama.cpp 支持多种硬件架构，包括\nApple Silico...",
        "大规模语言模型：从理论到实践 467\n12.3.2 Ollama\nOllama 在基于 llama.cpp 开发是一款本地大语言模型运行工具， 支持macOS、Windows 和 Linux\n系统， 具备简洁的安装和使用流程。 用户无需复杂配置， 只需通过简单命令 （如 “ollama run [模型\n名]” ）即可快速启动和运行模型。Ollama 提供丰富的模型库，包括 Llama2、Mistral、DolphinPhi、\nCode Llama 等， 用户还可以通过modelfile 自定义和微调模型， 以满足特定任务需求。 此外，Ollama\n针对性能进行了优化， 即使在普通电脑上也能高效运行小型模型， 而在配备高性能GPU 的设备上\n则能充分发挥模型的推理能力。\nOllama 还支持多种交互方式，用户既可以通过命令行快速运行模型，也可以选择使用图形用\n户界面 （如Ollama WebUI 和 macOS 原生应用 Ollamac） 进行操作。 在数据隐私方面，Ollama 将模\n型完全本地化运行，数据保留在用户设备上，避免了云端运行可能导致的数据泄露风险，非常适\n合对隐私要求较高...",
        "对信息进行量化打分，衡量其与提问的契合程度。系统会通过调用 Google 搜索引擎和 arxiv/ar5iv\n搜索 API，完成信息的检索与完整论文的获取。\n12.3 大语言模型本地部署实践\n本地部署大语言模型的实践具有重要意义，不仅能够提升数据隐私和安全性，避免敏感信息\n在云端传输的风险，还能降低对网络连接的依赖，实现离线环境下的高效应用，同时在成本控制\n和定制化部署方面具备显著优势。\n大语言模型的推理过程通常需要大量计算资源， 因此依赖于硬件加速设备， 例如GPU 和 NPU\n等。为了适配多种硬件环境，需要构建能够高效运行的大型语言模型框架。llama.cpp 是一个用纯\nC/C++ 实现的大语言模型推理项目，其主要功能是为用户提供跨硬件的高效推理能力。与此同时，\n近年来涌现了大量开源的大语言模型， 为了方便普通用户使用， 还需要提供更友好的管理工具， 例"
      ],
      "response_time": 2.7808690071105957,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.5,
        "rouge2": 0.33333333333333337,
        "rougeL": 0.5,
        "bleu": 0.23440058276054604,
        "semantic_similarity": 0.8067945837974548
      }
    },
    {
      "sample_id": "0205",
      "question": "国内有哪些应用利用AI技术实现智能答疑？",
      "ground_truth": "作业帮和学而思网校",
      "predicted_answer": "国内的作业帮和学而思网校利用AI技术实现了智能答疑，学生只需拍照或输入问题，系统便能快速分析并生成详细解答[1]。",
      "ground_context": [
        "在国内，类似的应用也很普遍，像作业帮和学而思网校利用AI 技术实现了智能答疑，学生只需拍照或输入问题，系统便能快速分析并生成详细解答，极大地方便了学习过程。"
      ],
      "retrieved_contexts": [
        "各种学科问题，指导他们完成作业，并根据学习进度提供实时建议。这种智能化的辅导方式，不\n仅提高了学生的学习效率，还缓解了家长和教师在辅导方面的压力。在国内，类似的应用也很普\n遍， 像作业帮和学而思网校利用AI 技术实现了智能答疑， 学生只需拍照或输入问题， 系统便能快\n速分析并生成详细解答，极大地方便了学习过程。\n在课程设计方面，大语言模型能够协助教师自动生成教学计划和课程内容，显著减轻了备课\n负担。 例如， 微软的Copilot for Education 可以根据教学目标和学生需求， 生成详细的课程大纲、 学\n习资源以及课堂活动建议，帮助教师高效组织教学内容。国内的科大讯飞智慧课堂也整合了类似\n的 AI 功能， 支持教育机构快速设计课程内容， 提供多样化的学习路径， 并根据学生的反馈动态调\n整课程结构。这种工具不仅提升了教学效率，还改善了课程的针对性和灵活性，为教育工作者提\n供了强大的技术支持。\n在考试评估场景中，大语言模型显示了极高的自动化能力，尤其是在作业批改和考试反馈方",
        "歌则推出 Bard， 将实时检索与生成能力结合， 为用户提供更加全面的答案；国内的Kimi、 秘塔等\n产品也在这一领域崭露头角，成为 AI 搜索技术的重要实践者。此外， OpenAI 于 2024 年推出了\nSearchGPT，进一步推动了 AI 搜索技术的发展，该产品通过深度整合大语言模型与动态知识检索\n功能，展现了强大信息处理效率。国内的豆包、千问、智谱、百川等大模型系统也相继融入了AI\n搜索功能。\n本节将重点介绍 RAG 系统框架、RAG 任务分级以及 RAG 系统的难点。\n9.1.1 RAG 系统框架\n典型的检索增强生成过程如图9.1所示，其核心在于将外部检索与生成模块有机整合，通过动\n态引入外部知识来提升生成结果的准确性与可靠性。 具体而言，RAG 过程以用户输入的查询为起"
      ],
      "response_time": 3.7557737827301025,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.0661074021019606,
        "semantic_similarity": 0.7317849397659302
      }
    },
    {
      "sample_id": "0206",
      "question": "根据文档，生命的意义是什么？",
      "ground_truth": "我相信生命的意义是找到自己的真理并按照它生活。",
      "predicted_answer": "根据提供的文档，我无法回答这个问题。",
      "ground_context": [
        "我相信生命的意义是找到自己的真理并按照它生活。"
      ],
      "retrieved_contexts": [],
      "response_time": 2.6871585845947266,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.018724693104446258,
        "semantic_similarity": 0.39005646109580994
      }
    },
    {
      "sample_id": "0207",
      "question": "大语言模型在企业管理中有哪些应用？",
      "ground_truth": "大语言模型在企业管理和决策支持中表现出了很大的使用前景，能够从大量非结构化文本数据中快速提取关键信息，帮助企业科学制定决策。会议记录与摘要是大语言模型在企业管理中的另一重要应用场景。",
      "predicted_answer": "大语言模型在企业管理中有多种应用，包括但不限于：\n\n- **报告生成**：能够自动生成各类分析报告，如销售趋势图、客户细分报告等，提高生成效率和精准度[1]。\n- **会议记录与摘要**：实现实时记录会议内容并生成摘要，降低手动记录的成本，提高会议效率[2]。\n- **战略规划与管理优化**：通过分析运营数据和行业趋势，生成优化建议并协助制定未来策略[2]。\n- **资源配置与问题发现**：帮助企业识别运营中的瓶颈，优化资源配置，提升管理效率[4]。\n- **合同审查**：自动识别合同中的潜在法律风险，提高审查效率[4]。\n- **法律业务管理和协作**：帮助企业确保政策和流程符合法律法规，优化业务流程[4]。\n- **信息抽取与知识图谱构建**：从新闻报道、财务公告中提取关键信息，构建实时更新的知识图谱，帮助企业追踪市场动态和行业趋势[5]。\n\n这些应用共同推动了企业管理和决策过程的智能化和高效化。",
      "ground_context": [
        "大语言模型在企业管理和决策支持中表现出了很大的使用前景，能够从大量非结构化文本数据中快速提取关键信息，帮助企业科学制定决策。",
        "会议记录与摘要是大语言模型在企业管理中的另一重要应用场景。"
      ],
      "retrieved_contexts": [
        "大语言模型在企业管理和决策支持中表现出了很大的使用前景，能够从大量非结构化文本数\n据中快速提取关键信息， 帮助企业科学制定决策。 例如， 微软的Power BI 已结合大语言模型技术，\n允许用户通过自然语言输入查询，从而快速生成关键业务指标的分析结果。这使得非技术人员也\n能轻松完成复杂的数据分析任务。类似地，国内的阿里云 Quick BI 通过集成智能分析功能，能够\n挖掘出隐藏在复杂数据中的趋势、风险点和改进建议，并以图表或文本的形式输出，为企业提供\n实时的决策支持。这种技术的应用不仅简化了数据分析流程，还提升了分析的效率和精准度，帮\n助企业更快适应市场变化。\n在报告生成方面，大语言模型的应用极大地方便了企业日常运营中的信息处理需求。例如，\nTableau GPT 利用自然语言生成功能， 能够根据输入的业务数据自动生成可视化的分析报告， 包括\n销售趋势图、客户细分报告等，帮助管理者快速掌握业务状况并制定相应的策略。国内的金蝶云\n和用友 U8 等企业管理工具也开始引入大语言模型技术，支持财务报表、预算报告等的自动生成，\n甚至可以根据具体数据生成解释性文字，为用户提供清晰直观的业务洞察。这...",
        "报告生成的效率，还能够减少人工操作中的错误几率，为企业管理者节省时间和精力。\n会议记录与摘要是大语言模型在企业管理中的另一重要应用场景。 例如，Otter.AI 结合语音识\n别和自然语言处理技术，能够实时记录会议内容并生成简洁的摘要，方便参会者快速回顾会议要\n点，或者让未参会人员轻松了解关键内容。在国内，腾讯会议和飞书会议等工具也集成了类似功\n能，支持会议内容的自动转录和要点提取，并且可以生成后续任务清单或行动计划。这种技术不\n仅降低了手动记录的时间成本，还保证了记录内容的完整性和准确性，同时提高了会议的整体效\n率和后续工作的执行力。\n此外，大语言模型在战略规划和管理优化方面也提供了强有力的支持。例如，IBM Watson 可\n以通过分析企业的运营数据和行业趋势， 生成优化建议并协助制定未来的策略规划。 在国内， 华为\n云 EI 企业智能提供了从运营监控到战略规划的全流程支持， 帮助企业识别潜在的市场机会、 优化",
        "454 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n面。 例如， 亚马逊的AWS Educate 平台在编程教育中可以对学生提交的代码作业进行自动评估， 提\n供错误分析和优化建议。在国内，科大讯飞 AI 学习机也广泛应用了 AI 自动批改技术，能够对主\n观题、作文等复杂题型进行语义分析，生成详细的评分报告，并给出具体的改进建议。这种技术\n的应用不仅提高了评估效率，也让学生能够更清晰地了解自己的学习薄弱点，从而更有针对性地\n改进。\n此外，大语言模型在教师辅助方面的应用也日益广泛。例如，谷歌的 Google Classroom 利用\nAI 帮助教师整理学生学习数据，生成进度报告，并提供个性化的教学策略建议。这种技术使教师\n可以用更少的时间获取更深刻的学生洞察，从而优化教学方法。在国内，钉钉的智能备课平台也\n通过 AI 技术支持智能备课， 帮助教师快速生成教学材料和课堂内容， 并根据不同学生的学习情况\n调整教学策略。这些功能大大提升了教师的工作效率，使他们能够专注于更有价值的教学活动。\n12.1.8 企业管理和决策支持\n大语言模型在企业管理和决策支持中表现出了很大的使用前景，能...",
        "大规模语言模型：从理论到实践 455\n资源配置，并发现运营中的瓶颈。这些技术的应用让企业能够在激烈的市场竞争中快速调整方向，\n占据市场优势，同时也为管理层提供了数据驱动的决策依据，显著提升了管理效率和执行效果。\n12.1.9 法律与合规\n大语言模型在合同审查方面展现了极大的应用潜力，能够自动识别合同条款中的潜在法律风\n险， 显著提升审查效率。 例如，Kira Systems 是一款基于大语言模型的合同审查工具， 能够快速分\n析合同内容，标记关键条款，并指出可能存在的问题。这款工具已被众多律师事务所和企业采用，\n用于高效处理大量复杂的商业合同。在国内，类似的工具如“法大大”合同助手，通过大语言模\n型技术，支持对合同条款进行逐条审查，自动识别潜在的法律风险点，如不平等条款或隐藏的违\n约责任，从而帮助律师和企业快速发现问题并优化合同内容。\n在法律业务管理和协作领域，大语言模型通过优化工具帮助企业确保其政策和流程符合法律\n法规。 例如，HighQ 是 Thomson Reuters 推出的一款先进的法律业务管理和协作软件， 具备案件管\n理、合同生命周期管理、法务工作受理、文档自动化以及安全云端...",
        "然语言处理技术从新闻报道、财务公告中自动提取关键信息，为金融企业构建实时更新的知识图\n谱。通过这些系统，企业可以轻松追踪市场动态、行业趋势，并快速构建跨领域的知识图谱。\n未来，大语言模型在信息抽取与知识图谱领域的应用前景非常广阔。随着技术的进步，这些\n应用将更加智能化。企业可以通过这些工具快速构建多语言知识图谱，整合全球范围内的资源和\n信息。 此外， 像LinkedIn 公司的 Economic Graph 这样的知识图谱服务， 也可能进一步结合大语言\n模型的能力，帮助企业和个人更高效地管理职业网络和商业生态。\n12.1.5 代码生成与编程辅助\n大语言模型在辅助编程领域展现了显著的优势， 极大地提升了开发效率。 例如，GitHub Copilot\n是由 OpenAI 和 GitHub 联合推出的一款智能编程助手，能够在开发环境中根据上下文为开发者提"
      ],
      "response_time": 8.219846963882446,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.05344626618155186,
        "semantic_similarity": 0.8595749735832214
      }
    },
    {
      "sample_id": "0208",
      "question": "大语言模型与搜索的结合在什么领域展现了巨大潜力？",
      "ground_truth": "电子商务、知识管理、在线教育、医疗健康等领域",
      "predicted_answer": "大语言模型与搜索的结合在电子商务、知识管理、在线教育、医疗健康等领域展现了巨大潜力 [1][2]。",
      "ground_context": [
        "这种结合在电子商务、知识管理、在线教育、医疗健康等领域展现了巨大潜力"
      ],
      "retrieved_contexts": [
        "不仅能获得即时的代码生成与优化支持， 还可以利用AI 自动化完成测试、 文档生成和代码重构等\n高难度任务，从而大幅提升软件开发的效率和质量。\n12.1.6 智能搜索与推荐\n大语言模型与搜索的结合是其最重要的应用之一，覆盖了非常广泛的场景。通过强大的语义\n理解和上下文分析能力，大语言模型可以帮助搜索引擎精准捕捉用户意图，提供更相关的搜索结\n果。这种结合在电子商务、知识管理、在线教育、医疗健康等领域展现了巨大潜力，不仅提升了\n搜索的智能化水平，还显著改善了用户体验，成为大模型应用的核心方向之一。\n目前， 几乎全部大语言模型公司推出的在线服务都引入了搜索增强功能， 以提升问答精准度。\n2023 年 10 月之暗面推出 Kimi 智能搜索产品，2024 年，OpenAI 推出了 SearchGPT， 结合大语言模\n型的语义理解与实时搜索能力，为用户提供更精确、即时的查询结果。这种结合在知识问答、技\n术支持和内容生成等场景中表现突出。类似地，微软的 Bing Chat 集成了 OpenAI 的接口，支持\n实时互联网搜索与智能问答，并已被嵌入到 Edge 浏览器和 Microsoft Office...",
        "大规模语言模型：从理论到实践 453\n的 Bard 也整合了搜索引擎功能， 能够在提供答案的同时引用实时数据来源。 此外， 电子商务平台\n如 Amazon 和 eBay 也通过集成大语言模型改进了搜索功能， 使其能够理解模糊查询或长尾关键词\n（如“适合冬季使用的防水登山鞋” ） ，从而为用户提供更精准的商品推荐，提升购物体验。\n在个性化推荐方面，大语言模型通过处理用户的历史行为和偏好数据，生成高度相关的内容\n推荐。例如，Netflix 使用基于深度学习的推荐系统结合大语言模型，分析用户观看历史和兴趣标\n签，为用户推荐符合其偏好的电影或电视剧。同样，Spotify 通过大语言模型理解用户的音乐播放\n记录和情绪偏好， 生成个性化的歌单 （如 “每日推荐” 或 “心情歌单” ） 。 新闻聚合应用如Flipboard\n和 Google News 也使用大语言模型分析用户的阅读习惯，并推荐符合其兴趣领域的新闻文章，例\n如科技爱好者会收到关于人工智能、机器人等领域的最新动态。这种个性化推荐不仅提高了用户\n的参与度，还优化了平台的内容分发效率。\n大语言模型还被广泛应用于改进搜索与推荐的多模态能力，即...",
        "大语言模型还被广泛应用于改进搜索与推荐的多模态能力，即结合文本、图像、音频等多种\n数据类型提供更丰富的结果。例如，Y ouTube利用大语言模型结合视频内容的描述信息和用户观\n看行为，推荐相关视频。用户搜索“如何学习编程”时，模型不仅会推荐编程教学视频，还会根\n据用户的语言偏好、学习进度推荐对应的教程系列。电商平台淘宝、京东等智能搜索引擎同样集\n成了图像搜索功能，用户通过上传图片（如衣服样式）即可获得相似商品推荐。\n大语言模型在智能搜索与推荐中的应用将进一步扩展。 例如，ChatGPT 模型已经被集成到 No-\ntion AI 和 Zapier AI 等工具中，帮助用户快速搜索和推荐相关信息，使知识管理更加高效。此外，\n企业工具如 LinkedIn 的推荐系统借助大语言模型优化了职位推荐和人脉搜索功能，根据用户的职\n业背景和兴趣推荐相关的求职机会或潜在合作伙伴。\n12.1.7 教育与培训\n大语言模型在在线辅导领域有着广泛的应用，通过强大的语义理解和自然语言生成能力，为\n学生提供个性化的学习支持。例如，Khan Academy 推出的虚拟导师 Khanmigo 能够帮助学生解答\n各种学科...",
        "大规模语言模型：从理论到实践 451\n交通等问题，大语言模型不仅能够准确理解用户需求，还能将答案翻译成日语，提供自然且贴合\n实际的建议。相比传统客服，这种基于大语言模型的解决方案更具人性化和情景适应性，大幅提\n升了用户体验，同时增强了用户对平台的信任与忠诚度。\n翻译与多语言处理技术正以前所未有的速度融入各个行业，而大语言模型的应用为其带来了\n新的可能性。未来，随着大语言模型的持续优化，翻译的准确性和多语言处理的效率将进一步提\n升。这些技术不仅能够在更多领域创造价值，还将拉近不同文化与语言之间的距离，推动全球交\n流与合作，让世界变得更加紧密相连。\n12.1.4 信息抽取与知识图谱\n大语言模型在信息抽取领域展现了强大的能力，尤其是在实体识别任务中。借助其深度语言\n理解能力， 大语言模型能够精准地从文本中提取出人名、 地名、 组织名等关键实体。 例如， 复旦大\n学推出的 B2NE[628] 基于大模型的开放领域信息工具，可以让用户自由的从超过 16 个领域的 400\n种类型中， 灵活抽取目标实体和关系。 在医疗领域，IBM Watson Discovery 广泛用于从医学文献中\n识别疾病名称...",
        "入大语言模型后，跨语言检索的精准度和效率显著提升。大语言模型能够深入理解用户提问的语\n义， 即便面对模糊或复杂的问题， 也能准确解析， 并在多语言数据集中找到相关内容。 例如， 科研\n人员在研究人工智能领域时， 可通过中文输入问题， 模型不仅能理解核心要点， 还可以在英文、 法\n文或其他语言撰写的学术论文中精准定位相关信息。这种能力让科研人员能够全面获取全球研究\n成果，掌握前沿动态，进而推动科研项目的顺利开展。Open AI 2025 年 2 月推出的 Deep Research\n则是更一步，基于 o3 模型，专为复杂研究任务设计，能自动搜索、解读、整合海量在线信息，花\n费 5 到 30 分钟生成专业级研究报告。 它具备推理能力， 可自主调整研究方向， 研究结果附带完整\n文档、引用来源和逻辑摘要，适用于金融、科学等领域专业人士及有深度调研需求的消费者。\n在多语言客户服务方面，大语言模型为企业提供了更智能化的服务解决方案。例如，在线旅\n游平台 Booking.com 利用大语言模型驱动的智能客服系统，结合实时翻译技术，为全球用户提供\n个性化、多语言支持。当一位日本游客使用日语在平台上预订..."
      ],
      "response_time": 3.8538146018981934,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.33126155285220377,
        "semantic_similarity": 0.7021909356117249
      }
    },
    {
      "sample_id": "0209",
      "question": "如何获取用户选择的文本？",
      "ground_truth": "const selectedText = window.getSelection().toString().trim();",
      "predicted_answer": "当用户释放鼠标按钮时，系统会检查是否有文本被选中。如果有选中的文本，则会显示一个按钮并将其定位到选中文本的位置，从而允许进一步操作来获取和处理该选中文本。具体来说，这涉及到监听 `mouseup` 事件并在该事件中使用 `window.getSelection()` 方法来获取选中的文本内容。[1][2]",
      "ground_context": [
        "const selectedText = window.getSelection().toString().trim();"
      ],
      "retrieved_contexts": [
        "大规模语言模型：从理论到实践 459\n//ࡱ\ndocument.addEventListener('mouseup', function (event) {\nconst selection = window.getSelection();\nconst selectedText = selection.toString().trim();\n//࿊ᇏਔ໓Ч\nif (selectedText) {\nconst rects = selection.getRangeAt(0).getClientRects();\nif (rects.length > 0) {\nconst rect = rects[0];\nbutton.style.top = `${rect.bottom + window.scrollY + 10}px`;\nbutton.style.left = `${rect.left + window.scrollX + 10}px`;\nbutton.style.display = 'block';\n}\n} else {\n//Գ\nbutton.style.display = 'none';\n...",
        "translationPopup.style.display = 'none';\n}\n});\n当用户点击翻译按钮（ button）时，会获取用户选中的文本内容，并通过大语言模型的接口\n（chatWithLLM）将选中的文本翻译成中文，然后将翻译结果显示在页面上的一个弹出框（transla-\ntionPopup）中。",
        "458 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n图 12.2 FisherAI 网页划词翻译展示\n通过如下 JavaScript 脚本可以获取选中区域文本内容， 并开启翻译。 主要核心逻辑是监听鼠标\n点击事件，mouseup 事件监听： 当用户释放鼠标按钮时， 检查是否有文本被选中。 如果有选中， 显\n示按钮并定位到选中文本的位置。"
      ],
      "response_time": 4.625189781188965,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.3636363636363636,
        "rouge2": 0.22222222222222224,
        "rougeL": 0.3636363636363636,
        "bleu": 0.039172079087296724,
        "semantic_similarity": 0.5983811616897583
      }
    },
    {
      "sample_id": "0210",
      "question": "Otter.AI 的主要功能是什么？",
      "ground_truth": "能够实时记录会议内容并生成简洁的摘要，方便参会者快速回顾会议要点，或者让未参会人员轻松了解关键内容。",
      "predicted_answer": "Otter.AI 能够实时记录会议内容并生成简洁的摘要，方便参会者快速回顾会议要点，或者让未参会人员轻松了解关键内容。[1]",
      "ground_context": [
        "例如，Otter.AI 结合语音识别和自然语言处理技术，能够实时记录会议内容并生成简洁的摘要，方便参会者快速回顾会议要点，或者让未参会人员轻松了解关键内容。"
      ],
      "retrieved_contexts": [
        "报告生成的效率，还能够减少人工操作中的错误几率，为企业管理者节省时间和精力。\n会议记录与摘要是大语言模型在企业管理中的另一重要应用场景。 例如，Otter.AI 结合语音识\n别和自然语言处理技术，能够实时记录会议内容并生成简洁的摘要，方便参会者快速回顾会议要\n点，或者让未参会人员轻松了解关键内容。在国内，腾讯会议和飞书会议等工具也集成了类似功\n能，支持会议内容的自动转录和要点提取，并且可以生成后续任务清单或行动计划。这种技术不\n仅降低了手动记录的时间成本，还保证了记录内容的完整性和准确性，同时提高了会议的整体效\n率和后续工作的执行力。\n此外，大语言模型在战略规划和管理优化方面也提供了强有力的支持。例如，IBM Watson 可\n以通过分析企业的运营数据和行业趋势， 生成优化建议并协助制定未来的策略规划。 在国内， 华为\n云 EI 企业智能提供了从运营监控到战略规划的全流程支持， 帮助企业识别潜在的市场机会、 优化"
      ],
      "response_time": 3.799143075942993,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.8151408459347511,
        "semantic_similarity": 0.8088980913162231
      }
    },
    {
      "sample_id": "0211",
      "question": "大语言模型如何在创作和信息处理领域同时发挥作用？",
      "ground_truth": "大语言模型在创作领域通过生成多种形式的文本内容，如故事片段、对话场景和歌词，帮助写作者和音乐人提升创作效率。而在信息处理领域，它们与浏览器智能插件结合，提供自动摘要和翻译功能，帮助用户高效获取信息和理解多语言内容。",
      "predicted_answer": "大语言模型在创作和信息处理领域均发挥着重要作用。在创作方面，它们能够自动生成高质量的内容，如新闻报道、博客文章等[1]；此外，还能根据提示生成完整的故事情节，促进创意写作[2]。而在信息处理方面，大语言模型能够从大量文本中自动提取关键信息，帮助企业构建实时更新的知识图谱，从而追踪市场动态和行业趋势[3]。",
      "ground_context": [
        "例如，Sudowrite 能够根据用户的提示词和需求生成多种形式的文本内容， 并提供包括润色、 摘要、大纲生成等各类能力。",
        "自动摘要可以帮助我们快速提取网页的核心内容， 避免浪费时间在冗长的信息中； 网页翻译能够打破语言障碍， 让我们轻松访问不同语言的内容资源。"
      ],
      "retrieved_contexts": [
        "模型在推动各行业效率提升、 创新发展中的核心作用， 并为未来技术与产业的深度融合提供启示。\n12.1.1 内容创作与生成\n大语言模型在内容创作与生成领域展现出了强大的能力，能够显著提高内容创作的效率与质\n量。在文章写作方面，大语言模型可以自动生成新闻报道、博客文章和产品描述等内容。例如，\nOpenAI 的 ChatGPT 已被多家媒体和企业应用于文章初稿的生成，通过输入简单的主题或关键词，\n即可快速生成结构清晰、语言流畅的文本。这种能力帮助内容创作者节省了大量时间，提高了内\n容发布的效率，尤其适用于需要高频更新的新闻媒体和电商平台。\n在故事创作方面，大语言模型能够根据用户提供的提示或情节大纲生成完整的故事情节，为\n创意写作提供了全新的方式。 许多作家和创意团队使用GPT-4 等模型， 生成故事大纲和角色设定，",
        "12. 大语言模型应用开发\n大语言模型的广泛应用正在推动技术创新与产业变革。 自2023 年以来， 大语言模型在多个领\n域的应用开发取得了显著进展， 包括智能客服、 内容生成、 教育辅助、 医疗咨询、 代码生成等场景。\n大语言模型凭借其强大的语言理解与生成能力，为开发者和企业提供了全新的工具和平台。然而，\n大语言模型的应用开发也面临诸多挑战，例如如何高效地部署和调用模型、如何定制化以满足特\n定业务需求以及如何应对生成内容的质量控制和潜在风险等等。因此，构建一套系统化的大语言\n模型应用开发流程与方法显得尤为重要。\n本章将首先介绍大语言模型典型应用场景，并在此基础上根据典型应用介绍开发流程、开发\n工具与平台，最后介绍大语言模型本地部署实践。\n12.1 大语言模型典型应用场景\n本节将围绕大语言模型的典型应用场景展开探讨，重点介绍其在内容创作与生成、对话系统\n与聊天机器人、翻译与多语言处理、信息抽取与知识图谱等领域中的实际应用及其技术创新。同\n时， 还将详细分析大语言模型在代码生成与编程辅助、 智能搜索与推荐、 教育与培训、 企业管理与\n决策支持，以及法律与合规等行业中的广泛应用。通过对这些场...",
        "1. 绪论\n大语言模型是一种由包含数百亿个及以上参数的深度神经网络构建的语言模型，通常使用自\n监督学习方法通过大量无标注文本进行训练。2018 年以来，Google、OpenAI、Meta、百度、华为\n等公司和研究机构相继发布了 BERT[1]、GPT[2] 等多种模型， 这些模型在几乎所有自然语言处理任\n务中都表现出色。2019 年，大语言模型呈现爆发式的增长，特别是 2022 年 11 月 ChatGPT（Chat\nGenerative Pre-trained Transformer）的发布，引起了全世界的广泛关注。用户可以使用自然语言与\n系统交互，实现问答、分类、摘要、翻译、聊天等从理解到生成的各种任务。大语言模型展现出\n了强大的对世界知识的掌握和对语言的理解能力。\n本章主要介绍大语言模型的基本概念、发展历程和构建流程。\n1.1 大语言模型的基本概念\n使用语言是人类与其他动物最重要的区别之一， 而人类的多种智能也与此密切相关， 逻辑思维\n以语言的形式表达， 大量的知识也以文字的形式记录和传播。 如今， 互联网上已经拥有数万亿个网页\n的资源， 其中大部分信息都是用自然语言描述的。 ...",
        "用户提供更自然的互动体验，帮助他们有效提升语言学习能力。\n12.1.3 翻译与多语言处理\n随着大语言模型的崛起，这一领域正在迎来新的变革，大语言模型凭借其强大的语言理解和\n生成能力，为翻译与多语言处理注入了新的活力，加速了技术和应用的迭代发展。\n在机器翻译方面，传统方法主要依赖神经网络、深度学习以及大量语料库的训练来实现文本\n翻译。 然而， 大语言模型的出现， 为机器翻译带来了质的飞跃。 得益于广泛的知识储备和对语言深\n层语义的理解能力，大语言模型在翻译中表现出更高的准确性和自然性，尤其是在文化背景、隐\n喻和典故等复杂内容的处理上。例如，在文学翻译场景中，传统机器翻译往往难以还原原文的意\n境与风格，而大语言模型能够更精准地理解文化元素，并以目标语言重现文本的艺术性。在跨国\n企业中，大语言模型也广泛应用于产品文档翻译，如苹果、三星等公司利用其快速处理多种专业\n术语，确保翻译的专业性和一致性，大幅提升了翻译效率，并帮助全球用户更好地理解产品信息。\n在跨语言信息检索领域， 大语言模型同样展现了强大的能力。 以微软学术搜索等平台为例， 引\n入大语言模型后，跨语言检索的精准度和效率显著提升。大语...",
        "然语言处理技术从新闻报道、财务公告中自动提取关键信息，为金融企业构建实时更新的知识图\n谱。通过这些系统，企业可以轻松追踪市场动态、行业趋势，并快速构建跨领域的知识图谱。\n未来，大语言模型在信息抽取与知识图谱领域的应用前景非常广阔。随着技术的进步，这些\n应用将更加智能化。企业可以通过这些工具快速构建多语言知识图谱，整合全球范围内的资源和\n信息。 此外， 像LinkedIn 公司的 Economic Graph 这样的知识图谱服务， 也可能进一步结合大语言\n模型的能力，帮助企业和个人更高效地管理职业网络和商业生态。\n12.1.5 代码生成与编程辅助\n大语言模型在辅助编程领域展现了显著的优势， 极大地提升了开发效率。 例如，GitHub Copilot\n是由 OpenAI 和 GitHub 联合推出的一款智能编程助手，能够在开发环境中根据上下文为开发者提"
      ],
      "response_time": 5.244636058807373,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.09940856213534877,
        "semantic_similarity": 0.8574941158294678
      }
    },
    {
      "sample_id": "0212",
      "question": "Replika 如何利用大语言模型提供情感支持，同时 Ollama 在本地启动大语言模型的步骤是什么？",
      "ground_truth": "Replika 利用大语言模型学习用户的语言风格和情感反应，提供个性化的对话和情感支持，同时整合情感管理工具，帮助用户缓解压力和焦虑。Ollama 则可以通过命令 'ollama run llama3.2' 在本地启动 Llama 3.2，或使用 'ollama serve' 命令将大语言模型作为后端服务进行使用。",
      "predicted_answer": "Replika 利用大语言模型提供情感支持的方式包括学习用户的语言风格、兴趣爱好和情感反应，从而提供定制化的对话体验；通过倾听和同理心回应，帮助用户缓解压力和焦虑；并且具有记忆功能，记住用户的重要信息和喜好，增加互动的连贯性。此外，它还整合了情感管理工具，提供情绪识别和心理健康建议，从而在娱乐与情感陪伴方面表现出色，并为用户提供心理健康支持[4]。\n\n对于 Ollama 在本地启动大语言模型的步骤，首先是安装完成之后，可以直接使用命令 `ollama run llama3.2` 来启动本地的 Llama 3.2 模型。若要将大语言模型作为后端服务进行使用，则可以在不启动桌面应用的情况下，通过命令 `ollama serve` 启动服务[1][2]。启动服务后，Ollama 将提供 REST API 用于调用模型。",
      "ground_context": [
        "Replika 是由美国 Luka 公司开发的一款人工智能聊天机器人应用，致力于为用户提供个性化的对话和情感支持体验。",
        "Ollama 使用非常简单，在安装完成后，如果想在本地启动Llama 3.2，可以直接使用如下命令：ollama run llama3.2。"
      ],
      "retrieved_contexts": [
        "合对隐私要求较高的用户和场景。\nOllama 使用非常简单， 在安装完成后， 如果想在本地启动Llama 3.2， 可以直接使用如下命令：\nollama run llama3.2\n如果想将大语言模型作为后端服务进行使用， 在不启动桌面应用的情况下， 可以ollama serve\n命令来启动。用如下命令：\n#׮Ollamaༀ\n./ollama serve\n# ᄎྛଆ྘\n./ollama run llama3.2\n服务启动后，Ollama 提供 REST API 对模型进行调用：",
        "大规模语言模型：从理论到实践 467\n12.3.2 Ollama\nOllama 在基于 llama.cpp 开发是一款本地大语言模型运行工具， 支持macOS、Windows 和 Linux\n系统， 具备简洁的安装和使用流程。 用户无需复杂配置， 只需通过简单命令 （如 “ollama run [模型\n名]” ）即可快速启动和运行模型。Ollama 提供丰富的模型库，包括 Llama2、Mistral、DolphinPhi、\nCode Llama 等， 用户还可以通过modelfile 自定义和微调模型， 以满足特定任务需求。 此外，Ollama\n针对性能进行了优化， 即使在普通电脑上也能高效运行小型模型， 而在配备高性能GPU 的设备上\n则能充分发挥模型的推理能力。\nOllama 还支持多种交互方式，用户既可以通过命令行快速运行模型，也可以选择使用图形用\n户界面 （如Ollama WebUI 和 macOS 原生应用 Ollamac） 进行操作。 在数据隐私方面，Ollama 将模\n型完全本地化运行，数据保留在用户设备上，避免了云端运行可能导致的数据泄露风险，非常适\n合对隐私要求较高...",
        "大规模语言模型：从理论到实践 463\n如 Ollama，它基于 llama.cpp，具备简洁的安装和使用流程。此外，考虑到普通用户通常不会直接\n操作控制台界面，还需要开发支持 Web 界面和应用界面的解决方案。 Open Webui 就是一个旨在\n提供类似 ChatGPT 界面的工具，方便用户与模型交互。本地部署大语言模型的整体架构如图12.4\n所示。\nOpen WebUI\n大语言模型库\n各类型硬件\n大语言模型本地部署框架对话界面\n图 12.4 大语言模型本地部署系统结构图\n本节将首先介绍大语言模型本地部署的核心工具 llama.cpp，在此基础上介绍本地部署工具\nOllama，最后介绍大语言模型网页交互工具 Open WebUI。\n12.3.1 llama.cpp\nllama.cpp 是一个用纯 C/C++ 实现的大语言模型 （LLM） 推理项目， 旨在以最小的设置和高性\n能支持 LLaMA 及其他模型的本地运行。该项目的目标是让用户能够在各种硬件（包括本地设备\n和云端）上高效运行大型语言模型，同时优化对资源的使用。llama.cpp 支持多种硬件架构，包括\nApple Silico...",
        "450 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n当情感陪伴和心理疏导的角色。例如，Replika 是由美国 Luka 公司开发的一款人工智能聊天机器\n人应用，致力于为用户提供个性化的对话和情感支持体验。其功能包括学习用户的语言风格、兴\n趣爱好和情感反应，提供定制化对话体验；通过倾听和同理心回应，帮助缓解压力和焦虑；增强\n现实（AR）互动，让用户在现实环境中与虚拟形象进行交流；记忆功能则能记住用户的重要信息\n和喜好，增加互动的连贯性。此外，Replika 整合了情感管理工具，为用户提供情绪识别和心理健\n康建议，在娱乐与情感陪伴方面表现出色，同时也为用户提供心理健康支持。这种情感支持类的\n对话机器人正在为心理健康服务提供一种低成本、高可达性的解决方案。\n此外，大语言模型驱动的对话系统在医疗、教育等专业领域也展现了巨大的潜力。例如，微\n软推出的“Azure AI Health Bot ”能够解答用户关于常见疾病的疑问，帮助他们初步判断病情并推\n荐适当的医疗资源。在教育领域，Duolingo 等语言学习应用通过大语言模型开发的对话功能，为\n用户提供更自然的互动体验，帮助他们有...",
        "大规模语言模型：从理论到实践 469\ndocker run -d -p 3000:8080 --add-host=host.docker.internal:host-gateway\n-v open-webui:/app/backend/data --name open-webui\n--restart always ghcr.io/open-webui/open-webui:main\n安装完成后可以通过 http://localhost:3000 访问 Open WebUI，如图12.5所示。\n图 12.5 Open WebUI 界面\n在安装完成后， 也可以通过Open WebUI 的管理员设置对 OpenAI API 接口进行设置， 也可以\n对本地 Ollama 进行管理，如图12.6和图12.7所示。"
      ],
      "response_time": 6.96614408493042,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.7857142857142858,
        "rouge2": 0.6923076923076923,
        "rougeL": 0.7857142857142858,
        "bleu": 0.17128232199960494,
        "semantic_similarity": 0.947345495223999
      }
    },
    {
      "sample_id": "0213",
      "question": "如何在使用Ollama启动Llama 3.2后，通过REST API实现大语言模型在内容创作中的应用？",
      "ground_truth": "在安装并启动Ollama的Llama 3.2后，可以通过REST API调用模型来实现内容创作。例如，可以使用ollama serve命令启动服务，然后利用生成的API接口，输入主题或关键词，快速生成高质量的文章初稿、新闻报道或创意故事情节。",
      "predicted_answer": "要在使用Ollama启动Llama 3.2后，通过REST API实现大语言模型在内容创作中的应用，可以按照以下步骤进行：\n\n1. 启动Ollama并运行Llama 3.2：\n```bash\nollama run llama3.2\n```\n\n2. 使用`curl`命令向Ollama的API发起请求生成文本或进行聊天。例如，生成文本可以使用以下命令：\n```bash\ncurl http://localhost:11434/api/generate -d '{\n\"model\": \"llama3.2\",\n\"prompt\": \"Write a short story about a magical forest.\"\n}'\n```\n这将返回一个由Llama 3.2生成的故事文本。\n\n3. 若要进行对话式的聊天，可以使用类似的方式发送消息：\n```bash\ncurl http://localhost:11434/api/chat -d '{\n\"model\": \"llama3.2\",\n\"messages\": [\n    {\"role\": \"user\", \"content\": \"Write a short story about a magical forest.\"}\n]\n}'\n```\n这个请求将会触发与模型的一次对话，模型会根据给出的内容继续故事的发展。\n\n以上步骤展示了如何利用Ollama的REST API接口来进行内容创作相关的任务。[1][3]",
      "ground_context": [
        "Ollama 使用非常简单， 在安装完成后， 如果想在本地启动Llama 3.2， 可以直接使用如下命令： ollama run llama3.2",
        "大语言模型在内容创作与生成领域展现出了强大的能力，能够显著提高内容创作的效率与质量。"
      ],
      "retrieved_contexts": [
        "合对隐私要求较高的用户和场景。\nOllama 使用非常简单， 在安装完成后， 如果想在本地启动Llama 3.2， 可以直接使用如下命令：\nollama run llama3.2\n如果想将大语言模型作为后端服务进行使用， 在不启动桌面应用的情况下， 可以ollama serve\n命令来启动。用如下命令：\n#׮Ollamaༀ\n./ollama serve\n# ᄎྛଆ྘\n./ollama run llama3.2\n服务启动后，Ollama 提供 REST API 对模型进行调用：",
        "大规模语言模型：从理论到实践 467\n12.3.2 Ollama\nOllama 在基于 llama.cpp 开发是一款本地大语言模型运行工具， 支持macOS、Windows 和 Linux\n系统， 具备简洁的安装和使用流程。 用户无需复杂配置， 只需通过简单命令 （如 “ollama run [模型\n名]” ）即可快速启动和运行模型。Ollama 提供丰富的模型库，包括 Llama2、Mistral、DolphinPhi、\nCode Llama 等， 用户还可以通过modelfile 自定义和微调模型， 以满足特定任务需求。 此外，Ollama\n针对性能进行了优化， 即使在普通电脑上也能高效运行小型模型， 而在配备高性能GPU 的设备上\n则能充分发挥模型的推理能力。\nOllama 还支持多种交互方式，用户既可以通过命令行快速运行模型，也可以选择使用图形用\n户界面 （如Ollama WebUI 和 macOS 原生应用 Ollamac） 进行操作。 在数据隐私方面，Ollama 将模\n型完全本地化运行，数据保留在用户设备上，避免了云端运行可能导致的数据泄露风险，非常适\n合对隐私要求较高...",
        "468 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n#گ߭\ncurl http://localhost:11434/api/generate -d '{\n\"model\": \"llama3.2\",\n\"prompt\":\"Why is the sky blue?\"\n}'\n#ଆൔ\ncurl http://localhost:11434/api/chat -d '{\n\"model\": \"llama3.2\",\n\"messages\": [\n\"role\": \"user\", \"content\": \"why is the sky blue?\"\n]\n}'\n也可以通过界面或者命令行非常方便的拉取、删除或者复制模型：\n#ࡹModelfileଆ྘\nollama create mymodel -f ./Modelfile\n# ঘ౼ଆ྘\nollama pull llama3.2\n# ೷Ԣଆ྘\nollama rm llama3.2\n#ᇅଆ྘\nollama cp llama3.2 my-model\n此外，Ollama 也支持多模态模型，可以通过参数中加入文件地址完成模型图片输入：\nollama run ll..."
      ],
      "response_time": 8.985446691513062,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.19753086419753085,
        "rouge2": 0.12658227848101267,
        "rougeL": 0.19753086419753085,
        "bleu": 0.028287399721572407,
        "semantic_similarity": 0.8667842149734497
      }
    }
  ]
}