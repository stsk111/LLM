{
  "evaluation_summary": {
    "total_samples": 214,
    "successful_evaluations": 214,
    "success_rate": 1.0,
    "evaluation_time": "2026-02-10 20:55:13"
  },
  "average_metrics": {
    "avg_recall@1": 0.32710280373831774,
    "avg_recall@3": 0.43457943925233644,
    "avg_recall@5": 0.46261682242990654,
    "avg_mrr": 0.37398753894080994,
    "avg_rouge_1": 0.25223385253934955,
    "avg_rouge_2": 0.11599776782667698,
    "avg_rouge_l": 0.2466918063439737,
    "avg_bleu": 0.14613738287386902,
    "avg_semantic_similarity": 0.7394920207351168,
    "avg_response_time": 2.041495958221293
  },
  "detailed_results": [
    {
      "sample_id": "0000",
      "question": "SFT 模型通常需要多少块 GPU 来完成训练？",
      "ground_truth": "通常需要数十块GPU，花费数天时间完成训练。",
      "predicted_answer": "根据文档中的描述，SFT 模型的训练通常需要数十块 GPU，花费数天时间完成[1][2]。具体来说，可能需要几到几十块不等的 GPU，但确切的数量未在文档中详细说明。",
      "ground_context": [
        "根据模型的大小和训练数据量，通常需要数十块GPU，花费数天时间完成训练。"
      ],
      "retrieved_contexts": [
        "大量的计算资源。 根据模型的大小和训练数据量， 通常需要数十块GPU， 花费数天时间完成训练。\nSFT 模型具备了初步的任务完成能力，可以开放给用户使用，很多类 ChatGPT 的模型都属于该类\n型， 包括Alpaca[35]、Vicuna[41]、MOSS、ChatGLM-6B 等。 很多这类模型的效果非常好， 甚至在一些\n评测中达到了 ChatGPT 的 90% 的效果[35, 41]。当前的一些研究表明，有监督微调阶段的数据选择\n对 SFT 模型效果有非常大的影响[42]，因此构造少量并且高质量的训练数据是本阶段的研究重点。",
        "大规模语言模型：从理论到实践 11\n花费 266.4 万 H800 GPU 小时。由于训练过程需要消耗大量的计算资源，并很容易受到超参数影\n响，因此，如何提升分布式计算效率并使模型训练稳定收敛是本阶段的研究重点。\n有监督微调（Supervised Fine Tuning，SFT） ， 也称为指令微调， 利用少量高质量数据集， 通过\n有监督训练使模型具备问题回答、翻译、写作等能力。有监督微调的数据包含用户输入的提示词\n和对应的理想输出结果。用户输入包括问题、闲聊对话、任务指令等多种形式和任务。\n例如：཮౵Ĥ\n֊ն࿐གྷႵ 4ࡾ\nਟ཮౵\nྍ౵b\n利用这些有监督数据，使用与预训练阶段相同的语言模型训练算法，在基础模型的基础上进行训\n练， 得到有监督微调模型 （SFT 模型） 。 经过训练的SFT 模型具备初步的指令理解能力和上下文理\n解能力，能够完成开放领域问答、阅读理解、翻译、生成代码等任务，也具备了一定的对未知任\n务的泛化能力。由于有监督微调阶段所需的训练数据量较少，SFT 模型的训练过程并不需要消耗\n大量的计算资源。 根据模型的大小和训练数据量， 通常需要数十块GPU， 花费数天时间完成...",
        "12 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n奖励建模（Reward Modeling） 阶段的目标是构建一个文本质量对比模型。 对于同一个提示词，\nSFT 模型对给出的多个不同输出结果的质量进行排序。奖励模型可以通过二分类模型，对输入的\n两个结果之间的优劣进行判断。奖励模型与基础模型和 SFT 模型不同，奖励模型本身并不能单独\n提供给用户使用。 奖励模型的训练通常和SFT 模型一样， 使用数十块GPU， 通过数天时间完成训\n练。由于奖励模型的准确率对强化学习阶段的效果有至关重要的影响，因此通常需要大规模的训\n练数据对该模型进行训练。Andrej Karpathy 在报告中指出，该部分需要百万量级的对比数据标注，\n而且其中很多标注需要很长时间才能完成。图1.4 给出了 InstructGPT 系统中奖励模型训练样本标\n注示例[24]。 可以看到， 示例中文本表达都较为流畅， 标注其质量排序需要制定非常详细的规范， 标\n注者也需要认真地基于标注规范进行标注，需要消耗大量的人力。同时，保持众包标注者之间的\n一致性，也是奖励建模阶段需要解决的难点问题之一。此外，奖励模型的泛...",
        "性知识 （Factual Knowledge） 和常识性知识 （Commonsense） 在内的世界知识 （World Knowledge） 。 根\n据文献 [39] 中的介绍，GPT-3 完成一次训练的总计算量是 3640PFLOPS， 按照NVIDIA A100 80GB\nGPU 和平均利用率达到 50% 计算， 需要花费近一个月的时间使用1000 块 GPU 完成。 由于GPT-3\n的训练采用 NVIDIA V100 32GB GPU ，其实际计算成本远高于上述计算。文献[29] 介绍了参数量\n同样是 1750 亿个的 OPT 模型，该模型训练使用 992 块 NVIDIA A100 80GB GPU ，整体训练时间\n将近 2 个月。BLOOM[31] 模型的参数量也是 1750 亿个，该模型训练一共花费 3.5 个月，使用包含\n384 块 NVIDIA A100 80GB GPU 集群完成。可以看到，大语言模型的训练需要花费大量的计算资\n源和时间。LLaMA、Falcon、 百川 （Baichuan） 等模型都属于基础语言模型。 即便是DeepSeek-V3[40]\n经过了大量...",
        "使用 48 个计算节点。每个计算节点包含 8 块 NVIDIA A100 80GB GPU（总计 384 块 GPU） ，并且\n使用 4×NVLink 用于节点内部 GPU 之间的通信。节点之间采用 4 个 Omni-Path 100 Gbps 网卡构\n建的增强 8 维超立方体全局拓扑网络进行通信。文献 [34] 并没有给出 LLaMA 模型训练中所使用\n的集群的具体配置和网络拓扑结构，但是给出了不同参数规模的总 GPU 小时数。LLaMA 模型训\n练使用 NVIDIA A100 80GB GPU ，LLaMA-7B 模型训练需要 82432 GPU 小时，LLaMA-13B 模型\n训练需要 135168 GPU 小时，LLaMA-33B 模型训练需要 530432 GPU 小时，而 LLaMA-65B 模型\n训练需要高达 1022362 GPU 小时。LLaMA 使用的训练数据量远超 OPT 和 BLOOM 模型，虽然模\n型参数量远小于上述两个模型，但是其所需计算量非常惊人。\n通过使用分布式训练系统，大语言模型的训练周期可以从单计算设备花费几十年，缩短到使\n用数千个计算设备花费几十天..."
      ],
      "response_time": 3.159262180328369,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.33333333333333337,
        "rouge2": 0.0,
        "rougeL": 0.33333333333333337,
        "bleu": 0.16772802394199504,
        "semantic_similarity": 0.8475821614265442
      }
    },
    {
      "sample_id": "0001",
      "question": "生成过程是如何进行的？",
      "ground_truth": "w1w2 · · · wm 的生成过程可以看作单词逐个生成的过程。首先生成 w1，之后根据 w1 生成 w2，然后根据 w1 和 w2 生成 w3，依此类推，根据前 m − 1 个单词生成最后一个单词 wm。",
      "predicted_answer": "生成过程首先通过检索模块（Retriever）根据用户的查询内容定位并查找相关数据源，然后筛选出与查询高度相关的信息作为检索结果[3]。接下来，这些检索结果会传递给生成模块（Generator），生成模块会基于检索到的相关内容生成连贯且准确的自然语言回答[3][5]。生成模块不仅要整合检索结果，还要根据用户查询的需求进行内容分析、推理和重组，以确保输出的答案具备逻辑性和针对性[1]。此外，生成过程中还需要保证生成内容的真实性、正确性以及连贯性和流畅性[5]。",
      "ground_context": [
        "w1w2 · · · wm 的生成过程可以看作单词逐个生成的过程。首先生成 w1，之后根据 w1 生成 w2，然后根据 w1 和 w2 生成 w3，依此类推，根据前 m − 1 个单词生成最后一个单词 wm。"
      ],
      "retrieved_contexts": [
        "生成模块则是基于检索器提供的相关内容生成最终的答案。 生成器通常由大语言模型构成， 通\n过结合用户输入的查询和检索器返回的上下文信息，生成连贯且准确的自然语言回答。生成器不\n仅需要对检索结果进行有效整合，还需根据用户查询的具体需求，进行内容分析、推理和再组织，\n以确保输出的答案既具备逻辑性又具有针对性。生成器的能力决定了系统在处理复杂问题时的表\n现，尤其是在需要融合多源信息或解答长尾知识时。\n检索增强生成也正逐步突破传统的文本模态限制， 扩展至图像、 音频、 代码等多模态场景， 为",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "大语言模型：从理论到实践 317\n点， 首先通过检索模块 （Retriever） 根据查询内容定位并查找相关数据源， 然后筛选出与查询高度\n相关的信息作为检索结果。这些检索结果随后与生成模块（Generator）协作，以增强生成过程的\n质量和效果。\n用户输入各种模态数据\n检索模块生成模块结果\n文本代码图像音频视频知识科学数据……3D\n数据\n编码索引\n稀疏检索稠密检索其他\nTr ansfor merLSTMDiffusionGAN……\n文本\n代码\n图片\n视频\n知识\n科学数据\n3D\n音频\n图 9.1 典型大模型检索增强生成过程 [413]\n检索模块负责从外部知识库或数据源中定位与用户查询相关的信息。检索器通常基于向量检\n索技术或其他高效的检索算法，将输入的自然语言查询转换为向量表示，并与外部数据源中的内\n容进行匹配。外部数据源可以是文档数据库、知识图谱、 API 接口或实时搜索引擎等。检索模块\n不仅需要快速准确地定位相关内容，还需对检索结果进行筛选和排序，以确保返回的内容与用户\n查询具有高度相关性。这一模块的性能直接影响生成器后续处理的质量和效率。\n生成模块则是基于检索器提供的相关内容生成...",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "cination） 。 因此， 生成组件需要确保其输出内容忠实于检索到的信息， 即生成的回答必须基于检索\n到的事实，而不是凭空捏造。真实性评估目标在于衡量生成内容是否准确反映了检索结果中的信\n息，避免出现事实错误或误导性内容。\n（3） 正确性 （Correctness） ： 在许多任务中， 生成的回答需要与给定的参考答案 （Ground Truth）\n保持一致。正确性评估目标旨在衡量生成内容与标准答案之间的一致性，特别是在有明确答案的\n任务（如问答或填空任务）中。这通常通过自动化指标（如 F1 分数或精确匹配率）来衡量。\n（4）连贯性与流畅性（Coherence and Fluency ） ：生成内容的连贯性和流畅性是评估生成组件\n的重要目标之一。连贯性指回答内容是否逻辑通顺，是否能够完整表达查询的意图；流畅性则关\n注语言表达是否符合自然语言的语法和用法。这些目标通常通过人工评估或语言模型的评分机制\n来实现。\n（5）生成内容的多维度要求（Multi-Dimensional Requirements） ：生成组件的评估需要覆盖多\n个维度，包括内容的可读性、丰富性和结构化程度。例如，在生成..."
      ],
      "response_time": 3.183119773864746,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.1111111111111111,
        "rouge2": 0.0,
        "rougeL": 0.1111111111111111,
        "bleu": 0.011841914914507572,
        "semantic_similarity": 0.6541200876235962
      }
    },
    {
      "sample_id": "0002",
      "question": "Baichuan2的发布月份是什么？",
      "ground_truth": "2023 年 9 月",
      "predicted_answer": "Baichuan2的发布月份是2023年9月[3]。",
      "ground_context": [
        "Baichuan2 2023 年 9 月 百川 530 亿 对话模型"
      ],
      "retrieved_contexts": [
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "186 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n# run_baichuan_7b.sh\n#!/bin/bash\n# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n# DeepSpeed Team\nOUTPUT=$1\nZERO_STAGE=$2\nif [ \"$OUTPUT\" == \"\" ]; then\nOUTPUT=./output_step1_baichuan_7b\nfi\nif [ \"$ZERO_STAGE\" == \"\" ]; then\nZERO_STAGE=3\nfi\nmkdir -p $OUTPUT\ndeepspeed main.py\n--data_path <my_data>/my_dataset\n #ඔऌ\n--data_split 10,0,0\n #࿞਀\n--model_name_or_path <my_model>/baichuan_7b\n #֥baichuan 7Bࣥ\nper_device_train_batch_size 4\n--per_device_...",
        "星火 2023 年 5 月 科大讯飞 - 对话模型\n浦语书生 2023 年 6 月 浦江实验室 - 对话模型\nClaude 2 2023 年 7 月 Anthropic - 对话模型\nBaichuan2 2023 年 9 月 百川 530 亿 对话模型\nKimi 2023 年 10 月 月之暗面 - 对话模型\nGemini 2023 年 12 月 Google - 对话模型\nGLM-4 2024 年 1 月 智谱华章 - 对话模型\nClaude 3 2024 年 1 月 Anthropic - 对话模型\nGPT-4o 2024 年 5 月 OpenAI 2000 亿 1 对话模型\n豆包 2024 年 5 月 字节跳动 - 对话模型\n星火 2.0 2024 年 6 月 科大讯飞 - 对话模型\nStep-2 2024 年 7 月 阶跃星辰 10000 亿 对话模型\nGPT-o1 2024 年 9 月 OpenAI 3000 亿 1 对话模型\nClaude 3.5 2024 年 10 月 Anthropic - 对话模型\nGPT-o3 2024 年 12 月 OpenAI - 推理模型\n豆包...",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "model = create_hf_model(AutoModelForCausalLM,\nargs.model_name_or_path,\ntokenizer,\nds_config,\ndisable_dropout=args.disable_dropout)\n#ູᆦӻBaichuan 7Bս઒\ntokenizer = BaiChuanTokenizer.from_pretrained(args.model_name_or_path)\nmodel = create_hf_model(BaiChuanForCausalLM,\nargs.model_name_or_path,\ntokenizer,\nds_config,\ndisable_dropout=args.disable_dropout)\n最后，在训练脚本中将 model_name_or_path 参数修改为 Baichuan 7B 的模型路径即可开始\n模型的训练。训练脚本中以 DeepSpeed-Chat 中的 run_llama2_7b.sh 为模板进行修改："
      ],
      "response_time": 0.7325961589813232,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 0.3333333333333333
      },
      "answer_quality_metrics": {
        "rouge1": 0.6666666666666666,
        "rouge2": 0.5,
        "rougeL": 0.6666666666666666,
        "bleu": 0.1933853138176172,
        "semantic_similarity": 0.6103490591049194
      }
    },
    {
      "sample_id": "0003",
      "question": "What function is used to apply softmax to the scores?",
      "ground_truth": "scores = F.softmax(scores, dim =-1)",
      "predicted_answer": "在代码片段中使用了`F.softmax`函数来对分数应用softmax操作[3]。具体来说，在掩码不是None的情况下，会对scores进行掩码填充处理，然后调用`F.softmax(scores, dim=-1)`对分数应用softmax操作。",
      "ground_context": [
        "scores = F.softmax(scores, dim =-1)"
      ],
      "retrieved_contexts": [
        "来加快计算速度，避免从全局内存中读取和写入注意力矩阵。达成该目标需要做到在不访问整个\n输入的情况下计算 Softmax 函数，并且后向传播中不能存储中间注意力矩阵。在标准 Attention 算\n法中，Softmax 计算按行进行，即在与 V 做矩阵乘法之前，需要完成 Q、K 每个分块中的一整行\n的计算。 在得到Softmax 的结果后， 再与矩阵V 分块做矩阵乘。 而在FlashAttention 中， 将输入分\n割成块，并在输入块上进行多次传递，以增量的方式执行 Softmax 计算。",
        "yourself unable to manage your emotions, it is strongly\nrecommended to seek professional assistance promptly.\nHuman: Depression is just temporary bad mood, isn’t it?\nWhat should be taken into consideration in daily life when\ndealing with depression? Assistant: That’s right. How-\never, if you can’t control it on your own for a long time and\nit affects your normal work and life, you should seek help\nfrom professional institutions in a timely manner. Here\nare some things that may help improve the d...",
        "#ݖSoftmaxູ0\nif mask is not None:\nmask = mask.unsqueeze(1)\nscores = scores.masked_fill(mask == 0, -1e9)\nscores = F.softmax(scores, dim =-1)\nif dropout is not None:\nscores = dropout(scores)\noutput = torch.matmul(scores, v)\nreturn output\ndef forward(self, q, k, v, mask =None):\nbs = q.size(0)\n#Ӯh๨\nk = self.k_linear(k).view(bs, -1, self.h, self.d_k)\nq = self.q_linear(q).view(bs, -1, self.h, self.d_k)\nv = self.v_linear(v).view(bs, -1, self.h, self.d_k)\n# इᆔሇᇂ\nk = k.transpose(1,2)\nq = q.transpose(1,2)\n...",
        ",→ material, while the mitochondria is responsible for energy production. Other organelles\n,→ include endoplasmic reticulum and Golgi apparatus.\ninstruction:\nProvide three possible solution to the following problem.\ninput:\nHow to reduce plastic waste?\noutput:\n1. Reducing single-use plastic by using reusable grocery bags, food containers, and water bottles.\n2. Recycling plastic waste and using the recycled material to create new products.\n3. Educating people about the importance and benefits of r...",
        "20 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n图 2.2 自注意力机制中的查询、键、值\n为了得到编码单词 xi 时所需要关注的上下文信息， 通过位置i 查询向量与其他位置的键向量\n做点积得到匹配分数 qi · k1, qi · k2, · · · , qi · kt。为了防止过大的匹配分数在后续 Softmax 计算过\n程中导致的梯度爆炸及收敛效率差的问题，这些得分会除以放缩因子\n√\nd 以稳定优化。放缩后的\n得分经过 Softmax 归一化为概率，与其他位置的值向量相乘来聚合希望关注的上下文信息，并最\n小化不相关信息的干扰。上述计算过程可以被形式化地表述如下：\nZ = Attention(Q, K, V ) = Softmax\n\u0012 QK ⊤\n√\nd\n\u0013\nV (2.6)\n其中 Q ∈ RL×dq , K ∈ RL×dk , V ∈ RL×dv 分别表示输入序列中的不同单词的 q, k, v 向量拼接组\n成的矩阵，L 表示序列长度，Z ∈ RL×dv 表示自注意力操作的输出。为了进一步增强自注意力机\n制聚合上下文信息的能力，提出了多头注意力机制，以关注上下文的不同侧面。..."
      ],
      "response_time": 1.5125701427459717,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 0.3333333333333333
      },
      "answer_quality_metrics": {
        "rouge1": 0.6666666666666666,
        "rouge2": 0.625,
        "rougeL": 0.6666666666666666,
        "bleu": 0.16206460049285412,
        "semantic_similarity": 0.7697149515151978
      }
    },
    {
      "sample_id": "0004",
      "question": "GPT-3 完成一次训练的总计算量是多少？",
      "ground_truth": "3640PFLOPS",
      "predicted_answer": "GPT-3 完成一次训练的总计算量是 3640 PFLOPS [1]。",
      "ground_context": [
        "根据文献 [39] 中的介绍，GPT-3 完成一次训练的总计算量是 3640PFLOPS， 按照NVIDIA A100 80GB GPU 和平均利用率达到 50% 计算， 需要花费近一个月的时间使用1000 块 GPU 完成。"
      ],
      "retrieved_contexts": [
        "性知识 （Factual Knowledge） 和常识性知识 （Commonsense） 在内的世界知识 （World Knowledge） 。 根\n据文献 [39] 中的介绍，GPT-3 完成一次训练的总计算量是 3640PFLOPS， 按照NVIDIA A100 80GB\nGPU 和平均利用率达到 50% 计算， 需要花费近一个月的时间使用1000 块 GPU 完成。 由于GPT-3\n的训练采用 NVIDIA V100 32GB GPU ，其实际计算成本远高于上述计算。文献[29] 介绍了参数量\n同样是 1750 亿个的 OPT 模型，该模型训练使用 992 块 NVIDIA A100 80GB GPU ，整体训练时间\n将近 2 个月。BLOOM[31] 模型的参数量也是 1750 亿个，该模型训练一共花费 3.5 个月，使用包含\n384 块 NVIDIA A100 80GB GPU 集群完成。可以看到，大语言模型的训练需要花费大量的计算资\n源和时间。LLaMA、Falcon、 百川 （Baichuan） 等模型都属于基础语言模型。 即便是DeepSeek-V3[40]\n经过了大量...",
        "==================================================\n2.3 大语言模型的结构\n当前， 绝大多数大语言模型都采用类似GPT 的架构， 使用基于Transformer 结构构建的仅由解\n码器组成的网络结构， 采用自回归的方式构建语言模型， 但是在位置编码、 层归一化位置、 激活函数\n等细节上各有不同。 文献[13] 介绍了 GPT-3 模型的训练过程， 包括模型架构、 训练数据组成、 训练过\n程及评估方法。 由于GPT-3 并没有开放源代码， 根据论文直接重现整个训练过程并不容易， 因此文\n献[29] 介绍了根据 GPT-3 的描述复现的过程， 构造并开源了系统OPT（Open Pre-trained Transformer\nLanguage Models） 。MetaAI 也仿照 GPT-3 的架构开源了 LLaMA 模型[34]， 公开评测结果及利用该模\n型 进 行 有 监 督 微 调 后 的 模 型 都 有 非 常 好 的 表 现。 GPT-3 模 型 之",
        "用数千个计算设备花费几十天。分布式训练系统需要克服计算墙、显存墙、通信墙等挑战，以确\n保集群内的所有资源得到充分利用，从而加速训练过程并缩短训练周期。\n• 计算墙：单个计算设备所能提供的计算能力与大语言模型所需的总计算量之间存在巨大差\n异。2022 年 3 月发布的 NVIDIA H100 SXM 的单卡 FP16 算力只有 2000 TFLOPS（Floating\nPoint Operations Per Second ） ， 而GPT-3 需要 314 ZFLOPS 的总计算量， 两者相差了8 个数量\n级。\n• 显存墙： 单个计算设备无法完整存储一个大语言模型的参数。GPT-3 包含 1750 亿个参数， 如\n果在推理阶段采用 FP32 格式进行存储，则需要 700GB 的计算设备内存空间，而 NVIDIA\nH100 GPU 只有 80GB 显存。\n• 通信墙： 分布式训练系统中各计算设备之间需要频繁地进行参数传输和同步。 由于通信的延\n迟和带宽限制，这可能成为训练的瓶颈。在 GPT-3 的训练过程中，如果分布式系统中存在\n128 个模型副本，那么在每次迭代过程中至少需要传输 89...",
        "258 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n这段代码实现了整个 MiniGPT-4 模型的前向传播过程， 包括图像和文本的编码、 提示处理、 多\n模态数据编码的连接， 以及最终损失的计算。通过在Conceptual Caption、SBU 等组合数据集上进\n行计算，即可获得预训练的 MiniGPT-4 模型。\n在第一轮训练完成后，MiniGPT-4 获得了关于图像的丰富知识，并且可以根据人类查询提供\n合理的描述。但是它在生成连贯的语句输出方面遇到了困难，例如，可能会产生重复的单词或句\n子、碎片化的句子或者完全不相关的内容。这样的问题降低了 MiniGPT-4 与人类进行真实交流时\n流畅的视觉对话能力。\n2. 高质量数据集构建\n研究人员注意到，预训练的 GPT-3 曾面临类似的问题。虽然在大量的语言数据集上做了预训\n练，但模型并不能直接生成符合用户意图的文本输出。 GPT-3 通过从人类反馈中进行指令微调和\n强化学习，产生了更加人性化的输出。借鉴这一点，研究人员期望预训练的 MiniGPT-4 也可以做\n到与用户意图对齐，增强模型的可用性。\n为此，研究人员精心构建了...",
        "大规模语言模型：从理论到实践 99\n效率进行优化，主要的技术手段有混合精度训练、算子融合、梯度累加等；在分布式训练系统中，\n随着计算设备数量的增加，理论上峰值计算速度会增加，然而受通信效率的影响，计算设备数量\n增多会造成加速比急速降低；多设备加速比是由计算和通信效率决定的，需要结合算法和网络拓\n扑结构进行优化，分布式训练并行策略的主要目标就是提升分布式训练系统中的多设备加速比。\n大语言模型的参数量和所使用的数据量都非常大，因此都采用了分布式训练架构完成训练。\n文献 [13] 仅在 GPT-3 的训练过程中提到全部使用 NVIDIA V100 GPU ，文献 [29] 介绍了 OPT 使\n用 992 块 NVIDIA A100 80GB GPU ，采用全分片数据并行（Fully Sharded Data Parallel ）[168] 以及\nMegatron-LM 张量并行（Tensor Parallelism）[169]，整体训练时间近两个月。BLOOM[31] 模型的研\n究人员则公开了更多在硬件和所采用的系统架构方面的细节。该模型的训练一共花费了 3.5 个月，\n使用 48 个计算节..."
      ],
      "response_time": 0.9754626750946045,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0.0,
        "bleu": 0.0,
        "semantic_similarity": 0.44275495409965515
      }
    },
    {
      "sample_id": "0005",
      "question": "有监督微调模型具备哪些能力？",
      "ground_truth": "经过训练的SFT模型具备初步的指令理解能力和上下文理解能力，能够完成开放领域问答、阅读理解、翻译、生成代码等任务，也具备了一定的对未知任务的泛化能力。",
      "predicted_answer": "有监督微调模型（SFT模型）具备初步的指令理解和上下文理解能力，能够完成开放领域问答、阅读理解、翻译、生成代码等任务，并且具备一定的对未知任务的泛化能力[1]。",
      "ground_context": [
        "经过训练的SFT模型具备初步的指令理解能力和上下文理解能力，能够完成开放领域问答、阅读理解、翻译、生成代码等任务，也具备了一定的对未知任务的泛化能力。"
      ],
      "retrieved_contexts": [
        "大规模语言模型：从理论到实践 11\n花费 266.4 万 H800 GPU 小时。由于训练过程需要消耗大量的计算资源，并很容易受到超参数影\n响，因此，如何提升分布式计算效率并使模型训练稳定收敛是本阶段的研究重点。\n有监督微调（Supervised Fine Tuning，SFT） ， 也称为指令微调， 利用少量高质量数据集， 通过\n有监督训练使模型具备问题回答、翻译、写作等能力。有监督微调的数据包含用户输入的提示词\n和对应的理想输出结果。用户输入包括问题、闲聊对话、任务指令等多种形式和任务。\n例如：཮౵Ĥ\n֊ն࿐གྷႵ 4ࡾ\nਟ཮౵\nྍ౵b\n利用这些有监督数据，使用与预训练阶段相同的语言模型训练算法，在基础模型的基础上进行训\n练， 得到有监督微调模型 （SFT 模型） 。 经过训练的SFT 模型具备初步的指令理解能力和上下文理\n解能力，能够完成开放领域问答、阅读理解、翻译、生成代码等任务，也具备了一定的对未知任\n务的泛化能力。由于有监督微调阶段所需的训练数据量较少，SFT 模型的训练过程并不需要消耗\n大量的计算资源。 根据模型的大小和训练数据量， 通常需要数十块GPU， 花费数天时间完成...",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "5. 指令微调\n指令微调又称有监督微调，是指在预训练大语言模型的基础上，通过使用有标注的自然语言\n形式的数据， 对模型参数进行微调， 使模型具备指令遵循（Instruction Following） 能力， 能够完成\n各类预先设计的任务，并可以在零样本情况下处理诸多下游任务。经过海量数据预训练后的语言\n模型虽然具备了大量的 “知识” ， 但是由于其训练时的目标仅是进行下一个词的预测， 因此不能够\n理解并遵循人类自然语言形式的指令。为了使模型具有理解并响应人类指令的能力，还需要使用\n指令数据对其进行调整。如何构造指令数据，如何高效低成本地进行指令微调训练，以及如何在\n语言模型基础上进一步扩大上下文等问题，是大语言模型在指令微调阶段的核心。\n本章先介绍大语言模型指令微调训练方法，在此基础上介绍高效模型微调及模型上下文窗口\n扩展方法，最后介绍指令微调的代码实践。\n5.1 指令微调训练\n指令微调具体训练过程并不复杂，主要分为如下三个步骤： （ 1）针对每一项任务去明确地\n定义相应的自然语言形式的指令或者提示，这些指令或提示对任务目标以及输出要求进行清晰描\n述； （2）把训练数据调整成包含指令以...",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "下游任务进行微调。 这些模型参数量大都在10 亿个以上， 由于微调的计算量很大， 这类模型的影\n响力在当时相较 BERT 类模型有不小的差距。\n能力探索阶段集中于 2019 年至 2022 年，由于大语言模型很难针对特定任务进行微调，研究\n人员开始探索在不针对单一任务进行微调的情况下如何发挥大语言模型的能力。2019 年，Radford\n等人在文献 [11] 中使用 GPT-2 模型研究了大语言模型在零样本情况下的任务处理能力。 在此基础\n上，Brown 等人在 GPT-3[13] 模型上研究了通过语境学习进行少样本学习的方法，将不同任务的少\n量有标注的实例拼接到待分析的样本之前输入语言模型，语言模型根据实例理解任务并给出正确\n的结果。基于 GPT-3 的语境学习在 TriviaQA、WebQS、CoQA 等评测集合中都展示出了非常强的\n能力，在有些任务中甚至超过了此前的有监督方法。上述方法不需要修改语言模型的参数，模型\n在处理不同任务时无须花费大量计算资源进行模型微调。仅依赖语言模型本身，其性能在很多任\n务上仍然很难达到有监督学习 （Supervised Learning） 的效果，..."
      ],
      "response_time": 1.4558699131011963,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.6666666666666666,
        "rouge2": 0.0,
        "rougeL": 0.6666666666666666,
        "bleu": 0.6232338235216929,
        "semantic_similarity": 0.8866622447967529
      }
    },
    {
      "sample_id": "0006",
      "question": "大语言模型的构建流程主要包含几个阶段？",
      "ground_truth": "主要包含四个阶段：预训练、 有监督微调、 奖励建模和强化学习。",
      "predicted_answer": "大语言模型的构建流程主要包含四个阶段：预训练、有监督微调、奖励建模和强化学习[1]。",
      "ground_context": [
        "OpenAI 使用的大语言模型构建流程主要包含四个阶段：预训练、 有监督微调、 奖励建模和强化学习。"
      ],
      "retrieved_contexts": [
        "10 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n1.3 大语言模型的构建流程\n根据 OpenAI 联合创始人 Andrej Karpathy 在微软 Build 2023 大会上公开的信息，OpenAI 使用\n的大语言模型构建流程如图1.3 所示， 主要包含四个阶段：预训练、 有监督微调、 奖励建模和强化\n学习。这四个阶段都需要不同规模的数据集及不同类型的算法，会产出不同类型的模型，所需要\n的资源也有非常大的差别。\n图 1.3 OpenAI 使用的大语言模型构建流程\n预训练（Pretraining） 阶段需要利用海量的训练数据 （数据来自互联网网页、 维基百科、 书籍、\nGitHub、 论文、 问答网站等） ， 构建包含数千亿甚至数万亿单词的具有多样性的内容。 利用由数千块\n高性能 GPU 和高速网络组成的超级计算机， 花费数十天完成深度神经网络参数训练， 构建基础模\n型 （Base Model） 。 基础模型对长文本进行建模， 使模型具有语言生成能力， 根据输入的提示词， 模\n型可以生成文本补全句子。有一部分研究人员认为，语言模型建模过程中隐含地构建了包括事实\n性知识...",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "152 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n据集。该数据集精炼后包含约 5.2 万条数据，能够用于提升大语言模型在开放领域信息抽取任务\n中的表现，从而显著增强其能力。\n图 5.1 适用于大模型开放领域命名实体识别任务 B 2NERD 数据集构建过程[188]\n3. 自动构建指令\n手动构建指令数据代价高昂，需要大量的人力投入。因此，一些研究尝试寻找更高效的替代\n方法。具有代表性的工作如 Self-Instruct[189]，利用大语言模型的生成能力自动构建指令。\nSelf-Instruct 数据生成是一个迭代过程。如图5.2 所示，它包含以下 4 个步骤。",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "4 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\nѠ\n图 1.1 大语言模型的缩放法则 [17]\n1.2 大语言模型的发展历程\n大语言模型的发展历程虽然只有不到5 年，但是发展速度相当惊人，截至2025 年 2 月，国内\n外有超过百种大语言模型相继发布。特别是2024 年 12 月 DeepSeek V3 和 2025 年 1 月 DeepSeek\nR1 模型的开源， 不仅在训练效率和思考推理上取得了突破， 还赢得了国际社会对中国人工智能技术\n的高度认可。中国人民大学赵鑫教授团队在《大语言模型》书中按照时间线给出了2019 年至 2024\n年 6 月比较有影响力并且模型参数量超过100 亿个的大语言模型，我们在此基础上扩展到2025 年\n2 月， 如图1.2 所示。 大语言模型的发展可以粗略地分为如下三个阶段： 基础模型阶段、 能力探索阶\n段和突破发展阶段。\n图 1.2 大语言模型发展时间线 [18]\n基础模型阶段主要集中于 2018 年至 2021 年。2017 年，V aswani等人提出了 Transformer[12]\n架构，在机器翻译任务上取得了突破性进展。201..."
      ],
      "response_time": 0.9740340709686279,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.6155748316594309,
        "semantic_similarity": 0.8412023782730103
      }
    },
    {
      "sample_id": "0007",
      "question": "第 5 章重点介绍什么技术？",
      "ground_truth": "模型微调技术，有监督微调数据的构造策略以及高效微调方法：LoRA、Delta Tuning 等方法。",
      "predicted_answer": "第 5 章重点介绍模型微调技术[1][2][3][4][5]。",
      "ground_context": [
        "第 5 章重点介绍模型微调技术，有监督微调数据的构造策略以及高效微调方法：LoRA、Delta Tuning 等方法；"
      ],
      "retrieved_contexts": [
        "和强化学习方法，使模型能够理解指令并生成类人回答。第 5 章重点介绍模型微调技术，有监督\n微调数据的构造策略以及高效微调方法：LoRA、Delta Tuning 等方法；第 6 章则围绕强化学习展\n开， 讲解其基础理论与近端策略优化（PPO）技术， 并结合实际案例， 以DeepSpeed-Chat 和 veRL\n框架为例，详细说明如何训练类 ChatGPT 系统。\n第 7 章、第 8 章和第 9 章围绕提升大语言模型能力展开详细探讨，内容涵盖多模态大语言模\n型、智能体实践及检索增强生成。第 7 章重点介绍多模态大语言模型的基础理论、架构设计与训\n练策略，并探讨其在实际场景中的应用实践；第 8 章聚焦智能体的发展历程与大语言模型智能体\n的架构设计，深入分析智能体的实现原理，并以 LangChain 为例详细阐述具体实践；第 9 章则围\n绕检索增强生成展开讨论，介绍其核心思想与实现方式，涵盖检索增强框架的设计、检索模块与",
        "大规模语言模型：从理论到实践 15\n生成模块的协作机制，以及其在具体任务场景中的应用方法与实践。\n第 10 章、 第11 章和第 12 章主要围绕如何应用大语言模型展开讨论， 内容涵盖提升模型效率\n的方法、大语言模型评估， 以及典型应用的开发与部署。第10 章重点介绍模型压缩与优化、训练\n效率优化和推理效率优化等提升模型效率的关键技术； 第11 章聚焦于大语言模型评估， 探讨其基\n本概念和难点， 阐述评估体系的构建、 评估方法的设计以及实际评估的实施； 第12 章则基于典型\n的大语言模型应用场景，详细介绍开发流程、开发工具及本地部署的实践方法。",
        "14 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n第2章大语言模型基础\n第3章大语言模型预训练数据第4章分布式训练预训练\n第7章多模态大语言模型第8章大模型智能体大模型增强\n第5章有监督微调第6章强化学习指令理解\n理论基础\n大模型应用\n第9章检索增强生成\n第10章大语言模型效率优化第11章大语言模型评估第12章大语言模型应用开发\n图 1.5 本书章节安排\n第 2 章介绍大语言模型的基础理论知识，包括语言模型的定义、Transformer 结构、大语言模\n型框架等内容，并以 LLaMA 使用的模型结构为例介绍代码实例。\n第 3 章和第 4 章围绕大语言模型预训练阶段的主要研究内容开展介绍，包括模型分布式训练\n中需要掌握的数据并行、 流水线并行、 模型并行及ZeRO 系列优化方法。 除此之外， 还将介绍预训\n练需要使用的数据分布和数据预处理方法， 并以DeepSpeed 为例介绍如何进行大语言模型预训练。\n第 5 章和第 6 章聚焦于大语言模型指令理解阶段的核心研究内容，探讨如何通过有监督微调\n和强化学习方法，使模型能够理解指令并生成类人回答。第 5 章重点介绍模型微调技术，...",
        "\u0015\u000f\u0015\u000f\u0012୑ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 131\n\u0015\u000f\u0015\u000f\u0013࡬134\n5 指令微调 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 147\n5.1 指令微调训练 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ....",
        "方法训练后的 RL 模型，就是最终提供给用户使用、具有理解用户指令和上下文的类 ChatGPT 系\n统。由于强化学习方法稳定性不高，并且超参数众多，使得模型收敛难度大，叠加奖励模型的准\n确率问题，使得在大语言模型上有效应用强化学习非常困难。\n1.4 本书的内容安排\n本书共分为 12 章， 围绕大语言模型基础理论、预训练、指令理解、大模型增强和大模型应用\n五个部分展开：第一部分介绍大语言模型的基础理论；第二部分介绍大语言模型的预训练，包括\n大语言模型预训练数据和分布式训练；第三部分介绍大语言模型如何理解并服从人类指令，包括\n有监督微调和强化学习；第四部分介绍大语言模型增强技术，包括多模态大语言模型、大模型智\n能体和检索增强生成；第五部分介绍大模型应用，包括大语言模型效率优化、大语言模型评估和\n大语言模型应用开发。具体章节安排如图1.5 所示。"
      ],
      "response_time": 0.849219799041748,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0.0,
        "bleu": 0.056191101519740785,
        "semantic_similarity": 0.6929798126220703
      }
    },
    {
      "sample_id": "0008",
      "question": "残差连接的主要作用是什么？",
      "ground_truth": "避免在优化过程中因网络过深而产生潜在的梯度消失问题。",
      "predicted_answer": "残差连接的主要作用是避免在优化过程中因网络过深而产生的潜在梯度消失问题[1]。具体来说，它通过使用一条直连通道直接将对应子层的输入连接到输出，使得误差可以在较浅层次就反馈回输入，从而改善深层网络的训练效果。公式表示为 \\(x_{l+1} = f(x_l) + x_l\\) [1]。",
      "ground_context": [
        "残差连接主要是指使用一条直连通道直接将对应子层的输入连接到输出，避免在优化过程中因网络过深而产生潜在的梯度消失问题。"
      ],
      "retrieved_contexts": [
        "def forward(self, x):\nx = self.dropout(F.relu(self.linear_1(x)))\nx = self.linear_2(x)\nreturn x\n2.1.4 残差连接与层归一化\n由 Transformer 结构组成的网络结构通常都非常庞大。 编码器和解码器均由很多层基本的Trans-\nformer 块组成，每一层中都包含复杂的非线性映射，这就导致模型的训练比较困难。因此，研究\n人员在 Transformer 块中进一步引入了残差连接与层归一化技术， 以进一步提升训练的稳定性。 具\n体来说，残差连接主要是指使用一条直连通道直接将对应子层的输入连接到输出，避免在优化过\n程中因网络过深而产生潜在的梯度消失问题：\nxl+1 = f (xl) + xl (2.10)\n其中 xl 表示第 l 层的输入，f (·) 表示一个映射函数。 此外， 为了使每一层的输入/输出稳定在一个\n合理的范围内，层归一化技术被进一步引入每个 Transformer 块中：\nLN(x) = α · x − µ\nσ + b (2.11)",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "列联表（Contingency Table） ，用于比较两个模型之间的预测结果。\n给定如图11.10 所示的用于麦克尼马尔检验的混淆矩阵， 可以得到模型1 的准确率为 A+B\nA+B+C+D ，\n其中 A+B +C +D 为整个测试集中的样本数 n。 同样地， 也可以得到模型2 的准确率为 A+C\nA+B+C+D 。\n这个矩阵中最重要的数字是 B 和 C，因为 A 和 D 表示了模型 1 和模型 2 都进行正确或错误预测\n的样本数。B 和 C 则反映了两个模型之间的差异。",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "进的位置编码和插值法。\n5.3.1 具有外推能力的位置编码\n位置编码的长度外推能力来源于位置编码中表征相对位置信息的部分，相对位置信息不同于\n绝对位置信息， 对于训练时的依赖较少。 位置编码的研究一直是基于Transformer 结构模型的重点。\n2017 年 Transformer 结构[12] 提出时， 介绍了两种位置编码， 一种是Naive Learned Position Embedding，\n也就是 BERT 模型中使用的位置编码；另一种是 Sinusoidal Position Embedding ，通过正弦函数为\n每个位置向量提供一种独特的编码。这两种最初的形式都是绝对位置编码的形式，依赖于训练过\n程中的上下文窗口大小，在推理时基本不具有外推能力。随后，2021 年提出的 RoPE[48] 在一定程\n度上缓解了绝对位置编码外推能力弱的问题。关于 RoPE 位置编码的具体细节， 已在2.3.1 节进行\n了介绍， 这里不再赘述。 后续在T5 架构[243] 中， 研究人员又提出了T5 Bias Position Embedding， 直"
      ],
      "response_time": 2.2996861934661865,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.10409371632988497,
        "semantic_similarity": 0.7201075553894043
      }
    },
    {
      "sample_id": "0009",
      "question": "在大语言模型的指令理解阶段，如何通过有监督微调和强化学习的方法提升模型的输出质量？",
      "ground_truth": "通过有监督微调和强化学习方法，可以使模型理解指令并生成类人回答，从而提升输出质量。",
      "predicted_answer": "在大语言模型的指令理解阶段，可以通过以下两种方法提升模型的输出质量：\n\n1. **有监督微调**：这种方法涉及使用有标注的自然语言形式的数据对模型参数进行微调，使其能够理解和响应人类指令，进而完成各种预先设定的任务。具体而言，需要针对每项任务明确定义相应的自然语言形式的指令或提示，并调整训练数据以包含这些指令[1]。\n\n2. **强化学习**：强化学习阶段利用前一阶段训练的奖励模型来评估SFT（Supervised Fine-Tuning）模型对提示词的回答质量，并结合模型建模目标综合得到更好的效果。通过这种方式，可以使模型具备更好的泛化能力和输出多样性，尽管它可能会降低模型的基础熵[2][5]。\n\n这两种方法各有优势，但都需要大量的计算资源和时间投入。",
      "ground_context": [
        "第 5 章和第 6 章聚焦于大语言模型指令理解阶段的核心研究内容，探讨如何通过有监督微调和强化学习方法，使模型能够理解指令并生成类人回答。",
        "如果奖励模型的目标是针对系统所有的输出都能够高质量地进行判决，那么该问题的难度在某种程度上与文本生成等价。"
      ],
      "retrieved_contexts": [
        "5. 指令微调\n指令微调又称有监督微调，是指在预训练大语言模型的基础上，通过使用有标注的自然语言\n形式的数据， 对模型参数进行微调， 使模型具备指令遵循（Instruction Following） 能力， 能够完成\n各类预先设计的任务，并可以在零样本情况下处理诸多下游任务。经过海量数据预训练后的语言\n模型虽然具备了大量的 “知识” ， 但是由于其训练时的目标仅是进行下一个词的预测， 因此不能够\n理解并遵循人类自然语言形式的指令。为了使模型具有理解并响应人类指令的能力，还需要使用\n指令数据对其进行调整。如何构造指令数据，如何高效低成本地进行指令微调训练，以及如何在\n语言模型基础上进一步扩大上下文等问题，是大语言模型在指令微调阶段的核心。\n本章先介绍大语言模型指令微调训练方法，在此基础上介绍高效模型微调及模型上下文窗口\n扩展方法，最后介绍指令微调的代码实践。\n5.1 指令微调训练\n指令微调具体训练过程并不复杂，主要分为如下三个步骤： （ 1）针对每一项任务去明确地\n定义相应的自然语言形式的指令或者提示，这些指令或提示对任务目标以及输出要求进行清晰描\n述； （2）把训练数据调整成包含指令以...",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "大规模语言模型：从理论到实践 13\n强化学习（Reinforcement Learning，RL） 阶段根据数十万条提示词， 利用前一阶段训练的奖励\n模型， 给出SFT 模型对提示词回答结果的质量评估， 并与语言模型建模目标综合得到更好的效果。\n该阶段使用的提示词数量与有监督微调阶段类似，数量在十万个量级，并且不需要人工提前给出\n该提示词所对应的理想回复。使用强化学习，在 SFT 模型的基础上调整参数，使最终生成的文本\n可以获得更高的奖励 （Reward） 。 该阶段需要的计算量较预训练阶段也少很多， 通常仅需要数十块\nGPU，数天即可完成训练。文献 [24] 给出了强化学习和有监督微调的对比，在模型参数量相同的\n情况下，强化学习可以得到相较于有监督微调好得多的效果。关于为什么强化学习相比有监督微\n调可以得到更好结果的问题， 截至2025 年 2 月还没有完整或得到普遍共识的解释。 目前相对得到\n认可的观点是，强化学习使得模型具备更好的泛化能力[43]。同时，Andrej Karpathy 也指出，强化\n学习并不是没有问题的，它会使基础模型的熵降低，从而减少模型输出的多样性。经过强化学...",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "6. 强化学习\n通过有监督微调，大语言模型已初步具备遵循人类指令并完成多类型任务的能力。然而该方\n法存在显著局限：首先需要构建海量指令 -答案对数据集，高质量回复标注需耗费高昂人力成本；\n其次交叉熵损失函数要求模型输出与标准答案逐字匹配，既无法适应自然语言的表达多样性，也\n难以解决输出对输入微小变动的敏感性，这在需要深度推理的复杂任务中尤为突出。\n当前大语言模型中的强化学习技术主要沿着两个方向演进：其一是基于人类反馈的强化学习\n（Reinforcement Learning from Human Feedback，RLHF） ， 通过奖励模型对生成文本进行整体质量评估，\n使模型能自主探索更优的回复策略， 并使得模型回复与人类偏好和价值观对齐。 典型如ChatGPT 等\n对话系统， 通过人类偏好数据训练奖励模型， 结合近端策略优化 （Proximal Policy Optimization，PPO）\n算法实现对齐优化。其二是面向深度推理的强化学习框架，以 OpenAI 的 O 系列模型和 DeepSeek\n的 R 系列为代表，通过答案校验引导模型进行多步推理。这类方法将复杂问题分解为长思..."
      ],
      "response_time": 4.342557430267334,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.02431117402606467,
        "semantic_similarity": 0.8314214944839478
      }
    },
    {
      "sample_id": "0010",
      "question": "在大语言模型中，如何通过 n 元语法模型和平滑技术来解决零概率问题？",
      "ground_truth": "n 元语法模型通过假设任意单词的出现概率只与过去 n-1 个词相关，从而减少了模型参数空间的复杂性。然而，尽管 n 元语言模型能缓解句子概率为零的问题，语言的多样性使得无法覆盖所有 n-gram。这时候，需要使用平滑技术来为所有可能出现的字符串分配一个非零的概率值，以避免零概率问题。平滑处理的基本思想是调整最大似然估计，提高低概率事件，降低高概率事件，使整体的概率分布趋于均匀。",
      "predicted_answer": "在大语言模型中，可以通过 n 元语法模型和平滑技术来解决零概率问题。n 元语法模型能够缓解句子概率为零的问题，但它仍存在不足之处，比如无法覆盖所有可能的 n-gram 组合，因此需要使用平滑技术来解决零概率问题。平滑技术通过对最大似然估计进行调整，为所有可能出现的字符串分配一个非零的概率值，从而避免零概率问题的发生[1][2]。",
      "ground_context": [
        "n 元语法或 n 元文法（n-gram）模型，其中，n-gram 表示由 n 个连续单词构成的单元。",
        "平滑是指为了产生更合理的概率，对最大似然估计进行调整的一类方法，也称为数据平滑（Data Smoothing）。"
      ],
      "retrieved_contexts": [
        "P (wi|w1w2 · · · wi−1) = P (wi|wi−(n−1)wi−(n−2) · · · wi−1)\nP (wi|wi−1\n1 ) = P (wi|wi−1\ni−n+1)\n(1.3)\n满足上述条件的模型被称为 n 元语法或 n 元文法（n-gram）模型。其中，n-gram 表示由 n 个连续\n单词构成的单元，也被称为n 元语法单元。\n虽然 n 元语言模型能缓解句子概率为零的问题，但语言是由人和时代创造的，具备无尽的可\n能性，再庞大的训练数据也无法覆盖所有的 n-gram，而训练数据中的零频率并不代表零概率。因\n此，需要使用平滑技术（Smoothing）解决，为所有可能出现的字符串分配一个非零的概率值，从\n而避免零概率问题。平滑是指为了产生更合理的概率，对最大似然估计进行调整的一类方法，也\n称为数据平滑（Data Smoothing）。平滑处理的基本思想是提高低概率事件，降低高概率事件，使\n整体的概率分布趋于均匀。 这类方法通常被称为统计语言模型（Statistical Language Models，SLM）\n。相关平滑算法细节可以参考《自然语言处理导论》的第 6 ...",
        "n 元语言模型从整体上看与训练数据规模和模型的阶数（考虑上下文的数量）有较大的关系，\n不同的平滑算法在不同情况下的表现有较大的差距。虽然平滑算法较好地解决了零概率问题，但\n是基于稀疏表示的 n 元语言模型仍然有以下三个较为明显的缺点。\n（1）无法对长度超过 n 的上下文建模。\n（2）依赖人工设计规则的平滑技术。\n（3）当 n 增大时，数据的稀疏性随之增大，模型的参数量更是呈指数级增加，受数据稀疏问\n题的影响，其参数难以被准确学习。\n此外，n 元文法中单词的离散表示也忽略了单词之间的相似性。 因此， 基于分布式表示和神经",
        "2. 大语言模型基础\n语言模型的核心目标是对自然语言的概率分布进行建模，这一任务在自然语言处理研究中占\n据重要地位，是其基础性工作之一。大量研究围绕这一目标，从不同角度展开了探索，包括 n 元\n语言模型（n-gram Language Models ） 、神经语言模型和预训练语言模型等。这些研究在不同发展\n阶段对自然语言处理任务产生了深远影响。 随着基于Transformer 架构的语言模型不断发展， 以及\n预训练-微调范式在各类自然语言处理任务中取得突破性成果，自 2020 年 OpenAI 发布 GPT-3 以\n来，大语言模型的研究逐步深入。尽管大语言模型参数规模庞大，并且通过有监督微调和强化学\n习可以完成众多任务，其理论基础仍然离不开对语言建模的核心研究。\n本章首先介绍 Transformer 结构， 并在此基础上讲解生成式预训练语言模型GPT、 大语言模型\n的网络结构、注意力机制优化及相关实践。关于 n 元语言模型、神经语言模型及其他预训练语言\n模型的内容，可参考《自然语言处理导论》第 6 章[4]。\n2.1 Transformer 结构\nTransformer 结构[44] ...",
        "大语言模型：从理论到实践 371\n图 10.1 大语言模型推理遵循自回归模式\n在每次 GPT 推理中， 对每个词元的自注意力操作需要其前面词元的键和值。 最简单且无状态\n的实现需要在每次迭代中重新计算所有的键和值，这会导致大量额外的计算开销。为了避免这种\n重新计算的开销，FAIRSEQ[470] 提出了键值缓存（Key-V alue Cache） ，即在迭代中保存键和值，以\n便重复使用。 根据上述方法和技术， 大语言模型的推理过程可以分为预填充阶段 （Prefilling Stage）\n和解码阶段（Decoding Stage）两个阶段，如图10.2所示。在预填充阶段，模型会计算并存储初始\n输入词元的键-值 （KV） 缓存， 同时生成第一个输出词元。 随后进入解码阶段， 模型逐个生成后续\n输出词元，并在每一步更新 KV 缓存，直至完成整个推理过程。",
        "下游任务通常需要利用有标注数据集进行训练，数据集使用 D 进行表示，每个样例由输入长度为\nn 的文本序列 x = x1, x2, · · · , xn 和对应的标签 y 构成。\n先将文本序列 x 输入 GPT 模型， 获得最后一层的最后一个词所对应的隐藏层输出h(L)\nn ， 在此"
      ],
      "response_time": 2.614204168319702,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.8333333333333334,
        "rouge2": 0.6,
        "rougeL": 0.6666666666666666,
        "bleu": 0.31893659830426646,
        "semantic_similarity": 0.9360364079475403
      }
    },
    {
      "sample_id": "0011",
      "question": "在自然语言处理领域，如何克服 n 元语言模型的局限性以实现更好的性能？",
      "ground_truth": "通过应用预训练语言模型（PLM）的方法，例如使用基于 Transformer 结构的 GPT 和 BERT 模型，可以有效克服 n 元语言模型的局限性。预训练模型能够通过海量数据学习有效的特征表示，并且在下游任务中只需进行微调，避免了 n 元模型对上下文长度的限制和对人工规则的依赖。",
      "predicted_answer": "为克服 n 元语言模型的局限性，可以转向更为先进的模型架构，如基于Transformer的大型预训练语言模型[2][4]。这些模型不仅能够更好地捕捉长距离依赖关系，还减少了对人工设计平滑技术的需求，并通过大规模预训练提高了泛化能力。此外，使用分布式表示代替离散表示也有助于捕捉词汇间的语义相似性[1]。综上所述，采用预训练语言模型并结合分布式表示可以有效提升自然语言处理任务的性能。",
      "ground_context": [
        "以GPT和BERT为代表的基于 Transformer 结构的大规模预训练语言模型的出现，使自然语言处理全面进入预训练微调范式新时代。",
        "n 元语言模型仍然有以下三个较为明显的缺点。（1）无法对长度超过 n 的上下文建模。"
      ],
      "retrieved_contexts": [
        "n 元语言模型从整体上看与训练数据规模和模型的阶数（考虑上下文的数量）有较大的关系，\n不同的平滑算法在不同情况下的表现有较大的差距。虽然平滑算法较好地解决了零概率问题，但\n是基于稀疏表示的 n 元语言模型仍然有以下三个较为明显的缺点。\n（1）无法对长度超过 n 的上下文建模。\n（2）依赖人工设计规则的平滑技术。\n（3）当 n 增大时，数据的稀疏性随之增大，模型的参数量更是呈指数级增加，受数据稀疏问\n题的影响，其参数难以被准确学习。\n此外，n 元文法中单词的离散表示也忽略了单词之间的相似性。 因此， 基于分布式表示和神经",
        "2. 大语言模型基础\n语言模型的核心目标是对自然语言的概率分布进行建模，这一任务在自然语言处理研究中占\n据重要地位，是其基础性工作之一。大量研究围绕这一目标，从不同角度展开了探索，包括 n 元\n语言模型（n-gram Language Models ） 、神经语言模型和预训练语言模型等。这些研究在不同发展\n阶段对自然语言处理任务产生了深远影响。 随着基于Transformer 架构的语言模型不断发展， 以及\n预训练-微调范式在各类自然语言处理任务中取得突破性成果，自 2020 年 OpenAI 发布 GPT-3 以\n来，大语言模型的研究逐步深入。尽管大语言模型参数规模庞大，并且通过有监督微调和强化学\n习可以完成众多任务，其理论基础仍然离不开对语言建模的核心研究。\n本章首先介绍 Transformer 结构， 并在此基础上讲解生成式预训练语言模型GPT、 大语言模型\n的网络结构、注意力机制优化及相关实践。关于 n 元语言模型、神经语言模型及其他预训练语言\n模型的内容，可参考《自然语言处理导论》第 6 章[4]。\n2.1 Transformer 结构\nTransformer 结构[44] ...",
        "P (wi|w1w2 · · · wi−1) = P (wi|wi−(n−1)wi−(n−2) · · · wi−1)\nP (wi|wi−1\n1 ) = P (wi|wi−1\ni−n+1)\n(1.3)\n满足上述条件的模型被称为 n 元语法或 n 元文法（n-gram）模型。其中，n-gram 表示由 n 个连续\n单词构成的单元，也被称为n 元语法单元。\n虽然 n 元语言模型能缓解句子概率为零的问题，但语言是由人和时代创造的，具备无尽的可\n能性，再庞大的训练数据也无法覆盖所有的 n-gram，而训练数据中的零频率并不代表零概率。因\n此，需要使用平滑技术（Smoothing）解决，为所有可能出现的字符串分配一个非零的概率值，从\n而避免零概率问题。平滑是指为了产生更合理的概率，对最大似然估计进行调整的一类方法，也\n称为数据平滑（Data Smoothing）。平滑处理的基本思想是提高低概率事件，降低高概率事件，使\n整体的概率分布趋于均匀。 这类方法通常被称为统计语言模型（Statistical Language Models，SLM）\n。相关平滑算法细节可以参考《自然语言处理导论》的第 6 ...",
        "发展，使得大规模文本非常容易获取，因此训练超大规模的基于神经网络的语言模型成为可能。\n受计算机视觉领域采用 ImageNet[9] 对模型进行一次预训练， 使模型可以通过海量图像充分学\n习如何提取特征，再根据任务目标进行模型精调的预训练范式影响，自然语言处理领域基于预训\n练语言模型的方法逐渐成为主流。 以ELMo[10] 为代表的动态词向量模型开启了语言模型预训练的\n大门。此后， 以GPT[11] 和 BERT[1] 为代表的基于 Transformer 结构[12] 的大规模预训练语言模型的\n出现，使自然语言处理全面进入预训练微调范式新时代。将预训练模型应用于下游任务时，不需\n要了解太多的任务细节，不需要设计特定的神经网络结构，只需要“微调”预训练模型，使用具\n体任务的标注数据在预训练语言模型上进行监督训练，就可以取得显著的性能提升。这类方法通\n常被称为预训练语言模型（Pre-trained Language Models，PLM）。\n2020 年，OpenAI 发布了由包含 1750 亿个参数的神经网络构成的生成式大规模预训练语言模\n型 GPT-3（Generative Pre-t...",
        "下游任务通常需要利用有标注数据集进行训练，数据集使用 D 进行表示，每个样例由输入长度为\nn 的文本序列 x = x1, x2, · · · , xn 和对应的标签 y 构成。\n先将文本序列 x 输入 GPT 模型， 获得最后一层的最后一个词所对应的隐藏层输出h(L)\nn ， 在此"
      ],
      "response_time": 2.8195645809173584,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.3636363636363636,
        "rouge2": 0.0,
        "rougeL": 0.1818181818181818,
        "bleu": 0.10247236813787432,
        "semantic_similarity": 0.7947680950164795
      }
    },
    {
      "sample_id": "0012",
      "question": "模型的参数维度是什么？",
      "ground_truth": "512",
      "predicted_answer": "根据提供的文档，我无法回答这个问题。文档中未具体提及模型参数的维度。",
      "ground_context": [
        "d_model = 512"
      ],
      "retrieved_contexts": [
        "家” ） 专注不同任务或数据层面。 混合专家架构的引入使得训练具有数千亿甚至万亿参数的模型成\n为可能，如开源的 1.6 万亿参数的 Switch Transformers[67] 等。\n在采用混合专家架构的大语言模型中，MoE 层通常由门控网络（Gating Network）G 和 N 个\n专家网络 （Experts Network）{f1, f2, ..., fN } 组成。 门控网络充当着选择器的角色， 也称为路由， 它\n负责决定将哪些输入数据发送给哪些专家。专家网络则分别处理特定的不同子任务。在这一过程\n中，并非所有专家都同时运作，而是由门控网络依据数据特性，精准地将数据路由到与之最为相\n关的专家那里，最终再根据一个或者多个专家输出的结果综合得到整体的预测结果。在模型架构\n的设计中，MoE 层通常安置于每个 Transformer 模块中前馈层（FFN） 。当模型不断扩大时，FFN\n层在计算方面的需求也越来越高。例如，在参数数量达 5400 亿的 PaLM[14] 模型中，90% 的参数",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "如，模型可以回答与图像相关的问题，或根据视觉线索指导机器人完成特定任务。 PaLM-E 拥有\n5620 亿个参数，在多模态推理与迁移学习方面表现卓越，能够在一系列具身任务中高效表现，而\n无需针对特定任务进行微调。这一特性使得 PaLM-E 成为人工智能研究与机器人实际应用中的一\n项多功能工具。\n3. ImageBind\nMeta 发布的 ImageBind[256] 是一个多模态对齐模型，旨在通过整合六种不同类型的数据（文\n本、 图像/视频、 音频、 深度信息、 热成像数据和运动传感器数据） 来创建一个统一的嵌入空间， 设\n计使得模型能够处理和理解来自多种感官的信息。与传统模型不同， ImageBind 不要求所有模态\n同时存在于同一数据集中，而是利用图像的固有链接性质，实现跨模态的对齐和理解，这为生成\n更复杂的虚拟环境提供了可能性。\nImageBind-LLM[257] 是基于 ImageBind 的多模态大语言模型，它使用 ImageBind 的联合嵌入\n空间来处理多模态数据。与现有主要专注于语言和图像视觉的大语言模型不同， ImageBind-LLM",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "为，并利用当前状态对未来的状态或输出进行预测。两个方程可以如下形式化表示：\nh′(t) = Ah(t) + Bx(t) (10.1)\ny(t) = Ch(t) + Dx(t) (10.2)\n其中，A 是状态转移矩阵、B 表示控制量对状态量的影响、C 表示当前状态量对输出影响和 D 表\n示当前控制量对输出影响， 上述四个矩阵都是可学习的， 也称为模型参数，h 表示中间状态，x 表\n示输入序列。\n状态空间模型的基本过程如图10.5所示。输入信号 x 与矩阵 B 相乘，生成一个向量，用于表\n示输入 x 对系统状态的影响。状态表示（State Representation）h 是一个隐向量，包含了系统的核\n心“知识” 。通过与矩阵A 相乘，状态表示描述了内部状态之间的关联，从而体现系统的动态特\n性。在预测输出之前，需要根据当前状态和输入信号更新状态。最后，通过矩阵C 将状态映射到\n输出空间，利用矩阵 D 提供从输入到输出的直接信号（通常被称为跳跃连接（Skip Connection） ） ，\n生成最终的输出。矩阵 C 描述了状态与输出之间的关系，即如何将状态转换为输出结果。\n跳跃连接\n图 10..."
      ],
      "response_time": 0.7588670253753662,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.0,
        "semantic_similarity": 0.4024509787559509
      }
    },
    {
      "sample_id": "0013",
      "question": "什么函数用于将输入 x 进行旋转处理？",
      "ground_truth": "rotate_half",
      "predicted_answer": "根据提供的文档，我无法回答这个问题。文档中没有提到具体的旋转处理函数。[3]",
      "ground_context": [
        "def rotate_half(x):"
      ],
      "retrieved_contexts": [
        "了四个不同的指令输入模板，包括零样本、少量样本、CoT 模板。Flan 2022 构建过程中还使用了\n任务混合和输入反转等技术。输入反转（Input Inversion）是指将原始输入中的某些元素或部分进\n行反转或重新排列，以生成新的输入，用于增强模型的泛化能力和鲁棒性。例如，在对话任务中，\n将对话历史中的上下文和响应进行反转，以测试模型在不同输入顺序下的表现。在代码生成任务\n中，可以将代码和问题进行反转，在链式推理任务（Chain-of-Thought，CoT）中，将查询、答案\n和解释进行反转。任务混合（Task Mixing）则将来自不同任务的示例混合在一起进行训练，其目\n标旨在增强模型的泛化能力和适应不同任务的能力。\n文献 [188] 针对提升大语言模型在开放领域命名实体识别中的能力进行了研究。通过整合 54\n个现有的中英文命名实体识别数据集， 并经过两步规范化， 构建了B2NERD 数据集。 研究指出， 整\n合多个现有数据集的主要挑战在于实体定义的不一致性和模糊性。 例如， 有些数据集会区分 “时代\n广场” 这样的地点和 “巴黎” 这样的地缘政治实体， 而另一些数据集则将两者统...",
        "def forward(self, x, e_outputs, src_mask, trg_mask):\nattn_output_1 = self.attn_1(x, x, x, trg_mask)\nattn_output_1 = self.dropout_1(attn_output_1)\nx = x + attn_output_1\nx = self.norm_1(x)\nattn_output_2 = self.attn_2(x, e_outputs, e_outputs, src_mask)\nattn_output_2 = self.dropout_2(attn_output_2)\nx = x + attn_output_2\nx = self.norm_2(x)\nff_output = self.ff(x)\nff_output = self.dropout_3(ff_output)\nx = x + ff_output\nx = self.norm_3(x)\nreturn x\nclass Decoder(nn.Module):\ndef __init__(self, vocab_size,...",
        "大规模语言模型：从理论到实践 43\n于 ReLU 函数；当 β 取值为 1 时，Swish 函数是光滑且非单调的。在 HuggingFace 的 transformers\n库中 Swish 函数被 SiLU 函数[47] 代替。\n图 2.5 Swish 激活函数在参数 β 取不同值时的形状\n3. RoPE\n在位置编码上，使用旋转位置嵌入[48] 代替原有的绝对位置编码。RoPE 借助复数的思想，出\n发点是通过绝对位置编码的方式实现相对位置编码。其目标是通过下述运算给 q, k 添加绝对位置\n信息：\n˜qm = f (q, m), ˜kn = f (k, n) (2.24)\n详细的证明和求解过程可以参考文献 [48]，最终可以得到二维情况下用复数表示的 RoPE：\nf (q, m) = Rf (q, m)eiΘf (q,m) = ||q||ei(Θ(q)+mθ) = qeimθ (2.25)\n根据复数乘法的几何意义，上述变换实际上是对应向量旋转，所以位置向量称为“旋转式位置编",
        "x = x + attn_output\nx = self.norm_1(x)\nff_output = self.ff(x)\nff_output = self.dropout_2(ff_output)\nx = x + ff_output\nx = self.norm_2(x)\nreturn x\nclass Encoder(nn.Module):\ndef __init__(self, vocab_size, d_model, N, heads, dropout):\nsuper().__init__()\nself.N = N\nself.embed = Embedder(vocab_size, d_model)\nself.pe = PositionalEncoder(d_model, dropout =dropout)\nself.layers = get_clones(EncoderLayer(d_model, heads, dropout), N)\nself.norm = Norm(d_model)\ndef forward(self, src, mask):\nx = self.embed(sr...",
        "X\nat\n[πθ(at|st)] = 0\n(6.13)\n常用的基线选择是状态价值函数 V (st) ，即 V (st) = Eτ ∼P (τ ;θ)[Rt|st] 。此时，Rt 可视为动作\n价值函数 Q(st, at) = E[Rt|st, at] 的蒙特卡洛估计，策略梯度更新公式进一步表示为：\n∇θJ(θ) =\n∞X\nt=0\nEτ ∼P (τ ;θ) [(Q(st, at) − V (st))∇θ log πθ(at|st)] (6.14)\n其中，A(st, at) = Q(st, at) − V (st) 被称为优势函数， 衡量动作at 相对于状态 st 的预期回报提升。\n当 Rt > V (st) 时，说明动作 at 带来的实际回报高于状态 st 的平均预期回报，应增加其选择概率；\n反之则降低概率。\n6.2.2 REINFORCE 算法\nREINFORCE 算法是最基础的策略梯度方法之一， 由Ronald J. Williams 于 1992 年提出。 其核\n心思想是通过蒙特卡洛采样方法直接估计策略梯度，利用轨迹的完整回报（ Complete Return）来\n更新策略参数 θ，..."
      ],
      "response_time": 0.7965140342712402,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0.0,
        "bleu": 0.0,
        "semantic_similarity": 0.4342643618583679
      }
    },
    {
      "sample_id": "0014",
      "question": "通过上述代码可以得到什么输出？",
      "ground_truth": "通过上述代码可以得到如下输出：",
      "predicted_answer": "根据提供的文档，我无法回答这个问题。文档中并没有提供具体的输入或上下文来说明将产生什么样的输出。这些文档主要涉及代码片段和模型输出的内容描述，但未明确指出特定代码的输出结果。",
      "ground_context": [
        "通过上述代码可以得到如下输出："
      ],
      "retrieved_contexts": [
        "embedded_query[:5]\n执行上述代码可以得到如下输出：\n(5, 1536)\n[0.0053587136790156364,\n-0.0004999046213924885,\n0.038883671164512634,\n-0.003001077566295862,\n-0.00900818221271038]",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "chain = LLMChain(llm=llm, prompt =prompt, callbacks =[handler])\nchain.run(number=2)\n#đ൐Ⴈverboseݔࢲ֥\nchain = LLMChain(llm=llm, prompt =prompt, verbose =True)\nchain.run(number=2)\n#֥callbacksݔࢲ֥\nchain = LLMChain(llm=llm, prompt =prompt)\nchain.run(number=2, callbacks =[handler])\n执行上述程序可以得到如下输出：",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "outputs[0] = torch.LongTensor([FR_TEXT.vocab.stoi['<sos>']])\nfor i in range(1, max_len):\ntrg_mask = np.triu(np.ones((1, i, i),\nk=1).astype('uint8')\ntrg_mask= Variable(torch.from_numpy(trg_mask) == 0).cuda()\nout = model.out(model.decoder(outputs[:i].unsqueeze(0),\ne_outputs, src_mask, trg_mask))\nout = F.softmax(out, dim =-1)\nval, ix = out[:, -1].data.topk(1)\noutputs[i] = ix[0][0]\nif ix[0][0] == FR_TEXT.vocab.stoi['<eos>']:\nbreak\nreturn ' ' .join(\n[FR_TEXT.vocab.itos[ix] for ix in outputs[:i]]\n)"
      ],
      "response_time": 1.3131511211395264,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.007213727908218376,
        "semantic_similarity": 0.6840671300888062
      }
    },
    {
      "sample_id": "0015",
      "question": "GPT-2 相较于 GPT 引入了哪种前置层归一化方法？",
      "ground_truth": "前置层归一化方法",
      "predicted_answer": "GPT-2 相较于 GPT 引入了前置层归一化方法，具体来说是将层归一化移动到了多头自注意力层和全连接层之前[1][3]。",
      "ground_context": [
        "为了使模型训练过程更加稳定，GPT-2 相较于 GPT 引入了前置层归一化方法，将第一个层归一化移动到多头自注意力层之前，将第二个层归一化移动到全连接层之前。"
      ],
      "retrieved_contexts": [
        "层归一化\n…\n文本预测文本分类\n12✕\n+\nAttenOutFC多头自注意力\nMatMul\nSoftmax\nQueryKeyValue\nQKVLinear\n图 2.4 GPT-2 的模型结构\n接下来，分别介绍 RMSNorm 归一化函数、SwiGLU 激活函数和 RoPE 的具体内容和实现。\n1. RMSNorm 归一化函数\n为了使模型训练过程更加稳定，GPT-2 相较于 GPT 引入了前置层归一化方法，将第一个层归\n一化移动到多头自注意力层之前，将第二个层归一化移动到全连接层之前。同时，残差连接的位",
        "典型的生成式预训练语言模型之一。GPT 的模型结构如图2.3 所示，它是由多层 Transformer 组成\n的单向语言模型，主要分为输入层、编码层和输出层三部分。\n图 2.3 GPT 的模型结构\n本节将重点介绍 GPT 自监督预训练、 有监督下游任务微调及基于HuggingFace 的预训练语言\n模型实践。\n2.2.1 自监督预训练\nGPT 采用生成式预训练方法，单向意味着模型只能从左到右或从右到左对文本序列建模，所\n采用的 Transformer 结构和解码策略保证了输入文本每个位置只能依赖过去时刻的信息。",
        "大规模语言模型：从理论到实践 41\n后，OpenAI 就不再开源（也没有开源模型） ，因此并不清楚ChatGPT 和 GPT-4 采用的模型架构。\n本节将以 LLaMA 模型为例，介绍大语言模型架构在 Transformer 原始结构上的改进，并介绍\nTransformer 结构中空间和时间占比最大的注意力机制的优化方法。\n2.3.1 LLaMA 的模型结构\n文献 [34] 介绍了 LLaMA 采用的 Transformer 结构和细节， 与2.1 节介绍的 Transformer 结构的\n不同之处为采用了前置层归一化 （Pre-normalization） 方法并使用RMSNorm 归一化函数 （Root Mean\nSquare Normalizing Function） ，激活函数更换为SwiGLU，使用了旋转位置嵌入（Rotary Positional\nEmbeddings，RoPE） ，使用的Transformer 结构与 GPT-2 类似，如图2.4 所示。\n层归一化\n全连接层\n多头自注意力\n+\n文本和位置嵌入表示\n层归一化\nTr ansfor mer\n层归一化\n…\n文本预测...",
        "层数。\nGPT 模型的输出层基于最后一层的表示 h(L)，预测每个位置上的条件概率，其计算过程可以\n表示为\nP (wi|w1, w2, · · · , wi−1) = Softmax(W eh(L)\ni + bout) (2.14)\n其中，W e ∈ R|V|×d 为词向量矩阵，|V| 为词表大小。\n单向语言模型按照阅读顺序输入文本序列 w，用常规语言模型目标优化 w 的最大似然估计，\n使之能根据输入历史序列对当前词做出准确的预测：\nLPT(w) = −\nnX\ni=1\nlog P (wi|w0, w1, · · · , wi−1; θ) (2.15)\n其中 θ 代表模型参数。也可以基于马尔可夫假设，只使用部分过去词进行训练。预训练时通常使\n用随机梯度下降法进行反向传播，优化该负对数似然函数。\n2.2.2 有监督下游任务微调\n通过自监督语言模型预训练， 使得GPT 模型具备了一定的通用语义表示能力。 下游任务微调\n（Downstream Task Fine-tuning） 的目的是在通用语义表示的基础上， 根据下游任务的特性进行适配。\n下游任务通常需要利用有标注数据集进行训练，数据集使用...",
        "大规模语言模型：从理论到实践 31\n给定文本序列 w = w1, w2, · · · , wn，GPT 首先在输入层中将其映射为稠密的向量：\nvi = vt\ni + vp\ni (2.12)\n其中，vt\ni 是词 wi 的词向量，vp\ni 是词 wi 的位置向量，vi 为第 i 个位置的单词经过模型输入层 （第0\n层）后的输出。GPT 模型的输入层与前文中介绍的神经网络语言模型的不同之处在于其需要添加\n位置向量， 这是Transformer 结构自身无法感知位置导致的， 因此需要来自输入层的额外位置信息。\n经过输入层编码，模型得到表示向量序列 v = v1, v2, · · · , vn，随后将 v 送入模型编码层。编\n码层由 L 个 Transformer 模块组成， 在自注意力机制的作用下， 每一层的每个表示向量都会包含之\n前位置表示向量的信息，使每个表示向量都具备丰富的上下文信息，而且，经过多层编码， GPT\n能得到每个单词层次化的组合式表示，其计算过程表示为：\nh(l) = Transformer-Block(l)(h(0)) (2.13)\n其中 h(l) ∈ Rd×n 表示第 ..."
      ],
      "response_time": 1.3999474048614502,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.06237923061531353,
        "semantic_similarity": 0.7017541527748108
      }
    },
    {
      "sample_id": "0016",
      "question": "在启动整个模型训练之前，需要对预训练数据进行什么处理？",
      "ground_truth": "将预训练数据根据训练好的词元分析器进行处理。",
      "predicted_answer": "在启动整个模型训练之前，需要将预训练数据根据训练好的词元分析器进行处理，并且如果文档长度超过 512 个词元，就需要直接截断[1]。",
      "ground_context": [
        "在启动整个模型训练之前，还需要将预训练数据根据训练好的词元分析器进行处理。"
      ],
      "retrieved_contexts": [
        "大规模语言模型：从理论到实践 35\n3. 预处理数据集\n在启动整个模型训练之前，还需要将预训练数据根据训练好的词元分析器进行处理。如果文\n档长度超过 512 个词元，就直接截断。数据处理代码如下所示：",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "144 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n6. 模型训练\n借助 DeepSpeed 框架实现对模型的训练，训练步骤大致分为以下几个阶段。\n（1） 训练前的准备： 使用print_rank_0 函数输出当前的训练状态。 该函数确保只有指定的进程\n（通常是主进程）会打印消息，避免了多进程环境下的重复输出。在开始训练之前，对模型进行一\n次评估，计算模型的困惑度。\n（2） 训练循环： 每个周期的开始， 都会打印当前周期和总周期数。 在每次迭代中， 数据批次先被\n移动到相应的 GPU 设备， 接着模型对这个批次进行前向传播计算损失。 使用model.backward(loss)\n计算梯度，并使用 model.step() 更新模型参数。对于主进程，还会使用 print_throughput 函数打印\n吞吐量，这有助于了解模型的训练速度和效率。\n（3）保存模型：如果指定了输出目录，则模型的状态和配置将被保存。模型可以在不同的格\n式中保存，例如 HuggingFace 的模型格式或 DeepSpeed 的 Zero Stage 3 特定格式。save_hf_format\n函数...",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "--max_seq_len 512\n--learning_rate 9.65e-6\n--weight_decay 0.\n--num_train_epochs 2\n--gradient_accumulation_steps 1\n--lr_scheduler_type cosine\n--num_warmup_steps 0\n--seed 1234\n--gradient_checkpointing\n--zero_stage $ZERO_STAGE\n--deepspeed\n--output_dir $OUTPUT\n&> $OUTPUT/training.log\n5.4.4 模型训练\n数据预处理和自定义模型的修改都完成后，就可以正式进行训练了。进入步骤一指令微调的\n路径 training/step1_supervised_finetuning 下，把上述构造的训练脚本放置到 training/"
      ],
      "response_time": 1.3234450817108154,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.2919481114753033,
        "semantic_similarity": 0.7468445897102356
      }
    },
    {
      "sample_id": "0017",
      "question": "DecoderLayer 的构造函数中定义了多少个 Norm 实例？",
      "ground_truth": "3",
      "predicted_answer": "DecoderLayer 的构造函数中定义了三个 Norm 实例 [1]。具体来说，这些实例分别为 self.norm_1、self.norm_2 和 self.norm_3。",
      "ground_context": [
        "self.norm_1 = Norm(d_model)\nself.norm_2 = Norm(d_model)\nself.norm_3 = Norm(d_model)"
      ],
      "retrieved_contexts": [
        "大规模语言模型：从理论到实践 27\nclass DecoderLayer(nn.Module):\ndef __init__(self, d_model, heads, dropout =0.1):\nsuper().__init__()\nself.norm_1 = Norm(d_model)\nself.norm_2 = Norm(d_model)\nself.norm_3 = Norm(d_model)\nself.dropout_1 = nn.Dropout(dropout)\nself.dropout_2 = nn.Dropout(dropout)\nself.dropout_3 = nn.Dropout(dropout)\nself.attn_1 = MultiHeadAttention(heads, d_model, dropout =dropout)\nself.attn_2 = MultiHeadAttention(heads, d_model, dropout =dropout)\nself.ff = FeedForward(d_model, dropout =dropout)\ndef ...",
        "大规模语言模型：从理论到实践 17\n运行多个独立注意力机制， 进而从多维度捕捉输入序列信息。 它使得序列中任意两个单词之\n间的依赖关系可以直接被建模而不基于传统的循环结构，从而更好地解决文本的长程依赖\n问题。\n• 位置感知前馈网络层（Position-wise Feed-Forward Network） ：通过全连接层对输入文本序列\n中的每个单词表示进行更复杂的变换。\n• 残差连接： 对应图中的Add 部分。 它是一条分别作用在上述两个子层中的直连通路， 被用于\n连接两个子层的输入与输出，使信息流动更高效，有利于模型的优化。\n• 层归一化：对应图中的 Norm 部分。它作用于上述两个子层的输出表示序列，对表示序列进\n行层归一化操作，同样起到稳定优化的作用。\n词元嵌入表示\n多头注意力\nAdd & Norm\nAdd & Norm\nAdd & Norm\n掩码多头注意力\n输出（迭代右移)\n×N\n输出概率\nLinearSoftmax\nN×\n词元嵌入表示\n多头注意力\n位置感知前馈网络Add & Norm\nAdd & Norm\n输入\n位置编码 位置编码\n位置感知前馈网络\n图 2.1 基于 Trans...",
        "48 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\nclass LlamaDecoderLayer(nn.Module):\ndef __init__(self, config: LlamaConfig):\nsuper().__init__()\nself.hidden_size = config.hidden_size\nself.self_attn = LlamaAttention(config=config)\nself.mlp = LlamaMLP(\nhidden_size=self.hidden_size,\nintermediate_size=config.intermediate_size,\nhidden_act=config.hidden_act,\n)\nself.input_layernorm = LlamaRMSNorm(config.hidden_size, eps =config.rms_norm_eps)\nself.post_attention_layernorm = LlamaRMSNorm(config.hidden_size, eps =config.r...",
        "大规模语言模型：从理论到实践 61\nEmbedding\nTransformerBlock\nTransformerBlock\nOutput\nInput\nEmbedding\nInput\nMulti-Head attention\nAdd & Norm\nAdd & Norm\nEmbedding\nRouter\nMulti-Head attention\nAdd & Norm\nAdd & Norm\nFFNFFN\nRouter\nMoE LoRAMoE\nInput\nFFN\n图 2.12 LoRAMoE 模型架构图 [83]\nLoRAMoE 模型架构如图2.12所示。 基于插件的微调能够将参数的改动集中在额外引入的插件\n中， 从而保证了模型知识的完整性， 有机会引入其他插件来通过与主模型的交互来缓解知识遗忘。\nLoRAMoE 引入了多个与前反馈神经网络并列的专家， 并通过路由相连， 如图2.12中标注了 “火焰”\n符号的部分， 这些部分也是需要在后续学习中进行参数学习的结构。LoRAMoE 在训练阶段使用局\n部平衡约束损失（Localized Balancing Constraint） ，这种约束能够让专...",
        "26 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\nclass EncoderLayer(nn.Module):\ndef __init__(self, d_model, heads, dropout =0.1):\nsuper().__init__()\nself.norm_1 = Norm(d_model)\nself.norm_2 = Norm(d_model)\nself.attn = MultiHeadAttention(heads, d_model, dropout =dropout)\nself.ff = FeedForward(d_model, dropout =dropout)\nself.dropout_1 = nn.Dropout(dropout)\nself.dropout_2 = nn.Dropout(dropout)\ndef forward(self, x, mask):\nattn_output = self.attn(x, x, x, mask)\nattn_output = self.dropout_1(attn_output)\nx = x + attn_ou..."
      ],
      "response_time": 1.1509671211242676,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.15384615384615385,
        "rouge2": 0.0,
        "rougeL": 0.15384615384615385,
        "bleu": 0.007805988703535686,
        "semantic_similarity": 0.40297943353652954
      }
    },
    {
      "sample_id": "0018",
      "question": "What does the Encoder class use to embed the source input?",
      "ground_truth": "self.embed = Embedder(vocab_size, d_model)",
      "predicted_answer": "根据提供的文档，我无法回答这个问题。文档中没有明确说明Encoder类如何对源输入进行嵌入。",
      "ground_context": [
        "self.embed = Embedder(vocab_size, d_model)"
      ],
      "retrieved_contexts": [
        "x = x + attn_output\nx = self.norm_1(x)\nff_output = self.ff(x)\nff_output = self.dropout_2(ff_output)\nx = x + ff_output\nx = self.norm_2(x)\nreturn x\nclass Encoder(nn.Module):\ndef __init__(self, vocab_size, d_model, N, heads, dropout):\nsuper().__init__()\nself.N = N\nself.embed = Embedder(vocab_size, d_model)\nself.pe = PositionalEncoder(d_model, dropout =dropout)\nself.layers = get_clones(EncoderLayer(d_model, heads, dropout), N)\nself.norm = Norm(d_model)\ndef forward(self, src, mask):\nx = self.embed(sr...",
        "大规模语言模型：从理论到实践 155\nGiven the classification task definition and the class labels, generate an input that corresponds\n,→ to each of the class labels. If the task doesn't require input, just generate the correct\n,→ class label.\nTask: Classify the sentiment of the sentence into positive, negative, or mixed.\nClass label: mixed\nSentence: I enjoy the flavor of the restaurant but their service is too slow.\nClass label: Positive\nSentence: I had a great day today. The weather was beautiful and I spent ti...",
        "类公开了两个方法：一个用于文档嵌入表示，另一个用于查询嵌入表示。前者输入多个文本，后\n者输入单个文本。之所以将它们作为两个单独的方法，是因为某些嵌入模型为文档和查询采用了\n不同的嵌入策略。以下是使用 OpenAI 的 API 接口完成文本嵌入的代码示例：\nfrom langchain.embeddings import OpenAIEmbeddings\nembeddings_model = OpenAIEmbeddings(openai_api_key=\"...\")\nembeddings = embeddings_model.embed_documents(\n[\n\"Hi there!\" ,\n\"Oh, hello!\" ,\n\"What's your name?\" ,\n\"My friends call me World\" ,\n\"Hello World!\"\n]\n)\nlen(embeddings), len(embeddings[0])\nembedded_query = embeddings_model.embed_query(\"What was the name mentioned in th...",
        "大语言模型：从理论到实践 297\n[\nDocument(page_content='---\n nsidebar_position: 0\n n---\n n# Document loaders\n n\n nUse document\nloaders to load data from a source as `Document`\n 's. A `Document` is a piece of text\n n and\nassociated metadata. For example, there are document loaders for loading a simple `.txt`\nfile, for loading the text\n ncontents of any web page, or even for loading a transcript of\na YouTube video.\n n\n nEvery document loader exposes two methods:\n n1. \"Load\": load documents\nfrom the configured s...",
        "252 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\ndef init_vision_encoder(\ncls, model_name, img_size, drop_path_rate, use_grad_checkpoint, precision\n):\n#֥ViT֥MiniGPT-4ൡ஥\nassert model_name == \"eva_clip_g\",\n\"vit model must be eva_clip_g for current version of MiniGPT-4\"\n#ࡹEva-ViT-GԤଆ྘\nvisual_encoder = create_eva_vit_g(\nimg_size, drop_path_rate, use_grad_checkpoint, precision\n)\n#ࡹLayerNorm߄\nln_vision = LayerNorm(visual_encoder.num_features)\n#Ҫ\nreturn visual_encoder, ln_vision\n在上段代码中，img_size 表示输入图像的尺寸；drop_path_rat..."
      ],
      "response_time": 0.8158044815063477,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0.0,
        "bleu": 0.0,
        "semantic_similarity": 0.48982638120651245
      }
    },
    {
      "sample_id": "0019",
      "question": "如何在使用BertTokenizerFast时保存模型配置，并利用填充掩码功能进行预测？",
      "ground_truth": "可以通过创建一个模型路径并检查其是否存在，若不存在则创建该目录，随后保存tokenizer的模型配置。同时，可以使用pipeline来进行填充掩码的预测。",
      "predicted_answer": "在使用`BertTokenizerFast`时，可以通过以下步骤保存模型配置，并利用填充掩码功能进行预测：\n\n1. 首先创建或检查指定路径是否存在，如果不存在则创建该路径：\n   ```python\n   model_path = \"pretrained-bert\"\n   if not os.path.isdir(model_path):\n       os.mkdir(model_path) \n   ```\n\n2. 然后将分词器配置保存到该路径下：\n   ```python\n   tokenizer.save_model(model_path)\n   ```\n\n3. 接着生成一个JSON文件来存储具体配置项：\n   ```python\n   with open (os.path.join(model_path, \"config.json\"), \"w\") as f:\n       tokenizer_cfg = {\n           \"do_lower_case\": True,\n           \"unk_token\": \"[UNK]\",\n           \"sep_token\": \"[SEP]\",\n           \"pad_token\": \"[PAD]\",\n           \"cls_token\": \"[CLS]\",\n           \"mask_token\": \"[MASK]\",\n           \"model_max_length\": max_length,\n           \"max_len\": max_length,\n       }\n       json.dump(tokenizer_cfg, f)\n   ```\n\n4. 使用`from_pretrained`方法加载分词器：\n   ```python\n   tokenizer = BertTokenizerFast.from_pretrained(model_path)\n   ```\n\n5. 对于预测任务，可以利用填充和掩码功能。假设有一个输入文本序列`input_text`以及对应的掩码位置`mask_positions`，可以这样处理：\n   ```python\n   inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True)\n   masked_inputs = inputs[\"input_ids\"].clone()\n   for i in mask_positions:\n       masked_inputs[i] = tokenizer.mask_token_id\n   outputs = model(masked_inputs).logits\n   predicted_masked_token_ids = torch.argmax(outputs, dim=-1)[0, mask_positions]\n   ```\n\n以上步骤详细说明了如何在使用`BertTokenizerFast`时保存模型配置，并利用填充掩码功能进行预测。[1][2]",
      "ground_context": [
        "model_path = 'pretrained-bert'",
        "tokenizer = BertTokenizerFast.from_pretrained(model_path)",
        "fill_mask = pipeline('fill-mask', model=model, tokenizer=tokenizer)",
        "tokenizer.save_model(model_path)",
        "with open(os.path.join(model_path, 'config.json'), 'w') as f: json.dump(tokenizer_cfg, f)"
      ],
      "retrieved_contexts": [
        "model_path = \"pretrained-bert\"\n#ࡃࡱ\nif not os.path.isdir(model_path):\nos.mkdir(model_path)\n#༅ఖଆ྘\ntokenizer.save_model(model_path)\n#֩؇\nwith open (os.path.join(model_path, \"config.json\"), \"w\") as f:\ntokenizer_cfg = {\n\"do_lower_case\": True,\n\"unk_token\": \"[UNK]\",\n\"sep_token\": \"[SEP]\",\n\"pad_token\": \"[PAD]\",\n\"cls_token\": \"[CLS]\",\n\"mask_token\": \"[MASK]\",\n\"model_max_length\": max_length,\n\"max_len\": max_length,\n}\njson.dump(tokenizer_cfg, f)\n#֞BertTokenizerFast\ntokenizer = BertTokenizerFast.from_pretrained(...",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "大语言模型：从理论到实践 253\ndef init_Qformer(cls, num_query_token, vision_width, cross_attention_freq =2):\n#֥BERTଆ྘஥ᇂQ-Former\nencoder_config = BertConfig.from_pretrained(\"bert-base-uncased\")\n#؇\nencoder_config.encoder_width = vision_width\nencoder_config.query_length = num_query_token\n# ᄝBERTҭᇿၩ৯Ҫ\nencoder_config.add_cross_attention = True\nencoder_config.cross_attention_freq = cross_attention_freq\n#֥BERTଆ྘ቔູQ-Formerଆॶ\nQformer = BertLMHeadModel(config=encoder_config)\n#༢\nquery_tokens = nn.Parameter(\ntorch.zeros...",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "大规模语言模型：从理论到实践 39\n[10135/79670 18:53:08 < 129:35:53, 0.15 it/s, Epoch 1.27/10]\nStep Training Loss Validation Loss\n1000 6.904000 6.558231\n2000 6.498800 6.401168\n3000 6.362600 6.277831\n4000 6.251000 6.172856\n5000 6.155800 6.071129\n6000 6.052800 5.942584\n7000 5.834900 5.546123\n8000 5.537200 5.248503\n9000 5.272700 4.934949\n10000 4.915900 4.549236\n5. 模型使用\n可以针对不同应用需求使用训练好的模型，以句子补全为例的代码如下所示：\n#ׄ\nmodel = BertForMaskedLM.from_pretrained(os.path.join(model_path, \"checkpoint-10000\"))\n#༅ఖ\ntokenizer = BertTok..."
      ],
      "response_time": 9.869232892990112,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.014084507042253521,
        "rouge2": 0.0,
        "rougeL": 0.014084507042253521,
        "bleu": 0.01564911312262541,
        "semantic_similarity": 0.7796725630760193
      }
    },
    {
      "sample_id": "0020",
      "question": "在 GPT-2 模型中，层归一化的实施与大规模语言模型 LLaMA 的超参数设置有什么关系？",
      "ground_truth": "GPT-2 通过引入前置层归一化方法来提高模型训练的稳定性，而 LLaMA 模型则使用不同规模的超参数配置，包括层数和自注意力头数，这影响了模型的复杂性和计算需求。",
      "predicted_answer": "在GPT-2模型中，引入了前置层归一化方法，即将第一个层归一化移动到多头自注意力层之前，第二个层归一化移动到全连接层之前[2]。然而，文档中并未详细说明这与LLaMA的超参数设置之间的具体关系[1]。根据提供的文档，我们无法进一步探讨两者之间的直接联系。",
      "ground_context": [
        "为了使模型训练过程更加稳定，GPT-2 相较于 GPT 引入了前置层归一化方法，将第一个层归一化移动到多头自注意力层之前。",
        "表 2.1 不同规模的 LLaMA 模型使用的超参数包括层数和自注意力头数，这影响了模型的复杂性和计算需求。"
      ],
      "retrieved_contexts": [
        "大规模语言模型：从理论到实践 41\n后，OpenAI 就不再开源（也没有开源模型） ，因此并不清楚ChatGPT 和 GPT-4 采用的模型架构。\n本节将以 LLaMA 模型为例，介绍大语言模型架构在 Transformer 原始结构上的改进，并介绍\nTransformer 结构中空间和时间占比最大的注意力机制的优化方法。\n2.3.1 LLaMA 的模型结构\n文献 [34] 介绍了 LLaMA 采用的 Transformer 结构和细节， 与2.1 节介绍的 Transformer 结构的\n不同之处为采用了前置层归一化 （Pre-normalization） 方法并使用RMSNorm 归一化函数 （Root Mean\nSquare Normalizing Function） ，激活函数更换为SwiGLU，使用了旋转位置嵌入（Rotary Positional\nEmbeddings，RoPE） ，使用的Transformer 结构与 GPT-2 类似，如图2.4 所示。\n层归一化\n全连接层\n多头自注意力\n+\n文本和位置嵌入表示\n层归一化\nTr ansfor mer\n层归一化\n…\n文本预测...",
        "层归一化\n…\n文本预测文本分类\n12✕\n+\nAttenOutFC多头自注意力\nMatMul\nSoftmax\nQueryKeyValue\nQKVLinear\n图 2.4 GPT-2 的模型结构\n接下来，分别介绍 RMSNorm 归一化函数、SwiGLU 激活函数和 RoPE 的具体内容和实现。\n1. RMSNorm 归一化函数\n为了使模型训练过程更加稳定，GPT-2 相较于 GPT 引入了前置层归一化方法，将第一个层归\n一化移动到多头自注意力层之前，将第二个层归一化移动到全连接层之前。同时，残差连接的位",
        "46 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n4. 模型整体框架\n基于上述模型和网络结构可以实现解码器层，根据自回归方式利用训练数据进行模型训练的\n过程与 2.2.3 节介绍的过程基本一致。不同规模的 LLaMA 模型使用的超参数如表2.1 所示。由于\n大语言模型的参数量非常大， 并且需要大量的数据进行训练， 因此仅利用单个GPU 很难完成训练，\n需要依赖分布式模型训练框架（第 4 章将详细介绍相关内容） 。",
        "2. 模型载入\n使用 transformers 库加载和配置 LLaMA 模型及其相关的词元分析器。从 transformers 库中导\n入 LLaMA 模型、 相应的词元分析器和模型配置后， 使用from_pretrained 方法加载预训练的 LLaMA\n模型、词元分析器和配置。为了确保词元分析器可以处理各种文本的长度，还需要进行填充设置。\n如果词元分析器还没有指定填充符号，则将其设置为 [PAD]，并确定填充行为发生在句子的右侧。\n此外，为了保证模型能够正确地处理句子结束和填充，还为模型配置设置了结束符号和填充符号\n的 ID。最后，为了优化模型在硬件上的性能，还需要调整模型的词汇表嵌入大小，使其成为8 的\n倍数。 通过这些步骤， 可以成功地加载并配置LLaMA 模型， 为后续的训练任务做好准备。 具体代\n码如下：",
        "174 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n的区间内。因此文献 [235] 中提出分块量化，以减小离群点的影响范围。为了恢复量化后的数据，\n需要存储每一块数据的放缩系数。 如果用32 位来存储放缩系数， 块的大小设为64， 放缩系数的存\n储将为每一个参数平均带来 32\n64 = 0.5 比特的额外开销，即 12.5% 的额外显存耗用。因此，需进一\n步对这些放缩系数进行量化， 即双重量化。 在QLoRA 中， 每256 个放缩系数会进行一次 8 比特量\n化，最终每个参数的额外开销由原本的 0.5 比特变为 8\n64 + 32/256\n64 = 0.127 比特。\n5.3 模型上下文窗口扩展\n随着更多长文本建模需求的出现，多轮对话、长文档摘要等任务在实际应用中越来越多，这些\n任务需要模型能够更好地处理超出常规上下文窗口大小的文本内容。尽管当前的大语言模型在处理\n短文本方面表现出色，但在支持长文本建模方面仍存在一些挑战，这些挑战包括预定义的上下文窗\n口大小限制等。以 MetaAI 在 2023 年 2 月开源的 LLaMA 模型[34] 为例，其规定输入文本的词元数\n量..."
      ],
      "response_time": 2.2300992012023926,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.7499999999999999,
        "rouge2": 0.6666666666666666,
        "rougeL": 0.7499999999999999,
        "bleu": 0.0639549461562271,
        "semantic_similarity": 0.8873125314712524
      }
    },
    {
      "sample_id": "0021",
      "question": "在训练大规模语言模型时，如何使用预训练的tokenizer和模型进行填充掩码操作？",
      "ground_truth": "在训练大规模语言模型时，首先需要定义模型参数并初始化模型，然后可以使用预训练的tokenizer（如BertTokenizerFast）和填充掩码pipeline来对输入进行处理。通过pipeline，可以使用模型和tokenizer对包含掩码的句子进行预测。",
      "predicted_answer": "根据提供的文档，我无法回答这个问题。文档中未详细说明预训练tokenizer和模型在大规模语言模型训练时的具体填充掩码操作过程。",
      "ground_context": [
        "model = Transformer(src_vocab, trg_vocab, d_model, N, heads)",
        "tokenizer = BertTokenizerFast.from_pretrained(model_path)"
      ],
      "retrieved_contexts": [
        "大语言模型：从理论到实践 235\n图片编码器\n文本编码器\nA photoof a bird遮盖图像和对应文本\n文本解码器\n图片解码器器\nA photoof a \nDecoder\n+ +\nA photoof a bird\n遮盖文本和对应图像\n图 7.3 视觉语言模型掩码预测范式 [271]\n基础语言和视觉对齐 （Foundational Language And Vision Alignment，FLA V A）[278] 是掩码预测\n范式中的代表性方法，其架构由图像编码器、文本编码器和多模态融合组件三部分组成，均基于\nTransformer 实现。 图像编码器采用Vision Transformer (ViT) [261]， 将图像分割为片段后嵌入并生成\n包含分类标记 （[CLSI ]） 的特征表示。 文本编码器使用标准Transformer [12]， 将文本分词并嵌入向\n量空间，通过上下文处理生成隐藏状态向量，同时输出包含分类标记（[CLST ]）的特征表示。两\n者均基于掩码预测任务进行训练。 多模态融合组件利用Transformer 的线性变换和交叉注意力机制\n整合图像与文本特征...",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "372 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n(a)预填充阶段(b)解码阶段\n图 10.2 大语言模型解码两阶段 [471]\n键值缓存在不同阶段的使用方式如图10.3 所示。在预填充阶段，即第一次迭代中，将输入的\n提示词进行处理， 为大语言模型的每个Transformer 层生成键值缓存。 在解码阶段， 大语言模型只\n需要计算新生成词元的查询、键和值。利用并更新键值缓存，逐步生成后面的词元。\n图 10.3 键值缓存在不同阶段的使用方式 [472]",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "大规模语言模型：从理论到实践 185\n# main.py\n#ࡱ\nmodeling_baichuan = import_module(\"models.Baichuan-7B.modeling_baichuan\")\ntokenization_baichuan = import_module(\"models.Baichuan-7B.tokenization_baichuan\")\n#౼Baichuanো\nBaiChuanForCausalLM = getattr(modeling_baichuan, \"BaiChuanForCausalLM\")\nBaiChuanTokenizer = getattr(tokenization_baichuan, \"BaiChuanTokenizer\")\n对模型代码文件路径做相应的修改，改为本地存储模型代码的路径。然后，同样在 main.py 中对\n对应的模型加载进行修改：\n# main.py\n# ჰ൓ս઒\ntokenizer = load_hf_tokenizer(args.model_name_or_path, fast_tokenizer =True)\nm..."
      ],
      "response_time": 1.050119161605835,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.33333333333333337,
        "rouge2": 0.0,
        "rougeL": 0.33333333333333337,
        "bleu": 0.03720180916786456,
        "semantic_similarity": 0.8160299062728882
      }
    },
    {
      "sample_id": "0022",
      "question": "代码的主要来源是什么？",
      "ground_truth": "编程问答社区（如 Stack Exchange）和公共软件仓库（如 GitHub）。",
      "predicted_answer": "代码的主要来源是编程问答社区（如 Stack Exchange[103, 104]）和公共软件仓库（如 GitHub[27, 100, 105]）。[1]",
      "ground_context": [
        "代码的主要来源是编程问答社区（如 Stack Exchange）和公共软件仓库（如 GitHub）。"
      ],
      "retrieved_contexts": [
        "66 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n式化语言， 它对应着长程依赖和准确的执行逻辑[102]。 代码的语法结构、 关键字和特定的编程范式\n都对其含义和功能起着重要的作用。代码的主要来源是编程问答社区（如 Stack Exchange[103, 104]）\n和公共软件仓库（如 GitHub[27, 100, 105]） 。编程问答社区中的数据包含了开发者提出的问题、其他\n开发者的回答及相关代码示例。这些数据提供了丰富的语境和真实世界中的代码使用场景。公共\n软件仓库中的数据包含了大量的开源代码，涵盖多种编程语言和不同领域。这些代码库中的很多\n代码经过了严格的代码评审和实际的使用测试，因此具有一定的可靠性。\n3.1.2 领域数据\n特定领域预训练语料库是为特定领域或主题量身定制的。这类语料库通常用于大语言模型的\n增量预训练阶段。在用通用预训练语料库训练出一个基础模型之后，如果该模型需要应用于某一\n特定领域的下游任务，就可以进一步利用特定领域预训练语料库对模型进行增量预训练。这一过\n程在基于初始通用预训练所获得的通用能力基础上，增强了模型在特定领域的能力。虽然领域数\n据...",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "表示，蛋白质序列可以用单字母代码或三字母代码表示。这样可以将不同格式的数据转换为统一\n的形式，使大语言模型更好地处理和分析科学文本数据。\n百科（Encyclopedia） 数据包含百科全书、 在线百科网站及其他知识数据库， 这些数据中蕴含着\n极为丰富的知识。百科知识内容通常是经由专家严谨编撰、志愿者无私奉献以及社区贡献者协同\n努力， 得以创作与完善， 具备一定的权威性与可靠性。 由于此类知识资源易于获取， 在大语言模型\n的预训练语料构建进程中发挥着至关重要的作用。最常见的百科语料库是维基百科（Wikipedia） 。\n它具有免费、开源、多语言以及文本价值高的特点。几乎所有的大语言模型预训练都会将维基百\n科作为其预训练语料库的一部分。就中文百科语料库而言，除了中文版维基百科外，还有百度百\n科、搜狗百科等来源。它们几乎涵盖了所有知识领域，TigerBot-wiki[99] 就是从百度百科的数据中\n筛选出来的。\n代码（Code） 是进行程序生成任务所必需的训练数据。 近期的研究和ChatGPT 的结果表明， 通\n过在大量代码上进行预训练，大语言模型可以有效提升代码生成的效果 [100, 10...",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "如图3.14 所示。其中，中文数据主要由WuDao Corpora 和 OSCAR[154] 组成。在虚拟抓取方面，由\n于很多语言的现有公开数据集较少， 因此这些语言的网页信息是十分重要的资源补充。 在ROOTS\n数据集中， 采用CommonCrawl 网页镜像， 选取了614 个域名， 从这些域名下的网页中提取文本内\n容补充到数据集中， 以提升语言的多样性。 在GitHub 代码方面， 针对程序语言，ROOTS 数据集采\n用了与 AlphaCode[101] 相同的方法：从 BigQuery 公开数据集中选取文件长度在 100 到 20 万字符，\n字母符号占比在 15% 至 65%，最大行数在 20 至 1000 行的代码。训练大语言模型时，网页数据对\n于数据的多样性和数据量支撑起到重要的作用[2, 19]，ROOTS 数据集中包含了 OSCAR 21.09 版本，\n对应的是 CommonCrawl 2021 年 2 月的快照，占整体 ROOTS 数据集规模的 38%。"
      ],
      "response_time": 1.3812129497528076,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.5,
        "rouge2": 0.2,
        "rougeL": 0.5,
        "bleu": 0.34917266802171815,
        "semantic_similarity": 0.8681443929672241
      }
    },
    {
      "sample_id": "0023",
      "question": "后缀数组 A 的元素是如何排列的？",
      "ground_truth": "A 中的元素按照后缀的字典顺序排列。",
      "predicted_answer": "后缀数组 A 的元素是按照后缀的字典顺序排列的[1]。对于序列 \"banana\" 的后缀 \"banana\" \"anana\" \"nana\" \"ana\" \"na\" \"a\" ，其对应的后缀数组 A 为 [6, 4, 2, 1, 5, 3]。",
      "ground_context": [
        "A 中的元素按照后缀的字典顺序排列。"
      ],
      "retrieved_contexts": [
        "缀数组 A 是一个整数数组， 其中每个元素表示S 中的一个后缀的起始位置。A 中的元素按照后缀\n的字典顺序排列。 例如， 序列 “banana” 的后缀包括 “banana” “anana” “nana” “ana” “na” “a” ， 对应的后\n缀数组 A 为 [6, 4, 2, 1, 5, 3]。 根据数组A， 可以很容易地找出相同的子串。 如果Si···i+|s| = Sj···j+|s|，\n那么 i 和 j 在数组 A 中一定在紧邻的位置上。文献 [124] 中设计了并行的后缀数组构造方法，针\n对 Wiki-40B 训练数据（约包含 4GB 文本内容） ，使用拥有96 核 CPU 以及 768GB 内存的服务器，\n可以在 140 秒内完成计算。 对于包含350GB 文本的 C4 数据集， 仅需要12 小时就可以完成后缀数\n组构造。\n在文档级别上，大部分大语言模型依靠文档之间的表面特征相似度（例如 n-gram 重叠比例）\n进行检测并删除重复文档[31, 34, 60, 124]。LLaMA[34] 采用 CCNet[125] 的处理模式，先将文档拆分为段\n落，并把所有字母转换...",
        "大规模语言模型：从理论到实践 109\n通过 AllReduce_Sum 通信¬， 跨设备求和， 得到完整的全量结果。 可以看出， 这里的输出结果和单\n计算设备执行的结果一致。\n图 4.9 单节点 Embedding 和两节点 Embedding 张量并行的示意图\n矩阵乘的张量并行要充分利用矩阵的分块乘法原理。举例来说，要实现如下矩阵乘法 Y =\nXA， 其中X 是维度为 M × N 的输入矩阵，A 是维度为 N × K 的参数矩阵，Y 是结果矩阵， 维\n度为 M × K。如果参数矩阵 A 非常大，甚至超出单张卡的显存容量，那么可以把参数矩阵 A 切\n分到多张卡上，并通过集合通信汇集结果，保证最终结果在数学计算上等价于单计算设备的计算\n结果。参数矩阵 A 存在以下两种切分方式。\n（1）参数矩阵 A 按列切块，将矩阵 A 按列切成\nA = [A1, A2] (4.2)\n（2）参数矩阵 A 按行切块，将矩阵 A 按行切成\nA =\n\f\f\f\f\f\nA1\nA2\n\f\f\f\f\f (4.3)\n图4.10 给出了参数矩阵按列切分的示例， 参数矩阵A 分别将 A1, A2 放置在两个计算设备上。\n两个计算设备分...",
        "274 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n前缀的词元，在成功识别工具方面起着更为关键的作用。标准的有监督微调训练会不加区分地最\n大化每个词元的条件概率，将所有词元视为同等重要。为解决这一局限，TL-Training 提出了一种\n根据词元相对重要性自适应调整其训练权重的方案。\n给定一个数据序列 (q, t0..s, o0..s)， 其中每个工具ti = (t0\ni , t1\ni , . . . , tli\ni ) 由 li 个词元构成， 将这些\n词元划分为两个集合：\nKi = {tm\ni ∈ ti | tm\ni 是关键词元} (8.2)\nN Ki = {tm\ni ∈ ti | tm\ni 不是关键词元} (8.3)\n然后，依据它们的相对重要性来调整 Ki 和 N Ki 的权重，使模型能够更侧重于关键词元。\nwm\ni =\n8\n<\n:\nCLIP\n\u0010\n|N Ki|\n|Ki| , 1, wmax\n\u0011\n如果tm\ni ∈ Ki\n1 否则\n(8.4)\n其中，wmax 是最大调整乘数，而 CLIP(x, min, max) 函数用于将调整因子限制在 [min, max] 这个区\n间...",
        "练。通过将预训练的 Vicuna 语言模型编码器与 CLIP ViT-L/14 视觉编码器的输出融合到相同的维\n度空间，LLaV A在合成指令跟随任务和 Science QA 基准测试中表现出显著的改进。LLaV A 1.5[320]\n在 LLaV A的基础上进一步优化了多模态文本对齐能力。其改进包括引入跨模态全连接多层感知\n机（MLP）层，并结合视觉问答（VQA）指令数据进行训练。LLaV A 1.5仅使用 60 万条图文对数\n据，在 8 张 A100 GPU 上约一天即可完成训练。LLaV A-NeXT (v1.6)[330] 在 LLaV A 1.5的基础上进\n行了多方面的改进，进一步推动了多模态文本对齐的性能。通过将全图和小图块的视觉特征分别\n输入视觉编码器，并将其拼接后处理，提高了图像分辨率的利用效率。优化了视觉指令调优数据\n集，新增了更好的视觉推理、OCR、世界知识和逻辑推理样本。\n由于高质量视觉指令调优数据的稀缺， LLaV A等模型可能在视觉和文本模态对齐上存在偏\n差， 甚至生成幻觉性输出。 为了解决这一问题，LLaV A-RLHF[262] 提出了基于人类反馈强化学习...",
        "下游任务通常需要利用有标注数据集进行训练，数据集使用 D 进行表示，每个样例由输入长度为\nn 的文本序列 x = x1, x2, · · · , xn 和对应的标签 y 构成。\n先将文本序列 x 输入 GPT 模型， 获得最后一层的最后一个词所对应的隐藏层输出h(L)\nn ， 在此"
      ],
      "response_time": 2.0991780757904053,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.11764705882352941,
        "rouge2": 0.0,
        "rougeL": 0.11764705882352941,
        "bleu": 0.06592309822529722,
        "semantic_similarity": 0.7101008892059326
      }
    },
    {
      "sample_id": "0024",
      "question": "稀疏混合专家模型中采用什么策略时，分配给不同专家的词元可能需要一些共有知识？",
      "ground_truth": "常规的门控策略",
      "predicted_answer": "在稀疏混合专家模型中，采用常规的门控策略时，分配给不同专家的词元可能需要一些共有知识或信息才能处理。[1]",
      "ground_context": [
        "稀疏混合专家模型中采用常规的门控策略时，分配给不同专家的词元可能需要一些共有知识"
      ],
      "retrieved_contexts": [
        "x2\n图 2.11 共享专家模型 [68]\n稀疏混合专家模型中采用常规的门控策略时，分配给不同专家的词元可能需要一些共有知识\n或信息才能处理。 因此， 多个专家可能会在各自的参数中获取同样的知识， 进而导致专家参数出现\n冗余。如果构建专门用于捕捉并整合不同情境下共有知识的共享专家，那么其他专家之间的参数\n冗余情况将可能得到缓解。这种冗余情况的缓解，有助于构建一个参数利用更高效且专家专业性\n更强的模型。因此，DeepSeekMoE[74] 提出了分离 Ks 个专家作为共享专家的思路。无论门控网络\n所给出的结果如何， 每个词元都将被确定性地分配给这些共享专家， 如图2.11 所示， 深色块Shared\nFFN 为共享专家，所有输入都会分配给共享专家。为保持计算成本恒定，其他经门控网络分配的\n专家中被激活专家的数量将减少 Ks 个。\n稀疏混合专家模型中的 MoE 层对于并行计算也十分友好，能更便捷地在单个 GPU 上实现高\n效计算。常规稠密模型中，全部参数都会参与对所有输入数据的处理流程。与之不同，稀疏混合\n专家模型具备的稀疏特性，使得计算仅在系统的特定局部展开。也就是说，并非所有参数在处理...",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "x1\n<latexit sha1_base64=\"IfvsN6LQ0T10Hxv1EOheRmClmyE=\">AAAB+nicbVC7TgJBFL2LL8QXamkzkZhYkV1iwJJoY4lRHglsyOwwCxNmZjczs0ay8gm22tsZW3/G1i9xgC0UPMlNTs65N+fmBDFn2rjul5NbW9/Y3MpvF3Z29/YPiodHLR0litAmiXikOgHWlDNJm4YZTjuxolgEnLaD8fXMbz9QpVkk780kpr7AQ8lCRrCx0t1jv9IvltyyOwdaJV5GSpCh0S9+9wYRSQSVhnCsdddzY+OnWBlGOJ0WeommMSZjPKRdSyUWVPvp/NUpOrPKAIWRsiMNmqu/L1IstJ6IwG4KbEZ62ZuJ/3qBWEo24aWfMhknhkqyCA4TjkyEZj2gAVOUGD6xBBPF7O+IjLDCxNi2CrYUb7mCVdKqlL1quXp7UapfZfXk4QRO4Rw8qEEdbqABTSAwhGd4gVfnyXlz3p2PxWrOyW6O4Q+czx9SH...",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "p=0.16\n图 2.10 混合专家模型三种主要类型 [68]\n2.4.1 稀疏混合专家模型\n稀疏混合专家模型，如图2.10(a) 所示，对于每个输入词元，在前向计算中仅激活专家集合中\n的一个子集。 门控网络对专家子集进行选择， 通过计算排名前K 位专家的输出加权和来实现稀疏\n性。这个过程可以形式化的表示为：\nF M oE\nSparse (x; Θ; {Wi}N\ni=1) =\nNX\ni=1\nG(x; Θ)ifi(x; Wi) (2.38)\nG(x; Θ)i = softmax(TopK(g(x; Θ) + Rnoise, K))i (2.39)\nTop-K(g(x; Θ), K)i =\n(\ng(x; Θ)i, g(x; Θ)i的值属于前 K 项\n−∞, 其他\n(2.40)\n其中，g(x; Θ) 表示在进行 softmax 操作之前的门控值，G(x; Θ)i 表示门控网络针对第 i 个专家的\n输出，TopK(·, K) 函数的目标是保持向量的前 K 项不变， 其它维度设置为−∞。 鉴于softmax 函数\n自身所具有的独特性质，当把其中某些项设置为 −∞ 时，这些项所对应的值会近似等同于..."
      ],
      "response_time": 1.2213566303253174,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.07030983742621925,
        "semantic_similarity": 0.6739157438278198
      }
    },
    {
      "sample_id": "0025",
      "question": "FlashAttention 算法是如何减少全局内存消耗的？",
      "ground_truth": "FlashAttention 就提出了不使用中间注意力矩阵，通过存储归一化因子来减少全局内存消耗的方法。",
      "predicted_answer": "FlashAttention 算法通过以下方式减少全局内存消耗：它不将整个注意力矩阵 S、P 整体写入全局内存，而是通过分块写入的方式存储前向传播的 Softmax 归一化因子，在后向传播中快速重新计算片上注意力[1][2]。这样既可以减少全局内存的访问量，又能在一定程度上提高计算效率。",
      "ground_context": [
        "FlashAttention 就提出了不使用中间注意力矩阵，通过存储归一化因子来减少全局内存消耗的方法。"
      ],
      "retrieved_contexts": [
        "大规模语言模型：从理论到实践 53\n自注意力算法的标准实现将计算过程中的矩阵 S、P 写入全局内存，而这些中间矩阵的大小\n与输入的序列长度有关且为二次型。因此，FlashAttention 就提出了不使用中间注意力矩阵，通过\n存储归一化因子来减少全局内存消耗的方法。FlashAttention 算法并没有将 S、P 整体写入全局内\n存，而是通过分块写入，存储前向传播的 Softmax 归一化因子，在后向传播中快速重新计算片上\n注意力，这比从全局内存中读取中间注意力矩阵的标准方法更快。虽然大幅减少了全局内存的访\n问量， 重新计算也导致FLOPS 增加， 但总体来看运行的速度更快且使用的显存更少。 具体算法如\n代码2.1 所示，其中内层循环和外层循环所对应的计算可以参考图2.9。\n代码 2.1: FlashAttention 算法\n输入: Q, K, V ∈ RN ×d 位于 HBM 中，GPU 芯片中的 SRAM 大小为 M\n输出: O\nBc = ⌈ M\n4d ⌉，Br = min(⌈ M\n4d ⌉, d) // ഡᇂॶնཬčblock size Ď\n在 HBM 中初始化 O = (0)...",
        "54 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n图 2.9 FlashAttention 计算流程图 [57]\nPyTorch 2.0 已经支持 FlashAttention，使用 torch.backends.cuda.enable_flash_sdp() 函数可以启\n用或者关闭 FlashAttention。\n3. 多查询注意力\n多查询注意力（Multi Query Attention）[58] 是多头注意力的一种变体。它的特点是，在多查询\n注意力中不同的注意力头共享一个键和值的集合，每个头只单独保留了一份查询参数，因此键和\n值的矩阵仅有一份，这大幅减少了显存占用，使其更高效。由于多查询注意力改变了注意力机制\n的结构，因此模型通常需要从训练开始就支持多查询注意力。文献 [59] 的研究结果表明，可以通\n过对已经训练好的模型进行微调来添加多查询注意力支持，仅需要约 5% 的原始训练数据量就可\n以达到不错的效果。 包括Falcon[60]、SantaCoder[61]、StarCoder[62] 在内的很多模型都采用了多查询\n注意力。\n以 LLM Foundry 为例，...",
        "大规模语言模型：从理论到实践 51\n[Dk, b/2] 的随机矩阵 R，LSH 函数的定义为\nh(x) = arg max([ xR; −xR]) (2.31)\n当 hqi = hkj 时，qi 才可以与相应的 Key-V alue对进行交互。\n2. FlashAttention\nNVIDIA GPU 中的不同类型的内存 （显存） 有不同的速度、 大小及访问限制。 这主要取决于\n它们物理上是在 GPU 芯片内部还是在板卡 RAM 存储芯片上。 GPU 显存分为全局内存（ Global\nMemory） 、本地内存（Local Memory） 、共享存储（Shared Memory，SRAM） 、寄存器（Register） 、\n常量内存 （Constant Memory） 、 纹理内存 （Texture Memory） 六大类。 图2.8 为 NVIDIA GPU 的整\n体内存结构示意图。全局内存、本地内存、共享存储和寄存器具有读写能力。全局内存和本地\n内存使用的高带宽显存 （High Bandwidth Memory，HBM） 位于板卡RAM 存储芯片上， 该部分内\n存容量很大。所有线程...",
        "过深度集成多进程架构到 AsyncLLM 核心，V1 创建了一个专注于调度器和模型执行器的独立执\n行循环，从而最大化了模型吞吐量并显著优化了执行效率。调度器架构得到了简化和统一，取消\n了传统的“预填充”和“解码”阶段的区分，统一处理用户输入的提示 token 和模型生成的输出\ntoken， 大幅提升了调度逻辑的灵活性。 为了进一步优化缓存性能，V1 实现了零开销的前缀缓存机\n制，即使缓存命中率为 0%，性能损失也几乎为零。\n在推理架构方面，V1 简化了张量并行推理， 通过缓存请求状态并仅传输增量更新， 减少了进\n程间通信，形成了一种对称设计，从而优化了推理效率。输入准备也得到了高效改进，采用持久\n化批次技术缓存输入张量， 只需处理增量更新， 显著降低了CPU 开销并提升数据处理效率。 针对\n多模态大语言模型（MLLM） ， 优化了输入预处理流程， 并引入前缀缓存和编码器缓存， 增强了多\n模态场景的处理能力。\n此外，vLLM V1 集成了 FlashAttention 3，用于优化动态性高的推理场景，例如在同一批次中\n同时处理预填充和解码任务。这些改进显著提升了推理的灵活性和性能，使得 ...",
        "52 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n图 2.8 NVIDIA GPU 的整体内存结构示意图\n前文介绍了自注意力机制的原理， 在GPU 中进行计算时， 传统的方法还需要引入两个中间矩\n阵 S 和 P 并存储到全局内存中。具体计算过程如下：\nS = QK, P = Softmax(S), O = P V (2.32)\n按照上述计算过程，需要先从全局内存中读取矩阵 Q 和 K，并将计算好的矩阵 S 写入全局内存，\n然后从全局内存中获取矩阵 S，计算 Softmax 得到矩阵 P ，再将其写入全局内存，最后读取矩阵\nP 和矩阵 V ，计算得到矩阵 O。这样的过程会极大地占用显存的带宽。在自注意力机制中，GPU\n的计算速度比内存速度快得多，因此计算效率越来越受全局内存访问的制约。\nFlashAttention[57] 利用 GPU 硬件中的特殊设计， 针对全局内存和共享存储的I/O 速度的不同， 尽\n可能地避免从 HBM 中读取或写入注意力矩阵。FlashAttention 的目标是尽可能高效地使用 SRAM\n来加快计算速度，避免从全局内存中读取和写入注意力矩阵。达成..."
      ],
      "response_time": 2.3113486766815186,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.2857142857142857,
        "rouge2": 0.0,
        "rougeL": 0.2857142857142857,
        "bleu": 0.04827657579843388,
        "semantic_similarity": 0.8641515374183655
      }
    },
    {
      "sample_id": "0026",
      "question": "如何删除隐私数据？",
      "ground_truth": "删除隐私数据最直接的方法是采用基于规则的算法，BigScience ROOTS Corpus 在构建过程中就采用了基于命名实体识别的方法，利用命名实体识别算法检测姓名、地址、电话号码等个人信息内容并进行删除或者替换。",
      "predicted_answer": "删除隐私数据的一种直接方法是采用基于规则的算法，比如BigScience ROOTS Corpus在构建过程中就采用了基于命名实体识别的方法来检测并删除或替换姓名、地址、电话号码等个人信息内容[1][3]。这种方法结合了基于Transformer的模型和机器翻译技术，能够处理多种语言的文本并消除其中的隐私信息。",
      "ground_context": [
        "删除隐私数据最直接的方法是采用基于规则的算法，BigScience ROOTS Corpus 在构建过程中就采用了基于命名实体识别的方法，利用命名实体识别算法检测姓名、地址、电话号码等个人信息内容并进行删除或者替换。"
      ],
      "retrieved_contexts": [
        "70 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n3.2.3 隐私消除\n由于绝大多数预训练数据源于互联网， 因此不可避免地会包含涉及敏感或个人信息 （Personally\nIdentifiable Information，PII）的用户生成内容，这可能会增加隐私泄露的风险[127]。如图3.3 所示，\n输入前缀词“East Stroudsburg Stroudsburg” ，语言模型在此基础上补全了姓名、电子邮件地址、电\n话号码、传真号码及实际地址。这些信息都是模型从预训练数据中学习得到的。因此，非常有必\n要从预训练语料库中删除包含个人身份信息的内容。\n图 3.3 从大语言模型中获得隐私数据的例子 [127]\n删除隐私数据最直接的方法是采用基于规则的算法，BigScience ROOTS Corpus [128] 在构建过\n程中就采用了基于命名实体识别的方法，利用命名实体识别算法检测姓名、地址、电话号码等个\n人信息内容并进行删除或者替换。该方法使用了基于 Transformer 的模型，并结合机器翻译技术，\n可以处理超过 100 种语言的文本，消除其中的隐私信息。该方法被...",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "并根据文档向量表示之间的海明距离 （Hamming Distance） 是否超过阈值进行过滤。 最后， 使用后\n缀数组（Suffix Array）删除存在 6000 个以上字符重复的文档。通过上述方法共发现 21.67% 的冗\n余信息。个人信息数据（包括邮件、电话、地址等）则使用正则表示的方法进行过滤。",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "典型的数据处理流程如图3.2 所示， 主要包括质量过滤、 冗余去除、 隐私消除、 词元切分这几个步\n骤。本节将依次介绍上述内容。\n图 3.2 典型的数据处理流程图 [18]\n3.2.1 质量过滤\n互联网上的数据质量参差不齐， 无论是OpenAI 联合创始人 Andrej Karpathy 在微软 Build 2023\n的报告，还是当前的一些研究都表明，训练数据的质量对于大语言模型效果具有重大影响。因此，\n从收集到的数据中删除低质量数据成为大语言模型训练中的重要步骤。大语言模型训练中所使用\n的低质量数据过滤方法可以大致分为两类：基于分类器的方法和基于启发式的方法。\n基于分类器的方法的目标是训练文本质量判断模型，利用该模型识别并过滤低质量数据。\nGPT-3[39]、PaLM[14] 和 GLaM[116] 模型在训练数据构造时都使用了基于分类器的方法。文献 [116]\n采用了基于特征哈希的线性分类器 （Feature Hash Based Linear Classifier ） ， 可以非常高效地完成文"
      ],
      "response_time": 1.9954276084899902,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.6666666666666666,
        "rouge2": 0.5714285714285715,
        "rougeL": 0.6666666666666666,
        "bleu": 0.4049235899296669,
        "semantic_similarity": 0.9548296332359314
      }
    },
    {
      "sample_id": "0027",
      "question": "什么模型被称为软混合专家模型？",
      "ground_truth": "软混合专家模型（SoftMoE）",
      "predicted_answer": "软混合专家模型（SoftMoE）[1] 被称为一种混合专家模型。[1]",
      "ground_context": [
        "（c）软混合专家模型（SoftMoE）"
      ],
      "retrieved_contexts": [
        "Y\n（c）软混合专家模型（SoftMoE）\n<latexit sha1_base64=\"17zqk6qKa6kdejpo5MShoZgkkWA=\">AAAB/XicbVC7SgNBFL0bXzG+opY2g0GwWnYlRBshaGMZwTwgWcLsZDYZMzO7zMwKYQn+gq32dmLrt9j6JU6SLTR64MLhnHs5lxMmnGnjeZ9OYWV1bX2juFna2t7Z3SvvH7R0nCpCmyTmseqEWFPOJG0aZjjtJIpiEXLaDsfXM7/9QJVmsbwzk4QGAg8lixjBxkqt5NJzq36/XPFcbw70l/g5qUCORr/81RvEJBVUGsKx1l3fS0yQYWUY4XRa6qWaJpiM8ZB2LZVYUB1k82+n6MQqAxTFyo40aK7+vMiw0HoiQrspsBnpZW8m/uuFYinZRBdBxmSSGirJIjhKOTIxmlWBBkxRYvjEEkwUs78jMsIKE2MLK9lS/OUK/pLWmevX3NpttVK/yuspwhEcwyn4cA51uIEGNIHAPTzBM7w4j8...",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "x\n<latexit sha1_base64=\"IVqKn81Phb8FhcRP6SaEHpTS2es=\">AAAB+HicbVDLTgJBEOzFF+IL9ehlIzHxRHaNQY9ELx4hkYeBDZkdemHCzOxmZtYECV/gVe/ejFf/xqtf4gB7ULCSTipV3alOhQln2njel5NbW9/Y3MpvF3Z29/YPiodHTR2nimKDxjxW7ZBo5ExiwzDDsZ0oJCLk2ApHtzO/9YhKs1jem3GCgSADySJGibFS/aFXLHllbw53lfgZKUGGWq/43e3HNBUoDeVE647vJSaYEGUY5TgtdFONCaEjMsCOpZII1MFk/ujUPbNK341iZUcad67+vpgQofVYhHZTEDPUy95M/NcLxVKyia6DCZNJalDSRXCUctfE7qwFt88UUsPHlhCqmP3dpUOiCDW2q4ItxV+uYJU0L8p+pVypX5aqN1k9eTiBUzgHH66gCndQgwZQQHiGF3h1npw35935WKzmnOzmGP7A+fwB8QeTjg...",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "p=0.16\n图 2.10 混合专家模型三种主要类型 [68]\n2.4.1 稀疏混合专家模型\n稀疏混合专家模型，如图2.10(a) 所示，对于每个输入词元，在前向计算中仅激活专家集合中\n的一个子集。 门控网络对专家子集进行选择， 通过计算排名前K 位专家的输出加权和来实现稀疏\n性。这个过程可以形式化的表示为：\nF M oE\nSparse (x; Θ; {Wi}N\ni=1) =\nNX\ni=1\nG(x; Θ)ifi(x; Wi) (2.38)\nG(x; Θ)i = softmax(TopK(g(x; Θ) + Rnoise, K))i (2.39)\nTop-K(g(x; Θ), K)i =\n(\ng(x; Θ)i, g(x; Θ)i的值属于前 K 项\n−∞, 其他\n(2.40)\n其中，g(x; Θ) 表示在进行 softmax 操作之前的门控值，G(x; Θ)i 表示门控网络针对第 i 个专家的\n输出，TopK(·, K) 函数的目标是保持向量的前 K 项不变， 其它维度设置为−∞。 鉴于softmax 函数\n自身所具有的独特性质，当把其中某些项设置为 −∞ 时，这些项所对应的值会近似等同于..."
      ],
      "response_time": 0.8856992721557617,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.5,
        "rouge2": 0.0,
        "rougeL": 0.5,
        "bleu": 0.2915369229944523,
        "semantic_similarity": 0.8853436708450317
      }
    },
    {
      "sample_id": "0028",
      "question": "哪些大语言模型采用了稀疏混合专家架构？",
      "ground_truth": "Switch Transformer、DeepSeekMoE、AdaMoE、Yuan 2.0-M32、OpenMoE、Qwen1.5-MoE-A2.7B",
      "predicted_answer": "采用了稀疏混合专家架构的大语言模型包括Switch Transformer[67]、DeepSeekMoE[74]、AdaMoE[75]、Yuán 2.0-M32[76]、OpenMoE[77]、Qwen1.5-MoE-A2.7B[78] 等[1]。",
      "ground_context": [
        "此外，众多大语言模型也都采用了稀疏混合专家架构，包括Switch Transformer、DeepSeekMoE、AdaMoE、Yuan 2.0-M32、OpenMoE、Qwen1.5-MoE-A2.7B等。"
      ],
      "retrieved_contexts": [
        "70B[37] 的性能。 此外， 众多大语言模型也都采用了稀疏混合专家架构， 包括Switch Transformer[67]、\nDeepSeekMoE[74]、AdaMoE[75]、Y uan 2.0-M32[76]、OpenMoE[77]、Qwen1.5-MoE-A2.7B[78] 等。 更多\n相关模型可以参考文献 [68]。\n共享FNNFNN1FNN2FNN3\n门控网络\n共享专家模型（SharedExpert）",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "型采用稠密混合专家结构的并不多， 主要包括EvoMoE[81]、MoLE[82]、LoRAMoE[83] 以及 DS-MoE[84]\n等。\n虽然稠密混合专家模型需要使用全部参数进行计算，并不能减少模型计算时间，但是研究人\n员却发现， 如果能够将LoRA 方法和 MoE 相结合， 可以在占用很少GPU 显存的同时， 减少微调数\n据的大规模扩增与模型世界知识维持之间存在的冲突。有监督微调是大语言模型应用的一个关键\n步骤，当模型需要与更广泛的下游任务保持一致，或者希望显著提高在特定任务上的表现时，大\n规模增加微调数据通常成为解决方案。然而当指令数据的大规模扩增可能会破坏大语言模型中之\n前储存的世界知识，即世界知识遗忘。LoRAMoE[83] 采用融合混合专家和 LoRA 插件的思想，插\n件形式确保了在训练阶段冻结主模型，保证了主模型世界知识的完整性。",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "x2\n图 2.11 共享专家模型 [68]\n稀疏混合专家模型中采用常规的门控策略时，分配给不同专家的词元可能需要一些共有知识\n或信息才能处理。 因此， 多个专家可能会在各自的参数中获取同样的知识， 进而导致专家参数出现\n冗余。如果构建专门用于捕捉并整合不同情境下共有知识的共享专家，那么其他专家之间的参数\n冗余情况将可能得到缓解。这种冗余情况的缓解，有助于构建一个参数利用更高效且专家专业性\n更强的模型。因此，DeepSeekMoE[74] 提出了分离 Ks 个专家作为共享专家的思路。无论门控网络\n所给出的结果如何， 每个词元都将被确定性地分配给这些共享专家， 如图2.11 所示， 深色块Shared\nFFN 为共享专家，所有输入都会分配给共享专家。为保持计算成本恒定，其他经门控网络分配的\n专家中被激活专家的数量将减少 Ks 个。\n稀疏混合专家模型中的 MoE 层对于并行计算也十分友好，能更便捷地在单个 GPU 上实现高\n效计算。常规稠密模型中，全部参数都会参与对所有输入数据的处理流程。与之不同，稀疏混合\n专家模型具备的稀疏特性，使得计算仅在系统的特定局部展开。也就是说，并非所有参数在处理..."
      ],
      "response_time": 2.1265451908111572,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.7368421052631579,
        "rouge2": 0.4444444444444444,
        "rougeL": 0.7368421052631579,
        "bleu": 0.1578452203400687,
        "semantic_similarity": 0.7534752488136292
      }
    },
    {
      "sample_id": "0029",
      "question": "NVIDIA H100 中全局内存的容量是多少？",
      "ground_truth": "80GB",
      "predicted_answer": "NVIDIA H100 中全局内存的容量是 80GB [1]。",
      "ground_context": [
        "NVIDIA H100 中全局内存有 80GB 空间"
      ],
      "retrieved_contexts": [
        "存容量很大。所有线程都可以访问全局内存，而本地内存只能由当前线程访问。 NVIDIA H100\n中全局内存有 80GB 空间，其访问速度虽然可以达到 3.35TB/s，但当全部线程同时访问全局内\n存时，其平均带宽仍然很低。共享存储和寄存器位于 GPU 芯片上，因此容量很小，并且只有在\n同一个 GPU 线程块 （Thread Block） 内的线程才可以并行访问共享存储， 而寄存器仅限于同一个\n线程内部访问。虽然 NVIDIA H100 中每个 GPU 线程块在流式多处理器（ Stream Multi-processor，\nSM）上可以使用的共享存储容量仅有 228KB，但是其速度比全局内存的访问速度快很多。",
        "单个服务器通常由 2∼16 个计算加速设备组成，这些计算加速设备之间的通信带宽也是影响\n分布式训练的重要因素。如果这些计算加速设备通过服务器 PCIe 总线互联，则会造成服务器内\n部计算加速设备之间的通信瓶颈。PCIe 5.0 总线也只能提供 128GB/s 的带宽，而 NVIDIA H100 采\n用的 HBM 可以提供 3350GB/s 的带宽。因此，服务器内部通常采用异构网络架构。NVIDIA HGX\nH100 8-GPU 服务器采用 NVLink 和 NVSwitch（NVLink 交换机） 技术， 如图4.19 所示。 每块H100\nGPU 都有多个 NVLink 端口， 并连接到所有 （4 个）NVSwitch 上。 每个NVSwitch 都是一个完全无\n阻塞的交换机，完全连接所有（8 块）H100 计算加速卡。NVSwitch 的这种完全连接的拓扑结构，\n使得服务器内任何 H100 加速卡之间都可以达到 900GB/s 的双向通信速度。",
        "用数千个计算设备花费几十天。分布式训练系统需要克服计算墙、显存墙、通信墙等挑战，以确\n保集群内的所有资源得到充分利用，从而加速训练过程并缩短训练周期。\n• 计算墙：单个计算设备所能提供的计算能力与大语言模型所需的总计算量之间存在巨大差\n异。2022 年 3 月发布的 NVIDIA H100 SXM 的单卡 FP16 算力只有 2000 TFLOPS（Floating\nPoint Operations Per Second ） ， 而GPT-3 需要 314 ZFLOPS 的总计算量， 两者相差了8 个数量\n级。\n• 显存墙： 单个计算设备无法完整存储一个大语言模型的参数。GPT-3 包含 1750 亿个参数， 如\n果在推理阶段采用 FP32 格式进行存储，则需要 700GB 的计算设备内存空间，而 NVIDIA\nH100 GPU 只有 80GB 显存。\n• 通信墙： 分布式训练系统中各计算设备之间需要频繁地进行参数传输和同步。 由于通信的延\n迟和带宽限制，这可能成为训练的瓶颈。在 GPT-3 的训练过程中，如果分布式系统中存在\n128 个模型副本，那么在每次迭代过程中至少需要传输 89...",
        "大规模语言模型：从理论到实践 51\n[Dk, b/2] 的随机矩阵 R，LSH 函数的定义为\nh(x) = arg max([ xR; −xR]) (2.31)\n当 hqi = hkj 时，qi 才可以与相应的 Key-V alue对进行交互。\n2. FlashAttention\nNVIDIA GPU 中的不同类型的内存 （显存） 有不同的速度、 大小及访问限制。 这主要取决于\n它们物理上是在 GPU 芯片内部还是在板卡 RAM 存储芯片上。 GPU 显存分为全局内存（ Global\nMemory） 、本地内存（Local Memory） 、共享存储（Shared Memory，SRAM） 、寄存器（Register） 、\n常量内存 （Constant Memory） 、 纹理内存 （Texture Memory） 六大类。 图2.8 为 NVIDIA GPU 的整\n体内存结构示意图。全局内存、本地内存、共享存储和寄存器具有读写能力。全局内存和本地\n内存使用的高带宽显存 （High Bandwidth Memory，HBM） 位于板卡RAM 存储芯片上， 该部分内\n存容量很大。所有线程...",
        "大规模语言模型：从理论到实践 105\n通过以下命令行启动上述程序：\nCUDA_VISIBLE_DEVICES=0,1 python -m torch.distributed.launch --nproc_per_node=2 main.py\n4.2.2 模型并行\n模型并行往往用于解决单节点内存不足的问题。 以包含1750 亿个参数的 GPT-3 模型为例， 如\n果模型中每一个参数都使用 32 位浮点数表示， 那么模型需要占用700GB 内存。 如果使用16 位浮\n点数表示， 那么每个模型副本需要占用350GB 内存。2022 年 3 月 NVIDIA 发布的 H100 加速卡仅\n支持 80GB 显存，无法将整个模型完整放入其中。模型并行可以从计算图角度，用以下两种形式\n进行切分。\n（1）按模型的层切分到不同设备，即层间并行或算子间并行（Inter-operator Parallelism），也\n称之为流水线并行（Pipeline Parallelism，PP）。\n（2） 将计算图层内的参数切分到不同设备， 即层内并行或算子内并行（Intra-operator Parallelism）\n..."
      ],
      "response_time": 0.7948002815246582,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.4,
        "rouge2": 0.0,
        "rougeL": 0.4,
        "bleu": 0.018724372764461875,
        "semantic_similarity": 0.6106727719306946
      }
    },
    {
      "sample_id": "0030",
      "question": "稀疏混合专家模型的p值是多少？",
      "ground_truth": "p=0.16",
      "predicted_answer": "根据提供的文档，我无法回答这个问题。文档中提到了稀疏混合专家模型[sparse MoE][4][5]，但没有提到与p值相关的信息。",
      "ground_context": [
        "p=0.16"
      ],
      "retrieved_contexts": [
        "p=0.28\n<latexit sha1_base64=\"2wDbko0etWTaF+OOQTtkDmzbXxM=\">AAAB/XicbZA9SwNBEIbn4leMX1FLm8UgWIU7kWgjBG0sI5gPSI6wt9lL1uzuHbt7QjiCf8FWezux9bfY+kvcXK7QxBcGXt6ZYYYniDnTxnW/nMLK6tr6RnGztLW9s7tX3j9o6ShRhDZJxCPVCbCmnEnaNMxw2okVxSLgtB2Mb2b99iNVmkXy3kxi6gs8lCxkBBsbteIrt+rV+uWKW3UzoWXj5aYCuRr98ndvEJFEUGkIx1p3PTc2foqVYYTTaamXaBpjMsZD2rVWYkG1n2bfTtGJTQYojJQtaVCW/t5IsdB6IgI7KbAZ6cXeLPy3F4iFyya89FMm48RQSeaHw4QjE6EZCjRgihLDJ9Zgopj9HZERVpgYC6xkoXiLCJZN68zyq9buziv16xxPEY7gGE7Bgwuowy00oAkEHuAZXuDVeXLenHfnYz5acPKdQ...",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "p=0.28\n<latexit sha1_base64=\"2wDbko0etWTaF+OOQTtkDmzbXxM=\">AAAB/XicbZA9SwNBEIbn4leMX1FLm8UgWIU7kWgjBG0sI5gPSI6wt9lL1uzuHbt7QjiCf8FWezux9bfY+kvcXK7QxBcGXt6ZYYYniDnTxnW/nMLK6tr6RnGztLW9s7tX3j9o6ShRhDZJxCPVCbCmnEnaNMxw2okVxSLgtB2Mb2b99iNVmkXy3kxi6gs8lCxkBBsbteIrt+rV+uWKW3UzoWXj5aYCuRr98ndvEJFEUGkIx1p3PTc2foqVYYTTaamXaBpjMsZD2rVWYkG1n2bfTtGJTQYojJQtaVCW/t5IsdB6IgI7KbAZ6cXeLPy3F4iFyya89FMm48RQSeaHw4QjE6EZCjRgihLDJ9Zgopj9HZERVpgYC6xkoXiLCJZN68zyq9buziv16xxPEY7gGE7Bgwuowy00oAkEHuAZXuDVeXLenHfnYz5acPKdQ...",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "p=0.16\n（a）稀疏混合专家模型（SparseMoE）\nAdd+NormalizeMergedFFN\nFNN1FNN2FNN3FNN4\n门控网络\n<latexit sha1_base64=\"HNgBwgBsr9ynSNzmxxjzG8mCdC8=\">AAAB+HicbVDLTgJBEOzFF+IL9ehlIjHxRHaNQY9ELx4hkUcCGzI79MKE2dnNzKwRCV/gVe/ejFf/xqtf4gB7ULCSTipV3alOBYng2rjul5NbW9/Y3MpvF3Z29/YPiodHTR2nimGDxSJW7YBqFFxiw3AjsJ0opFEgsBWMbmd+6wGV5rG8N+ME/YgOJA85o8ZK9cdeseSW3TnIKvEyUoIMtV7xu9uPWRqhNExQrTuemxh/QpXhTOC00E01JpSN6AA7lkoaofYn80en5MwqfRLGyo40ZK7+vpjQSOtxFNjNiJqhXvZm4r9eEC0lm/Dan3CZpAYlWwSHqSAmJrMWSJ8rZEaMLaFMcfs7YUOqKDO2q4ItxVuuYJU0L..."
      ],
      "response_time": 1.1974303722381592,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 1.0,
        "mrr": 0.2
      },
      "answer_quality_metrics": {
        "rouge1": 0.25,
        "rouge2": 0.0,
        "rougeL": 0.25,
        "bleu": 0.007469938459592591,
        "semantic_similarity": 0.546562135219574
      }
    },
    {
      "sample_id": "0031",
      "question": "在自然语言处理的模型中，如何处理未登录词的表示，并且这个处理对模型性能有什么影响？",
      "ground_truth": "在传统的自然语言处理模型中，未登录词（OOV）会被给予一个默认的通用表示，即使用一个标识为'[UNK]'的向量。词表示模型在训练过程中将'[UNK]'的向量作为词表示矩阵的一部分进行更新。当词表过小时，未登录词的比例较高，可能会影响模型的性能；而当词表过大时，低频词的词向量难以有效训练，亦会对模型性能产生负面影响。",
      "predicted_answer": "在自然语言处理的模型中，传统方法依赖预先确定的词表进行词表示，这会导致未登录词（OOV）缺乏有效的表示，可能降低模型性能[1][3]。为了解决这一问题，研究人员提出了子词词元化的方法，即将词汇分解为更细粒度的词元，这样不仅可以覆盖更多的词汇，还能有效减少未登录词的比例[3][5]。这种方法使得模型能够更好地泛化到新的词汇上，从而提高整体性能[1][3]。",
      "ground_context": [
        "在深度学习模型中，词表示模型会预先在词表中加入一个默认的 '[UNK]' （unknown）标识，表示未知词，并在训练的过程中将[UNK] 的向量作为词表示矩阵的一部分一起训练。",
        "当词表过小时，未登录词的比例较高，影响模型性能；当词表过大时，大量低频词出现在词表中，这些词的词向量很难。"
      ],
      "retrieved_contexts": [
        "3.2.4 词元切分\n传统的自然语言处理通常以单词为基本处理单元， 模型都依赖预先确定的词表V， 在对输入词\n序列编码时， 这些词表示模型只能处理词表中存在的词。 因此， 使用时， 如果遇到不在词表中的未登\n录词， 模型无法为其生成对应的表示， 只能给予这些未登录词（Out-of-V ocabulary，OOV） 一个默认\n的通用表示。 在深度学习模型中， 词表示模型会预先在词表中加入一个默认的 “[UNK]” （unknown）\n标识，表示未知词，并在训练的过程中将[UNK] 的向量作为词表示矩阵的一部分一起训练，通过\n引入某些相应机制来更新 [UNK] 向量的参数。使用时，对全部未登录词使用 [UNK] 向量作为表\n示向量。此外，基于固定词表的词表示模型对词表大小的选择比较敏感。当词表过小时，未登录\n词的比例较高，影响模型性能；当词表过大时，大量低频词出现在词表中，这些词的词向量很难",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "大规模语言模型：从理论到实践 71\n得到充分学习。理想模式下，词表示模型应能覆盖绝大部分的输入词，并避免词表过大所造成的\n数据稀疏问题。\n为了缓解未登录词问题，一些工作通过利用亚词级别的信息构造词表示向量。一种直接的解\n决思路是为输入建立字符级别表示，并通过字符向量的组合获得每个单词的表示，以解决数据稀\n疏问题。 然而， 单词中的词根、 词缀等构词模式往往跨越多个字符， 基于字符表示的方法很难学习\n跨度较大的模式。 为了充分学习这些构词模式， 研究人员提出了子词词元化（Subword Tokenization）\n方法，试图缓解上文介绍的未登录词问题。词元表示模型会维护一个词元词表，其中既存在完整\n的单词，也存在形如“c” “re” “ing”等单词的部分信息，称为子词（Subword）。词元表示模型\n对词表中的每个词元计算一个定长向量表示，供下游模型使用。对于输入的词序列，词元表示模\n型将每个词拆分为词表内的词元。例如， 将单词“reborn”拆分为“re”和“born” 。模型随后查询\n每个词元的表示，将输入重新组成词元表示序列。当下游模型需要计算一个单词或词组的表示时，\n可以将对应...",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "可以将对应范围内的词元表示合成需要的表示。因此，词元表示模型能够较好地解决自然语言处\n理系统中未登录词的问题。词元分析（Tokenization）是将原始文本分割成词元序列的过程。词元\n切分也是数据预处理中至关重要的一步。\n字节对编码（Byte Pair Encoding，BPE）[129] 是一种常见的子词词元算法。该算法采用的词表\n包含最常见的单词及高频出现的子词。使用时，常见词通常位于BPE 词表中，而罕见词通常能被\n分解为若干个包含在 BPE 词表中的词元， 从而大幅减小未登录词的比例。BPE 算法包括以下两个\n部分。\n（1）词元词表的确定。\n（2）全词切分为词元及词元合并为全词的方法。\nBPE 中词元词表的计算过程如图3.4 所示。 首先， 确定数据库中全词的词表和词频， 然后将每\n个单词切分为单个字符的序列，并在序列最后添加符号“</w>”作为单词结尾的标识。例如，单\n词“low”被切分为序列“l␣o␣w␣</w>” 。所切分出的序列元素称为字节，即每个单词都切分为字\n节的序列。之后，按照每个字节序列的相邻字节对和单词的词频，统计每个相邻字节对的出现频\n率，合并出现频率最高..."
      ],
      "response_time": 2.9159858226776123,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.2,
        "rouge2": 0.0,
        "rougeL": 0.2,
        "bleu": 0.11640074462341636,
        "semantic_similarity": 0.8137942552566528
      }
    },
    {
      "sample_id": "0032",
      "question": "Mixtral-8x7B模型在性能上如何与Huatuo-26M数据集相关的医疗问答对进行比较？",
      "ground_truth": "Mixtral-8x7B模型在很多基准测试中展现出了优于或等同于包含700亿参数的Llama-2-70B的性能，而Huatuo-26M数据集是规模最大的中文医疗问答数据集之一，包含逾2600万条高质量的医疗问答对，提供了疾病、症状、治疗方法以及药物信息等方面的知识。",
      "predicted_answer": "根据提供的文档，我无法回答这个问题。文档中并未提及Mixtral-8x7B模型与Huatuo-26M数据集相关的医疗问答对之间的性能比较。[1][2][3][4][5]",
      "ground_context": [
        "Mixtral-8x7B模型在很多基准测试中，展现出了优于或等同于包含了700亿参数的Llama-2-70B的性能。",
        "Huatuo-26M是目前规模最大的中文医疗问答数据集之一，包含逾2600万条高质量的医疗问答对，涵盖疾病、症状、治疗方法以及药物信息等诸多方面。"
      ],
      "retrieved_contexts": [
        "大规模语言模型：从理论到实践 59\n声项 Rnoise 是训练稀疏混合专家层的一种常用策略， 一方面， 它能够为模型创造更多的探索空间，\n促使不同专家模块之间展开多样化的尝试与协作，挖掘出潜在的优化路径；另一方面，通过打破\n可能出现的局部最优情况，提高了整个混合专家训练过程的稳定性[67]。\n由 Mixtral AI 公司推出的 Mixtral-8x7B 模型[66] 就采用了稀疏混合专家方式， 与早期的Mistral\n7B 模型[73] 共享基础架构。 但是，Mixtral-8x7B 模型使用了稀疏混合专家层代替每个 Transformer 块\n中的前馈层， 每个稀疏混合专家层包含8 个专家网络， 门控网络每次激活2 个专家。 但是在Mixtral-\n8x7B 模型中没有引入噪声项 Rnoise， 每个专家网络则使用了SwiGLU 结构[46]。 由于采用了稀疏混\n合专家方式，虽然 Mixtral-8x7B 模型的总参数量大约 560 亿，但是每次仅使用 130 亿个活跃参数。\n并且，Mixtral-8x7B 模型在很多基准测试中， 展现出了优于或等同于包含了700 亿参数的 Lla...",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "使用了可开放获取的医学百科全书和医学教科书数据。Huatuo-26M[112] 是目前规模最大的中文医\n疗问答数据集之一， 该数据集包含逾2600 万条高质量的医疗问答对， 涵盖疾病、症状、治疗方法\n以及药物信息等诸多方面。MedDialog[113] 是一个多语言的医疗对话数据集， 包含中文和英文的医",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "Attention，GQA） 。当MLA 和 GQA 使用相同大小的 KV 缓存时，MLA 表现出更强的能力。这\n是因为在某些情况下，MLA 能够在通道输出上展现更大的多样性，而 GQA 由于组内头部是复制\n的，导致组内所有头部的输出相同，无法捕捉到 MLA 所能处理的某些情况。文献 [64] 还提出了\nTransMLA 后训练方法， 该方法能够将广泛使用的基于GQA 的预训练模型 （例如LLaMA、Qwen、\nMixtral） 转换为基于MLA 的模型。 转换后， 通过进一步训练， 在不增加KV 缓存大小的前提下有\n效提升模型的表现力。\n2.4 混合专家模型\n随着 GPT-4[65]、Mixtral-8x7B[66]、DeepSeek-V3[40] 等模型的相继推出，混合专家模型 (Mixed\nExpert Models，MoEs) 日益受到关注。 依据大模型缩放法则， 模型规模是提升性能的关键， 然而规\n模扩大必然使计算资源大幅增加。因此，在有限计算资源预算下，如何用更少训练步数训练更大\n模型成为关键问题。为解决该问题，混合专家模型基于一个简洁的思想：模型不同部分（即“专\n家” ）..."
      ],
      "response_time": 1.5195691585540771,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 0.3333333333333333
      },
      "answer_quality_metrics": {
        "rouge1": 0.5555555555555556,
        "rouge2": 0.25,
        "rougeL": 0.4444444444444444,
        "bleu": 0.11243067438126955,
        "semantic_similarity": 0.8317223787307739
      }
    },
    {
      "sample_id": "0033",
      "question": "大语言模型训练中通用数据的重要性是什么，以及目前业界对预训练数据配比有何共识？",
      "ground_truth": "大语言模型训练中通用数据占比非常高，包括网页、对话文本、书籍等多种类型，提供了大规模且多样的训练数据。然而，业界截至2025年2月对预训练数据的配比尚未达成广泛的共识。",
      "predicted_answer": "大语言模型训练中通用数据非常重要，因为它们为模型提供了大规模且多样化的训练数据，有助于模型获得丰富的语言知识并增强泛化能力[1][3]。通用数据主要来源于网页、对话文本、书籍、代码、百科等多种类型，其中网页是最丰富的一类数据源[1][3]。\n\n截至2025年2月，业界关于预训练数据的具体配比还没有达成广泛的共识[1]。尽管如此，业界普遍认为预训练数据应涵盖多种类型的文本，并尽可能地覆盖不同的领域、语言、文化和视角，以提升大语言模型的泛化能力和适应性[2][3]。",
      "ground_context": [
        "通用数据在大语言模型训练数据中占比非常高，主要包括网页、对话文本、书籍、代码、百科等不同类型的数据，为大语言模型提供了大规模且多样的训练数据。",
        "截至2025年2月，业界关于预训练数据的配比还没达成广泛的共识。"
      ],
      "retrieved_contexts": [
        "64 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n型的任务解决能力。图3.1 给出了一些典型的大语言模型所使用数据类型的分布情况。可以看到，\n不同的大语言模型在训练数据类型分布上的差距很大， 截至2025 年 2 月， 业界关于预训练数据的\n配比还没达成广泛的共识。\n图 3.1 典型的大语言模型所使用数据类型的分布情况 [18]\n3.1.1 通用数据\n通用数据在大语言模型训练数据中占比非常高，主要包括网页、对话文本、书籍、代码、百\n科等不同类型的数据，为大语言模型提供了大规模且多样的训练数据。\n网页（Webpage）是通用数据中数量最多的一类。随着互联网的大规模普及，人们通过网站、\n论坛、 博客、App 创造了海量的数据。 根据2016 年 Google 公开的数据， 其搜索引擎索引处理了超\n过 130 万亿个网页数据。网页数据所包含的海量内容，使语言模型能够获得多样化的语言知识并\n增强其泛化能力[11, 19]。爬取和处理海量网页内容并不是一件容易的事情，因此一些研究人员构建\n了 ClueWeb09[89]、ClueWeb12[90]、SogouT-16[91]、Com...",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "3. 大语言模型预训练数据\n在预训练阶段，大语言模型从海量“高质量”文本数据中学习广泛的知识，随后这些知识存\n储在其模型参数当中。通过预训练使得大语言模型具备了一定程度的语言理解和生成能力。因此，\n如何构造海量“高质量”数据对于大语言模型预训练具有至关重要的作用。研究表明，预训练数\n据需要涵盖各种类型的文本，也需要覆盖尽可能多的领域、语言、文化和视角，从而提高大语言\n模型的泛化能力和适应性。当前大模型预训练使用的语料库涵盖网页内容、学术资料、百科、社\n交媒体和书籍等文本内容，同时也包含来自不同领域的文本内容，比如法律文件、年度财务报告、\n医学教科书等其他特定领域的数据。\n本章将介绍常见的大语言模型预训练数据的来源、处理方法、预训练数据对大语言模型影响\n的分析及开源数据集等。\n3.1 数据来源\n文献 [13] 介绍了 OpenAI 训练 GPT-3 使用的主要数据来源，包含经过过滤的 CommonCrawl\n数据集[19]、WebText 2、Books 1、Books 2 及英文 Wikipedia 等数据集。其中 CommonCrawl 的原\n始数据有 45TB，过滤后仅保留了 57...",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "始数据有 45TB，过滤后仅保留了 570GB 的数据。通过词元方式对上述数据进行切分，大约包含\n5000 亿个词元。为了保证模型使用更多高质量数据进行训练，在GPT-3 训练时，根据数据来源的\n不同，设置不同的采样权重。在完成 3000 亿个词元的训练时，英文 Wikipedia 的数据平均训练轮\n数为 3.4 次，而 CommonCrawl 和 Books 2 仅有 0.44 次和 0.43 次。由于 CommonCrawl 数据集的\n过滤过程烦琐复杂，Meta 公司的研究人员在训练 OPT[29] 模型时采用了混合 RoBERTa[86]、Pile[87]\n和 PushShift.io Reddit[88] 数据的方法。 由于这些数据集中包含的绝大部分数据都是英文数据， 因此\nOPT 也从 CommonCrawl 数据集中抽取了部分非英文数据加入训练数据。\n大语言模型预训练所需的数据来源大体上分为通用数据和专业数据两大类。通用数据（General\nData）包括网页、图书、新闻、对话文本等[14, 29, 39]。通用数据具有规模大、多样性和易获取等特\n点，因此支持大语言模型的语..."
      ],
      "response_time": 3.483917713165283,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.3636363636363636,
        "rouge2": 0.2222222222222222,
        "rougeL": 0.3636363636363636,
        "bleu": 0.16767146736524233,
        "semantic_similarity": 0.8868085741996765
      }
    },
    {
      "sample_id": "0034",
      "question": "团队使用了哪两种方法对数据集进行去重？",
      "ground_truth": "MinHash 和 URL",
      "predicted_answer": "团队使用了 MinHashLSH 方法和 URL 对数据集进行全面去重[1]。具体来说，MinHashLSH 方法用于过滤相似文档，而 URL 在这里可能用于识别不同文档之间的唯一性。",
      "ground_context": [
        "团队利用 MinHash 和 URL 对数据集进行全面去重，并按语言独立进行。"
      ],
      "retrieved_contexts": [
        "团队利用 MinHash 和 URL 对数据集进行全面去重，并按语言独立进行。其中，MinHashLSH[165]\n方法用于过滤相似文档，它基于 MinHash[158] 的多个哈希函数和 Jaccard 相似度，结合局部敏感哈",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "滤短文档、全局去重、文档交错、文档重排、训练集和保留集拆分，以及训练集与保留集中相似\n数据去重等步骤。所有步骤都假定整个数据集无法全部装载到内存中，并分布在多个进程中进行\n处理。使用 64 块 CPU，大约花费 60 多个小时就可以完成 1.21 万亿个词元的处理。整个处理过程\n所需内存峰值为 1.4TB。\n图 3.17 SlimPajama 数据集的处理过程 [166]\nSlimPajama 处理的详细流程如下。\n（1）NFC 正则化 （NFC Normalization） 的目标是去除非Unicode 字符，SlimPajama 遵循 GPT-2\n的规范，采用 NFC（Normalization Form C）正则化方法。NFC 正则化的命令示例如下：",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "据集，DataComp 提出了从多样化设计的数据集中进行采样的策略。 具体而言， 采样方法主要分为\n基于文本和基于图像两种： 基于文本的采样方法保留与ImageNet 类别相关联的图文对描述； 而基\n于图像的采样则通过利用 OpenAI CLIP 的 ViT-L/14模型对图像进行编码， 并借助FAISS 工具将大\n规模噪声图像聚类为 100,000 个组，然后根据 ImageNet 训练样本的嵌入，选择与这些样本最相近\n的聚类，从而生成具有多样性的图像数据集。尽管这些方法能有效提升数据的多样性，但它们对\nImageNet 等语义数据集的依赖可能会引入类别偏倚，从而限制模型在新下游任务中的泛化能力。\n此外，MetaCLIP[314] 提出了另一种方法， 利用来自Wikipedia 和 WordNet 的 500,000 个查询作为元\n数据，构建覆盖广泛概念的预训练数据分布。通过“平衡采样”算法，MetaCLIP 限制每个查询的\n样本数量（最多 20,000 个） ，在概念的多样性与代表性之间寻求平衡，从而进一步提升模型的泛\n化能力。\n7.3.2 视觉语义关联\n视觉语义关联是多模态大语言..."
      ],
      "response_time": 1.3661210536956787,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.28571428571428575,
        "rouge2": 0.0,
        "rougeL": 0.28571428571428575,
        "bleu": 0.015782280286300628,
        "semantic_similarity": 0.6812438368797302
      }
    },
    {
      "sample_id": "0035",
      "question": "数据质量对大语言模型训练效果的影响是什么？",
      "ground_truth": "数据质量通常被认为是影响大语言模型训练效果的关键因素之一。",
      "predicted_answer": "数据质量对大语言模型训练效果有很大影响。大量重复的低质量数据可能导致模型训练不稳定，甚至不收敛[122, 139]。高质量的数据有助于提高模型在自然语言生成和理解等任务上的性能，尤其是在自然语言生成任务中更为明显[116]。训练数据的构建时间、包含噪声或有害信息的情况以及数据重复率等因素都会显著影响模型性能[115, 122, 124, 140]。因此，业界普遍认为，在清洗过的高质量数据上训练的语言模型会有更好的性能[1]。",
      "ground_context": [
        "数据质量通常被认为是影响大语言模型训练效果的关键因素之一。"
      ],
      "retrieved_contexts": [
        "数据质量通常被认为是影响大语言模型训练效果的关键因素之一。大量重复的低质量数据甚\n至导致训练过程不稳定， 造成模型训练不收敛[122, 139]。 现有的研究表明， 训练数据的构建时间、 包\n含噪声或有害信息情况、 数据重复率等因素， 都对语言模型性能产生较大影响[115, 122, 124, 140]。 目前\n业界普遍的共识是语言模型在经过清洗的高质量数据上训练可以得到更好的性能。\n¬ M，即 Million，表示百万。\n­ B，即 Billion，表示十亿。",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "17 亿个参数的模型，针对下游少样本任务的性能进行了分析。使用相同超参数，对使用原始数据\n集和经过质量筛选后的数据训练得到的模型效果进行了对比，实验结果如图 3.8 所示。可以看到，\n使用高质量数据训练的模型在自然语言生成和自然语言理解任务上表现更好。特别是，高质量数\n据对自然语言生成任务的影响大于自然语言理解任务。这可能是因为自然语言生成任务通常需要\n生成高质量的语言， 过滤预训练语料库对语言模型的生成能力至关重要。 文献[116] 的研究强调了\n预训练数据的质量在下游任务的性能中也扮演着关键角色。\nGoogle Research 的研究人员针对数据构建时间、文本质量、是否包含有害信息进行了系统的\n研究[141]。 他们使用包含不同时间、 毒性水平、 文本质量和领域的数据， 训练了28 个具有 15 亿个\n参数的仅解码器（Decoder-only）结构的语言模型。研究结果表明，大语言模型训练数据的时间、\n内容过滤方法及数据源对下游模型行为具有显著影响。",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "数规模的大语言模型上进行实验的成本非常高，很多结论是在百亿甚至十亿规模的语言模型上进\n行的实验，其结果并不能完整地反映数据对大语言模型的影响。此外，一些观点仍处于猜想阶段，\n需要进一步验证。请各位读者甄别判断。\n3.3.1 数据规模\n随着大语言模型参数规模的增加， 为了有效地训练模型， 需要收集足够数量的高质量数据[34, 132]。\n在针对模型参数规模、训练数据量及总计算量与模型效果之间关系的研究 [132] 被提出之前，大部\n分大语言模型训练所采用的训练数据量相较于 LLaMA 等最新的大语言模型都少很多。 表3.1 给出\n了模型参数量与训练数据量的对比。 在Chinchilla 模型被提出之前， 大部分大语言模型都在着重提\n升模型的参数量，所使用的训练数据量都在 3000 亿个词元左右，LaMDA 模型使用的训练参数量\n仅有 1370 亿个。虽然 Chinchilla 模型的参数量不足 LaMDA 模型的一半，但是训练数据的词元数\n达到 1.4 万亿个，是 LaMDA 模型的 8 倍多。"
      ],
      "response_time": 3.2425293922424316,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.025027853286932106,
        "semantic_similarity": 0.8538264632225037
      }
    },
    {
      "sample_id": "0036",
      "question": "LLaMA 模型训练使用了哪些类型的数据来源？",
      "ground_truth": "包括网页、代码、论文、图书等。",
      "predicted_answer": "LLaMA 模型训练使用了以下几种类型的数据来源：CommonCrawl、C4、GitHub、Wikipedia、Books、arXiv 和 Stack Exchange[5]。",
      "ground_context": [
        "可以看到，LLaMA 模型训练混合了大量不同来源的数据，包括网页、代码、论文、图书等。"
      ],
      "retrieved_contexts": [
        "大规模语言模型：从理论到实践 41\n后，OpenAI 就不再开源（也没有开源模型） ，因此并不清楚ChatGPT 和 GPT-4 采用的模型架构。\n本节将以 LLaMA 模型为例，介绍大语言模型架构在 Transformer 原始结构上的改进，并介绍\nTransformer 结构中空间和时间占比最大的注意力机制的优化方法。\n2.3.1 LLaMA 的模型结构\n文献 [34] 介绍了 LLaMA 采用的 Transformer 结构和细节， 与2.1 节介绍的 Transformer 结构的\n不同之处为采用了前置层归一化 （Pre-normalization） 方法并使用RMSNorm 归一化函数 （Root Mean\nSquare Normalizing Function） ，激活函数更换为SwiGLU，使用了旋转位置嵌入（Rotary Positional\nEmbeddings，RoPE） ，使用的Transformer 结构与 GPT-2 类似，如图2.4 所示。\n层归一化\n全连接层\n多头自注意力\n+\n文本和位置嵌入表示\n层归一化\nTr ansfor mer\n层归一化\n…\n文本预测...",
        "134 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n{\n\"zero_optimization\": {\n\"stage\": 3,\n\"offload_optimizer\": {\n\"device\": \"cpu\"\n}\n\"offload_param\": {\n\"device\": \"cpu\"\n}\n},\n...\n}\n如果希望将更多的内存装载到 NVMe 中，则可以在配置文件中按照如下方式配置：\n{\n\"zero_optimization\": {\n\"stage\": 3,\n\"offload_optimizer\": {\n\"device\": \"nvme\",\n\"nvme_path\": \"/nvme_data\"\n}\n\"offload_param\": {\n\"device\": \"nvme\",\n\"nvme_path\": \"/nvme_data\"\n}\n},\n...\n}\n4.4.2 LLaMA 分布式训练实践\nLLaMA 模型是目前最流行、性能最强大的开源模型之一，基于LLaMA 构造的模型生态可以\n覆盖绝大部分模型使用场景。 在设置完必要的数据和环境配置后， 本节将逐步演示如何使用Deep-\nSpeed 框架训...",
        "46 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n4. 模型整体框架\n基于上述模型和网络结构可以实现解码器层，根据自回归方式利用训练数据进行模型训练的\n过程与 2.2.3 节介绍的过程基本一致。不同规模的 LLaMA 模型使用的超参数如表2.1 所示。由于\n大语言模型的参数量非常大， 并且需要大量的数据进行训练， 因此仅利用单个GPU 很难完成训练，\n需要依赖分布式模型训练框架（第 4 章将详细介绍相关内容） 。",
        "大规模语言模型：从理论到实践 161\n图 5.3 LLaMA 65B 模型使用 LIMA [42] 训练效果对比\n文献 [190] 研究也表明， 在模型构建过程中， 数据工程起着至关重要的作用， 可以通过提升数\n据集的多样性，显著增强模型的泛化能力。训练数据多样性的提升，可以从多个方面着手，例如\n使用来自不同源头、具备不同特征且呈现不同分布的数据。此外，实验结果也说明，在数据选择\n环节，多样性有着不可忽视的作用。对比随机选择、均匀选择这两种常见方式，具备多样性的数\n据选择策略展现出明显优势。此外，相较于单纯聚焦于挑选高质量数据，若能将数据质量与多样\n性标准有机结合，模型也可以达到更好的效果[223]。\n在问答任务方面，大语言模型的预训练依托于多样化的语料库来开展，这些语料库包含了多\n种类型的内容，并且涵盖了丰富的世界知识。大语言模型在预训练完成后，大量的知识被编码进\n了模型的参数之中。而通过监督微调的方式，就能够把这些已经编码进参数的知识有效地应用于\n问答任务里。然而，针对大语言模型的问答任务能力提升，存在着三个亟待解决的关键问题： （1）\n指令微调阶段，究竟需要多少数据量，才能使大语...",
        "大规模语言模型：从理论到实践 83\n表 3.2 LLaMA 模型训练所使用的数据集 [37]\n数据集 采样概率 训练轮数 存储空间\nCommonCrawl 67.0% 1.10 3.3 TB\nC4 15.0% 1.06 783 GB\nGitHub 4.5% 0.64 328 GB\nWikipedia 4.5% 2.45 83 GB\nBooks 4.5% 2.23 85 GB\narXiv 2.5% 1.06 92 GB\nStack Exchange 2.0% 1.03 78 GB\nGopher 模型[115] 在训练过程中进行了对数据分布的消融实验，以便验证混合来源对下游任务\n的影响。 针对MassiveText 子集设置了不同权重的数据组合， 并用于训练语言模型。 利用Wikitext103、\nLambada、C4 和 Curation Corpus 测试不同权重组合训练得到的语言模型在下游任务上的性能。 为了\n限制数据组合分布范围， 实验中固定了Wikipedia 和 GitHub 两个数据集的采样权重。 对于Wikipedia，\n要求对训练数据进行完整的学习， 因此将采样权重固定为..."
      ],
      "response_time": 1.194896936416626,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.013150919401790927,
        "semantic_similarity": 0.6206821203231812
      }
    },
    {
      "sample_id": "0037",
      "question": "Chinchilla 语言模型具有多少个参数？",
      "ground_truth": "700 亿个参数",
      "predicted_answer": "Chinchilla 语言模型具有 700 亿个参数[1]。",
      "ground_context": [
        "最终确定 Chinchilla 语言模型具有 700 亿个参数，使用了 1.4 万亿个词元进行训练。"
      ],
      "retrieved_contexts": [
        "放， 即模型大小加倍则训练词元数量也应该加倍。 为了验证该分析结果， 他们使用与Gopher 语言\n模型训练相同的计算资源，根据上述理论预测了 Chinchilla 语言模型的最优参数量与词元数量组\n合。最终确定 Chinchilla 语言模型具有 700 亿个参数，使用了 1.4 万亿个词元进行训练。通过实验\n发现，Chinchilla 在很多下游评估任务中都显著地优于 Gopher（280B） 、GPT-3（175B） 、Jurassic-1\n（178B）及 Megatron-Turing NLG（530B） 。\n图3.5 给出了在同等计算量情况下， 训练损失随参数量的变化情况。 针对9 种不同的训练参数\n量设置，使用不同词元数量的训练数据，训练不同大小的模型参数量，使得最终训练所需浮点运\n算数达到预定目标。 对于每种训练量预定目标， 图3.5(a) 所示为平滑后的训练损失与参数量之间的\n关系。可以看到，训练损失值存在明显的低谷，这意味着对于给定训练计算量目标，存在一个最\n佳模型参数量和训练数据量配置。利用这些训练损失低谷的位置，还可以预测更大的模型的最佳\n模型参数量和训练词元数量...",
        "数规模的大语言模型上进行实验的成本非常高，很多结论是在百亿甚至十亿规模的语言模型上进\n行的实验，其结果并不能完整地反映数据对大语言模型的影响。此外，一些观点仍处于猜想阶段，\n需要进一步验证。请各位读者甄别判断。\n3.3.1 数据规模\n随着大语言模型参数规模的增加， 为了有效地训练模型， 需要收集足够数量的高质量数据[34, 132]。\n在针对模型参数规模、训练数据量及总计算量与模型效果之间关系的研究 [132] 被提出之前，大部\n分大语言模型训练所采用的训练数据量相较于 LLaMA 等最新的大语言模型都少很多。 表3.1 给出\n了模型参数量与训练数据量的对比。 在Chinchilla 模型被提出之前， 大部分大语言模型都在着重提\n升模型的参数量，所使用的训练数据量都在 3000 亿个词元左右，LaMDA 模型使用的训练参数量\n仅有 1370 亿个。虽然 Chinchilla 模型的参数量不足 LaMDA 模型的一半，但是训练数据的词元数\n达到 1.4 万亿个，是 LaMDA 模型的 8 倍多。",
        "大规模语言模型：从理论到实践 77\n表 3.1 模型参数量与训练数据量的对比\n模型名称 参数量（个） 训练数据量（个词元）\nLaMDA[15] 1370 亿 1680 亿\nGPT-3[39] 1750 亿 3000 亿\nJurassic [133] 1780 亿 3000 亿\nGopher [115] 2800 亿 3000 亿\nMT-NLG 530B [134] 5300 亿 2700 亿\nChinchilla[132] 700 亿 14000 亿\nFalcon[60] 400 亿 10000 亿\nLLaMA[34] 630 亿 14000 亿\nLLaMA-2[37] 700 亿 20000 亿\nLLaMA-3[135] 4050 亿 150000 亿\nQwen2.5[136] 720 亿 180000 亿\nGLM-4[137] 1300 亿 100000 亿\nDeepMind 的研究人员在文献 [132] 中描述了他们训练 400 多个语言模型后得出的分析结果\n（模型的参数量从 7000 万个到 160 亿个，训练数据量从 5 亿个词元到 5000 亿个词元） 。研究发现，\n如果希...",
        "大规模语言模型：从理论到实践 179\n（1） 易用的类ChatGPT 模型的训练和强化推理： 只需要一个脚本就可以实现多个训练步骤， 包\n括使用 HuggingFace 预训练的模型，使用 InstructGPT 训练的所有三个步骤构建类 ChatGPT 模型。\n此外，还提供了一个易于使用的推理 API，用于用户在模型训练后进行对话式交互性测试。\n（2）DeepSpeed-RLHF 管道：DeepSpeed-RLHF 复现了 InstructGPT[24] 论文中的训练模式，包\n括指令微调、奖励模型微调和基于人类反馈的强化学习三个步骤。此外，还提供了数据抽象和混\n合功能，以支持用户使用多个不同来源的数据源进行训练。\n（3）DeepSpeed-RLHF 系统： 将DeepSpeed 的训练能力 （Training Engine） 和推理能力 （Inference\nEngine）整合到统一的混合引擎（DeepSpeed Hybrid Engine ，DeepSpeed-HE）中用于 RLHF 训练。\nDeepSpeed-HE 能够无缝地在推理和训练模式之间切换，使其能够利用来自 DeepS...",
        "78 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\nDopt ∝ C 0.51 (3.3)\n图 3.5 在同等计算量情况下，训练损失随参数量的变化情况 [132]\nLLaMA[34] 模型在训练时采用了与文献 [132] 相符的训练策略。研究发现，70 亿个参数的语\n言模型在训练超过 1 万亿个词元后，性能仍在持续增长。因此，Meta 的研究人员在 LLaMA-2[37]\n模型训练中，进一步增大了训练数据量，训练数据量达到 2 万亿个词元。LLaMA-3[135] 模型训练\n中，则是进一步将训练数据量增大到了惊人的 15 万亿个词元。Qwen2.5[136] 的 720 亿参数的开源\n版本，也使用了 18 万亿个词元进行了训练。文献 [132] 给出了不同参数量的 LLaMA 模型在训练\n期间， 随着训练数据量的增加， 模型在问答和常识推理任务上的效果演变， 如图3.6 所示。研究人\n员分别在 TriviaQA、HellaSwag、NaturalQuestions、SIQA、WinoGrande、PIQA 这 6 个数据集上进\n行了测试。可以看到，随着训练数据量的增加，模型..."
      ],
      "response_time": 0.7882099151611328,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.5,
        "rouge2": 0.0,
        "rougeL": 0.5,
        "bleu": 0.11609514815067874,
        "semantic_similarity": 0.7511911392211914
      }
    },
    {
      "sample_id": "0038",
      "question": "arXiv 上的论文是用什么编写的？",
      "ground_truth": "LaTeX",
      "predicted_answer": "arXiv 上的论文是用 LaTeX 编写的[5]。",
      "ground_context": [
        "arXiv 上的论文是用 LaTeX 编写的，其中公式、符号、表格等内容的表示非常适合语言模型学习。"
      ],
      "retrieved_contexts": [
        "大规模语言模型：从理论到实践 85\n（1）Pile-CC 是基于 CommonCrawl 的数据集，该数据集通过在 Web Archive 文件上使用 jus-\nText[143] 的方法进行提取，比直接使用 WET 文件产生更高质量的输出。\n（2）PubMed Central（PMC）是由美国国家生物技术信息中心（ NCBI）运营的 PubMed 生物\n医学在线资源库的一个子集，PubMed 是由美国国家医学图书馆运营的生物医学文章在线存储库，\n提供对近 500 万份出版物的开放全文访问。\n（3）Books 3 是一个图书数据集，来自 Shawn Presser 提供的 Bibliotik。Bibliotik 由小说和非小\n说类书籍组成，几乎是图书数据集（BookCorpus 2）数据量的十倍。\n（4）OpenWebText2 （OWT2）是一个基于 WebText[11] 和 OpenWebTextCorpus 的通用数据集。\n它包括来自多种语言的文本内容、网页文本元数据，以及多个开源数据集和开源代码库。\n（5）arXiv 是一个自 1991 年开始运营的论文预印版本发布服务平台。...",
        "speculative decoding[J]. arXiv preprint arXiv:2309.08168, 2023.\n[542] Liu X, Hu L, Bailis P , et al. Online speculative decoding[J]. arXiv preprint arXiv:2310.07177, 2023.",
        "主要集中在数学、计算机科学和物理领域。arXiv 上的论文是用 LaTeX 编写的，其中公式、符号、\n表格等内容的表示非常适合语言模型学习。\n（6）GitHub 是一个大型的开源代码库，对于语言模型完成代码生成、代码补全等任务具有非\n常重要的作用。\n（7）FreeLaw 是一个非营利项目，为法律领域的学术研究提供访问和分析工具。CourtListener\n是 FreeLaw 项目的一部分，包含美国联邦和州法院的数百万条法律意见，并提供批量下载服务。\n（8）Stack Exchange 是一个围绕用户提供问题和答案的网站集合。 Stack Exchange Data Dump\n包含了 Stack Exchange 网站集合中所有用户贡献的内容的匿名数据集。 它是截至2023 年 9 月公开\n可用的最大的问题-答案对数据集之一，包括编程、园艺、艺术等主题。\n（9）USPTO Backgrounds 是美国专利商标局授权的专利背景部分的数据集，来源于其公布的\n批量档案。由于专利通常包含任务背景介绍，给出了发明的背景和技术领域的概述，建立了问题\n空间的框架，因此该数据集包含了大量关于应用主题的...",
        "arXiv preprint arXiv:2305.05920, 2023.\n[473] Y uan Z, Shang Y , Zhou Y , et al. Llm inference unveiled: Survey and roofline model insights[J].\narXiv preprint arXiv:2402.16363, 2024.\n[474] Gu A, Dao T. Mamba: Linear-time sequence modeling with selective state spaces[J]. arXiv preprint\narXiv:2312.00752, 2023.",
        "大规模语言模型：从理论到实践 89\n图 3.15 ROOTS 数据集处理流程 [31]\n3.4.3 RefinedWeb\nRefinedWeb[60] 是由位于阿布扎比的技术创新研究院 （Technology Innovation Institute，TII） 在\n开发 Falcon 大语言模型时同步开源的大语言模型预训练集合， 其主要由CommonCrawl 数据集[156]\n过滤的高质量数据组成。CommonCrawl 数据集包含自 2008 年以来爬取的数万亿个网页， 由原始网\n页数据、 提取的元数据和文本提取结果组成， 总数据量超过1PB。CommonCrawl 数据集以 W ARC\n（Web ARChive） 格式或者WET 格式进行存储。W ARC是一种用于存档 Web 内容的国际标准格式，\n包含了原始网页内容、HTTP 响应头、URL 信息和其他元数据。WET 文件只包含抽取出的纯文本\n内容。\n文献 [60] 中给出了 RefinedWeb 中 CommonCrawl 数据集的处理流程和数据过滤百分比，如\n图3.16 所示。图中灰色部分是与前一个阶段相对应的移除率，阴影部..."
      ],
      "response_time": 0.6335453987121582,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.5,
        "rouge2": 0.0,
        "rougeL": 0.5,
        "bleu": 0.018724372764461875,
        "semantic_similarity": 0.5645149350166321
      }
    },
    {
      "sample_id": "0039",
      "question": "arXiv 是什么？",
      "ground_truth": "arXiv 是一个自 1991 年开始运营的论文预印版本发布服务平台。",
      "predicted_answer": "arXiv 是一个自 1991 年开始运营的论文预印版本发布服务平台[4]。",
      "ground_context": [
        "arXiv 是一个自 1991 年开始运营的论文预印版本发布服务平台。发布在 arXiv 上的论文主要集中在数学、计算机科学和物理领域。arXiv 上的论文是用 LaTeX 编写的，其中公式、符号、表格等内容的表示非常适合语言模型学习。"
      ],
      "retrieved_contexts": [
        "curated corpora with web data, and web data only[J]. arXiv preprint arXiv:2306.01116, 2023.\n[61] Allal L B, Li R, Kocetkov D, et al. Santacoder: don’t reach for the stars![J]. arXiv preprint\narXiv:2301.03988, 2023.\n[62] Li R, Allal L B, Zi Y , et al. Starcoder: may the source be with you![J]. arXiv preprint\narXiv:2305.06161, 2023.\n[63] Liu A, Feng B, Wang B, et al. Deepseek-v2: A strong, economical, and efficient mixture-of-experts\nlanguage model[J]. arXiv preprint arXiv:2405.04434, 2024.\n[64] M...",
        "主要集中在数学、计算机科学和物理领域。arXiv 上的论文是用 LaTeX 编写的，其中公式、符号、\n表格等内容的表示非常适合语言模型学习。\n（6）GitHub 是一个大型的开源代码库，对于语言模型完成代码生成、代码补全等任务具有非\n常重要的作用。\n（7）FreeLaw 是一个非营利项目，为法律领域的学术研究提供访问和分析工具。CourtListener\n是 FreeLaw 项目的一部分，包含美国联邦和州法院的数百万条法律意见，并提供批量下载服务。\n（8）Stack Exchange 是一个围绕用户提供问题和答案的网站集合。 Stack Exchange Data Dump\n包含了 Stack Exchange 网站集合中所有用户贡献的内容的匿名数据集。 它是截至2023 年 9 月公开\n可用的最大的问题-答案对数据集之一，包括编程、园艺、艺术等主题。\n（9）USPTO Backgrounds 是美国专利商标局授权的专利背景部分的数据集，来源于其公布的\n批量档案。由于专利通常包含任务背景介绍，给出了发明的背景和技术领域的概述，建立了问题\n空间的框架，因此该数据集包含了大量关于应用主题的...",
        "speculative decoding[J]. arXiv preprint arXiv:2309.08168, 2023.\n[542] Liu X, Hu L, Bailis P , et al. Online speculative decoding[J]. arXiv preprint arXiv:2310.07177, 2023.",
        "大规模语言模型：从理论到实践 85\n（1）Pile-CC 是基于 CommonCrawl 的数据集，该数据集通过在 Web Archive 文件上使用 jus-\nText[143] 的方法进行提取，比直接使用 WET 文件产生更高质量的输出。\n（2）PubMed Central（PMC）是由美国国家生物技术信息中心（ NCBI）运营的 PubMed 生物\n医学在线资源库的一个子集，PubMed 是由美国国家医学图书馆运营的生物医学文章在线存储库，\n提供对近 500 万份出版物的开放全文访问。\n（3）Books 3 是一个图书数据集，来自 Shawn Presser 提供的 Bibliotik。Bibliotik 由小说和非小\n说类书籍组成，几乎是图书数据集（BookCorpus 2）数据量的十倍。\n（4）OpenWebText2 （OWT2）是一个基于 WebText[11] 和 OpenWebTextCorpus 的通用数据集。\n它包括来自多种语言的文本内容、网页文本元数据，以及多个开源数据集和开源代码库。\n（5）arXiv 是一个自 1991 年开始运营的论文预印版本发布服务平台。...",
        "arXiv preprint arXiv:2305.05920, 2023.\n[473] Y uan Z, Shang Y , Zhou Y , et al. Llm inference unveiled: Survey and roofline model insights[J].\narXiv preprint arXiv:2402.16363, 2024.\n[474] Gu A, Dao T. Mamba: Linear-time sequence modeling with selective state spaces[J]. arXiv preprint\narXiv:2312.00752, 2023."
      ],
      "response_time": 0.8556344509124756,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.8,
        "rouge2": 0.6666666666666666,
        "rougeL": 0.8,
        "bleu": 0.769429739906649,
        "semantic_similarity": 0.964884340763092
      }
    },
    {
      "sample_id": "0040",
      "question": "CulturaX 研究团队使用了哪两种语言识别工具？",
      "ground_truth": "cld3 和 FastText",
      "predicted_answer": "CulturaX 研究团队在处理 mC4 和 OSCAR 数据集时，最初分别使用了 cld3 和 FastText 这两种不同的语言识别工具。然而，由于 cld3 在语言检测方面的表现不佳，CulturaX 团队最终改用了 FastText 对 mC4 中的文档语言进行了重新预测。[1][5]",
      "ground_context": [
        "在处理mC4 和 OSCAR 数据集时， 一个较为突出的问题是二者分别使用了cld3 和 FastText 这两种不同的语言识别工具。"
      ],
      "retrieved_contexts": [
        "度上说明高质量数据集对语言模型具有重要的影响。\n3.4.4 CulturaX\nCulturaX[159] 是一个可以用于预训练的多语言数据集， 涵盖167 种语言， 包含6.3 万亿个词元。\n它通过整合 mC4[160]（3.1.0 版本）和 OSCAR[161–163]（20.19、21.09、22.01 以及 23.01 版本）数据\n集，并经过语言识别、URL 过滤、基于度量的清洗、文档精炼以及数据去重等一系列严格的数据\n处理步骤，有效解决了现有多语言数据集存在的语言识别不准确、文档级去重缺失、数据清理不\n彻底等问题。该数据集具有多语言、开源、大规模和高质量的特点，旨在提升多语言场景下模型\n训练的数据质量，推动多语言学习的研究与发展，为训练高性能的多语言大语言模型提供了有力\n的数据支持，有助于打破训练数据不透明的现状。\nmC4 最初是为训练多语言编码器-解码器模型 mT5[160] 而创建， 涵盖101 种语言， 从Common-\nCrawl 的 71 个月度快照中获取数据，经过去除短行页面、不良词汇页面及重复行去除等处理，其\n语言识别借助 cld3[164] 工具。OSCAR 数...",
        "据集中删除。使用一系列全面的指标，包括单词数量、字符和单词重复比率等。同时高困惑度分\n数的文档也会被视为噪声排除。由于重复信息会对训练大语言模型产生不利影响， CulturaX 研究\n团队利用不同语言的停用词和标记词列表计算比率以删除文档，还通过 FastText 获取语言识别置\n信度辅助过滤。\n(4) 文档优化：由于 mC4 和 OSCAR 的文档是从互联网上抓取的 HTML 页面中提取的，其中\n很大一部分可能带有抓取和提取错误， 包括长JavaScript 行和无关内容。 因此， 对于每个文档， 文\n档优化步骤的目标是通过一系列操作去除其噪声或不相关的部分。首先，去除每个文档末尾的短\n行，因为这些行通常包含页脚细节或来自网站的无用信息。其次，删除包含JavaScript（JS）关键\n词列表中的单词（例如“<script>” ）的行，以避免不相关和非语言信息。\n(5) 冗余去除：尽管进行了全面的数据清洗， 但由于信息在网络上重新发布、 对同一文章的多\n次引用、样板内容和抄袭等各种原因，剩余数据集仍可能包含大量重复数据，这会导致大语言模\n型记忆和泛化能力受到影响，因此数据去重对保证训练...",
        "档将从数据集中剔除。这样做的目的在于避免那些会使 cld3 和 FastText 语言检测器产生混淆的文\n档，因为这些文档极有可能给数据带来噪声干扰。\n(2) 基于 URL 的过滤：为了降低数据中的有害信息， CulturaX 研究团队使用了图卢兹大学\n（University of Toulouse） 提供的最新UT1 URL 和域名黑名单， 将有毒和有害页面从数据中删除。 该\n列表包含来自色情、抱怨和黑客攻击等不同主题的网站，名单每周更新两到三次。目前该黑名单\n包含超过 370 万条由人类和机器 （如搜索引擎、 已知地址和索引） 共同贡献的记录[163]。mC4 数据\n集之前未使用过该黑名单进行过滤。OSCAR 数据集虽然使用过该黑名单进行数据清洗， 但是可以\n根据更新的名单进一步进行清洗。\n(3) 基于指标的清洗：受 ROOTS 语料库数据处理启发，CulturaX 数据集构建中也利用各种数\n据集指标的分布来识别和过滤异常文档。 每个指标为数据集中的文档提供量化特定属性的单一值，\n根据指标值及其范围确定阈值，将其分为正常和异常范围，异常范围的文档被视为噪声，并从数\n据集中删除。使用...",
        "Deduplication） 的目标是删除连续相同的序列字符串。 使用后缀数组进行逐个词元间的对比， 并删\n除 50 个以上的连续相同词元序列。URL 冗余去除（URL Deduplication）的目标是删除具有相同\nURL 的文档。CommonCrawl 数据集中存在一定量的具有重复 URL 的文档，并且这些文档的内容\n通常是完全相同的。构建 RefinedWeb 数据集时，对 CommonCrawl 数据集中不同部分之间相同的\nURL 进行了去除。该阶段处理完成后的数据集称为 RefinedWeb，仅保留了原始数据的 11.67%。\n以上三个阶段所包含的各个任务的详细处理规则可以参考文献[60] 的附录部分。 此外， 文献[60]\n还利用三个阶段产生的数据分别训练 10 亿和 30 亿参数规模的模型，并使用零样本泛化能力对模\n型结果进行评测。 评测后发现，RefinedWeb 的效果远好于 RW-Raw 和 RW-Filtered。 这也在一定程\n度上说明高质量数据集对语言模型具有重要的影响。\n3.4.4 CulturaX\nCulturaX[159] 是一个可以用于预训练的多语言...",
        "92 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n练。二者组合后，为后续处理提供了多达 135 亿份文档。其中，mC4 占比 66%，OSCAR 23.01 占\n比 11%，OSCAR 22.01 占比 7%，OSCAR 21.09 占比 9%，OSCAR 20.19 占比 7%。\n基于 mC4 和 OSCAR 合并后的数据集，CulturaX 研究团队通过一系列数据处理步骤来构造高\n质量的多语言数据集， 包括语言识别、 基于URL 的过滤、 基于指标的清洗、 文档优化、 冗余去除。\n具体清洗工作如下：\n(1) 语言识别： 在处理mC4 和 OSCAR 数据集时， 一个较为突出的问题是二者分别使用了cld3\n和 FastText 这两种不同的语言识别工具。此前的研究已经证实，cld3 在语言检测方面的表现远逊\n于 FastText， 这使得mC4 中出现了大量的语言检测错误[154]。 因此，CulturaX 团队使用 FastText 对\nmC4 中的文档语言重新进行预测。 若文档的预测语言与mC4 中原本提供的语言不一致， 那么该文\n档将从数据集中剔除。这样做的目的在..."
      ],
      "response_time": 2.203073501586914,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.3076923076923077,
        "rouge2": 0.18181818181818182,
        "rougeL": 0.3076923076923077,
        "bleu": 0.021065951393346185,
        "semantic_similarity": 0.6618748903274536
      }
    },
    {
      "sample_id": "0041",
      "question": "ROOTS数据集包含多少种语言？",
      "ground_truth": "59种语言",
      "predicted_answer": "ROOTS 数据集包含 46 种自然语言和 13 种编程语言，总计 59 种语言[3][5]。",
      "ground_context": [
        "ROOTS（Responsible Open-science Open-collaboration Text Sources ）数据集是 BigScience 项目在训练具有 1760 亿个参数的 BLOOM 大语言模型时使用的数据集。该数据集包含 46 种自然语言和 13 种编程语言，总计 59 种语言，整个数据集的大小约 1.6TB。"
      ],
      "retrieved_contexts": [
        "大规模语言模型：从理论到实践 87\n位的编程语言分布的华夫饼图（Waffle Plot） ，一个正方形大约对应3 万个文件。\n图 3.13 ROOTS 数据集中各语言所占比例 [128]\nROOTS 中的数据主要来自四个方面：公开数据、虚拟抓取、 GitHub 代码和网页数据。在公\n开数据方面，BigScience Data Sourcing 工作组的目标是收集尽可能多的各种类型的数据，包括自\n然语言处理数据集和各类型文档数据集。为此，还设计了 BigScience Catalogue[153] 用于管理和分\n享大型科学数据集，Masader Repository 用于收集阿拉伯语和文化资源的开放数据存储库。 在收集\n原始数据集的基础上，进一步从语言和统一表示方面对收集的文档进行规范化处理。识别数据集\n所属语言并分类存储，将所有数据都按照统一的文本和元数据结构进行表示。由于数据种类繁多，\nROOTS 数据集并没有公开其所包含数据集的情况，但是提供了 Corpus Map 及 Corpus Description\n工具， 以便查询各类数据集占比和数据情况。 在ROOTS 数据集中， 中文...",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "该数据集的加入可以帮助语言模型建模电子邮件通信的特性。\nPile 中不同数据子集所占比例及训练时的采样权重有很大不同， 高质量的数据会有更高的采样权\n重。 例如，Pile-CC 数据集包含227.12GB 数据， 整个训练周期中采样1 轮。 虽然Wikipedia (English)\n数据集仅有6.38GB 的数据， 但是整个训练周期中采样3 轮。 具体的采样权重和采样轮数可以参考文\n献 [87]。\n3.4.2 ROOTS\nROOTS（Responsible Open-science Open-collaboration Text Sources ）数据集[128] 是 BigScience\n项目在训练具有 1760 亿个参数的 BLOOM 大语言模型时使用的数据集。该数据集包含 46 种自然\n语言和 13 种编程语言，总计 59 种语言，整个数据集的大小约 1.6TB。ROOTS 数据集中各语言所\n占比例如图3.13 所示。图中左侧是以语言家族的字节为单位表示的自然语言占比树状图，其中欧\n亚大陆语言占据了绝大部分（1321.89GB） 。右侧橙色矩形对应的是印度尼西亚语（18GB） ...",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "如图3.14 所示。其中，中文数据主要由WuDao Corpora 和 OSCAR[154] 组成。在虚拟抓取方面，由\n于很多语言的现有公开数据集较少， 因此这些语言的网页信息是十分重要的资源补充。 在ROOTS\n数据集中， 采用CommonCrawl 网页镜像， 选取了614 个域名， 从这些域名下的网页中提取文本内\n容补充到数据集中， 以提升语言的多样性。 在GitHub 代码方面， 针对程序语言，ROOTS 数据集采\n用了与 AlphaCode[101] 相同的方法：从 BigQuery 公开数据集中选取文件长度在 100 到 20 万字符，\n字母符号占比在 15% 至 65%，最大行数在 20 至 1000 行的代码。训练大语言模型时，网页数据对\n于数据的多样性和数据量支撑起到重要的作用[2, 19]，ROOTS 数据集中包含了 OSCAR 21.09 版本，\n对应的是 CommonCrawl 2021 年 2 月的快照，占整体 ROOTS 数据集规模的 38%。"
      ],
      "response_time": 1.0852961540222168,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.2857142857142857,
        "rouge2": 0.0,
        "rougeL": 0.2857142857142857,
        "bleu": 0.05454618257151321,
        "semantic_similarity": 0.6546259522438049
      }
    },
    {
      "sample_id": "0042",
      "question": "在训练大语言模型时，为什么多语言场景下的网络爬虫数据集比精选数据集更具优势？",
      "ground_truth": "多语言场景下，网络爬虫数据集可以高效收集多种语言的数据，尽管其原始数据质量参差不齐，但经过清洗后可以很好应用于大语言模型训练。而精选数据集通常有限，可能无法覆盖多种语言的需求。",
      "predicted_answer": "在网络爬虫数据集中，虽然原始数据的质量可能参差不齐，但在经过清洗处理后，它可以有效支持大语言模型的训练[1]。这使得网络爬虫数据集能够在多语言场景下提供更为广泛的数据覆盖，从而帮助高效收集多语言数据，这是精选数据集（如The Pile和BookCorpus）所不具备的优势[1]。",
      "ground_context": [
        "在多语言场景下，网络爬虫数据集更具优势，它有助于高效收集多语言数据。",
        "在训练大语言模型之前，构建一个准备充分的预训练语料库尤为重要。"
      ],
      "retrieved_contexts": [
        "管道，对 166 种不同语言的网页数据进行分类和过滤。区别于以往依赖精选数据集（如 The Pile\n和 BookCorpus）训练大语言模型的做法。在多语言场景下，网络爬虫数据集更具优势，它有助于\n高效收集多语言数据。尽管其原始数据质量参差不齐，但经清洗后可以很好应用于大语言模型训",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "据集，会对大语言模型在问答任务上的表现产生怎样的影响？ （3）不同的大语言模型在指令微调\n阶段，对于数据的需求方面存在着怎样的差异呢？\n针对上述问题， 文献[224] 给出了详细的分析。 研究人员使用了ENTITYQUESTIONS[225]， 这\n是一个包含维基百科上 24 个不同话题知识的问答数据集。选择了其中 12 个与地点相关的原始训\n练集作为训练数据， 将它们对应的测试集作为测试集， 并将剩余12 个话题的测试集作为领域外测\n试集。通过设计的多模板补全机制，能够可靠地评估大语言模型对不同知识的记忆程度。利用该\n机制，根据其知识记忆水平将训练和测试集均进行了 5 个级别的划分。\n文献 [224] 中将训练数据划分为六个不同的数据量级别，从 60 个样本到完整数据集不等，并\n通过从 12 个话题中均匀抽样来构建训练集。实验结果表明，仅需 60 个训练样本的指令微调，就\n足以使大语言模型高效执行问答任务，并展现出强大的泛化能力。如图5.4所示。无论基础模型或\n记忆水平如何，大语言模型在使用较少训练样本时的表现优于使用 960 个或全部样本。增加训练",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "大规模语言模型：从理论到实践 161\n图 5.3 LLaMA 65B 模型使用 LIMA [42] 训练效果对比\n文献 [190] 研究也表明， 在模型构建过程中， 数据工程起着至关重要的作用， 可以通过提升数\n据集的多样性，显著增强模型的泛化能力。训练数据多样性的提升，可以从多个方面着手，例如\n使用来自不同源头、具备不同特征且呈现不同分布的数据。此外，实验结果也说明，在数据选择\n环节，多样性有着不可忽视的作用。对比随机选择、均匀选择这两种常见方式，具备多样性的数\n据选择策略展现出明显优势。此外，相较于单纯聚焦于挑选高质量数据，若能将数据质量与多样\n性标准有机结合，模型也可以达到更好的效果[223]。\n在问答任务方面，大语言模型的预训练依托于多样化的语料库来开展，这些语料库包含了多\n种类型的内容，并且涵盖了丰富的世界知识。大语言模型在预训练完成后，大量的知识被编码进\n了模型的参数之中。而通过监督微调的方式，就能够把这些已经编码进参数的知识有效地应用于\n问答任务里。然而，针对大语言模型的问答任务能力提升，存在着三个亟待解决的关键问题： （1）\n指令微调阶段，究竟需要多少数据量，才能使大语..."
      ],
      "response_time": 2.170466661453247,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.12072911810613608,
        "semantic_similarity": 0.9038066267967224
      }
    },
    {
      "sample_id": "0043",
      "question": "在多语言数据集的构建过程中，如何确保数据的质量和去重，特别是在处理来自网页的数据时？",
      "ground_truth": "CulturaX研究团队使用一系列全面的指标，包括单词数量、字符和单词重复比率等来删除噪声文档，同时利用不同语言的停用词和标记词列表计算比率以删除文档。此外，通过FastText获取语言识别信度辅助过滤。对于网页数据，尽管原始数据质量参差不齐，但经过清洗后，这些数据可以很好地应用于大语言模型训练。冗余去除是保证训练数据质量的关键，CulturaX研究团队利用MinHash和URL对数据集进行全面去重，并按语言独立进行。",
      "predicted_answer": "在多语言数据集构建过程中，特别是处理来自网页的数据时，确保数据质量和去重的方法包括：\n\n1. 定义质量指标：将高质量文本定义为“由人类撰写，面向人类”，并使用字母重复度、单词重复度、特殊字符、困惑度等指标进行衡量。这些指标根据不同语言进行调整，并通过人工浏览每个数据来源来确定最有效的识别方法[1]。\n\n2. 清理非自然语言文本：对于预处理错误、SEO页面或垃圾邮件（包括色情垃圾邮件），需要进行一定的处理。例如，采用启发式质量过滤算法通过整体长度、符号与单词比率及其他标准剔除离群值，以确保文档是实际的自然语言[5]。\n\n3. 使用SimHash算法计算文档的哈希值，以检测和去除冗余信息[1]。\n\n4. 对于来自网页的数据，还可以使用MinHash算法快速估算两个文档之间的相似度，从而有效过滤掉重叠度高的文档[5]。\n\n综上所述，在构建多语言数据集的过程中，通过综合运用多种技术和方法可以有效地保证数据的质量和去重。",
      "ground_context": [
        "CulturaX研究团队利用不同语言的停用词和标记词列表计算比率以删除文档，还通过FastText获取语言识别信度辅助过滤。",
        "在多语言场景下，网络爬虫数据集更具优势，它有助于高效收集多语言数据。"
      ],
      "retrieved_contexts": [
        "88 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n图 3.14 在 ROOTS 数据集中，中文数据集的种类及所占比例\n在数据准备完成后， 还要进行清洗、 过滤、 去重及隐私信息删除等工作，ROOTS 数据集处理\n流程如图3.15 所示。 整个处理工作并非完全依赖自动计算， 而是采用人工与自动相结合的方法。 针\n对数据中存在的一些非自然语言的文本，例如预处理错误、SEO 页面或垃圾邮件（包括色情垃圾\n邮件） ，构建ROOTS 数据集时会进行一定的处理。首先，定义一套质量指标，其中高质量的文本\n被定义为 “由人类撰写， 面向人类” （written by humans for humans） ， 不区分内容 （专业人员根据来\n源对内容进行选择）或语法正确性的先验判断。所使用的指标包括字母重复度、单词重复度、特\n殊字符、困惑度等。完整的指标列表可以参考文献[128]。这些指标根据来源的不同，进行了两种\n主要的调整：针对每种语言单独选择参数，如阈值等；人工浏览每个数据来源，以确定哪些指标\n最可能识别出非自然语言。其次，针对冗余信息，采用 SimHash 算法[155]，计算文档的...",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "OpenCLIP[305] 则需数百卡 GPU 集群运行数天至数周。 文献[304] 提出通过构建高效的数据处理管\n道，可实现性能提升，同时避免成本大幅上升。\n如图 7.12 所示，数据是训练多模态大语言模型的核心要素之一。构建一个多样且平衡的数据\n集对于模型学习覆盖足够多概念的良好世界模型至关重要。清除大型数据集中常见的重复数据同\n样重要，这不仅能够节省大量计算资源，还能降低模型过度记忆的风险。与此同时，数据剪枝也\n是数据处理的重要环节，确保文本描述与图像内容高度相关，有助于模型更好地理解和对齐多模\n态信息。可以通过改进模型对视觉语义关联（Grounding）能力来增强对图文关系的理解，并通过\n引入人类偏好优化对齐效果。 在OCR 任务中， 使用专门的增强技术可以进一步提升文本读取和翻\n译能力。通过结合高效数据处理、合理的模型架构选择和针对性优化策略，可以显著提升多模态\n大语言模型的训练效果和应用能力。",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "大规模语言模型：从理论到实践 91\n的垃圾信息，这些页面主要由关键词列表、样板文本或特殊字符序列组成。采用文献 [115] 中提\n出的启发式质量过滤算法，通过整体长度、符号与单词比率及其他标准剔除离群值，以确保文档\n是实际的自然语言。逐行纠正（Line-wise Correction） 的目标是过滤文档中不适合语言模型训练的\n行（例如社交媒体计数器、 导航按钮等） 。使用基于规则的方法进行逐行纠正过滤， 如果删除超过\n5%，则完全删除该文档。经过过滤阶段，仅有 23.34% 的原始数据得以保留，所得的数据集称为\nRW-Filtered。\n冗余去除阶段包含模糊冗余去除、严格冗余去除及 URL 冗余去除三个任务。 模糊冗余去除\n（Fuzzy Deduplication） 的目标是删除内容相似的文档。RefinedWeb 构建时使用了 MinHash 算法[158]，\n能快速估算两个文档间的相似度。利用该算法可以有效过滤重叠度高的文档。RefinedWeb 数据集\n构建时，使用的是 5-gram 并分成 20 个桶，每个桶采用 450 个 Hash 函数。严格冗余去除（Exact\nDedup..."
      ],
      "response_time": 5.771878719329834,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.12500000000000003,
        "rouge2": 0.0,
        "rougeL": 0.12500000000000003,
        "bleu": 0.01240297678262629,
        "semantic_similarity": 0.747489333152771
      }
    },
    {
      "sample_id": "0044",
      "question": "如何通过加强数学问题数据集的方式提升大语言模型在数学任务上的能力？",
      "ground_truth": "通过在Pile数据集中专门增加数学问题数据集，期望增强通过Pile数据集训练的语言模型的数学能力。",
      "predicted_answer": "根据提供的文档，我无法回答这个问题。文档中主要讨论了数据多样性和数量对大语言模型整体性能的影响，但并未具体提到如何通过加强数学问题数据集的方式来提升大语言模型在数学任务上的能力。[1][2][3][5]",
      "ground_context": [
        "大语言模型在数学任务上的表现较差， 这可能是由于训练集中缺乏数学问题。因此，Pile 数据集中专门增加了数学问题数据集，期望增强通过 Pile 数据集训练的语言模型的数学能力。",
        "pair_freqs = compute_pair_freqs(splits)"
      ],
      "retrieved_contexts": [
        "大规模语言模型：从理论到实践 161\n图 5.3 LLaMA 65B 模型使用 LIMA [42] 训练效果对比\n文献 [190] 研究也表明， 在模型构建过程中， 数据工程起着至关重要的作用， 可以通过提升数\n据集的多样性，显著增强模型的泛化能力。训练数据多样性的提升，可以从多个方面着手，例如\n使用来自不同源头、具备不同特征且呈现不同分布的数据。此外，实验结果也说明，在数据选择\n环节，多样性有着不可忽视的作用。对比随机选择、均匀选择这两种常见方式，具备多样性的数\n据选择策略展现出明显优势。此外，相较于单纯聚焦于挑选高质量数据，若能将数据质量与多样\n性标准有机结合，模型也可以达到更好的效果[223]。\n在问答任务方面，大语言模型的预训练依托于多样化的语料库来开展，这些语料库包含了多\n种类型的内容，并且涵盖了丰富的世界知识。大语言模型在预训练完成后，大量的知识被编码进\n了模型的参数之中。而通过监督微调的方式，就能够把这些已经编码进参数的知识有效地应用于\n问答任务里。然而，针对大语言模型的问答任务能力提升，存在着三个亟待解决的关键问题： （1）\n指令微调阶段，究竟需要多少数据量，才能使大语...",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "据集，会对大语言模型在问答任务上的表现产生怎样的影响？ （3）不同的大语言模型在指令微调\n阶段，对于数据的需求方面存在着怎样的差异呢？\n针对上述问题， 文献[224] 给出了详细的分析。 研究人员使用了ENTITYQUESTIONS[225]， 这\n是一个包含维基百科上 24 个不同话题知识的问答数据集。选择了其中 12 个与地点相关的原始训\n练集作为训练数据， 将它们对应的测试集作为测试集， 并将剩余12 个话题的测试集作为领域外测\n试集。通过设计的多模板补全机制，能够可靠地评估大语言模型对不同知识的记忆程度。利用该\n机制，根据其知识记忆水平将训练和测试集均进行了 5 个级别的划分。\n文献 [224] 中将训练数据划分为六个不同的数据量级别，从 60 个样本到完整数据集不等，并\n通过从 12 个话题中均匀抽样来构建训练集。实验结果表明，仅需 60 个训练样本的指令微调，就\n足以使大语言模型高效执行问答任务，并展现出强大的泛化能力。如图5.4所示。无论基础模型或\n记忆水平如何，大语言模型在使用较少训练样本时的表现优于使用 960 个或全部样本。增加训练",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "210 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n解决数据受限的问题。大语言模型可通过强化学习中的奖励机制，学习如何探索不同的推理路径，\n从而扩大训练数据的范围。但此前的相关研究成果未达到理想的竞争效果，Kimi k1.5 旨在探索一\n条更有效的技术路线。\n1. 技术路线\n强化学习提示数据集构建： RL 提示数据集的质量和多样性对强化学习的有效性至关重要。\nKimi k1.5 定义了高质量 RL 提示集的三个关键属性：\n多样覆盖：提示应涵盖 STEM、编程和一般推理等广泛学科，以增强模型的适应性和跨领域\n应用能力。为此，采用自动过滤器选择需要丰富推理且易于评估的问题，数据集来源广泛，包括\n不同领域的问题以及纯文本和图像 - 文本问答数据。\n平衡难度：提示集应包含不同难度级别的问题，以促进模型的逐步学习并防止过度拟合。利\n用模型自身能力自适应评估提示难度， 通过SFT 模型多次生成答案计算通过率作为难度代理指标，\n并开发标签系统按领域和学科分类提示，实现难度平衡。\n准确评估能力：提示应能被验证者客观可靠地评估，确保基于正确推理衡量模型性能。为避\n免奖励操纵，排除易出现验证..."
      ],
      "response_time": 1.6652085781097412,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0.0,
        "bleu": 0.0560031303210914,
        "semantic_similarity": 0.7159660458564758
      }
    },
    {
      "sample_id": "0045",
      "question": "混合并行将哪些并行策略混合使用？",
      "ground_truth": "混合并行将多种并行策略如数据并行、流水线并行和张量并行等混合使用。",
      "predicted_answer": "混合并行将数据并行、流水线并行和张量并行等多种并行策略混合使用[1][3]。具体来说，针对千亿规模的大语言模型，通常在每个服务器内部使用张量并行策略，通过流水线并行将模型的不同层划分给不同的机器计算，并在外层叠加数据并行策略以增加并发数量[1]。",
      "ground_context": [
        "混合并行将多种并行策略如数据并行、流水线并行和张量并行等混合使用。"
      ],
      "retrieved_contexts": [
        "4.2.3 混合并行\n混合并行将多种并行策略如数据并行、流水线并行和张量并行等混合使用。通过结合不同的\n并行策略，混合并行可以充分发挥各种并行策略的优点，最大限度地提高计算性能和效率。针对\n千亿规模的大语言模型，通常，在每个服务器内部使用张量并行策略，由于该策略涉及的网络通\n信量较大，因此需要利用服务器内部的不同计算设备之间的高速通信带宽。通过流水线并行，将\n模型的不同层划分为多个阶段，每个阶段由不同的机器负责计算。这样可以充分利用多台机器的\n计算能力， 并通过机器之间的高速通信传递计算结果和中间数据， 以提高整体的计算速度和效率。\n最后，在外层叠加数据并行策略，以增加并发数量，加快整体训练速度。通过数据并行，将训练\n数据分发到多组服务器上进行并行处理，每组服务器处理不同的数据批次。这样可以充分利用多\n台服务器的计算资源，并增加训练的并发度，从而加快整体训练速度。\nBLOOM 使用 Megatron-DeepSpeed[134] 框架进行训练，主要包含两个部分： Megatron-LM 提",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "def to_dist_tensor(t): return distribute_tensor(t, mesh, rowwise_placement)\nmod._apply(to_dist_tensor)\nsharded_module = distribute_module(MyModule(), mesh, partition_fn =shard_params)\ndef shard_fc(mod_name, mod, mesh):\nrowwise_placement = [Shard(0),Replicate()]\nif mod_name == \"fc1\":\nmod.weight = torch.nn.Parameter(distribute_tensor(mod.weight, mesh, rowwise_placement))\n# ᄝ൐Ⴈൈაభ૫ shard_paramsॖၛ࿊ᄴః၂\nsharded_module = distribute_module(MyModule(), mesh, partition_fn =shard_fc)\n4.2.3 混合并行\n混合并行将多种并行策略...",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "同时在计算设备 1 中并行计算 F12。 相比于最原始的流水线并行方法，GPipe 流水线方法可以有效\n减少并行气泡。\n图 4.7 GPipe 策略流水线并行样例 [170]\n虽然 GPipe 策略可以减少一定的并行气泡，但是只有当一个小批次中所有的前向计算都完成\n时，才能执行后向计算。因此，还是会产生很多并行气泡，从而降低系统的并行效率。Megatron-\nLM[171] 采用了 1F1B 流水线并行策略， 即一个前向通道和一个后向通道。1F1B 流水线并行策略引\n入了任务调度机制，使得下游设备能够在等待上游计算的同时执行其他可并行的任务，从而提高\n设备的利用率。1F1B 给出了非交错式和交错式两种调度模式，如图4.8 所示。\n1F1B 非交错式调度模式可分为三个阶段。 首先是热身阶段， 在计算设备中进行不同数量的前\n向计算。接下来的阶段是前向-后向阶段，计算设备按顺序执行一次前向计算，然后进行一次后向\n计算。最后一个阶段是后向阶段，计算设备完成最后一次后向计算。相比于GPipe 策略，1F1B 非\n交错式调度模式在节省内存方面表现得更好。然而，它需要与 GPipe 策略一样的时间来..."
      ],
      "response_time": 2.152332067489624,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.14625679357970386,
        "semantic_similarity": 0.8774125576019287
      }
    },
    {
      "sample_id": "0046",
      "question": "大语言模型是基于什么结构的？",
      "ground_truth": "Transformer 结构",
      "predicted_answer": "当前，绝大多数大语言模型都采用类似GPT的架构，使用基于Transformer结构构建的仅由解码器组成的网络结构[3]。",
      "ground_context": [
        "大语言模型都是以Transformer 结构为基础"
      ],
      "retrieved_contexts": [
        "1. 绪论\n大语言模型是一种由包含数百亿个及以上参数的深度神经网络构建的语言模型，通常使用自\n监督学习方法通过大量无标注文本进行训练。2018 年以来，Google、OpenAI、Meta、百度、华为\n等公司和研究机构相继发布了 BERT[1]、GPT[2] 等多种模型， 这些模型在几乎所有自然语言处理任\n务中都表现出色。2019 年，大语言模型呈现爆发式的增长，特别是 2022 年 11 月 ChatGPT（Chat\nGenerative Pre-trained Transformer）的发布，引起了全世界的广泛关注。用户可以使用自然语言与\n系统交互，实现问答、分类、摘要、翻译、聊天等从理解到生成的各种任务。大语言模型展现出\n了强大的对世界知识的掌握和对语言的理解能力。\n本章主要介绍大语言模型的基本概念、发展历程和构建流程。\n1.1 大语言模型的基本概念\n使用语言是人类与其他动物最重要的区别之一， 而人类的多种智能也与此密切相关， 逻辑思维\n以语言的形式表达， 大量的知识也以文字的形式记录和传播。 如今， 互联网上已经拥有数万亿个网页\n的资源， 其中大部分信息都是用自然语言描述的。 ...",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "==================================================\n2.3 大语言模型的结构\n当前， 绝大多数大语言模型都采用类似GPT 的架构， 使用基于Transformer 结构构建的仅由解\n码器组成的网络结构， 采用自回归的方式构建语言模型， 但是在位置编码、 层归一化位置、 激活函数\n等细节上各有不同。 文献[13] 介绍了 GPT-3 模型的训练过程， 包括模型架构、 训练数据组成、 训练过\n程及评估方法。 由于GPT-3 并没有开放源代码， 根据论文直接重现整个训练过程并不容易， 因此文\n献[29] 介绍了根据 GPT-3 的描述复现的过程， 构造并开源了系统OPT（Open Pre-trained Transformer\nLanguage Models） 。MetaAI 也仿照 GPT-3 的架构开源了 LLaMA 模型[34]， 公开评测结果及利用该模\n型 进 行 有 监 督 微 调 后 的 模 型 都 有 非 常 好 的 表 现。 GPT-3 模 型 之",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "64 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n型的任务解决能力。图3.1 给出了一些典型的大语言模型所使用数据类型的分布情况。可以看到，\n不同的大语言模型在训练数据类型分布上的差距很大， 截至2025 年 2 月， 业界关于预训练数据的\n配比还没达成广泛的共识。\n图 3.1 典型的大语言模型所使用数据类型的分布情况 [18]\n3.1.1 通用数据\n通用数据在大语言模型训练数据中占比非常高，主要包括网页、对话文本、书籍、代码、百\n科等不同类型的数据，为大语言模型提供了大规模且多样的训练数据。\n网页（Webpage）是通用数据中数量最多的一类。随着互联网的大规模普及，人们通过网站、\n论坛、 博客、App 创造了海量的数据。 根据2016 年 Google 公开的数据， 其搜索引擎索引处理了超\n过 130 万亿个网页数据。网页数据所包含的海量内容，使语言模型能够获得多样化的语言知识并\n增强其泛化能力[11, 19]。爬取和处理海量网页内容并不是一件容易的事情，因此一些研究人员构建\n了 ClueWeb09[89]、ClueWeb12[90]、SogouT-16[91]、Com..."
      ],
      "response_time": 1.0263571739196777,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.5,
        "rouge2": 0.0,
        "rougeL": 0.5,
        "bleu": 0.0211187082675828,
        "semantic_similarity": 0.6083805561065674
      }
    },
    {
      "sample_id": "0047",
      "question": "1F1B 交错式调度模式要求微批次的数量是流水线阶段的什么倍数？",
      "ground_truth": "整数倍",
      "predicted_answer": "1F1B 交错式调度模式要求微批次的数量是流水线阶段的整数倍[1]。",
      "ground_context": [
        "1F1B 交错式调度模式要求微批次的数量是流水线阶段的整数倍。"
      ],
      "retrieved_contexts": [
        "大规模语言模型：从理论到实践 107\n1F1B 交错式调度模式要求微批次的数量是流水线阶段的整数倍。每个设备不仅负责连续多\n个层的计算， 还可以处理多个层的子集， 这些子集被称为模型块。 具体而言， 在之前的模式中， 设\n备 1 可能负责层 1∼4， 设备2 负责层 5∼8， 依此类推。 在新的模式下， 设备1 可以处理层 1、2、9、\n10，设备 2 处理层 3、4、11、12，依此类推。在这种模式下，每个设备在流水线中被分配到多个\n阶段。例如，设备 1 可能参与热身阶段、前向计算阶段和后向计算阶段的某些子集任务。每个设\n备可以并行执行不同阶段的计算任务，从而更好地利用流水线并行的优势。这种模式不仅在内存\n消耗方面表现出色，还能提高计算效率，使大型模型的并行系统能够更高效地完成计算任务。\n图 4.8 1F1B 流水线并行策略样例 [171]\nPyTorch 中也包含了实现流水线的 API 函数 Pipe，具体实现参考“ torch.distributed.pipeline.\nsync.Pipe”类。可以使用这个 API 构造一个模型，其包含两个线性层，分别放置在两个计算设备\n中的样例...",
        "同时在计算设备 1 中并行计算 F12。 相比于最原始的流水线并行方法，GPipe 流水线方法可以有效\n减少并行气泡。\n图 4.7 GPipe 策略流水线并行样例 [170]\n虽然 GPipe 策略可以减少一定的并行气泡，但是只有当一个小批次中所有的前向计算都完成\n时，才能执行后向计算。因此，还是会产生很多并行气泡，从而降低系统的并行效率。Megatron-\nLM[171] 采用了 1F1B 流水线并行策略， 即一个前向通道和一个后向通道。1F1B 流水线并行策略引\n入了任务调度机制，使得下游设备能够在等待上游计算的同时执行其他可并行的任务，从而提高\n设备的利用率。1F1B 给出了非交错式和交错式两种调度模式，如图4.8 所示。\n1F1B 非交错式调度模式可分为三个阶段。 首先是热身阶段， 在计算设备中进行不同数量的前\n向计算。接下来的阶段是前向-后向阶段，计算设备按顺序执行一次前向计算，然后进行一次后向\n计算。最后一个阶段是后向阶段，计算设备完成最后一次后向计算。相比于GPipe 策略，1F1B 非\n交错式调度模式在节省内存方面表现得更好。然而，它需要与 GPipe 策略一样的时间来...",
        "MLFQ 调度器。该调度器采用 k 个不同优先级的队列 Q1, Q2, · · · , Qk，Q1 优先级最高，其中的作\n业运行时间是最短的， 将Q1 中作业的运行时间片 （Quantum） 设置为一个迭代最小花费时间，Qi\n和 Qi−1 之间的作业运行时间片比率 （Quantum Ratio） 设置为2。 当一个批次执行完成时，Skip-join\nMLFQ 调度器会根据刚进入队列的作业情况，构造下一个批次的作业列表。与原始的MLFQ 调度\n器不同，Skip-join MLFQ 调度器不完全根据队列优先级选择执行批次，而是结合作业进入时间及\n执行情况确定每个批次的作业列表。同时，针对被抢占的作业会立即返回所生成的词元，而不是\n等待整个任务全部完成，从而优化用户体验。\n此前的研究表明， 大语言模型的能力符合缩放法则， 也就是说模型参数量越大其能力越强。 然\n而， 大语言模型所需的显存使用量也与其参数量成正比。 例如， 将GPT-3 175B 的所有参数以 FP16\n方式进行存储， 所需的GPU 显存就达到了 350GB， 在运行时还需要更多显存来存储中间状态。 因\n此， 大语言模型通常需...",
        "520 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n索引\nn-gram, 2\nn 元文法, 2\nn 元语法, 2\nn 元语法单元, 2\n16 位浮点数, 116\n1F1B 交错式调度模式, 107\n1F1B 非交错式调度模式, 106\n32 位浮点数, 116\nAction, 191\nAction Space, 192\nActor-critic Agent, 193\nAgent, 191, 261\nAll Gather, 125\nAll Reduce, 124\nAll to All, 125\nAuto-CoT, 276\nAutomatic Evaluation, 425\nBF16, 116\nBFloat16, 116\nBlack-box KD, 383\nBroadcast, 123\nCatastrophic Forgetting, 32\nChain-of-Thought Prompting, 275\nChain-of-Thought，CoT, 276\nChunk, 324\nClassification, 421\nCohen’s Kappa, 427\nCollective ...",
        "106 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n的下游设备 （Downstream Device） 需要长时间持续处于空闲状态， 等待上游设备 （Upstream Device）\n计算完成， 才能开始计算自身的任务。 这种情况导致设备的平均使用率大幅降低， 形成了模型并行气\n泡（Model Parallelism Bubble），也称为流水线气泡（Pipeline Bubble）。\n图 4.6 流水线并行样例\n朴素流水线策略所产生的并行气泡，使得系统无法充分利用计算资源，降低了系统整体的计\n算效率。为了减少并行气泡，文献 [170] 提出了 GPipe 方法，将小批次（Mini-batch）进一步划分\n成更小的微批次（Micro-batch），利用流水线并行方法，每次处理一个微批次的数据。在当前阶\n段计算完成得到结果后，将该微批次的结果发送给下游设备，同时开始处理后一个微批次的数据，\n这样可以在一定程度上减少并行气泡。图4.7 给出了 GPipe 策略流水线并行样例。前向 F1 计算被\n拆解为 F11、F12、F13、F14，在计算设备 1 中计算完成 F11 后，会..."
      ],
      "response_time": 0.9574036598205566,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.014238348381961592,
        "semantic_similarity": 0.4317382872104645
      }
    },
    {
      "sample_id": "0048",
      "question": "What is the default value for 'local_rank' in the argument parser?",
      "ground_truth": "0",
      "predicted_answer": "The default value for 'local_rank' in the argument parser is 0. [1]",
      "ground_context": [
        "parser.add_argument('--local_rank', default =0, type=int, help='node rank for distributed training' )"
      ],
      "retrieved_contexts": [
        "metavar='N')\nparser.add_argument('--local_rank', default =0, type=int,\nhelp='node rank for distributed training' )\nargs = parser.parse_args()\ntorch.distributed.init_process_group(backend=\"nccl\") #߄\nprint(\"Use GPU: {} for training\" .format(args.local_rank))\n#ଆ྘\nmodel = DeepLab()\ntorch.cuda.set_device(args.local_rank) #భཁव\nmodel = model.cuda()\nmodel = torch.nn.parallel.DistributedDataParallel(model, device_ids =[args.local_rank],\noutput_device=args.local_rank, find_unused_parameters =True) # ඔऌѩྛ\n...",
        "yourself unable to manage your emotions, it is strongly\nrecommended to seek professional assistance promptly.\nHuman: Depression is just temporary bad mood, isn’t it?\nWhat should be taken into consideration in daily life when\ndealing with depression? Assistant: That’s right. How-\never, if you can’t control it on your own for a long time and\nit affects your normal work and life, you should seek help\nfrom professional institutions in a timely manner. Here\nare some things that may help improve the d...",
        "from dataset import Cityscaples\n# ҕඔഡᇂ\nparser = argparse.ArgumentParser(description='DeepLab')\nparser.add_argument('-j', '--workers', default =4, type=int, metavar ='N',\nhelp='number of data loading workers (default: 4)' )\nparser.add_argument('--epochs', default =100, type=int, metavar ='N',\nhelp='number of total epochs to run' )\nparser.add_argument('--start-epoch', default =0, type=int, metavar ='N',\nhelp='manual epoch number (useful on restarts)' )\nparser.add_argument('-b', '--batch-size', def...",
        "354 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\nYou will be presented with a/an {query name}, an analysis of the {queryname}, a/an {doc name},\nand an analysis of the {doc name}.\nYour task is to assess if the {doc name} {definition of relevance} the {query name} in one word:\n- Yes: If the {doc name} {definition of relevance} the {query name}.\n- No: Otherwise.\nImportant: Respond using only one of the following two words without quotation marks: Yes or No.\nHere is the {query name}:\n{query}\nHere is the analysis of...",
        "大语言模型：从理论到实践 361\n1. 检索模块的评估指标\n检索模块的评估指标需要全面反映系统在复杂信息环境中的表现，不仅关注检索结果的相关\n性和准确性，还需涵盖多样性与鲁棒性，以衡量系统在动态、海量且可能包含误导性信息的知识\n库中的适应能力。针对检索模块的评估指标可以分为基于排序和非基于排序两大类。\n基于排序的指标评估相关项目在排序列表中的呈现顺序， 重视相关项目在列表中的排名位置。\n评测指标主要有：\n平均倒数排名（Mean Reciprocal Rank, MRR ）是一组查询中，第一个正确答案的倒数排名的\n平均值，公式为：\nM RR = 1\n|Q|\n|Q|X\ni=1\n1\nranki\n(9.16)\n其中 |Q| 是查询的数量，ranli 是第 i 个查询的第一个相关文档的排名位置。\n平均准确率均值 （Mean Average Precision, MAP） 是每个查询的平均准确率得分的平均值， 公\n式为:\nM AP = 1\n|Q|\n|Q|X\nq=1\nPn\nk=1(P (k) × rel(k))\f\f第 q 个查询的相关文档数量\n\f\f (9.17)\n其中 P (k) 是指在排名列表中截..."
      ],
      "response_time": 0.7093002796173096,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.14285714285714288,
        "rouge2": 0.0,
        "rougeL": 0.14285714285714288,
        "bleu": 0.013609139615996616,
        "semantic_similarity": 0.5143072605133057
      }
    },
    {
      "sample_id": "0049",
      "question": "单个计算设备在推理阶段采用 FP32 格式进行存储时，需要多少GB的计算设备内存空间来存储GPT-3的参数？",
      "ground_truth": "700GB",
      "predicted_answer": "单个计算设备在推理阶段采用 FP32 格式进行存储时，需要 700GB 的计算设备内存空间来存储 GPT-3 的参数 [1]。",
      "ground_context": [
        "如果在推理阶段采用 FP32 格式进行存储，则需要 700GB 的计算设备内存空间"
      ],
      "retrieved_contexts": [
        "用数千个计算设备花费几十天。分布式训练系统需要克服计算墙、显存墙、通信墙等挑战，以确\n保集群内的所有资源得到充分利用，从而加速训练过程并缩短训练周期。\n• 计算墙：单个计算设备所能提供的计算能力与大语言模型所需的总计算量之间存在巨大差\n异。2022 年 3 月发布的 NVIDIA H100 SXM 的单卡 FP16 算力只有 2000 TFLOPS（Floating\nPoint Operations Per Second ） ， 而GPT-3 需要 314 ZFLOPS 的总计算量， 两者相差了8 个数量\n级。\n• 显存墙： 单个计算设备无法完整存储一个大语言模型的参数。GPT-3 包含 1750 亿个参数， 如\n果在推理阶段采用 FP32 格式进行存储，则需要 700GB 的计算设备内存空间，而 NVIDIA\nH100 GPU 只有 80GB 显存。\n• 通信墙： 分布式训练系统中各计算设备之间需要频繁地进行参数传输和同步。 由于通信的延\n迟和带宽限制，这可能成为训练的瓶颈。在 GPT-3 的训练过程中，如果分布式系统中存在\n128 个模型副本，那么在每次迭代过程中至少需要传输 89...",
        "大规模语言模型：从理论到实践 117\n图 4.15 FP32、FP16 和 BF16 的表示\n混合精度优化的过程如图 4.16 所示。Adam 优化器状态包括采用 FP32 保存的模型参数备份，\n一阶动量和二阶动量也都采用 FP32 格式存储。 假设模型参数量为Φ， 模型参数和梯度都是用FP16\n格式存储， 则共需要2Φ + 2Φ + (4Φ + 4Φ + 4Φ) = 16 Φ 字节存储。其中，Adam 状态占比 75%。动\n态损失缩放在反向传播前，将损失变化（dLoss）手动增大 2K 倍，因此反向传播时得到的激活函\n数梯度不会溢出；反向传播后，将权重梯度缩小 2K 倍，恢复正常值。举例来说，有 75 亿个参数\n的模型，如果用 FP16 格式，只需要 15GB 计算设备内存，但是在训练阶段，模型状态实际上需要\n耗费 120GB 内存。计算卡内存占用中除了模型状态，还有剩余状态（Residual States） ，包括激活\n值（Activation） 、各种临时缓冲区（Buffer）及无法使用的显存碎片（Fragmentation）等。可以使\n用激活值检查点（Activation Ch...",
        "于表示指数，第 6 位∼第 0 位用于表示尾数。由于 FP16 的值区间比 FP32 的值区间小很多，所以\n在计算过程中很容易出现上溢出和下溢出。BF16 相较于 FP16 以精度换取更大的值区间范围。由\n于 FP16 和 BF16 相较 FP32 精度低，训练过程中可能会出现梯度消失和模型不稳定的问题，因此，\n需要使用一些技术解决这些问题，例如动态损失缩放（Dynamic Loss Scaling ）和混合精度优化器\n（Mixed Precision Optimizer）等。",
        "性知识 （Factual Knowledge） 和常识性知识 （Commonsense） 在内的世界知识 （World Knowledge） 。 根\n据文献 [39] 中的介绍，GPT-3 完成一次训练的总计算量是 3640PFLOPS， 按照NVIDIA A100 80GB\nGPU 和平均利用率达到 50% 计算， 需要花费近一个月的时间使用1000 块 GPU 完成。 由于GPT-3\n的训练采用 NVIDIA V100 32GB GPU ，其实际计算成本远高于上述计算。文献[29] 介绍了参数量\n同样是 1750 亿个的 OPT 模型，该模型训练使用 992 块 NVIDIA A100 80GB GPU ，整体训练时间\n将近 2 个月。BLOOM[31] 模型的参数量也是 1750 亿个，该模型训练一共花费 3.5 个月，使用包含\n384 块 NVIDIA A100 80GB GPU 集群完成。可以看到，大语言模型的训练需要花费大量的计算资\n源和时间。LLaMA、Falcon、 百川 （Baichuan） 等模型都属于基础语言模型。 即便是DeepSeek-V3[40]\n经过了大量...",
        "型规模、自注意力机制和解码方法。\n1. 模型规模的影响： 主流的大型语言模型通常包含数十亿到数万亿的参数。 例如，LLaMA-70B\n拥有 700 亿参数， 而GPT-3 的规模更是高达 1750 亿参数。 这类超大规模模型显著增加了推理过程\n中的计算成本、内存访问成本和内存使用量。随着模型参数规模的增大，推理所需的计算资源和\n显存容量也随之增加。 同时， 模型权重需要频繁从高带宽内存 （HBM） 加载到GPU 芯片， 这不仅\n加剧了内存访问延迟，还显著提高了能耗。此外，大规模模型的权重存储和处理会占用大量显存\n资源，从而降低整体的资源利用效率。\n2. 自注意力操作的影响： 在推理过程中， 自注意力机制是计算复杂度的主要来源之一。 正如前\n文所述， 在预填充阶段， 自注意力操作的计算复杂度随着输入长度的增加呈现出二次增长 （O(n2)） 。\n这意味着，当输入长度较长时，自注意力机制会显著增加计算成本、内存访问成本和内存使用量。\n例如，在处理长文本时，模型需要为每个词元计算注意力权重矩阵，这不仅显著加重了计算负担，\n还导致显存占用大幅上升。因此，自注意力机制的高复杂度成为推理效率低下的关..."
      ],
      "response_time": 1.3343660831451416,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.33333333333333337,
        "rouge2": 0.0,
        "rougeL": 0.33333333333333337,
        "bleu": 0.0092991042878441,
        "semantic_similarity": 0.5404314994812012
      }
    },
    {
      "sample_id": "0050",
      "question": "What does the DistributedSampler class require to be available?",
      "ground_truth": "Requires distributed package to be available",
      "predicted_answer": "The `DistributedSampler` class requires the distributed package to be available [1]。",
      "ground_context": [
        "if not dist.is_available(): raise RuntimeError(\"Requires distributed package to be available\" )"
      ],
      "retrieved_contexts": [
        "102 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\nclass DistributedSampler(Sampler):\ndef __init__(self, dataset, num_replicas =None, rank =None, shuffle =True, seed =0):\nif num_replicas is None:\nif not dist.is_available():\nraise RuntimeError(\"Requires distributed package to be available\" )\nnum_replicas = dist.get_world_size()\nif rank is None:\nif not dist.is_available():\nraise RuntimeError(\"Requires distributed package to be available\" )\nrank = dist.get_rank()\nself.dataset = dataset #ࠢ\nself.num_replicas = num_rep...",
        "大规模语言模型：从理论到实践 155\nGiven the classification task definition and the class labels, generate an input that corresponds\n,→ to each of the class labels. If the task doesn't require input, just generate the correct\n,→ class label.\nTask: Classify the sentiment of the sentence into positive, negative, or mixed.\nClass label: mixed\nSentence: I enjoy the flavor of the restaurant but their service is too slow.\nClass label: Positive\nSentence: I had a great day today. The weather was beautiful and I spent ti...",
        "大规模语言模型：从理论到实践 103\nreturn iter (indices)\ndef __len__(self):\nreturn self .num_samples\ndef set_epoch(self, epoch):\nr\"\"\"\n࿞਀੽ඔ\n֒attr:`shuffle=True`ඨ྽\nඨ྽\nArguments:\nepoch (int): ࿞਀੽ඔ\n\"\"\"\nself.epoch = epoch\n利用 DistributedSampler 类构造的完整的训练程序样例 main.py 如下：",
        "大规模语言模型：从理论到实践 135\npip install deepspeed\n1. 训练数据配置\n使用 PyTorch 和 transformers 库来设置预训练模型的数据加载器， 以实现在单机或多机分布式\n训练环境中对数据的加载和采样。需要导入的模块如下。\n• DataLoader 是 PyTorch 提供的工具，用于从数据集加载数据到模型进行训练或评估。\n• RandomSampler 和 SequentialSampler 是 PyTorch 提供的两种采样器。RandomSampler 随机采\n样数据，而 SequentialSampler 顺序采样数据。\n• DistributedSampler 是用于分布式训练的数据采样器。\n• default_data_collator 是 transformers 库提供的默认数据收集器，用于将多个样本整合为一个\n批量数据。\n• create_pretrain_dataset 是一个自定义函数，用于创建预训练数据集。\n通过检查 args.local_rank 是否为 −1，代码会选择使用普通的采样器（单机）还是分布式采样\n器 （多...",
        "Proceedings of the 44th international ACM SIGIR conference on research and development in in-\nformation retrieval. 2021: 2342-2348.\n[590] Guo J, Guo Y , Li M, et al. Flame: Financial large-language model assessment and metrics evaluation\n[J]. arXiv preprint arXiv:2501.06211, 2025.\n[591] Jin D, Pan E, Oufattole N, et al. What disease does this patient have? a large-scale open domain\nquestion answering dataset from medical exams[J]. Applied Sciences, 2021, 11(14):6421."
      ],
      "response_time": 0.6506633758544922,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.7058823529411764,
        "rouge2": 0.5333333333333333,
        "rougeL": 0.7058823529411764,
        "bleu": 0.22894156860669912,
        "semantic_similarity": 0.7846457958221436
      }
    },
    {
      "sample_id": "0051",
      "question": "1F1B 流水线并行策略引入了什么机制？",
      "ground_truth": "任务调度机制",
      "predicted_answer": "1F1B 流水线并行策略引入了任务调度机制，使得下游设备能够在等待上游计算的同时执行其他可并行的任务，从而提高设备的利用率[1][2]。",
      "ground_context": [
        "1F1B 流水线并行策略引入了任务调度机制，使得下游设备能够在等待上游计算的同时执行其他可并行的任务，从而提高设备的利用率。"
      ],
      "retrieved_contexts": [
        "同时在计算设备 1 中并行计算 F12。 相比于最原始的流水线并行方法，GPipe 流水线方法可以有效\n减少并行气泡。\n图 4.7 GPipe 策略流水线并行样例 [170]\n虽然 GPipe 策略可以减少一定的并行气泡，但是只有当一个小批次中所有的前向计算都完成\n时，才能执行后向计算。因此，还是会产生很多并行气泡，从而降低系统的并行效率。Megatron-\nLM[171] 采用了 1F1B 流水线并行策略， 即一个前向通道和一个后向通道。1F1B 流水线并行策略引\n入了任务调度机制，使得下游设备能够在等待上游计算的同时执行其他可并行的任务，从而提高\n设备的利用率。1F1B 给出了非交错式和交错式两种调度模式，如图4.8 所示。\n1F1B 非交错式调度模式可分为三个阶段。 首先是热身阶段， 在计算设备中进行不同数量的前\n向计算。接下来的阶段是前向-后向阶段，计算设备按顺序执行一次前向计算，然后进行一次后向\n计算。最后一个阶段是后向阶段，计算设备完成最后一次后向计算。相比于GPipe 策略，1F1B 非\n交错式调度模式在节省内存方面表现得更好。然而，它需要与 GPipe 策略一样的时间来...",
        "大规模语言模型：从理论到实践 107\n1F1B 交错式调度模式要求微批次的数量是流水线阶段的整数倍。每个设备不仅负责连续多\n个层的计算， 还可以处理多个层的子集， 这些子集被称为模型块。 具体而言， 在之前的模式中， 设\n备 1 可能负责层 1∼4， 设备2 负责层 5∼8， 依此类推。 在新的模式下， 设备1 可以处理层 1、2、9、\n10，设备 2 处理层 3、4、11、12，依此类推。在这种模式下，每个设备在流水线中被分配到多个\n阶段。例如，设备 1 可能参与热身阶段、前向计算阶段和后向计算阶段的某些子集任务。每个设\n备可以并行执行不同阶段的计算任务，从而更好地利用流水线并行的优势。这种模式不仅在内存\n消耗方面表现出色，还能提高计算效率，使大型模型的并行系统能够更高效地完成计算任务。\n图 4.8 1F1B 流水线并行策略样例 [171]\nPyTorch 中也包含了实现流水线的 API 函数 Pipe，具体实现参考“ torch.distributed.pipeline.\nsync.Pipe”类。可以使用这个 API 构造一个模型，其包含两个线性层，分别放置在两个计算设备\n中的样例...",
        "106 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n的下游设备 （Downstream Device） 需要长时间持续处于空闲状态， 等待上游设备 （Upstream Device）\n计算完成， 才能开始计算自身的任务。 这种情况导致设备的平均使用率大幅降低， 形成了模型并行气\n泡（Model Parallelism Bubble），也称为流水线气泡（Pipeline Bubble）。\n图 4.6 流水线并行样例\n朴素流水线策略所产生的并行气泡，使得系统无法充分利用计算资源，降低了系统整体的计\n算效率。为了减少并行气泡，文献 [170] 提出了 GPipe 方法，将小批次（Mini-batch）进一步划分\n成更小的微批次（Micro-batch），利用流水线并行方法，每次处理一个微批次的数据。在当前阶\n段计算完成得到结果后，将该微批次的结果发送给下游设备，同时开始处理后一个微批次的数据，\n这样可以在一定程度上减少并行气泡。图4.7 给出了 GPipe 策略流水线并行样例。前向 F1 计算被\n拆解为 F11、F12、F13、F14，在计算设备 1 中计算完成 F11 后，会...",
        "520 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n索引\nn-gram, 2\nn 元文法, 2\nn 元语法, 2\nn 元语法单元, 2\n16 位浮点数, 116\n1F1B 交错式调度模式, 107\n1F1B 非交错式调度模式, 106\n32 位浮点数, 116\nAction, 191\nAction Space, 192\nActor-critic Agent, 193\nAgent, 191, 261\nAll Gather, 125\nAll Reduce, 124\nAll to All, 125\nAuto-CoT, 276\nAutomatic Evaluation, 425\nBF16, 116\nBFloat16, 116\nBlack-box KD, 383\nBroadcast, 123\nCatastrophic Forgetting, 32\nChain-of-Thought Prompting, 275\nChain-of-Thought，CoT, 276\nChunk, 324\nClassification, 421\nCohen’s Kappa, 427\nCollective ...",
        "， 也称之为张量并行（Tensor Parallelism，TP） 。 两节点模型并行训练系统样例如图4.5 所示， 图4.5(a)\n为流水线并行， 模型的不同层被切分到不同的设备中； 图4.5(b) 为张量并行， 同一层中的不同参数\n被切分到不同的设备中进行计算。\n图 4.5 两节点模型并行训练系统样例\n1. 流水线并行\n流水线并行是一种并行计算策略，将模型的各个层分段处理，并将每个段分布在不同的计算设\n备上， 使得前后阶段能够流水式、 分批工作。 流水线并行通常应用于大语言模型的并行系统中， 以有\n效解决单个计算设备内存不足的问题。 图4.6 给出了一个由四个计算设备组成的流水线并行系统， 包\n含前向计算和后向计算。其中F1、F2、F3、F4 分别代表四个前向路径，位于不同的设备上；而B4、\nB3、B2、B1 则代表逆序的后向路径， 也分别位于四个不同的设备上。 从图4.6 中可以看出， 计算图中"
      ],
      "response_time": 1.3824913501739502,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.016531706181466276,
        "semantic_similarity": 0.6272812485694885
      }
    },
    {
      "sample_id": "0052",
      "question": "数据并行和GPipe策略在并行计算中有什么主要区别？",
      "ground_truth": "数据并行系统中，每个计算设备都有整个神经网络模型的模型副本，进行前向计算和梯度同步，而GPipe策略在前向计算完成后才能执行后向计算，且可能产生并行气泡。虽然GPipe减少了并行气泡，但在执行后向计算时仍然依赖于前向计算的完成。",
      "predicted_answer": "数据并行方法中，每个计算设备拥有完整的模型副本，仅处理部分数据样本，并在前向计算后进行梯度的广播和聚合[5]。而GPipe策略则通过将小批次进一步划分为微批次，实现更细粒度的并行计算，从而减少并行气泡[1][3]。",
      "ground_context": [
        "在数据并行系统中，每个计算设备都有整个神经网络模型的模型副本，进行迭代时，每个计算设备只分配一个批次数据样本的子集。",
        "GPipe策略可以减少一定的并行气泡，但是只有当一个小批次中所有的前向计算都完成时，才能执行后向计算。"
      ],
      "retrieved_contexts": [
        "同时在计算设备 1 中并行计算 F12。 相比于最原始的流水线并行方法，GPipe 流水线方法可以有效\n减少并行气泡。\n图 4.7 GPipe 策略流水线并行样例 [170]\n虽然 GPipe 策略可以减少一定的并行气泡，但是只有当一个小批次中所有的前向计算都完成\n时，才能执行后向计算。因此，还是会产生很多并行气泡，从而降低系统的并行效率。Megatron-\nLM[171] 采用了 1F1B 流水线并行策略， 即一个前向通道和一个后向通道。1F1B 流水线并行策略引\n入了任务调度机制，使得下游设备能够在等待上游计算的同时执行其他可并行的任务，从而提高\n设备的利用率。1F1B 给出了非交错式和交错式两种调度模式，如图4.8 所示。\n1F1B 非交错式调度模式可分为三个阶段。 首先是热身阶段， 在计算设备中进行不同数量的前\n向计算。接下来的阶段是前向-后向阶段，计算设备按顺序执行一次前向计算，然后进行一次后向\n计算。最后一个阶段是后向阶段，计算设备完成最后一次后向计算。相比于GPipe 策略，1F1B 非\n交错式调度模式在节省内存方面表现得更好。然而，它需要与 GPipe 策略一样的时间来...",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "106 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n的下游设备 （Downstream Device） 需要长时间持续处于空闲状态， 等待上游设备 （Upstream Device）\n计算完成， 才能开始计算自身的任务。 这种情况导致设备的平均使用率大幅降低， 形成了模型并行气\n泡（Model Parallelism Bubble），也称为流水线气泡（Pipeline Bubble）。\n图 4.6 流水线并行样例\n朴素流水线策略所产生的并行气泡，使得系统无法充分利用计算资源，降低了系统整体的计\n算效率。为了减少并行气泡，文献 [170] 提出了 GPipe 方法，将小批次（Mini-batch）进一步划分\n成更小的微批次（Micro-batch），利用流水线并行方法，每次处理一个微批次的数据。在当前阶\n段计算完成得到结果后，将该微批次的结果发送给下游设备，同时开始处理后一个微批次的数据，\n这样可以在一定程度上减少并行气泡。图4.7 给出了 GPipe 策略流水线并行样例。前向 F1 计算被\n拆解为 F11、F12、F13、F14，在计算设备 1 中计算完成 F11 后，会...",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "大规模语言模型：从理论到实践 101\n4.2.1 数据并行\n在数据并行系统中， 每个计算设备都有整个神经网络模型的模型副本（Model Replica） ， 进行\n迭代时，每个计算设备只分配一个批次数据样本的子集，并根据该批次样本子集的数据进行网络\n模型的前向计算。 假设一个批次的训练样本数为N ， 使用M 个计算设备并行计算， 每个计算设备\n会分配到 N /M 个样本。前向计算完成后，每个计算设备都会根据本地样本计算损失误差，得到\n梯度 Gi（i 为加速卡编号） ，并将本地梯度Gi 进行广播。所有计算设备需要聚合其他加速卡给出\n的梯度值，然后使用平均梯度 (PN\ni=1 Gi)/N 对模型进行更新，完成该批次训练。图4.4 给出了由\n两个计算设备组成的数据并行训练系统样例。\n图 4.4 由两个计算设备组成的数据并行训练系统样例\n数据并行训练系统可以通过增加计算设备，有效提升整体训练吞吐量，即 每秒全局批次数\n（Global Batch Size Per Second ）。与单个计算设备训练相比，其最主要的区别在于反向计算中的\n梯度需要在所有计算设备中进行同步，以保证每个计算设备上最终..."
      ],
      "response_time": 2.0351245403289795,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.3333333333333333,
        "rouge2": 0.0,
        "rougeL": 0.3333333333333333,
        "bleu": 0.16576082887962762,
        "semantic_similarity": 0.8971790075302124
      }
    },
    {
      "sample_id": "0053",
      "question": "在张量并行与1F1B交错式调度模式中，如何解决大规模语言模型中设备间的计算和存储问题？",
      "ground_truth": "张量并行需要根据模型的具体结构和算子类型，将参数切分到不同设备，并保证数学一致性。对于大语言模型，嵌入式表示算子的参数存储需求很高，因此可以按词维度切分，以减少每个设备的显存需求。同时，1F1B交错式调度模式通过让每个设备处理多个层的子集，从而更高效地利用计算资源，提升了内存消耗和计算效率。",
      "predicted_answer": "在张量并行中，需要解决如何将参数切分到不同设备以及保持切分后的数学一致性的问题。对于嵌入式表示算子，当词表数量较大时，可能会导致单个计算设备的显存不足。此时可以通过按词维度切分参数，使得每个设备只存储部分词向量，最后通过汇总各个设备上的部分词向量来获得完整的词向量[3]。\n\n1F1B交错式调度模式则通过调整微批次的数量使其成为流水线阶段的整数倍，让每个设备不仅负责连续多个层的计算，还可以处理这些层的不同子集（称为模型块）。这种方式有助于更好地利用流水线并行的优势，在内存消耗和计算效率上都有所提升[1]。",
      "ground_context": [
        "张量并行需要根据模型的具体结构和算子类型，解决如何将参数切分到不同设备，以及如何保证切分后的数学一致性这两个问题。",
        "在新的模式下，设备1 可以处理层 1、2、9、10，设备 2 处理层 3、4、11、12，依此类推。"
      ],
      "retrieved_contexts": [
        "大规模语言模型：从理论到实践 107\n1F1B 交错式调度模式要求微批次的数量是流水线阶段的整数倍。每个设备不仅负责连续多\n个层的计算， 还可以处理多个层的子集， 这些子集被称为模型块。 具体而言， 在之前的模式中， 设\n备 1 可能负责层 1∼4， 设备2 负责层 5∼8， 依此类推。 在新的模式下， 设备1 可以处理层 1、2、9、\n10，设备 2 处理层 3、4、11、12，依此类推。在这种模式下，每个设备在流水线中被分配到多个\n阶段。例如，设备 1 可能参与热身阶段、前向计算阶段和后向计算阶段的某些子集任务。每个设\n备可以并行执行不同阶段的计算任务，从而更好地利用流水线并行的优势。这种模式不仅在内存\n消耗方面表现出色，还能提高计算效率，使大型模型的并行系统能够更高效地完成计算任务。\n图 4.8 1F1B 流水线并行策略样例 [171]\nPyTorch 中也包含了实现流水线的 API 函数 Pipe，具体实现参考“ torch.distributed.pipeline.\nsync.Pipe”类。可以使用这个 API 构造一个模型，其包含两个线性层，分别放置在两个计算设备\n中的样例...",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "张量并行需要根据模型的具体结构和算子类型，解决如何将参数切分到不同设备，以及如何\n保证切分后的数学一致性这两个问题。 大语言模型都是以Transformer 结构为基础，Transformer 结\n构主要由嵌入式表示（Embedding） 、矩阵乘（MatMul）和交叉熵损失（Cross Entropy Loss）计算\n构成。这三种类型的算子有较大的差异，需要设计对应的张量并行策略 [169] 才可以实现将参数切\n分到不同的设备。\n对于嵌入式表示算子，如果总的词表数非常大，会导致单计算设备显存无法容纳 Embedding\n层参数。举例来说，如果词表数量是 64000，嵌入式表示维度为 5120，类型采用 32 位精度浮点\n数， 那么整层参数需要的显存大约为64000 × 5120 × 4/1024/1024 = 1250 MB， 反向梯度同样需要\n1250MB 显存， 仅仅存储就需要将近2.5GB。 对于嵌入表示层的参数， 可以按照词维度切分， 每个\n计算设备只存储部分词向量，然后通过汇总各个设备上的部分词向量，得到完整的词向量。图4.9\n给出了单节点 Embedding 和两节点 E...",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "392 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n序列并行 序列并行张量并行A和B：参数（FP8），Y和Z：激活值\n图 10.11 采用 FP8 张量和序列并行的 Transformer 层 [530]\n零冗余优化器 （Zero Redundancy Data Parallelism，ZeRO）[173–175] 也是大模型训练中的另一种\n常用分布式学习技术。ZeRO 的核心思想是将模型状态分片到各设备，使每个设备仅保存训练步\n骤所需数据 （如主权重、 梯度和优化器状态） 的一部分。 为了减少内存消耗，ZeRO 方法通常将单\n个张量分割为多个子张量，并将其分布到不同的设备上。\n直接将 FP8 应用于 ZeRO 也是不可行的，因为难以处理与 FP8 分片相关的缩放因子。每个张\n量的缩放因子需要与 FP8 分片一起分发。为了解决这一问题，FP8-LM 实现了一种新的 FP8 分布\n方案，该方案将整个张量分布到设备上，而不是像ZeRO 那样将张量分割为多个子张量进行分布。\nFP8 张量的分布采用贪婪策略，具体过程如算法 1 所述。具体来说，我们的方法首先根据张量状\n态的大小对..."
      ],
      "response_time": 4.063264846801758,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.5,
        "rouge2": 0.0,
        "rougeL": 0.5,
        "bleu": 0.12847526661366396,
        "semantic_similarity": 0.9132169485092163
      }
    },
    {
      "sample_id": "0054",
      "question": "在分布式系统中，如何处理多个计算设备的输出以确保最终结果与单个计算设备一致？",
      "ground_truth": "在分布式系统中，多个计算设备的输出需要进行合并，最终得到与单个计算设备等价的计算结果。这是因为多个计算设备并行执行各自的子任务，从而加速整体计算。",
      "predicted_answer": "在分布式系统中，为了确保多个计算设备的输出结果与单个计算设备的一致性，通常会在反向传播阶段通过同步梯度来实现这一点。具体而言，在数据并行系统中，每个计算设备都会独立地对局部样本进行前向和反向计算，并获得各自的梯度。这些局部梯度随后会被收集起来并在所有计算设备之间进行同步，通常采取的是平均的方式[1]。这样做的目的是确保即使是在分布式的环境下，最终更新的模型参数也能反映整个数据集的特性，而不是仅仅局限于某个特定计算设备上的样本。此外，这种同步机制也是提高整体训练吞吐量的关键步骤之一[1]。",
      "ground_context": [
        "一个模型训练任务往往会有大量的训练样本作为输入，可以利用一个计算设备完成，也可以将整个模型的训练任务拆分成多个子任务，分发给不同的计算设备，实现并行计算。此后，还需要对每个计算设备的输出进行合并，最终得到与单个计算设备等价的计算结果。",
        "由于同一个服务器内部的多个计算设备之间可能并不共享内存，因此无论这些计算设备是处于一个服务器还是多个服务器中，其系统架构都属于分布式系统范畴。"
      ],
      "retrieved_contexts": [
        "大规模语言模型：从理论到实践 101\n4.2.1 数据并行\n在数据并行系统中， 每个计算设备都有整个神经网络模型的模型副本（Model Replica） ， 进行\n迭代时，每个计算设备只分配一个批次数据样本的子集，并根据该批次样本子集的数据进行网络\n模型的前向计算。 假设一个批次的训练样本数为N ， 使用M 个计算设备并行计算， 每个计算设备\n会分配到 N /M 个样本。前向计算完成后，每个计算设备都会根据本地样本计算损失误差，得到\n梯度 Gi（i 为加速卡编号） ，并将本地梯度Gi 进行广播。所有计算设备需要聚合其他加速卡给出\n的梯度值，然后使用平均梯度 (PN\ni=1 Gi)/N 对模型进行更新，完成该批次训练。图4.4 给出了由\n两个计算设备组成的数据并行训练系统样例。\n图 4.4 由两个计算设备组成的数据并行训练系统样例\n数据并行训练系统可以通过增加计算设备，有效提升整体训练吞吐量，即 每秒全局批次数\n（Global Batch Size Per Second ）。与单个计算设备训练相比，其最主要的区别在于反向计算中的\n梯度需要在所有计算设备中进行同步，以保证每个计算设备上最终...",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "100 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n加，这些问题变得更加突出。同时，当大型集群进行长时间训练时，设备故障可能会影响或中断\n训练，对分布式系统的问题处理也提出了很高的要求。\n4.2 分布式训练的并行策略\n分布式训练系统的目标是将单节点模型训练转换成等价的分布式并行模型训练。对于大语言\n模型来说，训练过程就是根据数据和损失函数，利用优化算法对神经网络模型参数进行更新的过\n程。 单个计算设备模型训练系统的结构如图4.3 所示， 其主要由数据和模型两个部分组成。 训练过\n程由多个数据小批次（Mini-batch）完成。图中数据表示一个数据小批次。训练系统会利用数据小\n批次根据损失函数和优化算法计算梯度，从而对模型参数进行修正。针对大语言模型多层神经网\n络的执行过程，可以由一个 计算图（Computational Graph ）表示。这个图有多个相互连接的算子\n（Operator） ，每个算子实现一个神经网络层（Neural Network Layer ） ，而参数则代表了这个层在训\n练中所更新的权重。\nा䃎ツ \nा \n䃎ツ \n图 4.3 单个计算设备模型训练系统的...",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "用数千个计算设备花费几十天。分布式训练系统需要克服计算墙、显存墙、通信墙等挑战，以确\n保集群内的所有资源得到充分利用，从而加速训练过程并缩短训练周期。\n• 计算墙：单个计算设备所能提供的计算能力与大语言模型所需的总计算量之间存在巨大差\n异。2022 年 3 月发布的 NVIDIA H100 SXM 的单卡 FP16 算力只有 2000 TFLOPS（Floating\nPoint Operations Per Second ） ， 而GPT-3 需要 314 ZFLOPS 的总计算量， 两者相差了8 个数量\n级。\n• 显存墙： 单个计算设备无法完整存储一个大语言模型的参数。GPT-3 包含 1750 亿个参数， 如\n果在推理阶段采用 FP32 格式进行存储，则需要 700GB 的计算设备内存空间，而 NVIDIA\nH100 GPU 只有 80GB 显存。\n• 通信墙： 分布式训练系统中各计算设备之间需要频繁地进行参数传输和同步。 由于通信的延\n迟和带宽限制，这可能成为训练的瓶颈。在 GPT-3 的训练过程中，如果分布式系统中存在\n128 个模型副本，那么在每次迭代过程中至少需要传输 89..."
      ],
      "response_time": 3.673548698425293,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.06660199517879695,
        "semantic_similarity": 0.784806489944458
      }
    },
    {
      "sample_id": "0055",
      "question": "什么是训练的批量大小？",
      "ground_truth": "GLOBAL_BATCH_SIZE",
      "predicted_answer": "训练的批量大小是指在一次权重更新时使用的样本数量。具体来说，在多 GPU 训练环境中，`micro_batch_size_per_gpu` 参数指定了每个 GPU 上的微批次大小，而 `ppo_mini_batch_size` 则表示整体的小批次大小。例如，`ppo_mini_batch_size` 设置为 64，意味着每次从训练集中选取 64 个样本组成一个小批次；而 `micro_batch_size_per_gpu` 设置为 4，则表示每个 GPU 每次处理 4 个样本的微批次。[1][3]",
      "ground_context": [
        "\"train_batch_size\": GLOBAL_BATCH_SIZE"
      ],
      "retrieved_contexts": [
        "避免因数据量过大导致的显存溢出问题。 log_prob_micro_batch_size_per_gpu 表示每个 GPU\n上用于计算对数概率的微批次大小。在强化学习中，对数概率用于计算策略梯度，它反映了模型\n在当前策略下采取某个动作的概率。将这个参数设置为 8，即每个 GPU 在计算对数概率时，每次\n处理 8 个样本的微批次，这样可以优化计算过程，提高训练效率。",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "大规模语言模型：从理论到实践 219\n动相关领域的发展，也为开发者在大模型强化学习领域的创新提供了支持。本节将介绍使用 verl\n框架进行大模型中强化学习的实践。\n1. 训练脚本与参数配置\n以推理任务为例，我们按照官方教程选用 Qwen2.5-0.5B-Instruct 模型在 GSM8K 数据集上进\n行强化学习训练。下面是一些关键的 RL 训练参数：\n在近端策略优化算法中，ppo_mini_batch_size 表示小批次的大小。在训练过程中，我们并\n不会一次性使用整个训练集来更新模型参数，而是将训练集划分为多个小批次。这个参数设置\n为 64，意味着每次从训练集中选取 64 个样本组成一个小批次，用于计算梯度和更新演员模型\n的参数。通过使用小批次，可以减少内存的占用，并且在一定程度上提高训练的稳定性和效率。\nppo_micro_batch_size_per_gpu 指的是每个 GPU 上的微批次大小。在多 GPU 训练环境下，为\n了更高效地利用 GPU 资源， 会将小批次进一步划分为微批次。 这里设置为4， 表示每个GPU 每次\n处理 4 个样本的微批次。 这种细粒度的划分有助于在G...",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "大规模语言模型：从理论到实践 139\n4. DeepSpeed 设置\n在配置代码的开始， 定义了两个关键参数GLOBAL_BA TCH_SIZE和 MICRO_BA TCH_ SIZE。\nGLOBAL_BA TCH_SIZE定义了全局的批次大小。这通常是所有 GPU 加起来的总批次大小。MI-\nCRO_BA TCH_SIZE定义了每块 GPU 上的微批次大小。因为微批次处理每次只加载并处理一小部\n分数据，所以可以帮助大语言模型在有限的 GPU 内存中运行。训练配置函数 get_train_ds_config\n主要包括以下内容。\n（1）ZeRO 优化配置：ZeRO 是 DeepSpeed 提供的一种优化策略，旨在减少训练中的冗余并加\n速模型的训练。其中的参数，如 offload_param 和 offload_optimizer，允许用户选择是否将模型参\n数或优化器状态卸载到 CPU。\n（2）混合精度训练：通过设置 FP16 字段，使模型可以使用 16 位浮点数进行训练，加速训练\n过程并减少内存使用。\n（3）梯度裁剪：通过 gradient_clipping 字段，可以防止训练过程中出现..."
      ],
      "response_time": 3.0699923038482666,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.13333333333333333,
        "rouge2": 0.07142857142857144,
        "rougeL": 0.13333333333333333,
        "bleu": 0.0032281681220314167,
        "semantic_similarity": 0.5173048973083496
      }
    },
    {
      "sample_id": "0056",
      "question": "GLOBAL_BATCH_SIZE定义了什么？",
      "ground_truth": "GLOBAL_BATCH_SIZE定义了全局的批次大小。",
      "predicted_answer": "GLOBAL_BA TCH_SIZE定义了全局的批次大小。这通常是所有 GPU 加起来的总批次大小。[1]",
      "ground_context": [
        "GLOBAL_BATCH_SIZE定义了全局的批次大小。这通常是所有 GPU 加起来的总批次大小。"
      ],
      "retrieved_contexts": [
        "大规模语言模型：从理论到实践 139\n4. DeepSpeed 设置\n在配置代码的开始， 定义了两个关键参数GLOBAL_BA TCH_SIZE和 MICRO_BA TCH_ SIZE。\nGLOBAL_BA TCH_SIZE定义了全局的批次大小。这通常是所有 GPU 加起来的总批次大小。MI-\nCRO_BA TCH_SIZE定义了每块 GPU 上的微批次大小。因为微批次处理每次只加载并处理一小部\n分数据，所以可以帮助大语言模型在有限的 GPU 内存中运行。训练配置函数 get_train_ds_config\n主要包括以下内容。\n（1）ZeRO 优化配置：ZeRO 是 DeepSpeed 提供的一种优化策略，旨在减少训练中的冗余并加\n速模型的训练。其中的参数，如 offload_param 和 offload_optimizer，允许用户选择是否将模型参\n数或优化器状态卸载到 CPU。\n（2）混合精度训练：通过设置 FP16 字段，使模型可以使用 16 位浮点数进行训练，加速训练\n过程并减少内存使用。\n（3）梯度裁剪：通过 gradient_clipping 字段，可以防止训练过程中出现...",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "ds_config[\n'train_micro_batch_size_per_gpu'] = args.per_device_train_batch_size\nds_config[\n'train_batch_size'] = args.per_device_train_batch_size * torch.distributed.get_world_size(\n) * args.gradient_accumulation_steps\n# ഡᇂ࿞਀ᇕሰ\nset_random_seed(args.seed)\ntorch.distributed.barrier()\n# ൐ႨDeepSpeed߄\nmodel, optimizer, _, lr_scheduler = deepspeed.initialize(\nmodel=model,\noptimizer=optimizer,\nargs=args,\nconfig=ds_config,\nlr_scheduler=lr_scheduler,\ndist_init_required=True)\nif args.gradient_checkpointin...",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "大规模语言模型：从理论到实践 145\n#ٳ\nprint_rank_0(\"***** Running training *****\" , args .global_rank)\nprint_rank_0(\nf\"***** Evaluating perplexity,\nEpoch {0}/{args.num_train_epochs} *****\",\nargs.global_rank)\nperplexity = evaluation(model, eval_dataloader)\nprint_rank_0(f\"ppl: {perplexity}\", args .global_rank)\nfor epoch in range(args.num_train_epochs):\nprint_rank_0(\nf\"Beginning of Epoch {epoch+1}/{args.num_train_epochs},\nTotal Micro Batches {len(train_dataloader)}\",\nargs.global_rank)\nmodel.train()\nimport time\nfor..."
      ],
      "response_time": 0.8867146968841553,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.4444444444444444,
        "rouge2": 0.0,
        "rougeL": 0.4444444444444444,
        "bleu": 0.3032929624979452,
        "semantic_similarity": 0.8108065128326416
      }
    },
    {
      "sample_id": "0057",
      "question": "ZeRO-3 配置参数的 stage 是多少？",
      "ground_truth": "3",
      "predicted_answer": "ZeRO-3 配置参数的 stage 设置为 3。[1][2][5]",
      "ground_context": [
        "以下是 DeepSpeed 使用 ZeRO-3 配置参数的样例：{\"zero_optimization\": {\"stage\": 3,},"
      ],
      "retrieved_contexts": [
        "大规模语言模型：从理论到实践 133\n以下是 DeepSpeed 使用 ZeRO-3 配置参数的样例：\n{\n\"zero_optimization\": {\n\"stage\": 3,\n},\n\"fp16\": {\n\"enabled\": true\n},\n\"optimizer\": {\n\"type\": \"AdamW\",\n\"params\": {\n\"lr\": 0.001,\n\"betas\": [\n0.8,\n0.999\n],\n\"eps\": 1e-8,\n\"weight_decay\": 3e-7\n}\n},\n...\n}\n如果希望在 ZeRO-3 的基础上继续使用 ZeRO-Infinity 将优化器状态和计算转移到 CPU 中， 则\n可以在配置文件中按照如下方式配置：\n{\n\"zero_optimization\": {\n\"stage\": 3,\n\"offload_optimizer\": {\n\"device\": \"cpu\"\n}\n},\n...\n}\n甚至可以进一步将模型参数也装载到 CPU 内存中，在配置文件中按照如下方式配置：",
        "132 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n{\n\"optimizer\": {\n\"type\": \"OneBitAdam\",\n\"params\": {\n\"lr\": 0.001,\n\"betas\": [\n0.8,\n0.999\n],\n\"eps\": 1e-8,\n\"weight_decay\": 3e-7,\n\"freeze_step\": 400,\n\"cuda_aware\": false,\n\"comm_backend_name\": \"nccl\"\n}\n}\n...\n}\nDeepSpeed 中也支持多种类型 ZeRO 的分片机制， 包括ZeRO-0、ZeRO-1、ZeRO-2、ZeRO-3 以及\nZeRO-Infinity。ZeRO-0 禁用所有类型的分片， 仅将DeepSpeed 当作分布式数据并行使用；ZeRO-1\n对优化器状态进行分片，占用内存为原始的 1/4，通信容量与数据并行性相同；ZeRO-2 对优化器\n状态和梯度进行分片，占用内存为原始的 1/8，通信容量与数据并行性相同；ZeRO-3 对优化器状\n态、梯度及模型参数进行分片，内存减少与数据并行度和复杂度成线性关系，同时通信容...",
        "\"max_out_tokens\": max_out_tokens,\n\"inference_tp_size\": inference_tp_size,\n\"release_inference_cache\": release_inference_cache,\n\"pin_parameters\": pin_parameters,\n\"tp_gather_partition_size\": tp_gather_partition_size,\n},\n\"tensorboard\": {\n\"enabled\": enable_tensorboard,\n\"output_path\": f\"{tb_path}/ds_tensorboard_logs/\",\n\"job_name\": f\"{tb_name}_tensorboard\"\n}\n}\ndef get_eval_ds_config(offload, stage =0):\n#֥DeepSpeed஥ᇂ\ndevice = \"cpu\" if offload else \"none\"\nzero_opt_dict = {\n\"stage\": stage,\n\"stage3_param_p...",
        "（3） 对模型参数进行分区， 图4.17 中的 Pos+g+p 部分。 此时， 每个计算设备所需内存是16Φ\nN B。\n当 N 比较大时，每个计算设备占用内存趋向于 0。\n图 4.17 三种 ZeRO 方法的单个设备内存占用\n在 DeepSpeed 框架中，Pos 对应 Zero-1，Pos+g 对应 Zero-2，Pos+g+p 对应 Zero-3。文献 [175]\n中也对 ZeRO 优化方法所带来的通信量增加的情况进行了分析，Zero-1 和 Zero-2 对整体通信量没\n有影响，虽然对通信有一定延迟影响，但是整体性能受到的影响很小。Zero-3 所需的通信量则是\n正常通信量的 1.5 倍。\nPyTorch中也实现了 ZeRO 优化方法， 可以使用ZeroRedundancyOptimizer 调用， 也可与 “torch.nn.parallel.DistributedDataParallel”\n结合使用， 以减少每个计算设备的内存峰值消耗。 使用ZeroRedundancyOptimizer 的参考代码如下\n所示：",
        "\"device\": device\n},\n\"stage3_param_persistence_threshold\": 1e4,\n\"stage3_max_live_parameters\": 3e7,\n\"stage3_prefetch_bucket_size\": 3e7,\n\"memory_efficient_linear\": False\n}\nreturn {\n\"train_batch_size\": GLOBAL_BATCH_SIZE,\n\"train_micro_batch_size_per_gpu\": MICRO_BATCH_SIZE,\n\"steps_per_print\": 10,\n\"zero_optimization\": zero_opt_dict,\n\"fp16\": {\n\"enabled\": True,\n\"loss_scale_window\": 100\n},\n\"gradient_clipping\": 1.0,\n\"prescale_gradients\": False,\n\"wall_clock_breakdown\": False,\n\"hybrid_engine\": {\n\"enabled\": e..."
      ],
      "response_time": 0.7922930717468262,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.25,
        "rouge2": 0.0,
        "rougeL": 0.25,
        "bleu": 0.013038525962482981,
        "semantic_similarity": 0.46095818281173706
      }
    },
    {
      "sample_id": "0058",
      "question": "如何初始化进程组？",
      "ground_truth": "dist.init_process_group(\"gloo\", rank =rank, world_size =world_size)",
      "predicted_answer": "使用 `torch.multiprocessing` 开启多个进程时，可以通过以下步骤初始化进程组[3]：\n\n```python\ndef init_process(rank: int, size: int, fn: Callable[[int, int], None], backend=\"gloo\"):\n    os.environ[\"MASTER_ADDR\"] = \"127.0.0.1\"\n    os.environ[\"MASTER_PORT\"] = \"29500\"\n    dist.init_process_group(backend, rank=rank, world_size=size)\n    fn(rank, size)\n\n# 下面是使用 torch.multiprocessing 模块启动多个进程的例子\nimport torch.multiprocessing as mp\n\ndef run(rank, size):\n    print(f'Running on rank {rank} / size {size}')\n\nmp.spawn(init_process, args=(4, run), nprocs=4, join=True)\n```\n\n这段代码设置了主地址和端口，并使用指定的后端（这里是 `gloo`）初始化了进程组。",
      "ground_context": [
        "dist.init_process_group(\"gloo\", rank =rank, world_size =world_size)"
      ],
      "retrieved_contexts": [
        "142 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n5. DeepSpeed 初始化\n设置 DeepSpeed 的配置参数后， 可以利用DeepSpeed 进行模型训练的初始化， 初始化流程如下。\n（1）确定运行的设备：首先，检查代码是否有指定的本地 GPU（通过 args.local_rank） 。如果\n没有指定，则程序默认使用 CUDA 设备。否则，它会为进程设置指定的 GPU。\n（2） 初始化分布式后端： 在分布式训练中， 使用deepspeed.init_distributed() 函数实现每个进程\n与其他进程的同步，初始化分布式环境。\n（3）获取当前进程的全局排序：在分布式训练中，使用torch.distributed.get_rank() 函数获得每\n个进程的唯一排序或 ID。\n（4）设置 DeepSpeed 配置：根据用户参数（如是否进行 offload、使用哪个 Zero Stage 等）构\n建一个 DeepSpeed 配置字典，来决定训练设置。\n（5） 同步所有工作进程： 使用torch.distributed.barrier() 确保在进一步的初始化之...",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "def init_process(rank: int, size: int, fn: Callable[[ int, int], None], backend =\"gloo\"):\n\"\"\"࣢ߌ\"\"\"\nos.environ[\"MASTER_ADDR\"] = \"127.0.0.1\"\nos.environ[\"MASTER_PORT\"] = \"29500\"\ndist.init_process_group(backend, rank =rank, world_size =size)\nfn(rank, size)\n接下来使用“torch.multiprocessing”开启多个进程，本例中共开启了 4 个进程：",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "已同步。\n（6）DeepSpeed 初始化：这是最关键的一步。通过 deepspeed.initialize 函数，可以将模型、优\n化器、参数和先前构建的 DeepSpeed 配置传递给库，进行初始化。这个函数会返回一个已经根据\nDeepSpeed 配置进行了优化的模型和优化器。\n（7）梯度检查点：对于特别大的模型，梯度检查点是一种节省显存的技巧，即只在需要时计\n算模型的中间梯度。 如果用户启用了这个选项， 则会调用model.gradient_checkpointing_enable() 方\n法来实现相关功能。\n具体代码如下所示："
      ],
      "response_time": 4.8175225257873535,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.2325581395348837,
        "rouge2": 0.14285714285714288,
        "rougeL": 0.20930232558139536,
        "bleu": 0.07477860209616596,
        "semantic_similarity": 0.5799599885940552
      }
    },
    {
      "sample_id": "0059",
      "question": "DeepSpeed 支持哪些类型的 ZeRO 分片机制？",
      "ground_truth": "ZeRO-0、ZeRO-1、ZeRO-2、ZeRO-3 以及 ZeRO-Infinity。",
      "predicted_answer": "DeepSpeed 支持包括 ZeRO-0、ZeRO-1、ZeRO-2、ZeRO-3 以及 ZeRO-Infinity 在内的多种类型 ZeRO 分片机制。[2][4]",
      "ground_context": [
        "DeepSpeed 中也支持多种类型 ZeRO 的分片机制，包括ZeRO-0、ZeRO-1、ZeRO-2、ZeRO-3 以及 ZeRO-Infinity。"
      ],
      "retrieved_contexts": [
        "（3） 对模型参数进行分区， 图4.17 中的 Pos+g+p 部分。 此时， 每个计算设备所需内存是16Φ\nN B。\n当 N 比较大时，每个计算设备占用内存趋向于 0。\n图 4.17 三种 ZeRO 方法的单个设备内存占用\n在 DeepSpeed 框架中，Pos 对应 Zero-1，Pos+g 对应 Zero-2，Pos+g+p 对应 Zero-3。文献 [175]\n中也对 ZeRO 优化方法所带来的通信量增加的情况进行了分析，Zero-1 和 Zero-2 对整体通信量没\n有影响，虽然对通信有一定延迟影响，但是整体性能受到的影响很小。Zero-3 所需的通信量则是\n正常通信量的 1.5 倍。\nPyTorch中也实现了 ZeRO 优化方法， 可以使用ZeroRedundancyOptimizer 调用， 也可与 “torch.nn.parallel.DistributedDataParallel”\n结合使用， 以减少每个计算设备的内存峰值消耗。 使用ZeroRedundancyOptimizer 的参考代码如下\n所示：",
        "132 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n{\n\"optimizer\": {\n\"type\": \"OneBitAdam\",\n\"params\": {\n\"lr\": 0.001,\n\"betas\": [\n0.8,\n0.999\n],\n\"eps\": 1e-8,\n\"weight_decay\": 3e-7,\n\"freeze_step\": 400,\n\"cuda_aware\": false,\n\"comm_backend_name\": \"nccl\"\n}\n}\n...\n}\nDeepSpeed 中也支持多种类型 ZeRO 的分片机制， 包括ZeRO-0、ZeRO-1、ZeRO-2、ZeRO-3 以及\nZeRO-Infinity。ZeRO-0 禁用所有类型的分片， 仅将DeepSpeed 当作分布式数据并行使用；ZeRO-1\n对优化器状态进行分片，占用内存为原始的 1/4，通信容量与数据并行性相同；ZeRO-2 对优化器\n状态和梯度进行分片，占用内存为原始的 1/8，通信容量与数据并行性相同；ZeRO-3 对优化器状\n态、梯度及模型参数进行分片，内存减少与数据并行度和复杂度成线性关系，同时通信容...",
        "130 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n通信可以并行。通过减少通信量和增加局部性与并行性，数据并行通信的有效带宽有效增大。\n图4.29 给出了 DeepSpeed 3D 并行策略示意图。图中给出了 32 个计算设备进行 3D 并行的例\n子。神经网络的各层分为 4 个流水线阶段。每个流水线阶段中的层在 4 个张量并行计算设备之间\n进一步划分。最后，每个流水线阶段有两个数据并行实例，使用 ZeRO 内存优化在这 2 个副本之\n间划分优化器状态量。\n~ ~\n~\n~\n~\n~\n~~\n图 4.29 DeepSpeed 3D 并行策略示意图 [177]\nDeepSpeed 软件架构如图4.30 所示，主要包含以下三部分。\n（1）API：DeepSpeed 提供了易于使用的 API 接口，简化了训练模型和推断的过程。用户只需\n调用几个 API 接口即可完成任务。 通过 “initialize” 接口可以初始化引擎， 并在参数中配置训练参\n数、优化技术等。这些配置参数通常保存在名为“ds_config.json”的文件中。\n（2）RunTime：RunTime 是 DeepSp...",
        "数）和 MT-NLG[134]（5400 亿个参数）都采用 DeepSpeed 框架完成训练。\nDeepSpeed 的主要优势在于支持大规模神经网络模型、提供了更多的优化策略和工具。Deep-\nSpeed 通过实现三种并行方法的灵活组合， 即ZeRO 支持的数据并行、 流水线并行和张量并行， 可\n以应对不同工作负载的需求。 特别是通过3D 并行性的支持，DeepSpeed 可以处理具有万亿个参数\n的超大规模模型。DeepSpeed 还引入了 ZeRO-Offload，使单个 GPU 能够训练比其显存容量大 10\n倍的模型。为了充分利用 CPU 和 GPU 的内存来训练大语言模型， DeepSpeed 还扩展了 ZeRO-2。\n此外，DeepSpeed 还提供了稀疏注意力核 （Sparse Attention Kernel） ， 支持处理包括文本、 图像和语\n音等长序列输入的模型。DeepSpeed 还集成了 1 比特 Adam 算法（1-bit Adam） ，该算法可以只使\n用原始 Adam 算法 1/5 的通信量，达到与 Adam 类似的收敛率，显著提高分布式训练的效率，降\n低通信开...",
        "态的大小对其进行排序，然后根据每个 GPU 的剩余内存大小将张量分发到不同的 GPU。分布遵\n循一个原则：剩余内存较大的 GPU 优先接收新的分布张量。\n通过这种方式，可以将张量的缩放因子与张量一并顺利分发，同时降低通信和计算复杂性。\n图10.12展示了在包含和不包含缩放因子的情况下，ZeRO 张量分片方式的差异。ZeRO 张量分区方\n式可以分为两种：有缩放因子和无缩放因子。左图展示了原始的高精度 ZeRO 方法，其中一个张\n量被分割成多个分区后分配到不同的设备上。右图展示了提出的 FP8 ZeRO 方法，该方法将每个\n张量的完整副本分配到设备上，同时保留并考虑张量的缩放因子。"
      ],
      "response_time": 1.3902459144592285,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 0.5
      },
      "answer_quality_metrics": {
        "rouge1": 0.8333333333333333,
        "rouge2": 0.8181818181818181,
        "rougeL": 0.8333333333333333,
        "bleu": 0.5283419944690572,
        "semantic_similarity": 0.7799792885780334
      }
    },
    {
      "sample_id": "0060",
      "question": "What optimizer class is used in the document?",
      "ground_truth": "torch.optim.Adam",
      "predicted_answer": "The optimizer class used in the document is `torch.optim.Adam` [1][5]。",
      "ground_context": [
        "optimizer_class=torch.optim.Adam, # ЇልਔAdam"
      ],
      "retrieved_contexts": [
        "ddp_model.parameters(),\noptimizer_class=torch.optim.Adam, # ЇልਔAdam\nlr=0.01\n)\nelse:\noptimizer = torch.optim.Adam(ddp_model.parameters(), lr =0.01)\n# భཟԮѬ\noutputs = ddp_model(torch.randn(20, 2000).to(rank))\nlabels = torch.randn(20, 2000).to(rank)\n#ཟԮѬ\nloss_fn(outputs, labels) .backward()\n#ྍҕඔ\nprint_peak_memory(\"Max memory allocated before optimizer step()\" , rank)\noptimizer.step()\nprint_peak_memory(\"Max memory allocated after optimizer step()\" , rank)\nprint(f\"params sum is: {sum(model.parameters(...",
        "大规模语言模型：从理论到实践 155\nGiven the classification task definition and the class labels, generate an input that corresponds\n,→ to each of the class labels. If the task doesn't require input, just generate the correct\n,→ class label.\nTask: Classify the sentiment of the sentence into positive, negative, or mixed.\nClass label: mixed\nSentence: I enjoy the flavor of the restaurant but their service is too slow.\nClass label: Positive\nSentence: I had a great day today. The weather was beautiful and I spent ti...",
        "},\n{\n\"params\": [\np for n, p in model.named_parameters()\nif (any(nd in n\nfor nd in no_decay_name_list) and p.requires_grad)\n],\n\"weight_decay\": 0.0,\n},\n]\nreturn optimizer_grouped_parameters",
        "yourself unable to manage your emotions, it is strongly\nrecommended to seek professional assistance promptly.\nHuman: Depression is just temporary bad mood, isn’t it?\nWhat should be taken into consideration in daily life when\ndealing with depression? Assistant: That’s right. How-\never, if you can’t control it on your own for a long time and\nit affects your normal work and life, you should seek help\nfrom professional institutions in a timely manner. Here\nare some things that may help improve the d...",
        "138 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\nfrom transformers import get_scheduler\nfrom deepspeed.ops.adam import DeepSpeedCPUAdam, FusedAdam\n#ఖ\noptimizer_grouped_parameters = get_optimizer_grouped_parameters(\nmodel, args .weight_decay, args .learning_rate)\nAdamOptimizer = DeepSpeedCPUAdam if args.offload else FusedAdam\noptimizer = AdamOptimizer(optimizer_grouped_parameters,\nlr=args.learning_rate,\nbetas=(0.9, 0.95))\nnum_update_steps_per_epoch = math.ceil(\nlen(train_dataloader) / args.gradient_accumulation_..."
      ],
      "response_time": 0.6374585628509521,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.375,
        "rouge2": 0.2857142857142857,
        "rougeL": 0.375,
        "bleu": 0.161692143534558,
        "semantic_similarity": 0.7851157188415527
      }
    },
    {
      "sample_id": "0061",
      "question": "如何在使用 ZeRO-3 的基础上配置优化器以便将状态转移到 CPU 中，并且选择适当的优化器类型？",
      "ground_truth": "在使用 ZeRO-3 的基础上，可以通过在配置文件中设置 'offload_optimizer' 为 'cpu' 来将优化器状态转移到 CPU 中。同时，可以根据是否选择了 offload 选项来决定使用 DeepSpeedCPUAdam 还是 FusedAdam 作为优化器。",
      "predicted_answer": "要在使用 ZeRO-3 的基础上配置优化器以便将状态转移到 CPU 中，可以在配置文件中添加 `offload_optimizer` 参数并将 `device` 设置为 `\"cpu\"`，如下所示：\n\n```json\n{\n    \"zero_optimization\": {\n        \"stage\": 3,\n        \"offload_optimizer\": {\n            \"device\": \"cpu\"\n        }\n    },\n    ...\n}\n```\n\n关于选择合适的优化器类型，可以考虑使用 AdamW 作为优化器类型，其配置如下：\n\n```json\n{\n    \"optimizer\": {\n        \"type\": \"AdamW\",\n        \"params\": {\n            \"lr\": 0.001,\n            \"betas\": [\n                0.8,\n                0.999\n            ],\n            \"eps\": 1e-8,\n            \"weight_decay\": 3e-7\n        }\n    },\n    ...\n}\n```\n\n这些配置可以帮助你在使用 ZeRO-3 的同时将优化器的状态转移到 CPU 上。[1][2]",
      "ground_context": [
        "\"zero_optimization\": { \"stage\": 3, \"offload_optimizer\": { \"device\": \"cpu\" } }",
        "AdamOptimizer = DeepSpeedCPUAdam if args.offload else FusedAdam"
      ],
      "retrieved_contexts": [
        "大规模语言模型：从理论到实践 133\n以下是 DeepSpeed 使用 ZeRO-3 配置参数的样例：\n{\n\"zero_optimization\": {\n\"stage\": 3,\n},\n\"fp16\": {\n\"enabled\": true\n},\n\"optimizer\": {\n\"type\": \"AdamW\",\n\"params\": {\n\"lr\": 0.001,\n\"betas\": [\n0.8,\n0.999\n],\n\"eps\": 1e-8,\n\"weight_decay\": 3e-7\n}\n},\n...\n}\n如果希望在 ZeRO-3 的基础上继续使用 ZeRO-Infinity 将优化器状态和计算转移到 CPU 中， 则\n可以在配置文件中按照如下方式配置：\n{\n\"zero_optimization\": {\n\"stage\": 3,\n\"offload_optimizer\": {\n\"device\": \"cpu\"\n}\n},\n...\n}\n甚至可以进一步将模型参数也装载到 CPU 内存中，在配置文件中按照如下方式配置：",
        "132 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n{\n\"optimizer\": {\n\"type\": \"OneBitAdam\",\n\"params\": {\n\"lr\": 0.001,\n\"betas\": [\n0.8,\n0.999\n],\n\"eps\": 1e-8,\n\"weight_decay\": 3e-7,\n\"freeze_step\": 400,\n\"cuda_aware\": false,\n\"comm_backend_name\": \"nccl\"\n}\n}\n...\n}\nDeepSpeed 中也支持多种类型 ZeRO 的分片机制， 包括ZeRO-0、ZeRO-1、ZeRO-2、ZeRO-3 以及\nZeRO-Infinity。ZeRO-0 禁用所有类型的分片， 仅将DeepSpeed 当作分布式数据并行使用；ZeRO-1\n对优化器状态进行分片，占用内存为原始的 1/4，通信容量与数据并行性相同；ZeRO-2 对优化器\n状态和梯度进行分片，占用内存为原始的 1/8，通信容量与数据并行性相同；ZeRO-3 对优化器状\n态、梯度及模型参数进行分片，内存减少与数据并行度和复杂度成线性关系，同时通信容...",
        "118 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n零冗余优化器（Zero Redundancy Data Parallelism ，ZeRO） 的目标是针对模型状态的存储进行\n去除冗余的优化[173–175]。ZeRO 使用分区的方法， 即将模型状态量分割成多个分区， 每个计算设备\n只保存其中的一部分。这样整个训练系统内只需要维护一份模型状态，减少了内存消耗和通信开\n销。具体来说，如图4.17 所示，ZeRO 包含以下三种方法。\n（1） 对Adam 优化器状态进行分区， 图4.17 中的 Pos 部分。 模型参数和梯度依然是每个计算设\n备保存一份。此时，每个计算设备所需内存是 4Φ + 12Φ\nN 字节，其中 N 是计算设备总数。当 N 比\n较大时，每个计算设备占用内存趋向于 4ΦB，也就是 16ΦB 的 1\n4 。\n（2）对模型梯度进行分区，图4.17 中的 Pos+g 部分。模型参数依然是每个计算设备保存一份。\n此时，每个计算设备所需内存是 2Φ + 2Φ+12Φ\nN 字节。当 N 比较大时，每个计算设备占用内存趋向\n于 2ΦB，也就是 16ΦB 的 1/8。\n（3...",
        "（3） 对模型参数进行分区， 图4.17 中的 Pos+g+p 部分。 此时， 每个计算设备所需内存是16Φ\nN B。\n当 N 比较大时，每个计算设备占用内存趋向于 0。\n图 4.17 三种 ZeRO 方法的单个设备内存占用\n在 DeepSpeed 框架中，Pos 对应 Zero-1，Pos+g 对应 Zero-2，Pos+g+p 对应 Zero-3。文献 [175]\n中也对 ZeRO 优化方法所带来的通信量增加的情况进行了分析，Zero-1 和 Zero-2 对整体通信量没\n有影响，虽然对通信有一定延迟影响，但是整体性能受到的影响很小。Zero-3 所需的通信量则是\n正常通信量的 1.5 倍。\nPyTorch中也实现了 ZeRO 优化方法， 可以使用ZeroRedundancyOptimizer 调用， 也可与 “torch.nn.parallel.DistributedDataParallel”\n结合使用， 以减少每个计算设备的内存峰值消耗。 使用ZeroRedundancyOptimizer 的参考代码如下\n所示：",
        "大规模语言模型：从理论到实践 127\n表 4.1 GLOO、MPI 和 NCCL 在 CPU 和 GPU 环境下对通信原语的支持情况\n通信原语 GLOO MPI NCCL\nCPU GPU CPU GPU CPU GPU\nSend ✓ × ✓ ? × ✓\nReceive ✓ × ✓ ? × ✓\nBroadcast ✓ ✓ ✓ ? × ✓\nScatter ✓ × ✓ ? × ✓\nReduce ✓ × ✓ ? × ✓\nAll Reduce ✓ ✓ ✓ ? × ✓\nGather ✓ × ✓ ? × ✓\nAll Gather ✓ × ✓ ? × ✓\nReduce Scatter × × × × × ✓\nAll To All × × ✓ ? × ✓\nBarrier ✓ × ✓ ? × ✓\n以 PyTorch 为例，介绍如何使用上述通信原语完成多计算设备间通信。先使用“ torch.\ndistributed”初始化分布式环境：\nimport os\nfrom typing import Callable\nimport torch\nimport torch.distributed as dist\ndef..."
      ],
      "response_time": 5.200587272644043,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.2745098039215686,
        "rouge2": 0.163265306122449,
        "rougeL": 0.23529411764705882,
        "bleu": 0.08215083473398258,
        "semantic_similarity": 0.8888741135597229
      }
    },
    {
      "sample_id": "0062",
      "question": "在使用分布式训练时，如何选择合适的数据采样器？",
      "ground_truth": "在分布式训练环境中，需要使用 DistributedSampler 确保每个进程或节点获得数据的一个不重复的子集，而在单机环境中则可以使用普通的随机或顺序采样器。",
      "predicted_answer": "在使用分布式训练时，应选择 `DistributedSampler` 来实现数据的分布式采样[3]。如果是在单机环境中，则可以选择 `RandomSampler` 进行随机采样或 `SequentialSampler` 进行顺序采样[3]。判断条件是检查 `args.local_rank` 是否为 -1，如果不是则使用分布式采样器[3]。",
      "ground_context": [
        "使用 PyTorch 和 transformers 库来设置预训练模型的数据加载器， 以实现在单机或多机分布式训练环境中对数据的加载和采样。",
        "通过检查 args.local_rank 是否为 −1，代码会选择使用普通的采样器（单机）还是分布式采样器 （多机） 。DistributedSampler 确保在分布式训练环境中， 每个进程或节点都能获得数据的一个不重复的子集。"
      ],
      "retrieved_contexts": [
        "重复的子集，这使得分布式训练变为可能。而在单机环境中，使用常规的随机或顺序采样器即可。\n具体代码如下所示：",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "大规模语言模型：从理论到实践 135\npip install deepspeed\n1. 训练数据配置\n使用 PyTorch 和 transformers 库来设置预训练模型的数据加载器， 以实现在单机或多机分布式\n训练环境中对数据的加载和采样。需要导入的模块如下。\n• DataLoader 是 PyTorch 提供的工具，用于从数据集加载数据到模型进行训练或评估。\n• RandomSampler 和 SequentialSampler 是 PyTorch 提供的两种采样器。RandomSampler 随机采\n样数据，而 SequentialSampler 顺序采样数据。\n• DistributedSampler 是用于分布式训练的数据采样器。\n• default_data_collator 是 transformers 库提供的默认数据收集器，用于将多个样本整合为一个\n批量数据。\n• create_pretrain_dataset 是一个自定义函数，用于创建预训练数据集。\n通过检查 args.local_rank 是否为 −1，代码会选择使用普通的采样器（单机）还是分布式采样\n器 （多...",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "4. 分布式训练\n随着大语言模型参数量和所需训练数据量的急速增长，单个机器上有限的资源已无法满足其\n训练的要求。需要设计分布式训练系统来解决海量的计算和内存资源需求问题。在分布式训练系\n统环境下，需要将一个模型训练任务拆分成多个子任务，并将子任务分发给多个计算设备，从而\n解决资源瓶颈。如何才能利用数万个计算加速芯片的集群，训练千亿甚至万亿参数规模的大语言\n模型？这其中涉及集群架构、并行策略、模型架构、内存优化、计算优化等一系列的技术。\n本章将介绍分布式机器学习系统的基础概念、分布式训练的并行策略、分布式训练的集群架\n构，并以 DeepSpeed 为例，介绍如何在集群上训练大语言模型。\n4.1 分布式训练概述\n分布式训练（Distributed Training）是指将机器学习或深度学习模型训练任务分解成多个子任\n务， 并在多个计算设备上并行训练。 图4.1 给出了单个计算设备和多个计算设备的示例， 这里计算\n设备可以是中央处理器（ Central Processing Unit ，CPU） 、图形处理器（Graphics Processing Unit ，\nGPU） 、张量处理器（Te..."
      ],
      "response_time": 2.170534372329712,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 0.3333333333333333
      },
      "answer_quality_metrics": {
        "rouge1": 0.18181818181818182,
        "rouge2": 0.0,
        "rougeL": 0.18181818181818182,
        "bleu": 0.04397806956215858,
        "semantic_similarity": 0.8408037424087524
      }
    },
    {
      "sample_id": "0063",
      "question": "在使用参数服务器架构进行模型训练时，如何确保模型能够正确处理文本的长度和填充设置？",
      "ground_truth": "在参数服务器架构中，训练服务器需要提供大量计算资源，同时在加载和配置模型时，需要确保词元分析器能够处理各种文本的长度，并进行适当的填充设置。",
      "predicted_answer": "根据提供的文档，我无法回答这个问题。文档中未提及关于参数服务器架构下处理文本长度和填充的具体方法。",
      "ground_context": [
        "参数服务器需要提供充足的内存资源和通信资源，训练服务器需要提供大量的计算资源。",
        "使用 transformers 库加载和配置 LLaMA 模型及其相关的词元分析器，从 transformers 库中导入 LLaMA 模型、相应的词元分析器和模型配置后，使用from_pretrained 方法加载预训练的 LLaMA 模型、词元分析器和配置。"
      ],
      "retrieved_contexts": [
        "122 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n图 4.19 NVIDIA HGX H100 8-GPU NVLink 和 NVSwitch 连接框图 [167]\n4.3.2 参数服务器架构\n参数服务器（Parameter Server，PS）架构的分布式训练系统中有两种服务器角色：训练服务\n器和参数服务器。参数服务器需要提供充足的内存资源和通信资源，训练服务器需要提供大量的\n计算资源。图4.20 为参数服务器的分布式训练集群的示意图。该集群包括两个训练服务器和两个\n参数服务器。 假设有一个可分为两个参数分区的模型， 每个分区由一个参数服务器负责参数同步。\n在训练过程中， 每个训练服务器都拥有完整的模型， 将分配到此服务器的训练数据集切片 （Dataset\nShard）并进行计算，将得到的梯度推送到相应的参数服务器。参数服务器会等待两个训练服务器\n都完成梯度推送，再计算平均梯度并更新参数。之后，参数服务器会通知训练服务器拉取最新的\n参数，并开始下一轮训练迭代。\n图 4.20 参数服务器的分布式训练集群的示意图 [167]",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "大规模语言模型：从理论到实践 123\n参数服务器架构的分布式训练过程可以细分为同步训练和异步训练两种模式。\n• 同步训练： 训练服务器在完成一个小批次的训练后， 将梯度推送给参数服务器。 参数服务器\n在收到所有训练服务器的梯度后，进行梯度聚合和参数更新。\n• 异步训练： 训练服务器在完成一个小批次的训练后， 将梯度推送给参数服务器。 参数服务器\n不再等待接收所有训练服务器的梯度，而是直接基于已收到的梯度进行参数更新。\n在同步训练的过程中，参数服务器会等待所有训练服务器完成当前小批次的训练，有诸多的\n等待或同步机制，导致整个训练速度较慢。异步训练去除了训练过程中的等待机制，训练服务器\n可以独立进行参数更新， 极大地加快了训练速度。 引入异步更新的机制会导致训练效果有所波动。\n应根据具体情况和需求选择适合的训练模式。\n4.3.3 去中心化架构\n去中心化 （Decentralized Network ）架构采用集合通信实现分布式训练系统。在去中心化\n架构中，没有中央服务器或控制节点，而是由节点之间进行直接通信和协调。这种架构的好处\n是可以减少通信瓶颈，提高系统的可扩展性。由于节点之间可以并行...",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "大规模语言模型：从理论到实践 219\n动相关领域的发展，也为开发者在大模型强化学习领域的创新提供了支持。本节将介绍使用 verl\n框架进行大模型中强化学习的实践。\n1. 训练脚本与参数配置\n以推理任务为例，我们按照官方教程选用 Qwen2.5-0.5B-Instruct 模型在 GSM8K 数据集上进\n行强化学习训练。下面是一些关键的 RL 训练参数：\n在近端策略优化算法中，ppo_mini_batch_size 表示小批次的大小。在训练过程中，我们并\n不会一次性使用整个训练集来更新模型参数，而是将训练集划分为多个小批次。这个参数设置\n为 64，意味着每次从训练集中选取 64 个样本组成一个小批次，用于计算梯度和更新演员模型\n的参数。通过使用小批次，可以减少内存的占用，并且在一定程度上提高训练的稳定性和效率。\nppo_micro_batch_size_per_gpu 指的是每个 GPU 上的微批次大小。在多 GPU 训练环境下，为\n了更高效地利用 GPU 资源， 会将小批次进一步划分为微批次。 这里设置为4， 表示每个GPU 每次\n处理 4 个样本的微批次。 这种细粒度的划分有助于在G..."
      ],
      "response_time": 0.9440197944641113,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.04227892872917365,
        "semantic_similarity": 0.7441999316215515
      }
    },
    {
      "sample_id": "0064",
      "question": "如何计算训练中的 TFLOPs?",
      "ground_truth": "train_tflops = train_flops_per_iteration / (e2e_time * gpus_per_model * (10**12))",
      "predicted_answer": "在训练过程中，TFLOPs（每秒万亿次浮点运算）的计算方法如下：\n\n```\ntrain_tflops = train_flops_per_iteration / (e2e_time * gpus_per_model * (10**12))\n```\n\n这里的 `train_flops_per_iteration` 表示每次迭代的浮点运算次数，`e2e_time` 是端到端的时间（以秒为单位），`gpus_per_model` 是每个模型使用的GPU数量。[1]",
      "ground_context": [
        "train_tflops = train_flops_per_iteration / (e2e_time * gpus_per_model * (10**12))"
      ],
      "retrieved_contexts": [
        "checkpoint_activations_factor, batch_size, seq_length, hf_config)\ntrain_tflops = train_flops_per_iteration / (e2e_time * gpus_per_model *\n(10**12))\nparam_string = f\"{params_in_billions:.3f} B\" if params_in_billions != 0 else \"NA\"\nprint(\nf\"Model Parameters: {param_string}, Latency: {e2e_time:.2f}s,\nTFLOPs: {train_tflops:.2f}, Samples/sec: {samples_per_second:.2f},\nTime/seq {e2e_time/batch_size:.2f}s, Batch Size: {batch_size},\nSequence Length: {seq_length}\"\n)\ndef save_hf_format(model, tokenizer, a...",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "i=1∼πθold (O|q)[ 1\nG\nGX\ni=1\n1\n|oi|\n|oi|X\nt=1\n[min( πθ(oi,t|q, oi,<t)\nπθold(oi,t|q, oi,<t)\nˆAi,t,\nclip\n\u0012 πθ(oi,t|q, oi,<t)\nπθold(oi,t|q, oi,<t) , 1 − ϵ, 1 + ϵ\n\u0013\nˆAi,t)] − βDKL [πθ||πref]]\n(6.34)\n在这个公式中 πθ 代表当前正在优化的策略模型，其参数为 θ，πθold 是旧的策略模型，用于提供参\n考和对比。G 表示组大小，即从旧策略 πθold 中抽取的多个输出 oi 的数量。每个 oi 都是一个完整\n的输出序列，|oi| 表示序列 oi 的长度。 ˆAi,t 是基于组内奖励计算得到的优势值，它衡量了在时间\n步 t 采取动作 oi,t 相对于平均水平的优势程度，用于指导策略的更新。ϵ 和 β 是超参数。ϵ 用于控\n制梯度剪切，防止策略更新幅度过大导致不稳定，β 则控制 KL 散度 DKL 的权重，DKL [πθ||πref]\n用于约束当前策略 πθ 和参考策略 πref 之间的差异，确保策略不会偏...",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "seq_length = args.max_seq_len\nbatch_size = args.per_device_train_batch_size\nsamples_per_second = batch_size / e2e_time\ncheckpoint_activations_factor = 4 if args.gradient_checkpointing else 3\nif args.lora_dim > 0:\nk = args.lora_dim * 2 / hidden_size\ncheckpoint_activations_factor -= (1 - k)\nhf_model._num_params = sum([\np.ds_numel if hasattr (p, \"ds_tensor\") else p.numel()\nfor p in hf_model.parameters()\n])\nparams_in_billions = hf_model._num_params / (1e9)\n# ໓ང[169]ෘ࿞਀FLOPSൔ\ntrain_flops_per_iteratio..."
      ],
      "response_time": 2.6923718452453613,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.6842105263157895,
        "rouge2": 0.6666666666666666,
        "rougeL": 0.6842105263157895,
        "bleu": 0.27785117304047363,
        "semantic_similarity": 0.8225629925727844
      }
    },
    {
      "sample_id": "0065",
      "question": "在生成任务输入和输出的步骤中，如何处理非分类任务？",
      "ground_truth": "对于非分类任务，使用输入优先的方法，先根据任务产生输入，再根据任务指令和输入生成输出。",
      "predicted_answer": "在生成任务输入和输出的步骤中，对于非分类任务，使用输入优先的方法，即先根据任务产生输入，再根据任务指令和输入生成输出。[1]",
      "ground_context": [
        "对于非分类任务，使用输入优先的方法，先根据任务产生输入，再根据任务指令和输入生成输出。"
      ],
      "retrieved_contexts": [
        "154 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n分类。\n步骤 3：生成任务输入和输出\n通过步骤 1，语言模型已经生成了面向新任务的指令，然而指令数据中还没有相应的输入和\n输出。 本步骤将为此前生成的指令生成输入和输出， 让指令数据变得完整。 与之前的步骤相同， 本\n步骤同样使用语境学习，使用来自其他任务的“指令” “输入” “输出”上下文示例做提示，预训\n练模型就可以为新任务生成输入–输出对。针对不同的任务类别， 分别使用“输入优先”或“输出\n优先”方法：对于非分类任务，使用输入优先的方法，先根据任务产生输入，再根据任务指令和\n输入生成输出；而对于分类任务，为了避免模型过多地生成某些特定类别的输入（而忽略其他的\n类别） ， 使用输出优先的方法， 先产生所有可能的输出标签， 再根据任务指令和输出， 补充相应的\n输入。\n“输入优先”提示模板如下所示：\nCome up with examples for the following tasks. Try to generate multiple examples when possible. If\n,→ the task d...",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "Task 5: {instruction for existing task 5}\nTask 6: {instruction for existing task 6}\nTask 7: {instruction for existing task 7}\nTask 8: {instruction for existing task 8}\nTask 9:\n步骤 2：确定指令是否代表分类任务\n由于后续对于分类任务和非分类任务有两种不同的处理方法，因此需要在本步骤对指令是否\n为分类任务进行判断，同样是利用拼接几个上下文示例的方法让模型自动判断任务类型是否是",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "大规模语言模型：从理论到实践 421\n分类任务（Classification） 是将输入样本分为不同的类别或标签的机器学习任务。 很多自然语\n言处理任务都可以转换为分类任务，包括分词、词性标注、情感分析等。例如情感分析中的一个\n常见任务就是判断输入的评论是正面评论还是负面评论。这个任务就转换成了二分类问题。再比\n如新闻类别分类任务的目标就是根据新闻内容将新闻划分为经济、军事、体育等类别，可以使用\n多分类机器学习算法完成。分类任务通常采用精确率、召回率、准确率、PR 曲线等评估指标，利\n用测试数据，根据系统预测结果与真实结果之间的对比，计算各类指标来对算法性能进行评估。\n回归任务（Regression） 是根据输入样本预测连续数值的机器学习任务。 一些自然语言处理任\n务都转换为回归任务进行建模，包括情感强度判断、作文评分、垃圾邮件识别等。例如作文评分\n任务就是对于给定的作文输入，按照评分标准自动给出 1∼10 分的评分结果，其目标是与人工评\n分尽可能接近。回归任务的评估指标主要衡量模型预测值与真实值之间的差距，主要包括平均绝\n对误差、平均绝对百分比误差、均方误差、均方误差根、均方误差对数..."
      ],
      "response_time": 1.2200994491577148,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.5838779546149977,
        "semantic_similarity": 0.9525454640388489
      }
    },
    {
      "sample_id": "0066",
      "question": "指令微调在模型获取各类关键能力的进程中发挥着什么作用？",
      "ground_truth": "不可或缺的作用",
      "predicted_answer": "指令微调在模型获取各类关键能力的进程中发挥着不可或缺的作用，它能够使模型具备指令遵循（Instruction Following）能力，从而能够完成各类预先设计的任务，并且可以在零样本情况下处理诸多下游任务[3]。",
      "ground_context": [
        "然而，指令微调在模型获取各类关键能力的进程中却发挥着不可或缺的作用。"
      ],
      "retrieved_contexts": [
        "据构造的难度。\n5.1.4 指令微调训练策略\n尽管从整体流程来看，指令微调的步骤并不繁杂，其训练代码甚至与预训练阶段的代码大体\n相同，然而，指令微调在模型获取各类关键能力的进程中却发挥着不可或缺的作用。此外，开源",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "5. 指令微调\n指令微调又称有监督微调，是指在预训练大语言模型的基础上，通过使用有标注的自然语言\n形式的数据， 对模型参数进行微调， 使模型具备指令遵循（Instruction Following） 能力， 能够完成\n各类预先设计的任务，并可以在零样本情况下处理诸多下游任务。经过海量数据预训练后的语言\n模型虽然具备了大量的 “知识” ， 但是由于其训练时的目标仅是进行下一个词的预测， 因此不能够\n理解并遵循人类自然语言形式的指令。为了使模型具有理解并响应人类指令的能力，还需要使用\n指令数据对其进行调整。如何构造指令数据，如何高效低成本地进行指令微调训练，以及如何在\n语言模型基础上进一步扩大上下文等问题，是大语言模型在指令微调阶段的核心。\n本章先介绍大语言模型指令微调训练方法，在此基础上介绍高效模型微调及模型上下文窗口\n扩展方法，最后介绍指令微调的代码实践。\n5.1 指令微调训练\n指令微调具体训练过程并不复杂，主要分为如下三个步骤： （ 1）针对每一项任务去明确地\n定义相应的自然语言形式的指令或者提示，这些指令或提示对任务目标以及输出要求进行清晰描\n述； （2）把训练数据调整成包含指令以...",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "练数据对预训练模型进行微调操作。从训练方式的角度来看，指令微调与预训练大体上较为相似，\n不过指令微调的目标函数往往只是针对输出部分来计算损失。\n本节将从指令微调数据构造、数据评估和影响、指令微调训练策略以及开源指令微调数据等\n方面进行介绍。\n5.1.1 指令微调数据\n指令微调数据通常由文本对构成，包含“指令输入”与“答案输出”两个关键部分。 “指令输\n入” ， 是指人们向模型提出的各类请求， 包含定义精准、 清晰的指令或者提示信息， 其核心作用在\n于详细阐释任务的目标究竟是什么，以及明确规定输出需要满足的各项要求。指令涵盖的范畴极\n为广泛，包括问题回答、信息分类、内容总结、文本改写等。 “答案输出” ，则是期望模型依据所"
      ],
      "response_time": 1.484337329864502,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.02369695807350934,
        "semantic_similarity": 0.5568110346794128
      }
    },
    {
      "sample_id": "0067",
      "question": "Flan 2022 在构建过程中使用了哪些技术？",
      "ground_truth": "任务混合和输入反转等技术。",
      "predicted_answer": "Flan 2022 在构建过程中使用了任务混合和输入反转等技术[1][2]。具体来说，任务混合是指将来自不同任务的示例混合在一起进行训练，而输入反转则是指将原始输入中的某些元素或部分进行反转或重新排列，以生成新的输入，从而增强模型的泛化能力和鲁棒性[2]。",
      "ground_context": [
        "Flan 2022 构建过程中还使用了任务混合和输入反转等技术。"
      ],
      "retrieved_contexts": [
        "了四个不同的指令输入模板，包括零样本、少量样本、CoT 模板。Flan 2022 构建过程中还使用了\n任务混合和输入反转等技术。输入反转（Input Inversion）是指将原始输入中的某些元素或部分进\n行反转或重新排列，以生成新的输入，用于增强模型的泛化能力和鲁棒性。例如，在对话任务中，\n将对话历史中的上下文和响应进行反转，以测试模型在不同输入顺序下的表现。在代码生成任务\n中，可以将代码和问题进行反转，在链式推理任务（Chain-of-Thought，CoT）中，将查询、答案\n和解释进行反转。任务混合（Task Mixing）则将来自不同任务的示例混合在一起进行训练，其目\n标旨在增强模型的泛化能力和适应不同任务的能力。\n文献 [188] 针对提升大语言模型在开放领域命名实体识别中的能力进行了研究。通过整合 54\n个现有的中英文命名实体识别数据集， 并经过两步规范化， 构建了B2NERD 数据集。 研究指出， 整\n合多个现有数据集的主要挑战在于实体定义的不一致性和模糊性。 例如， 有些数据集会区分 “时代\n广场” 这样的地点和 “巴黎” 这样的地缘政治实体， 而另一些数据集则将两者统...",
        "大规模语言模型：从理论到实践 151\n源数据集的质量情况，同时还要对数据的格式进行标准化处理，这涉及多方面细致的工作以及对\n不同数据原有特点的把握等，操作起来较为复杂且容易出现遗漏等情况。此外，大部分已有数据\n集都是为传统自然语言处理任务准备，并没有包含多样性的提示词，如何构造大量多样性且语义\n相同的提示词也是需要解决的难点。目前已经很多指令微调数据集采用这种方式进行构建。\nOIG（Open Instruction Generation）[184] 是一个大型指令微调数据集， 由LAION 社区成员创建，\n包含 30 个数据集和 4300 万条指令，包含使用来自多种数据源的数据增强创建的指令。它不仅涵\n盖标准数据集（如 Natural Questions 和 Natural Instructions） ，还涵盖与对话、总结、教育等相关的\n数据。Flan 2022[185] 数据集则是由五个部分组成， 分别是Flan 2021[186]、T0[16]、SUPER-NA TURAL\nINSTRUCTIONS[187]） 、CoT 数据集和对话数据集。它涵盖了多达 1836 个数据集。每个指...",
        "大规模语言模型：从理论到实践 7\n表 1.1 典型开源大语言模型汇总\n模型名称 发布时间 参数量（个） 模型类型 预训练数据量\nT5[19] 2019 年 10 月 110 亿 基础模型 1 万亿个词元\nPanGu-α[22] 2021 年 4 月 130 亿 基础模型 1.1 万亿个词元\nCPM-2[26] 2021 年 6 月 1980 亿 基础模型 2.6 万亿个词元\nCodeGen[27] 2022 年 3 月 160 亿 基础模型 5770 亿个词元\nGPT-NeoX-20B[28] 2022 年 4 月 200 亿 基础模型 825GB\nOPT[29] 2022 年 5 月 1750 亿 基础模型 1800 亿个词元\nGLM[30] 2022 年 10 月 1300 亿 基础模型 4000 亿个词元\nFlan-T5[23] 2022 年 10 月 110 亿 对话模型 -\nBLOOM[31] 2022 年 11 月 1760 亿 基础模型 3660 亿个词元\nBLOOMZ[32] 2022 年 11 月 1760 亿 对话模型 -\nOPT-IML[33] 2022 年 12...",
        "166 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n表 5.2 部分开源通用指令微调数据集的汇总信息\n指令数据集名称 发布单位 指令数据集规模 语言 是否公开\nAlpaca Data Standford Alpaca 5.2 万条 英文 公开\nAya Collection Cohere For AI 等 5.13 亿条 多语言 公开\nAya Dataset Cohere For AI 等 20.4 万条 多语言 公开\nBELLE 贝壳研究院 350 万条 中文 公开\nCOIG 北京智源研究院 19.11 万条 中文 公开\nDialogStudio Salesforce AI 87 个数据集 多语言 公开\nDolly Databricks 1.5 万条 英语 公开\nFirefly Y eungNLP 115 万条 中文 公开\nFlan 2022 Google Research 1836 个数据集 多语言 部分\nInstructionWild V2 新加坡国立大学 11 万条 中英文 公开\nLCCC 清华大学 1200 万条 中文 公开\nLMSYS-Chat-1M 加州大学伯...",
        "大语言模型：从理论到实践 291\n8.4.2 LangChain 框架\n大语言模型的调用相对简单， 仍需要完成大量的定制开发工作， 包括API 集成、 交互逻辑、 数\n据存储等。为了解决这个问题，从 2022 年开始，多家机构和个人陆续推出了大量开源项目，帮\n助开发者快速创建基于大语言模型的端到端应用程序或流程， 其中较为著名的是LangChain 框架。\nLangChain 框架是一种利用大语言模型的能力开发各种下游应用的开源框架，旨在为各种大语言\n模型应用提供通用接口，简化大语言模型应用的开发难度。它可以实现数据感知和环境交互，即\n能够使语言模型与其他数据源连接起来，并允许语言模型与其环境进行交互。\n本节将重点介绍 LangChain 框架以及其核心模块组成。\n1. LangChain 框架核心模块\n使用 LangChain 框架的核心目标是连接多种大语言模型 （如ChatGPT、LLaMA 等） 和外部资\n源（如 Google、Wikipedia、Notion 及 Wolfram 等） ，提供抽象组件和工具以在文本输入和输出之\n间进行接口处理。大语言模型和组件通过“链（Chain..."
      ],
      "response_time": 2.2076125144958496,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.07277006492157584,
        "semantic_similarity": 0.7708375453948975
      }
    },
    {
      "sample_id": "0068",
      "question": "What happens if args.global_rank is 0?",
      "ground_truth": "save_hf_format(model, tokenizer, args)",
      "predicted_answer": "If args.global_rank is 0, several actions occur based on the provided code snippets:\n\n- In `print_rank_0`, the message will be printed because the condition `if rank <= 0` evaluates to true when `rank` is 0 [1].\n- In `print_throughput`, the throughput of the model will also be printed under the same condition [2].\n\nThese functions check whether `rank` is less than or equal to 0 and only execute their operations (printing messages) when this condition is met.",
      "ground_context": [
        "if args.global_rank == 0:\nsave_hf_format(model, tokenizer, args)"
      ],
      "retrieved_contexts": [
        "if args.global_rank == 0:\nsave_hf_format(model, tokenizer, args)\nif args.zero_stage == 3:\n#ႿZero Stage 3 đૄॶGPUඔ\nsave_zero_three_model(model,\nargs.global_rank,\nargs.output_dir,\nzero_stage=args.zero_stage)\ndef print_rank_0(msg, rank =0):\nif rank <= 0 :\nprint(msg)\n#ႨႿյႆZero Stage 1ބStage 2๽๳ਈ\ndef print_throughput(hf_model, args, e2e_time, rank =0):\nif rank <= 0 :\nhf_config = hf_model.config\nnum_layers, hidden_size, vocab_size = get_hf_configs(hf_config)\ngpus_per_model = torch.distributed.get_world...",
        "start = time.time()\nbatch = to_device(batch, device)\noutputs = model(**batch, use_cache =False)\nloss = outputs.loss\nif args.print_loss:\nprint(\nf\"Epoch: {epoch}, Step: {step},\nRank: {torch.distributed.get_rank()}, loss = {loss}\"\n)\nmodel.backward(loss)\nmodel.step()\nend = time.time()\nif torch.distributed.get_rank() == 0:\nprint_throughput(model.model, args, end - start,\nargs.global_rank)\nif args.output_dir is not None:\nprint_rank_0('saving the final model ... ' , args .global_rank)\nmodel = convert_l...",
        "102 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\nclass DistributedSampler(Sampler):\ndef __init__(self, dataset, num_replicas =None, rank =None, shuffle =True, seed =0):\nif num_replicas is None:\nif not dist.is_available():\nraise RuntimeError(\"Requires distributed package to be available\" )\nnum_replicas = dist.get_world_size()\nif rank is None:\nif not dist.is_available():\nraise RuntimeError(\"Requires distributed package to be available\" )\nrank = dist.get_rank()\nself.dataset = dataset #ࠢ\nself.num_replicas = num_rep...",
        "def main():\nworld_size = 2\nprint(\"=== Using ZeroRedundancyOptimizer ===\" )\nmp.spawn(example,\nargs=(world_size, True),\nnprocs=world_size,\njoin=True)\nprint(\"=== Not Using ZeroRedundancyOptimizer ===\" )\nmp.spawn(example,\nargs=(world_size, False),\nnprocs=world_size,\njoin=True)\nif __name__==\"__main__\":\nmain()",
        "292 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n图 8.8 LangChain 模型输入/输出模块的基本流程\nPrompts 部分的主要功能是提示词模板、 提示词动态选择和输入管理。 提示词是指输入模型的\n内容。 该输入通常由模板、 示例和用户输入组成。LangChain 提供了几个类和函数， 使得构建和处\n理提示词更加容易。LangChain 中的 PromptTemplate 类可以根据模板生成提示词，它包含了一个\n文本字符串（模板） ，可以根据从用户处获取的一组参数生成提示词。以下是一个简单的示例：\nfrom langchain import PromptTemplate\ntemplate = \"\"\"\nYou are a naming consultant for new companies.\nWhat is a good name for a company that makes {product}?\n\"\"\"\nprompt = PromptTemplate.from_template(template)\nprompt.format(product=\"color..."
      ],
      "response_time": 2.6510279178619385,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.04705882352941176,
        "rouge2": 0.0,
        "rougeL": 0.02352941176470588,
        "bleu": 0.005192084003616371,
        "semantic_similarity": 0.35619932413101196
      }
    },
    {
      "sample_id": "0069",
      "question": "在进行指令微调时，如何确保模型的输出符合预期的要求？",
      "ground_truth": "在进行指令微调时，确保模型的输出符合预期的要求，可以通过构造包含精准、清晰的指令输入的数据对来实现。这些指令输入详细阐释了任务的目标，并明确规定了输出需要满足的各项要求。同时，在模型训练的过程中，可以通过保存模型的不同阶段，并利用打印功能来监控训练过程中的各种指标，例如通过打印吞吐量来评估训练效率。",
      "predicted_answer": "在进行指令微调时，确保模型输出符合预期要求的关键在于明确定义指令和高质量的数据准备。具体而言，需要针对每一项任务明确地定义相应的自然语言形式的指令或提示，清晰描述任务目标和输出要求[1]。同时，指令微调数据应由包含“指令输入”与“答案输出”的文本对构成，“指令输入”详细解释了任务目标和输出要求，“答案输出”则规定了模型应当生成的内容。因此，通过高质量且明确的指令数据对模型进行微调，能够有效提升模型产出结果的质量，使其更符合预期[3][5]。",
      "ground_context": [
        "指令微调数据通常由文本对构成，包含“指令输入”与“答案输出”两个关键部分。",
        "在进行指令微调时，确保模型的输出符合预期的要求，可以通过构造包含精准、清晰的指令输入的数据对来实现。",
        "save_hf_format(model, tokenizer, args) 和 save_zero_three_model(model, args.global_rank, args.output_dir, zero_stage=args.zero_stage) 这类函数可以帮助保存模型的不同阶段，从而监控训练过程中的各种指标。"
      ],
      "retrieved_contexts": [
        "5. 指令微调\n指令微调又称有监督微调，是指在预训练大语言模型的基础上，通过使用有标注的自然语言\n形式的数据， 对模型参数进行微调， 使模型具备指令遵循（Instruction Following） 能力， 能够完成\n各类预先设计的任务，并可以在零样本情况下处理诸多下游任务。经过海量数据预训练后的语言\n模型虽然具备了大量的 “知识” ， 但是由于其训练时的目标仅是进行下一个词的预测， 因此不能够\n理解并遵循人类自然语言形式的指令。为了使模型具有理解并响应人类指令的能力，还需要使用\n指令数据对其进行调整。如何构造指令数据，如何高效低成本地进行指令微调训练，以及如何在\n语言模型基础上进一步扩大上下文等问题，是大语言模型在指令微调阶段的核心。\n本章先介绍大语言模型指令微调训练方法，在此基础上介绍高效模型微调及模型上下文窗口\n扩展方法，最后介绍指令微调的代码实践。\n5.1 指令微调训练\n指令微调具体训练过程并不复杂，主要分为如下三个步骤： （ 1）针对每一项任务去明确地\n定义相应的自然语言形式的指令或者提示，这些指令或提示对任务目标以及输出要求进行清晰描\n述； （2）把训练数据调整成包含指令以...",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "练数据对预训练模型进行微调操作。从训练方式的角度来看，指令微调与预训练大体上较为相似，\n不过指令微调的目标函数往往只是针对输出部分来计算损失。\n本节将从指令微调数据构造、数据评估和影响、指令微调训练策略以及开源指令微调数据等\n方面进行介绍。\n5.1.1 指令微调数据\n指令微调数据通常由文本对构成，包含“指令输入”与“答案输出”两个关键部分。 “指令输\n入” ， 是指人们向模型提出的各类请求， 包含定义精准、 清晰的指令或者提示信息， 其核心作用在\n于详细阐释任务的目标究竟是什么，以及明确规定输出需要满足的各项要求。指令涵盖的范畴极\n为广泛，包括问题回答、信息分类、内容总结、文本改写等。 “答案输出” ，则是期望模型依据所",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "大规模语言模型：从理论到实践 149\nUser:\n൉હ൞ AIĤ\nAssistant:\nᆦ,ఖb\nUser:\nĤ\nAssistant:\nPython\nUser:\nູ൉હ൐Ⴈ Python ଻Ĥ\nAssistant:\nႿնଆ྘টඪđPythonე࿽bཞ TensorFlowބPyTorchਔ Python१b\n通过这样的对话数据，可以把最后一轮“Assistant”回答前的所有数据当做“输入” ，最后一\n轮“Assistant”回答做为“输出” ，模型针对这样的“输入-输出”微调后，就能够具备理解上下文\n对话并给出符合期望输出的能力。\n可以看到， 指令微调数据由一系列文本对构成， 其中每一对都涵盖了 “指令输入” 与 “答案输\n出” 两个关键部分。 乍一看， 指令微调数据构造并不复杂， 但其实构建指令微调数据集是极具挑战\n性的任务，复杂性在诸多层面均有体现。在数据收集阶段，获取高质量指令数据集需耗费大量时\n间与资源，既要广泛招募参与者，精心规划有效的收集策略，还要全力保证收集到的数据兼具多\n样性与高质量。收集来的数据后续必经重写与筛选流程，研究人员常运用深度演化、广度演化策\n略以及主题多..."
      ],
      "response_time": 3.3234193325042725,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 0.3333333333333333
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.10144651236008818,
        "semantic_similarity": 0.8412167429924011
      }
    },
    {
      "sample_id": "0070",
      "question": "在训练模型中，如何结合选择子集的硬掩码或软权重与损失函数的优化来提高模型的性能？",
      "ground_truth": "在训练模型时，可以通过选择子集的硬掩码或软权重来优化模型参数，并结合带有自监督语言建模损失的双层优化问题，从而提高模型的性能。在片段A中，模型在训练过程中计算损失并进行反向传播，而在片段B中，优化问题的形式化表示表明可以通过负对数似然损失来优化所选子集的模型参数。通过这种方式，可以更有效地训练模型。",
      "predicted_answer": "在训练模型过程中，可以通过以下步骤结合选择子集的硬掩码或软权重与损失函数的优化来提高模型的性能：\n\n1. 从数据集 \\( S \\) 中选择子集 \\( S_b \\)，这是通过最小化目标函数 \\( \\mathcal{N}_{LLA}^{|Q_i|} \\) 来完成的[3]。\n2. 对选定的子集 \\( S_b \\)，优化模型参数 \\( \\theta \\)，使得在该子集上达到最优的负对数似然值[3]。\n3. 这种方法允许我们使用较小规模的模型进行学习，例如 MPT 125M[219]，从而提高训练效率和模型性能。\n\n这种方法能有效利用有限的数据资源，优化模型参数，进而提高整体性能。",
      "ground_context": [
        "loss = outputs.loss",
        "从 S 中选择子集的硬掩码或软权重；内循环用于优化在 Sb 上的模型参数 θ。",
        "model.backward(loss)",
        "可以将带有自监督语言建模损失的双层优化问题，按照如下方法形式化表示："
      ],
      "retrieved_contexts": [
        "对于剩余的 4 个子集（MassiveWeb、News、Books 和 C4）设置了 7 种不同的组合。图3.11 给出了\n7 种不同子集采样权重训练得到 Gopher 模型在下游任务上的性能。可以看到，使用不同数量子集\n采样权重训练，获得的模型效果差别很大。在所有任务中表现良好且在 Curation Corpus 上取得最\n佳表现的绿色配置是 10% 的 C4、50% 的 MassiveWeb、30% 的 Books 和 10% 的 News。增加书籍\n数据的比例可以提高模型从文本中捕获长期依赖关系的能力， 降低Lambada 数据集[142] 上的损失，\n而使用更高比例的 C4 数据集[19] 则有助于在 C4 验证集[115] 上获得更好的表现。\n图 3.11 使用不同采样权重训练得到的 Gopher 语言模型在下游任务上的性能 [115]",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "从 S 中选择子集的硬掩码或软权重；2) 内循环用于优化在 Sb 上的模型参数 θ。 可以将带有自监督\n语言建模损失的双层优化问题，按照如下方法形式化表示：\nSb = arg min\nS′\nb⊂S,|Sb|=b\nX\nxi∈S′\nb,θ=θ∗\nN LLA|Q\ni (5.4)\ns.t.θ∗ = arg min\nθ\nX\nxi∈S′\nb\nN LLA|Q\ni (5.5)\nN LLi = 1\n|xi|\n|xi|X\nj=1\n− log P (xi(j)|xi(<j); θ) (5.6)\n其中 N LLi 表示针对每个数据 xi 的负对数似然（Negative Log Likelihood） ，可以使用较小的模型\n进行学习，比如 MPT 125M[219] 等。",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "OpenCLIP[305] 则需数百卡 GPU 集群运行数天至数周。 文献[304] 提出通过构建高效的数据处理管\n道，可实现性能提升，同时避免成本大幅上升。\n如图 7.12 所示，数据是训练多模态大语言模型的核心要素之一。构建一个多样且平衡的数据\n集对于模型学习覆盖足够多概念的良好世界模型至关重要。清除大型数据集中常见的重复数据同\n样重要，这不仅能够节省大量计算资源，还能降低模型过度记忆的风险。与此同时，数据剪枝也\n是数据处理的重要环节，确保文本描述与图像内容高度相关，有助于模型更好地理解和对齐多模\n态信息。可以通过改进模型对视觉语义关联（Grounding）能力来增强对图文关系的理解，并通过\n引入人类偏好优化对齐效果。 在OCR 任务中， 使用专门的增强技术可以进一步提升文本读取和翻\n译能力。通过结合高效数据处理、合理的模型架构选择和针对性优化策略，可以显著提升多模态\n大语言模型的训练效果和应用能力。"
      ],
      "response_time": 4.326415777206421,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 0.3333333333333333
      },
      "answer_quality_metrics": {
        "rouge1": 0.09523809523809525,
        "rouge2": 0.0,
        "rougeL": 0.09523809523809525,
        "bleu": 0.1125616465424873,
        "semantic_similarity": 0.8560431003570557
      }
    },
    {
      "sample_id": "0071",
      "question": "如何在命名实体识别和情感分类任务中确保数据的多样性和有效性？",
      "ground_truth": "在命名实体识别中，通过均匀选择每种实体类型的样本并强调语义多样性，确保数据的多样性。而在情感分类任务中，针对每个类标签生成相应的输入，确保对情感的准确分类。",
      "predicted_answer": "为确保命名实体识别和情感分类任务中数据的多样性和有效性，可以采取以下措施：\n\n1. 均匀选择各类实体类型的样本，同时强调语义多样性，通过选择文本相似度较低的样本来确保数据的多样性[1]。\n2. 对于不同数据集中存在不一致的实体定义进行标准化处理，通过基于模型的交叉验证和基于规则的筛选自动检测这些定义冲突，并为每种独特实体类型分配明确且可区分的标签[3]。\n3. 避免过度标注常见实体，增加对长尾实体的样本标注，从而提高数据集的多样性，防止模型过拟合[3]。\n4. 使用来自不同源头、具备不同特征且呈现不同分布的数据，提升训练数据的多样性[2]。\n5. 将数据质量和多样性相结合，不仅关注数据的质量，也要注重其多样性[2]。\n\n综上所述，通过标准化实体定义、增加语义多样性、合理平衡常见与少见实体的标注数量、利用多样化数据源及结合数据质量和多样性标准等手段，可以在命名实体识别和情感分类任务中确保数据的有效性和多样性。",
      "ground_context": [
        "均匀选择每种实体类型的样本，同时强调语义多样性，通过选择文本相似度较低的样本来确保数据的多样性。",
        "Given the classification task definition and the class labels, generate an input that corresponds to each of the class labels."
      ],
      "retrieved_contexts": [
        "策略来减少冗余。具体而言，均匀选择每种实体类型的样本，同时强调语义多样性，通过选择文\n本相似度较低的样本来确保数据的多样性。 最终， 在54 个中英双语命名实体识别数据集中应用该\n方法，得到了 B2NERD，这是一个包含 16 个主要领域、400 多种实体类型的高级命名实体识别数",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "广场” 这样的地点和 “巴黎” 这样的地缘政治实体， 而另一些数据集则将两者统一标注为 “LOC” 。\n如果直接使用未经处理的混合数据，大语言模型在训练中可能会与这些不一致的数据对齐，导致\n模型记住特定数据集的标注规则， 并在推理时对常见实体类型产生混淆。 此外， 合并数据集还容易\n引入大量冗余数据。许多数据集对常见实体进行了过多标注，而对长尾实体的样本标注较少。这\n种缺乏多样性的情况可能使大语言模型出现过拟合现象，并进一步导致知识遗忘和泛化能力下降\n的问题。\n为了解决数据集合并中的定义歧义以及数据冗余等问题，文献 [188] 提出了一种多数据集合\n并方法，如图5.1所示。该方法分为两个步骤，第一步是系统地标准化所有收集到的数据集中的实\n体定义。针对不同数据集中存在的不一致实体定义，方法通过基于模型的交叉验证和基于规则的\n筛选自动检测这些定义冲突。随后，根据特定原则为每种独特的实体类型分配明确且可区分的标\n签， 以消除模糊性。 在此阶段， 构建了一个通用的实体分类体系， 涵盖了常见实体类型， 并为新的\nNER 任务提供了标签命名的指导依据。 第二步则通过采用一种基于类别和语义多样性的数...",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "大规模语言模型：从理论到实践 161\n图 5.3 LLaMA 65B 模型使用 LIMA [42] 训练效果对比\n文献 [190] 研究也表明， 在模型构建过程中， 数据工程起着至关重要的作用， 可以通过提升数\n据集的多样性，显著增强模型的泛化能力。训练数据多样性的提升，可以从多个方面着手，例如\n使用来自不同源头、具备不同特征且呈现不同分布的数据。此外，实验结果也说明，在数据选择\n环节，多样性有着不可忽视的作用。对比随机选择、均匀选择这两种常见方式，具备多样性的数\n据选择策略展现出明显优势。此外，相较于单纯聚焦于挑选高质量数据，若能将数据质量与多样\n性标准有机结合，模型也可以达到更好的效果[223]。\n在问答任务方面，大语言模型的预训练依托于多样化的语料库来开展，这些语料库包含了多\n种类型的内容，并且涵盖了丰富的世界知识。大语言模型在预训练完成后，大量的知识被编码进\n了模型的参数之中。而通过监督微调的方式，就能够把这些已经编码进参数的知识有效地应用于\n问答任务里。然而，针对大语言模型的问答任务能力提升，存在着三个亟待解决的关键问题： （1）\n指令微调阶段，究竟需要多少数据量，才能使大语..."
      ],
      "response_time": 5.824449300765991,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.05824043634578505,
        "semantic_similarity": 0.8420102596282959
      }
    },
    {
      "sample_id": "0072",
      "question": "find_and_replace 会根据什么参数从基础模型中找出模块？",
      "ground_truth": "config 中的参数",
      "predicted_answer": "`find_and_replace` 会根据 `config` 中的参数从基础模型的 `named_parameters` 中找出包含指定名称的模块（默认为“q” “v”，即注意力模块的Q和V矩阵），然后创建一个新的自定义类 `Linear` 模块，并替换原来的模块。[1]",
      "ground_context": [
        "find_and_replace 会根据 config 中的参数从基础模型的 named_parameters 中找出包含指定名称的模块（默认为“q” “v” ，即注意力模块的Q 和 V 矩阵）"
      ],
      "retrieved_contexts": [
        "大规模语言模型：从理论到实践 171\n_find_and_replace 会根据 config 中的参数从基础模型的 named_parameters 中找出包含指定名称的\n模块（默认为“q” “v” ，即注意力模块的Q 和 V 矩阵） ，创建一个新的自定义类Linear 模块，并\n替换原来的。\nclass Linear(nn.Linear, LoraLayer):\n# LoraҪᇏ\ndef __init__(\nself,\nadapter_name: str,\nin_features: int,\nout_features: int,\nr: int = 0,\nlora_alpha: int = 1,\nlora_dropout: float = 0.0,\nfan_in_fan_out: bool = False,\nis_target_conv_1d_layer: bool = False,\n**kwargs,\n):\ninit_lora_weights = kwargs.pop(\"init_lora_weights\", True)\nnn.Linear.__init__(self, in_fe...",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "342 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n查询词\n生成\n回答融合检索\n生成\n生成\n生成\n查明变换\n图 9.13 RAG flow 的后检索分支模式 [421]\n9.3.4 循环模式\n循环模式的核心是检索与生成步骤之间的相互依赖性。 循环模式通过引入调度模块进行控制，\n确保系统可以根据需要在特定模块之间重复执行某些操作。 这一模式可以被抽象为一个有向图， 其\n中节点代表系统的各个模块，边表示模块之间的控制流或数据流。当一个模块能够返回到之前的\n模块时，该系统就形成了一个循环结构。这种循环设计允许系统在流程中对某些步骤进行重复优\n化，从而提升任务的完成效果。\n循环模式的关键在于判断模块（Judge Module） ，用于决定流程是否需要返回到之前的模块或\n继续向下执行。例如，当一个模块完成后，判断模块可以决定是进入下一个模块还是返回到前置\n模块。如果系统决定返回，则执行循环操作；如果系统决定不返回，则流程继续向前。这种灵活\n的控制机制使得循环模式能够动态调整整个流程，从而提高系统的适应性、灵活性以及对复杂任\n务的处理能力。\n循环模式可以进一步细分为三种类型：迭代型、...",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "大语言模型：从理论到实践 361\n1. 检索模块的评估指标\n检索模块的评估指标需要全面反映系统在复杂信息环境中的表现，不仅关注检索结果的相关\n性和准确性，还需涵盖多样性与鲁棒性，以衡量系统在动态、海量且可能包含误导性信息的知识\n库中的适应能力。针对检索模块的评估指标可以分为基于排序和非基于排序两大类。\n基于排序的指标评估相关项目在排序列表中的呈现顺序， 重视相关项目在列表中的排名位置。\n评测指标主要有：\n平均倒数排名（Mean Reciprocal Rank, MRR ）是一组查询中，第一个正确答案的倒数排名的\n平均值，公式为：\nM RR = 1\n|Q|\n|Q|X\ni=1\n1\nranki\n(9.16)\n其中 |Q| 是查询的数量，ranli 是第 i 个查询的第一个相关文档的排名位置。\n平均准确率均值 （Mean Average Precision, MAP） 是每个查询的平均准确率得分的平均值， 公\n式为:\nM AP = 1\n|Q|\n|Q|X\nq=1\nPn\nk=1(P (k) × rel(k))\f\f第 q 个查询的相关文档数量\n\f\f (9.17)\n其中 P (k) 是指在排名列表中截..."
      ],
      "response_time": 1.8615238666534424,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.15384615384615385,
        "rouge2": 0.0,
        "rougeL": 0.15384615384615385,
        "bleu": 0.01929364748650571,
        "semantic_similarity": 0.5656428933143616
      }
    },
    {
      "sample_id": "0073",
      "question": "线性插值法的优点是什么？",
      "ground_truth": "线性插值法具有良好的数值稳定性，并且不需要修改模型架构，只需要少量微调即可将 LLaMA.",
      "predicted_answer": "线性插值法的优点在于它能够平滑且数值稳定地计算注意力分数，从而减小上下文窗口扩展对注意力得分计算的影响[1][2]。",
      "ground_context": [
        "线性插值法具有良好的数值稳定性（具体推导请参考文献 [242]），并且不需要修改模型架构，只需要少量微调（例如，在 pile 数据集上进行 1000 步的微调）即可将 LLaMA"
      ],
      "retrieved_contexts": [
        "大规模语言模型：从理论到实践 177\n将所有三角函数视为基函数 ϕj(s) := e isθj ，可以将式 (5.16) 展开为\na(s) = Re\n2\n4\nd/2−1X\nj=0\nhjeisθj\n3\n5 (5.17)\n其中 s 是查询和键之间的相对距离，hj := ( q2j + iq2j+1)(k2j − ik2j+1) 是取决于查询和键的复系\n数。作为基函数的三角函数具有非常强的拟合能力，基本上可以拟合任何函数，因此在不训练的\n情况下， 对于预训练2048 的上下文窗口总会存在与 [0, 2048] 中的小函数值相对应但在 [0, 2048] 之\n外的区域中大很多的系数 hj（键和查询） ，如图5.9(a) 所示，但线性插值法得到的结果平滑且数值\n稳定，如图5.9(b) 所示。\n图 5.9 不同相对距离下外推法和线性插值法的注意力分数比较。\n因此，可以利用位置插值修改式 (5.15) 的位置编码函数：\nf ′(x, m) = f\n\u0012\nx, mL\nL′\n\u0013\n(5.18)\n这种方法对齐了位置索引和相对距离的范围， 减小了上下文窗口扩展对注意力得分计算的影响， 使\n得模型更容易适应。线性...",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "176 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n图 5.8 线性插值法的示意图 [242]\n给定一个位置索引 m ∈ [0, c) 和一个嵌入向量 x := [ x0, x1, · · · , xd−1]，其中 d 是注意力头的\n维度，RoPE 位置编码定义为如下函数：\nf (x, m) =\n\u0002\n(x0 + ix1)eimθ0, (x2 + ix3)eimθ1, · · · , (xd−2 + ixd−1)eimθd/2−1\n\u0003⊤\n(5.15)\n其中，i := √−1 是虚数单位，θj = 10000 −2j/d。虽然 RoPE 位置编码所得的注意力分数只依赖于\n相对位置，但是其外推能力并不理想，当直接扩展上下文窗口时，模型的困惑度会飙升。具体来\n说，RoPE 应用于注意力分数可以得到以下结果：\na(m, n) = Re ⟨f (q, m), f(k, m)⟩\n=\nd/2−1X\nj=0\n(q2j + iq2j+1)(k2j − ik2j+1) cos((m − n)θj)\n+ (q2j + iq2j+1)(k2j − ik2j+1) sin((m − n)θj)\n= ...",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "28 。 通常情况下， 对于n 个注意头， 斜率值是\n从 2\n−8\nn 开始，并使用相同的值作为其比率。ALiBi 的计算过程如图5.7 所示。\nm\n图 5.7 ALiBi 计算过程示例\nALiBi 对最近性具有归纳偏差， 它对远程查询–键对之间的注意力分数进行惩罚， 随着键和查询\n之间的距离增加， 惩罚也增加。不同的注意力头以不同的速率增加其惩罚， 这取决于斜率幅度。实\n验证明，这组斜率参数适用于各种文本领域和模型尺寸，不需要在新的数据和架构上调整斜率值。\n5.3.2 插值法\n不同的预训练大语言模型使用不同的位置编码，修改位置编码意味着重新训练，因此对于已\n训练的模型，通过修改位置编码扩展上下文窗口大小的适用性仍然有限。为了不改变模型架构而\n直接扩展大语言模型上下文窗口大小， 文献[242] 提出了位置插值法， 使现有的预训练大语言模型\n（包括 LLaMA、Falcon、Baichuan 等） 能直接扩展上下文窗口。 其关键思想是， 直接缩小位置索引，\n使最大位置索引与预训练阶段的上下文窗口限制相匹配。线性插值法的示意图如图5.8 所示。"
      ],
      "response_time": 1.168330192565918,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0.0,
        "bleu": 0.022989113556055568,
        "semantic_similarity": 0.7442994713783264
      }
    },
    {
      "sample_id": "0074",
      "question": "如何获取评估数据？",
      "ground_truth": "return self.raw_datasets[\"eval\"]",
      "predicted_answer": "根据提供的文档，我无法回答这个问题。文档中未提及关于如何获取评估数据的具体信息。",
      "ground_context": [
        "return self .raw_datasets[\"eval\"]"
      ],
      "retrieved_contexts": [
        "时对多个系统输出进行比较，可以评估不同系统之间的性能差异。\n（5）评估者是否提供解释是指是否要求评估者为自己的决策提供必要的说明。提供决策的解\n释有助于开发者了解评估过程中的决策依据和评估结果的可靠性，从而更好地优化系统性能，但\n缺点是极大地增加了评估者的时间花费。\n对于每个数据，通常会有多个不同人员进行评估，因此需要一定的方法整合最终评分。最简\n单的最终评分整合方法是计算平均主观得分（Mean Opinion Score，MOS）， 即对所有评估者的评\n分求平均值：",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "出。 例如， 对一篇文章的质量进行评估， 虽然可以使用自动评估的方法计算出一些指标， 如拼写错\n误的数量、 语法错误的数量等， 但是对于文章的流畅性、 逻辑性、 观点表达等方面的评估则需要人\n工阅读并进行分项打分。 这种方法被称为人工评估（Human Evaluation） 。 人工评估是一种耗时耗力\n的评估方法， 因此研究人员提出了一种新的评估方法， 即利用能力较强的大语言模型 （如GPT-4） ，\n构建合适的指令来评估系统结果[196, 598–601]。 这种评估方法可以大幅度减少人工评估所需的时间和\n人力成本，具有更高的效率。这种方法被称为 大语言模型评估 （LLM Evaluation ） 。此外，有时\n我们还希望对比不同系统之间或者系统不同版本之间的差别，这需要采用对比评估（Comparative\nEvaluation） 方法针对系统之间的不同进行量化。 自动评估在前面介绍评估指标时已经给出了对应\n的计算方法和公式，本节将分别针对人工评估、大语言模型评估和对比评估进行介绍。\n1. 人工评估\n人工评估是一种广泛应用于评估模型生成结果质量和准确性的方法，它通过人类参与对生成\n结...",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "用哪些评估指标，以及如何进行评估等。本节将围绕上述两个问题进行介绍。\n11.3.1 评估指标\n传统的自然语言处理算法通常针对单一任务，因此单个评估指标相对简单。然而，不同任务\n的评估指标有非常大的区别， HELM 评估[557] 集成了自然语言处理领域的不同评估数据集，共\n计构造了 42 类评估场景，但是评估指标高达 59 种。本节将针对分类与回归任务、语言模型、文\n本生成等不同任务所使用的评估指标，以及大语言模型评估指标体系进行介绍。\n1. 分类与回归任务评估指标"
      ],
      "response_time": 0.751013994216919,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.0,
        "semantic_similarity": 0.49303996562957764
      }
    },
    {
      "sample_id": "0075",
      "question": "在第一轮训练中，Rank 4 的损失是多少？",
      "ground_truth": "1.6396484375",
      "predicted_answer": "在第一轮训练中，Rank 4 的损失为 2.005859375。[1]",
      "ground_context": [
        "running - Rank: 4, Epoch 1/2, Step 1/341, trained samples: 128/341, Loss 1.6396484375"
      ],
      "retrieved_contexts": [
        "running - Rank: 4, Epoch 1/2, Step 1/341, trained samples: 128/341, Loss 1.6396484375\nrunning - Rank: 6, Epoch 1/2, Step 1/341, trained samples: 128/341, Loss 1.94140625\n...\nrunning - Rank: 4, Epoch 1/2, Step 341/341, trained samples: 43584/341, Loss 2.005859375\nrunning - Rank: 5, Epoch 1/2, Step 341/341, trained samples: 43584/341, Loss 1.6533203125\nrunning - ***** Evaluating perplexity, Epoch 1/2 *****\nrunning - Rank: 7, Epoch 1/2, Step 341/341, trained samples: 43584/341, Loss 2.076171875\nrun...",
        "大规模语言模型：从理论到实践 125\n图 4.25 集合通信 Gather 原语示例\n（6）All Gather：每个节点都收集所有其他节点上的数据，All Gather 相当于一个 Gather 操作\n之后跟着一个 Broadcast 操作。如图4.26 所示，All Gather 操作将所有计算设备上的数据收集到每\n个计算设备中。\n图 4.26 集合通信 All Gather 原语示例\n（7）Reduce Scatter：将每个节点中的张量切分为多个块，每个块被分配给不同的节点。接收\n到的块会在每个节点上进行特定的操作，例如求和、取平均值等。如图4.27 所示，每个计算设备\n都将其中的张量切分为 4 块，并分发到 4 个不同的计算设备中，每个计算设备分别对接收的分块\n进行特定操作。\n图 4.27 集合通信 Reduce Scatter 原语示例\n（8）All to All： 将每个节点的张量切分为多个块， 每个块分别发送给不同的节点。 如图4.28 所\n示，每个计算设备都将其中的张量切分为 4 块，并分发到 4 个不同的计算设备中。",
        "running - ppl: 6.88722562789917\nrunning - Beginning of Epoch 1/2, Total Micro Batches 341\nrunning - Rank: 0, Epoch 1/2, Step 1/341, trained samples: 128/341, Loss 1.916015625\nrunning - Rank: 3, Epoch 1/2, Step 1/341, trained samples: 128/341, Loss 1.6083984375\nrunning - Rank: 2, Epoch 1/2, Step 1/341, trained samples: 128/341, Loss 1.7587890625\nrunning - Rank: 5, Epoch 1/2, Step 1/341, trained samples: 128/341, Loss 1.658203125\nrunning - Rank: 4, Epoch 1/2, Step 1/341, trained samples: 128/341, ...",
        "值序列， 其大小为L × d。 在这种情况下， 计算合并专家的输出会产生大约L × 4 × d × m 次浮点运\n算 （FLOPs） 的计算成本， 而采用N 个专家的稠密混合专家模型则需要 N × L × 4 × d × m 次浮点\n运算。 此外， 软混合专家模型还必须对N 个专家的参数进行平均， 这又会额外产生N × 2 × d × m\n次浮点运算的成本。整体上 SMEAR 算法的计算复杂度是 (L × 4 + N × 2) × d × m。综合整体计\n算成本，软混合专家模型计算复杂度仍然远低于稠密混合专家模型。",
        "running - ppl: 6.158349514007568\nrunning - Beginning of Epoch 2/2, Total Micro Batches 341\nrunning - Rank: 0, Epoch 2/2, Step 1/341, trained samples: 128/341, Loss 1.7919921875\nrunning - Rank: 2, Epoch 2/2, Step 341/341, trained samples: 43584/341, Loss 1.291015625\nrunning - ***** Evaluating perplexity, Epoch 2/2 *****\nrunning - Rank: 5, Epoch 2/2, Step 341/341, trained samples: 43584/341, Loss 1.4794921875\nrunning - Rank: 6, Epoch 2/2, Step 341/341, trained samples: 43584/341, Loss 2.017578125\n..."
      ],
      "response_time": 0.9996793270111084,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.28571428571428575,
        "rouge2": 0.0,
        "rougeL": 0.28571428571428575,
        "bleu": 0.0,
        "semantic_similarity": 0.5003732442855835
      }
    },
    {
      "sample_id": "0076",
      "question": "ChatDoctor指令数据集的规模是多少？",
      "ground_truth": "11.5 万",
      "predicted_answer": "ChatDoctor指令数据集的规模是11.5万条。[1]",
      "ground_context": [
        "ChatDoctor 德克萨斯大学西南医学中心 11.5 万 医疗 公开"
      ],
      "retrieved_contexts": [
        "大规模语言模型：从理论到实践 167\n表 5.3 部分开源领域指令微调数据集的汇总信息\n指令数据集名称 发布单位 指令数据集规模（条） 领域 是否公开\nChatDoctor 德克萨斯大学西南医学中心 11.5 万 医疗 公开\nDISC-Med-SFT 复旦大学 46.49 万 医疗 公开\nHuatuo-26M 香港中文大学等 265 万 医疗 公开\nMedDialog 加州大学圣地亚哥分校 366 万 医疗 公开\nMedical Meadow 亚琛大学医院等 16 万 医疗 公开\nBELLE School Math 贝壳研究院 24.85 万 数学 公开\nGoat 新加坡国立大学 175 万 数学 公开\nOpenMathInstruct-1 NVIDIA 180 万 数学 公开\nCode Alpaca 20K Sahil Chaudhary 2 万 代码 公开\nCodeContest DeepMind 1.36 万 代码 公开\nCommitPackFT Bigcode 70.21 万 代码 公开\nDISC-Law-SFT 复旦大学 40.3 万 法律 部分\nHanFei 1.0 中国科...",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "含 110,000 个指令。LCCC[183] 是一个中文对话数据集， 包含LCCC-base 和 LCCC-large 两个版本。 其\n中 LCCC-base 采用两阶段数据收集方案，首先挑选专注发布新闻的微博帐号作为高质量用户，再\n收集其微博帖子下方评论并把评论路径视为对话一部分； LCCC-large 则是从包括中国 Chatterbot\n语料库、PTT 闲话语料库等多个开源存储库收集语料库，并与青云语料库、贴吧语料库一同清洗\n后处理成单轮对话数据集。\n2. 现有数据集转换\n收集和改进现有数据集也是一种用于构建指令微调数据集的方法，它涉及整合和修改多个开\n源数据集， 最终将它们合并成一个新数据集用于大模型指令微调。 文献[106] 指出这种构建方式具\n有以下优点： （1）多样性和全面性，生成的数据集具有丰富的数据来源、多样化的任务类型和广\n泛的领域覆盖； （2）规模大，选择的数据集越多，规模越大； （3）节省时间，这种构建方式可以\n减少数据集构建所需的时间。这种数据集构造的主要是难点是质量与格式标准化。需要全面考量",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "大规模语言模型：从理论到实践 7\n表 1.1 典型开源大语言模型汇总\n模型名称 发布时间 参数量（个） 模型类型 预训练数据量\nT5[19] 2019 年 10 月 110 亿 基础模型 1 万亿个词元\nPanGu-α[22] 2021 年 4 月 130 亿 基础模型 1.1 万亿个词元\nCPM-2[26] 2021 年 6 月 1980 亿 基础模型 2.6 万亿个词元\nCodeGen[27] 2022 年 3 月 160 亿 基础模型 5770 亿个词元\nGPT-NeoX-20B[28] 2022 年 4 月 200 亿 基础模型 825GB\nOPT[29] 2022 年 5 月 1750 亿 基础模型 1800 亿个词元\nGLM[30] 2022 年 10 月 1300 亿 基础模型 4000 亿个词元\nFlan-T5[23] 2022 年 10 月 110 亿 对话模型 -\nBLOOM[31] 2022 年 11 月 1760 亿 基础模型 3660 亿个词元\nBLOOMZ[32] 2022 年 11 月 1760 亿 对话模型 -\nOPT-IML[33] 2022 年 12..."
      ],
      "response_time": 0.7504470348358154,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.6666666666666666,
        "rouge2": 0.5,
        "rougeL": 0.6666666666666666,
        "bleu": 0.018724372764461875,
        "semantic_similarity": 0.4303074777126312
      }
    },
    {
      "sample_id": "0077",
      "question": "如何设置ds_accelerator?",
      "ground_truth": "Setting ds_accelerator to cuda (auto detect)",
      "predicted_answer": "`ds_accelerator` 的设置可以通过以下方式完成：\n\n```bash\n[2023-09-06 21:17:36,034] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator\nto cuda (auto detect)\n```\n\n这表明 `ds_accelerator` 被设置为 `cuda` （自动检测）。如果需要手动设置，可以在启动脚本或配置文件中指定。[1][5]",
      "ground_context": [
        "Setting ds_accelerator to cuda (auto detect)"
      ],
      "retrieved_contexts": [
        "大规模语言模型：从理论到实践 181\n[2023-09-06 21:17:36,034] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator\nto cuda (auto detect)\nDetected CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 but ignoring it because one or several of --include/\n--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't\npass any of these arguments to deepspeed.\n[2023-09-06 21:17:37,575] [INFO] [runner.py:555:main] cmd = /opt/miniconda3/envs/baichuan_sft/bin/\npython -u -m deepspeed....",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.\n1 --master_port=29500 --enable_each_rank_log=None main.py --model_name_or_path facebook/opt-1.3b\n--gradient_accumulation_steps 8 --lora_dim 128 --zero_stage 0 --enable_tensorboard --tensorboard_pa\nth /root/workspace/DeepSpeed-Chat/output/actor-models/1.3b --deepspeed --output_dir /root/workspace\n/DeepSpeed-Chat/output/actor-models/1.3b\n[2023-09-06 21:17:38,322] [INFO] [real_accelerator.py:110:get_...",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "[2023-09-06 21:17:41,099] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator\nto cuda (auto detect)\n[2023-09-06 21:17:43,194] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed\nnot yet implemented\n[2023-09-06 21:17:43,194] [INFO] [comm.py:594:init_distributed] cdb=None\n[2023-09-06 21:17:43,194] [INFO] [comm.py:625:init_distributed] Initializing TorchBackend in\nDeepSpeed with backend nccl\nDownloading pytorch_model.bin: 0%| | 0.00/2.63G [00:00<?, ?B/s]\nDow..."
      ],
      "response_time": 2.716691017150879,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.3888888888888889,
        "rouge2": 0.35294117647058826,
        "rougeL": 0.3888888888888889,
        "bleu": 0.08823360577385578,
        "semantic_similarity": 0.8293948173522949
      }
    },
    {
      "sample_id": "0078",
      "question": "文献 [239] 中提出了什么方案来平滑化敏感性？",
      "ground_truth": "文献 [239] 中提出了一种新的方案来平滑化敏感性，以及量化其不确定性。",
      "predicted_answer": "文献 [239] 中提出了一种新的方案来平滑化敏感性，并量化其不确定性。具体来说，通过以下公式实现了这一点：\n\n\\[\n\\bar{I}(t)(w_{ij}) = \\beta_1 \\bar{I}(t-1)(w_{ij}) + (1 - \\beta_1)I(t)(w_{ij})\n\\]\n\n\\[\n\\bar{U}(t)(w_{ij}) = \\beta_2 \\bar{U}(t-1)(w_{ij}) + (1 - \\beta_2)|I(t)(w_{ij}) - \\bar{I}(t)(w_{ij})|\n\\]\n\n\\[\ns(t)(w_{ij}) = \\frac{\\bar{I}(t)}{\\bar{U}(t)}\n\\]\n\n其中 \\(I(t)(w_{ij})\\) 是第 \\(t\\) 步时参数 \\(w_{ij}\\) 的敏感性，而 \\(\\bar{I}(t)\\) 和 \\(\\bar{U}(t)\\) 分别是对敏感性和不确定性的平滑化处理结果[1]。",
      "ground_context": [
        "然而，根据文献 [239] 中的实验结果，该敏感性度量受限于小批量采样带来的高方差和不确定性，因此并不完全可靠。相应地，文献 [239] 中提出了一种新的方案来平滑化敏感性，以及量化其不确定性。"
      ],
      "retrieved_contexts": [
        "大规模语言模型：从理论到实践 173\n其中，wij 表示可训练参数。该式估计了某个参数变为 0 后， 损失函数值的变化。因此，I(wij) 越\n大，表示模型对该参数越敏感，这个参数也就越应该被保留。然而，根据文献 [239] 中的实验结\n果，该敏感性度量受限于小批量采样带来的高方差和不确定性，因此并不完全可靠。相应地，文\n献 [239] 中提出了一种新的方案来平滑化敏感性，以及量化其不确定性。\n¯I (t)(wij) = β1 ¯I (t−1) + (1 − β1)I (t)(wij) (5.10)\n¯U (t)(wij) = β2 ¯U (t−1) + (1 − β2)|I (t)(wij) − ¯I (t)(wij)| (5.11)\ns(t)(wij) = ¯I (t) ¯U (t) (5.12)\n通过实验对上述几种重要性定义方法进行对比，发现由式 (5.11) 计算得到的重要性分数，即平滑\n后的参数敏感性，效果最优。故最终的重要性分数计算式为\nSk,i = s(λk,i) + 1\nd1\nd1X\nj=1\ns(Pk,ji ) + 1\nd2\nd2X\nj=1\ns(Qk,ij ) (5.1...",
        "一步改进了这一方法，通过限制起始词元的选择，仅选择那些属于顶级语言列表中语言类别的词\n元。该策略能够显著提升量化模型在各种任务上的泛化能力。同时建议在量化后训练 LayerNorm\n层，并使用知识蒸馏来匹配量化模型的输出分布与 FP16 模型的输出分布，从而实现与 LLM-QA T\n类似的效果，同时避免高昂的训练成本。\n为降低计算成本， 许多研究采用参数高效调优策略来加速量化感知训练 （QA T） 。QLoRA[235] 提\n出将大语言模型的权重量化为 4 位， 并使用BF16 对每个 4 位权重矩阵进行 LoRA[501] 微调。QLoRA\n使得在单个 GPU 上仅使用 30GB 的内存即可对 65B 参数的大模型进行高效微调。QA-LoRA[502] 则\n在 QLoRA 的基础上引入了组内量化。作者指出，QLoRA 中的量化参数数量远少于 LoRA 参数数\n量， 导致量化和低秩适应之间的不平衡问题。 为解决这一问题，QA-LoRA 提议增加量化操作的参\n数数量， 使用组内量化操作， 并将LoRA 项合并到相应的量化权重矩阵中， 以提升性能。LoftQ[503]\n则发现 QLoRA ...",
        "文献 [528] 提出了一种名为 Distilling Step-by-Step 的方法，该方法包括两个主要步骤： （1）给\n定一个教师大语言模型和一个无标签数据集，利用教师模型生成输出标签，并同时生成用于证明",
        "大规模语言模型：从理论到实践 49\n2.3.2 注意力机制优化\n在 Transformer 结构中， 自注意力机制的时间和存储复杂度与序列的长度呈平方的关系， 因此\n占用了大量的计算设备内存并消耗了大量的计算资源。如何优化自注意力机制的时空复杂度、增\n强计算效率是大语言模型面临的重要问题。一些研究从近似注意力出发，旨在减少注意力计算和\n内存需求，提出了稀疏近似、低秩近似等方法。此外，有一些研究从计算加速设备本身的特性出\n发， 研究如何更好地利用硬件特性对Transformer 中的注意力层进行高效计算。 本节将分别介绍上\n述方法。\n1. 稀疏注意力机制\n对一些训练好的 Transformer 结构中的注意力矩阵进行分析时发现， 其中很多是稀疏的， 因此可\n以 通 过 限 制 Query-Key 对 的 数 量 来 降 低 计 算 复 杂 度。 这 类 方 法 称 为稀 疏 注 意 力（Sparse\nAttention）机制。可以将稀疏化方法进一步分成基于位置的和基于内容的两类。\n基于位置的稀疏注意力机制的基本类型如图2.6 所示，主要包含如下五种类型。\n（1）全局注意力（Global ...",
        "模型微调方法。LoRA 方法可以在缩减训练参数量和 GPU 显存占用的同时，使训练后的模型具有\n与全量微调相当的性能。\n5.2.1 LoRA\n文献 [232] 的研究表明，语言模型针对特定任务微调之后，权重矩阵通常具有很低的本征秩\n（Intrinsic Rank） 。研究人员认为，参数更新量即便投影到较小的子空间中，也不会影响学习的有"
      ],
      "response_time": 5.7085840702056885,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.02777777777777778,
        "rouge2": 0.0,
        "rougeL": 0.02777777777777778,
        "bleu": 0.05923231580774757,
        "semantic_similarity": 0.827386736869812
      }
    },
    {
      "sample_id": "0079",
      "question": "get_peft_model 函数的功能是什么？",
      "ground_truth": "该函数封装了基础模型并得到一个 PeftModel 类的模型。",
      "predicted_answer": "`get_peft_model` 函数用于将指定的 PEFT （Parameter Efficient Fine-Tuning）配置应用到给定的模型上，以便进行高效的微调操作[1]。在这个例子中，它被用来将LoRA（Low-Rank Adaptation）配置应用到 `AutoModelForSeq2SeqLM` 模型上[1]。",
      "ground_context": [
        "接下来介绍 peft 库对 LoRA 的实现， 也就是上述代码中get_peft_model 函数的功能。 该函数封装了基础模型并得到一个 PeftModel 类的模型。"
      ],
      "retrieved_contexts": [
        "大规模语言模型：从理论到实践 169\nfrom transformers import AutoModelForSeq2SeqLM\nfrom peft import get_peft_config, get_peft_model, LoraConfig, TaskType\nmodel_name_or_path = \"bigscience/mt0-large\"\ntokenizer_name_or_path = \"bigscience/mt0-large\"\npeft_config = LoraConfig(\ntask_type=TaskType.SEQ_2_SEQ_LM, inference_mode =False, r =8, lora_alpha =32, lora_dropout =0.1\n)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name_or_path)\nmodel = get_peft_model(model, peft_config)\n接下来介绍 peft 库对 LoRA 的实现， 也就是上述代码中get_pe...",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "12 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n奖励建模（Reward Modeling） 阶段的目标是构建一个文本质量对比模型。 对于同一个提示词，\nSFT 模型对给出的多个不同输出结果的质量进行排序。奖励模型可以通过二分类模型，对输入的\n两个结果之间的优劣进行判断。奖励模型与基础模型和 SFT 模型不同，奖励模型本身并不能单独\n提供给用户使用。 奖励模型的训练通常和SFT 模型一样， 使用数十块GPU， 通过数天时间完成训\n练。由于奖励模型的准确率对强化学习阶段的效果有至关重要的影响，因此通常需要大规模的训\n练数据对该模型进行训练。Andrej Karpathy 在报告中指出，该部分需要百万量级的对比数据标注，\n而且其中很多标注需要很长时间才能完成。图1.4 给出了 InstructGPT 系统中奖励模型训练样本标\n注示例[24]。 可以看到， 示例中文本表达都较为流畅， 标注其质量排序需要制定非常详细的规范， 标\n注者也需要认真地基于标注规范进行标注，需要消耗大量的人力。同时，保持众包标注者之间的\n一致性，也是奖励建模阶段需要解决的难点问题之一。此外，奖励模型的泛...",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "if len (self.peft_config) > 1 and self.peft_config[adapter_name].bias != \"none\":\nraise ValueError(\n\"LoraModel supports only 1 adapter with bias. When using multiple adapters,\nset bias to 'none' for all adapters.\"\n)\nmark_only_lora_as_trainable(self.model, self.peft_config[adapter_name].bias)\nif self .peft_config[adapter_name].inference_mode:\n_freeze_adapter(self.model, adapter_name)\nLoraModel 类通过 add_adapter 方法添加 LoRA 层。 该方法包括_find_and_replace 和 mark_only_lora_as_trainable 两\n个主要函数。 mark_only_lora..."
      ],
      "response_time": 2.0372989177703857,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0.0,
        "bleu": 0.012620590225409331,
        "semantic_similarity": 0.6655418872833252
      }
    },
    {
      "sample_id": "0080",
      "question": "Alpaca数据集的规模是多少条?",
      "ground_truth": "5.2 万条",
      "predicted_answer": "Alpaca数据集的规模是5.2万条。[1]",
      "ground_context": [
        "Alpaca Data Standford Alpaca 5.2 万条 英文 公开"
      ],
      "retrieved_contexts": [
        "166 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n表 5.2 部分开源通用指令微调数据集的汇总信息\n指令数据集名称 发布单位 指令数据集规模 语言 是否公开\nAlpaca Data Standford Alpaca 5.2 万条 英文 公开\nAya Collection Cohere For AI 等 5.13 亿条 多语言 公开\nAya Dataset Cohere For AI 等 20.4 万条 多语言 公开\nBELLE 贝壳研究院 350 万条 中文 公开\nCOIG 北京智源研究院 19.11 万条 中文 公开\nDialogStudio Salesforce AI 87 个数据集 多语言 公开\nDolly Databricks 1.5 万条 英语 公开\nFirefly Y eungNLP 115 万条 中文 公开\nFlan 2022 Google Research 1836 个数据集 多语言 部分\nInstructionWild V2 新加坡国立大学 11 万条 中英文 公开\nLCCC 清华大学 1200 万条 中文 公开\nLMSYS-Chat-1M 加州大学伯...",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "指令数据的相似度， 只有当它和池中任何一条指令数据的ROUGE-L 相似度都低于 0.7 时， 才可能\n将其加入指令池。为保证数据的质量，还制定了一系列的启发式规则进行筛选：删除包含某些关\n键词（如“图片” ）的指令数据、重复的指令数据、过长或过短的数据等。\n使用 Self-Instruct 方法可以生成大量具有多样性的高质量数据。 斯坦福的研究人员借助这种方\n法生成指令数据，在 LLaMA 模型上进行指令微调得到 Alpaca 模型，其在各项评估指标上都展现\n出了优异的性能。Alpaca 所使用的指令数据集的示例如下所示：",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "该数据集的加入可以帮助语言模型建模电子邮件通信的特性。\nPile 中不同数据子集所占比例及训练时的采样权重有很大不同， 高质量的数据会有更高的采样权\n重。 例如，Pile-CC 数据集包含227.12GB 数据， 整个训练周期中采样1 轮。 虽然Wikipedia (English)\n数据集仅有6.38GB 的数据， 但是整个训练周期中采样3 轮。 具体的采样权重和采样轮数可以参考文\n献 [87]。\n3.4.2 ROOTS\nROOTS（Responsible Open-science Open-collaboration Text Sources ）数据集[128] 是 BigScience\n项目在训练具有 1760 亿个参数的 BLOOM 大语言模型时使用的数据集。该数据集包含 46 种自然\n语言和 13 种编程语言，总计 59 种语言，整个数据集的大小约 1.6TB。ROOTS 数据集中各语言所\n占比例如图3.13 所示。图中左侧是以语言家族的字节为单位表示的自然语言占比树状图，其中欧\n亚大陆语言占据了绝大部分（1321.89GB） 。右侧橙色矩形对应的是印度尼西亚语（18GB） ..."
      ],
      "response_time": 0.6998004913330078,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.6666666666666666,
        "rouge2": 0.5,
        "rougeL": 0.6666666666666666,
        "bleu": 0.04774753205784679,
        "semantic_similarity": 0.6496869325637817
      }
    },
    {
      "sample_id": "0081",
      "question": "在训练过程中，如何使用不同的位置编码方法来增强模型的外推能力，并且在训练中如何对OPT模型进行微调和奖励模型的训练？",
      "ground_truth": "位置编码的外推能力来源于相对位置信息，而在训练OPT模型时，通过train.py脚本对OPT-1.3b模型进行监督微调和对OPT-350m模型进行奖励模型的训练。",
      "predicted_answer": "在训练过程中，可以通过改进的位置编码方法来增强模型的外推能力。例如，RoPE (Rotary Position Embedding) [48] 和 T5 Bias Position Embedding 都是在原有绝对位置编码的基础上进行了改进，增强了其在外推场景下的表现[1]。此外，ALiBi[240] 和 LeX[241] 也是有效的改进方案[3]。\n\n对于 OPT 模型，可以通过增加上下文窗口的微调来适应长文本建模的需求。这种方式直接扩大了模型在微调过程中的上下文窗口大小，但这种方法在扩展到更大的上下文窗口时适应速度较慢，可能需要大量的训练批次才能显著提升窗口大小[3]。\n\n至于奖励模型的训练，则文档中未详细说明具体的训练方法或步骤。因此，根据提供的文档，我无法回答有关奖励模型训练的具体细节。",
      "ground_context": [
        "位置编码的长度外推能力来源于位置编码中表征相对位置信息的部分，相对位置信息不同于绝对位置信息， 对于训练时的依赖较少。",
        "表示通过 train.py 脚本进行步骤一和步骤二的训练，分别对OPT-1.3b 模型进行监督微调和对 OPT-350m 模型进行奖励模型的训练。"
      ],
      "retrieved_contexts": [
        "进的位置编码和插值法。\n5.3.1 具有外推能力的位置编码\n位置编码的长度外推能力来源于位置编码中表征相对位置信息的部分，相对位置信息不同于\n绝对位置信息， 对于训练时的依赖较少。 位置编码的研究一直是基于Transformer 结构模型的重点。\n2017 年 Transformer 结构[12] 提出时， 介绍了两种位置编码， 一种是Naive Learned Position Embedding，\n也就是 BERT 模型中使用的位置编码；另一种是 Sinusoidal Position Embedding ，通过正弦函数为\n每个位置向量提供一种独特的编码。这两种最初的形式都是绝对位置编码的形式，依赖于训练过\n程中的上下文窗口大小，在推理时基本不具有外推能力。随后，2021 年提出的 RoPE[48] 在一定程\n度上缓解了绝对位置编码外推能力弱的问题。关于 RoPE 位置编码的具体细节， 已在2.3.1 节进行\n了介绍， 这里不再赘述。 后续在T5 架构[243] 中， 研究人员又提出了T5 Bias Position Embedding， 直",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "量不得超过 2048 个。这会限制模型对长文本的理解和表达能力。当涉及长时间对话或长文档摘要\n时，传统的上下文窗口大小可能无法捕捉到全局语境，从而导致信息丢失或模糊的建模结果。\n为了更好地满足长文本需求，有必要探索如何扩展现有的大语言模型，使其能够有效地处理\n更大范围的上下文信息。具体来说，扩展语言模型的长文本建模能力主要有以下方法。\n• 增加上下文窗口的微调： 采用直接的方式， 即通过使用一个更大的上下文窗口来微调现有的\n预训练 Transformer，以适应长文本建模需求。\n• 位置编码：改进的位置编码，如ALiBi[240]、LeX[241] 等能够实现一定程度上的长度外推。这\n意味着它们可以在小的上下文窗口上进行训练，在大的上下文窗口上进行推理。\n• 插值法：将超出上下文窗口的位置编码通过插值法压缩到预训练的上下文窗口中。\n文献 [242] 指出， 采用增大上下文窗口微调的方式训练的模型， 对上下文的适应速度较慢。 在\n经过了超过 10000 个批次的训练后， 模型上下文窗口只有小幅度的增长， 从2048 增加到 2560。 实\n验结果显示，这种朴素的方法在扩展到更大的上下文窗...",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "重复的子集，这使得分布式训练变为可能。而在单机环境中，使用常规的随机或顺序采样器即可。\n具体代码如下所示："
      ],
      "response_time": 4.572063684463501,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.23076923076923078,
        "rouge2": 0.0,
        "rougeL": 0.23076923076923078,
        "bleu": 0.03917564246295651,
        "semantic_similarity": 0.7813373804092407
      }
    },
    {
      "sample_id": "0082",
      "question": "在使用 DeepSpeed 进行模型训练时，如何设置训练步骤和模型参数，以便同时满足片段A和片段B中的要求？",
      "ground_truth": "可以通过命令行参数设置训练步骤和模型参数，例如在片段A中提到的 'deepspeed main.py --data_path <my_data>/my_dataset' 和在片段B中提到的 '--step 训练步骤参数' 来实现。",
      "predicted_answer": "要同时满足片段A和片段B中的要求，在使用 DeepSpeed 进行模型训练时，可以通过以下步骤设置训练步骤和模型参数：\n\n1. **初始化 DeepSpeed**: 根据用户参数（如是否进行 offload、使用哪个 Zero Stage 等），构建一个 DeepSpeed 配置字典[1]。这一步骤涉及设置训练所需的优化技术和参数。\n   \n2. **数据加载和分发**: 将数据批次移动到相应的 GPU 设备上。这是在每次迭代中都需要执行的操作[4]。\n\n3. **前向传播和计算损失**: 使用模型对数据批次进行前向传播计算损失值。这一步骤是在每个训练周期内多次执行的[4]。\n\n4. **反向传播和参数更新**: 使用 `model.backward(loss)` 计算梯度，并使用 `model.step()` 更新模型参数。主进程中还会记录吞吐量以监测训练效率[4]。\n\n综上所述，通过合理配置 DeepSpeed 参数并在训练过程中正确执行上述操作，可以满足特定的训练需求。需要注意的是，具体细节可能依赖于实际应用场景的具体要求以及所使用的框架版本。\n\n[1][4]",
      "ground_context": [
        "--data_path <my_data>/my_dataset",
        "--step 训练步骤参数， 表示运行哪个步骤， 可选参数为1、2、3。"
      ],
      "retrieved_contexts": [
        "142 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n5. DeepSpeed 初始化\n设置 DeepSpeed 的配置参数后， 可以利用DeepSpeed 进行模型训练的初始化， 初始化流程如下。\n（1）确定运行的设备：首先，检查代码是否有指定的本地 GPU（通过 args.local_rank） 。如果\n没有指定，则程序默认使用 CUDA 设备。否则，它会为进程设置指定的 GPU。\n（2） 初始化分布式后端： 在分布式训练中， 使用deepspeed.init_distributed() 函数实现每个进程\n与其他进程的同步，初始化分布式环境。\n（3）获取当前进程的全局排序：在分布式训练中，使用torch.distributed.get_rank() 函数获得每\n个进程的唯一排序或 ID。\n（4）设置 DeepSpeed 配置：根据用户参数（如是否进行 offload、使用哪个 Zero Stage 等）构\n建一个 DeepSpeed 配置字典，来决定训练设置。\n（5） 同步所有工作进程： 使用torch.distributed.barrier() 确保在进一步的初始化之...",
        "130 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n通信可以并行。通过减少通信量和增加局部性与并行性，数据并行通信的有效带宽有效增大。\n图4.29 给出了 DeepSpeed 3D 并行策略示意图。图中给出了 32 个计算设备进行 3D 并行的例\n子。神经网络的各层分为 4 个流水线阶段。每个流水线阶段中的层在 4 个张量并行计算设备之间\n进一步划分。最后，每个流水线阶段有两个数据并行实例，使用 ZeRO 内存优化在这 2 个副本之\n间划分优化器状态量。\n~ ~\n~\n~\n~\n~\n~~\n图 4.29 DeepSpeed 3D 并行策略示意图 [177]\nDeepSpeed 软件架构如图4.30 所示，主要包含以下三部分。\n（1）API：DeepSpeed 提供了易于使用的 API 接口，简化了训练模型和推断的过程。用户只需\n调用几个 API 接口即可完成任务。 通过 “initialize” 接口可以初始化引擎， 并在参数中配置训练参\n数、优化技术等。这些配置参数通常保存在名为“ds_config.json”的文件中。\n（2）RunTime：RunTime 是 DeepSp...",
        "数）和 MT-NLG[134]（5400 亿个参数）都采用 DeepSpeed 框架完成训练。\nDeepSpeed 的主要优势在于支持大规模神经网络模型、提供了更多的优化策略和工具。Deep-\nSpeed 通过实现三种并行方法的灵活组合， 即ZeRO 支持的数据并行、 流水线并行和张量并行， 可\n以应对不同工作负载的需求。 特别是通过3D 并行性的支持，DeepSpeed 可以处理具有万亿个参数\n的超大规模模型。DeepSpeed 还引入了 ZeRO-Offload，使单个 GPU 能够训练比其显存容量大 10\n倍的模型。为了充分利用 CPU 和 GPU 的内存来训练大语言模型， DeepSpeed 还扩展了 ZeRO-2。\n此外，DeepSpeed 还提供了稀疏注意力核 （Sparse Attention Kernel） ， 支持处理包括文本、 图像和语\n音等长序列输入的模型。DeepSpeed 还集成了 1 比特 Adam 算法（1-bit Adam） ，该算法可以只使\n用原始 Adam 算法 1/5 的通信量，达到与 Adam 类似的收敛率，显著提高分布式训练的效率，降\n低通信开...",
        "144 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n6. 模型训练\n借助 DeepSpeed 框架实现对模型的训练，训练步骤大致分为以下几个阶段。\n（1） 训练前的准备： 使用print_rank_0 函数输出当前的训练状态。 该函数确保只有指定的进程\n（通常是主进程）会打印消息，避免了多进程环境下的重复输出。在开始训练之前，对模型进行一\n次评估，计算模型的困惑度。\n（2） 训练循环： 每个周期的开始， 都会打印当前周期和总周期数。 在每次迭代中， 数据批次先被\n移动到相应的 GPU 设备， 接着模型对这个批次进行前向传播计算损失。 使用model.backward(loss)\n计算梯度，并使用 model.step() 更新模型参数。对于主进程，还会使用 print_throughput 函数打印\n吞吐量，这有助于了解模型的训练速度和效率。\n（3）保存模型：如果指定了输出目录，则模型的状态和配置将被保存。模型可以在不同的格\n式中保存，例如 HuggingFace 的模型格式或 DeepSpeed 的 Zero Stage 3 特定格式。save_hf_format\n函数...",
        "大规模语言模型：从理论到实践 129\n[3] data = 4.0\n[0] data = 4.0\n[1] data = 4.0\n[2] data = 4.0\n4.4 DeepSpeed 实践\nDeepSpeed[172] 是一个由 Microsoft 公司开发的开源深度学习优化库，旨在提高大语言模型训\n练的效率和可扩展性，使研究人员和工程师能够更快地迭代和探索新的深度学习模型和算法。它\n采用了多种技术手段来加速训练，包括模型并行化、梯度累积、动态精度缩放、本地模式混合精\n度等。 此外，DeepSpeed 还提供了一些辅助工具， 例如分布式训练管理、 内存优化和模型压缩， 以\n帮助开发者更好地管理和优化大规模深度学习训练任务。DeepSpeed 是基于 PyTorch 构建的， 因此\n将现有的 PyTorch 训练代码迁移到 DeepSpeed 上通常只需要进行简单的修改。这使得开发者可以\n快速利用 DeepSpeed 的优化功能来加速训练任务。DeepSpeed 已经在许多大规模深度学习项目中\n得到了应用，包括语言模型、图像分类、目标检测等领域。大语言模型BLOOM[31]（1750 ..."
      ],
      "response_time": 6.0337584018707275,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.21621621621621623,
        "rouge2": 0.0,
        "rougeL": 0.16216216216216217,
        "bleu": 0.024753878174314035,
        "semantic_similarity": 0.8373633623123169
      }
    },
    {
      "sample_id": "0083",
      "question": "Kimi k1.5 的多模态数据包括哪五类?",
      "ground_truth": "字幕、图像-文本交错数据、OCR、知识和一般问题回答五类。",
      "predicted_answer": "Kimi k1.5 的多模态数据包括字幕、图像-文本交错数据、OCR、知识和一般问题回答五类。[3]",
      "ground_context": [
        "作为多模态模型，Kimi k1.5 的多模态数据包括字幕、图像-文本交错数据、OCR、知识和一般问题回答五类。"
      ],
      "retrieved_contexts": [
        "免奖励操纵，排除易出现验证错误的问题类型，并通过特定方法识别和移除易被操纵的提示。\n此外，为提高模型的图像推理能力，数据还来源于现实世界数据、合成视觉推理数据和文本\n渲染数据这三个类别。\n预训练数据集的构建与处理： Kimi k1.5 的预训练数据集涵盖英语、 中文、 代码、 数学与推理\n以及知识数据五个领域，以确保数据多样性。为保证数据高质量，采用多种清洗方法：\n针对英语和中文文本数据，建立多维质量过滤框架，包括基于规则的过滤、基于 FastText 的\n分类、基于嵌入的相似性分析和基于大模型的质量评估，最后通过动态采样率对不同质量的文档\n进行处理。\n对于代码数据， 对纯代码数据和文本-代码交错数据分别进行处理， 前者遵循BigCode 方法进\n行预处理和采样调整，后者采用基于句向量的方法召回高质量数据。数学与推理数据通过开发专\n门的数据清洗程序和 OCR 模型， 以及两阶段数据清洗过程， 提高数据质量。 知识数据通过精心策\n划，利用内部语言模型添加多维标签，并实施复杂的过滤和采样管道，优化数据组成。\n微调数据集的构建： Kimi k1.5 的 SFT 数据集包含约 100 万个文本...",
        "作为负样本（包括错误长响应和正确但超长响应） ，形成成对偏好数据用于DPO 训练，DPO 细节\n参见下文 [245]。\n4. 长到短强化学习：在标准 RL 训练后，选择性能和 token 效率平衡最佳的模型作为基础模\n型，进行单独的长到短 RL 训练，应用长度惩罚方案惩罚超长响应。\n改进的策略优化：Kimi k1.5 推导出带有长推理链的强化学习公式， 并采用在线镜像下降的变\n体进行策略优化。该算法通过以下方式进一步改进：\n1. 采样策略：采用课程采样和优先级采样策略。课程采样从简单任务开始训练，逐渐过渡到\n困难任务，利用数据的难度标签提高训练效率；优先级采样跟踪每个问题的成功率，按比例采样\n问题，使模型专注于薄弱领域。\n2. 长度惩罚：针对 RL 训练期间模型响应长度增加的问题，引入长度奖励限制 token 长度增\n长。在正确答案中提倡简短回答并惩罚较长回答，对错误答案的长回答明确惩罚。为缓解长度惩\n罚在训练初期对训练速度的影响，采用逐渐增加长度惩罚的方式。\n3. 训练架构及工程框架\nKimi k1.5 模型的训练分为三个阶段：\n1. 视觉语言预训练阶段：模型最初仅在语言数据上训练，...",
        "如一般问答、 编程、 数学和科学等。 此外， 还构建了100 万个文本 - 视觉示例， 涵盖图表解读、OCR\n等多种类别。\n多模态数据：作为多模态模型，Kimi k1.5 的多模态数据包括字幕、 图像-文本交错数据、OCR、\n知识和一般问题回答五类。对每类数据进行了针对性处理：\n标题数据整合开源和内部数据，并严格限制合成数据比例，同时进行质量控制和图像分辨率\n调整。 图像-文本交错数据则考虑开源数据集并构建自建数据， 还通过数据重排序确保图像和文本\n顺序正确。OCR 数据来源多样， 包括公开数据和自建数据集， 并进行数据增强以提高模型的OCR\n能力。",
        "210 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n解决数据受限的问题。大语言模型可通过强化学习中的奖励机制，学习如何探索不同的推理路径，\n从而扩大训练数据的范围。但此前的相关研究成果未达到理想的竞争效果，Kimi k1.5 旨在探索一\n条更有效的技术路线。\n1. 技术路线\n强化学习提示数据集构建： RL 提示数据集的质量和多样性对强化学习的有效性至关重要。\nKimi k1.5 定义了高质量 RL 提示集的三个关键属性：\n多样覆盖：提示应涵盖 STEM、编程和一般推理等广泛学科，以增强模型的适应性和跨领域\n应用能力。为此，采用自动过滤器选择需要丰富推理且易于评估的问题，数据集来源广泛，包括\n不同领域的问题以及纯文本和图像 - 文本问答数据。\n平衡难度：提示集应包含不同难度级别的问题，以促进模型的逐步学习并防止过度拟合。利\n用模型自身能力自适应评估提示难度， 通过SFT 模型多次生成答案计算通过率作为难度代理指标，\n并开发标签系统按领域和学科分类提示，实现难度平衡。\n准确评估能力：提示应能被验证者客观可靠地评估，确保基于正确推理衡量模型性能。为避\n免奖励操纵，排除易出现验证...",
        "大规模语言模型：从理论到实践 211\n2. 算法创新\n长上下文扩展： Kimi k1.5 将 RL 的上下文窗口扩展到 128k， 实验表明， 上下文长度与模型解\n决问题的能力强相关，增加上下文长度可提升模型在困难推理基准测试中的性能。为解决长上下\n文带来的计算量增加问题，采用部分回放（ partial rollouts）技术，通过重用之前轨迹的大部分来\n采样新轨迹，减少计算开销。具体操作时，部分展开系统将长响应分解为多个段，在多个迭代中\n逐步处理，加快训练速度。\nLong2short 的上下文压缩策略\n长上下文模型虽性能强大，但测试时 token 消耗较多。Kimi k1.5 提出多种方法将长上下文模\n型的思维先验转移到短上下文模型，以提高短上下文模型的性能：\n1. 模型合并： 通过简单平均长上下文模型和短上下文模型的权重， 获得无需训练的新模型， 有\n助于保持泛化能力。\n2. 最短拒绝采样： 基于模型对同一问题生成响应长度变化大的特点， 对同一问题多次采样， 选\n择最短的正确响应。\n3. DPO ：利用长上下文模型生成多个响应样本，将最短正确解决方案作为正样本，较长响应\n作为负样本..."
      ],
      "response_time": 1.1535894870758057,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 0.3333333333333333
      },
      "answer_quality_metrics": {
        "rouge1": 0.33333333333333337,
        "rouge2": 0.0,
        "rougeL": 0.33333333333333337,
        "bleu": 0.6068410610880497,
        "semantic_similarity": 0.7362501621246338
      }
    },
    {
      "sample_id": "0084",
      "question": "RLOO算法是基于哪个算法发展而来的？",
      "ground_truth": "REINFORCE算法",
      "predicted_answer": "RLOO算法是基于REINFORCE算法发展而来的[1][3]。",
      "ground_context": [
        "REINFORCE Leave-One-Out（RLOO）算法是在 REINFORCE 算法基础上发展而来的一种改进算法"
      ],
      "retrieved_contexts": [
        "θn+1 = arg max\nθ\nJPPO(θn)\n7: 通过最小化均方误差来更新价值函数：\nϕn+1 = arg min\nϕ\nL(ϕn)\n8: end for\n6.2.5 RLOO\nREINFORCE Leave-One-Out（RLOO）算法是在 REINFORCE 算法基础上发展而来的一种改\n进算法，它主要针对 REINFORCE 算法梯度估计方差较高的问题，通过利用多个在线样本构建更\n有效的基线来降低方差，从而提升算法性能。\n1. 算法原理\nRLOO 的核心在于改进基线的构建方式。在 REINFORCE 算法中，通常使用简单的移动平均\n基线，这种基线在处理复杂环境和多样本情况时存在一定局限性。RLOO 则利用每次采样得到的\n多个样本之间的关系，为每个样本单独构建基线。",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "204 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n(1) 方差降低效果：REINFORCE 算法使用简单基线 （如移动平均基线） ， 在降低方差方面效果有\n限。 而RLOO 通过利用多个样本构建动态基线， 能更有效地降低梯度估计的方差。 例如， 在\n实验中，RLOO 在相同训练条件下， 其奖励方差明显低于REINFORCE 算法， 这使得RLOO\n在优化过程中更加稳定，能够更快地收敛到较优的策略。\n(2) 样本利用效率：REINFORCE 算法在更新策略时，每个样本主要用于自身的梯度计算，样本\n之间的信息利用不足。RLOO 则充分利用了多个样本之间的关系， 每个样本不仅用于自身的\n梯度计算， 还参与构建其他样本的基线， 大大提高了样本的利用效率。实验表明， 在相同采\n样预算下，RLOO 能够实现更好的优化效果，如在多个数据集和模型上的实验显示，RLOO\n在胜率和奖励优化方面均优于 REINFORCE 算法。\n(3) 计算复杂度： 虽然RLOO 在样本利用和方差降低上具有优势， 但它的计算复杂度相对REIN-\nFORCE 算法有所增加。在构建基线时，RLOO 需要对每个样...",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "随着样本数量 k 的增加， 计算量会相应增大。 不过， 在实际应用中， 由于其在性能上的显著\n提升，这种计算复杂度的增加在可接受范围内。\n4. 算法特性分析\nRLOO 算法在继承 REINFORCE 算法优点的同时，有效改进了其部分缺陷。它通过多样本构\n建基线的方式，降低了梯度估计的方差，提高了策略更新的稳定性和准确性，使得算法在复杂环\n境和大规模任务中表现更优。然而，RLOO 算法也并非完美无缺。在处理大规模样本时，其计算\n复杂度的增加可能会成为限制因素，需要消耗更多的计算资源和时间。此外，RLOO 算法对样本\n的独立性假设较为依赖，如果样本之间存在较强的相关性，可能会影响基线的有效性，进而影响\n算法性能。 在实际应用中， 需要根据具体问题的特点和资源情况， 合理选择是否使用RLOO 算法。\n6.2.6 GRPO\n图 6.2 GRPO 算法流程图（需要重画）"
      ],
      "response_time": 0.7478883266448975,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.4,
        "rouge2": 0.0,
        "rougeL": 0.4,
        "bleu": 0.036422694300114855,
        "semantic_similarity": 0.6932128667831421
      }
    },
    {
      "sample_id": "0085",
      "question": "随着样本数量 k 的增加，计算量会如何变化？",
      "ground_truth": "计算量会相应增大。",
      "predicted_answer": "随着样本数量 \\( k \\) 的增加，计算量也会相应增大 [1][4]。尽管如此，在实际应用中，这种计算复杂度的增加仍在可接受范围内 [1]。",
      "ground_context": [
        "随着样本数量 k 的增加，计算量会相应增大。"
      ],
      "retrieved_contexts": [
        "随着样本数量 k 的增加， 计算量会相应增大。 不过， 在实际应用中， 由于其在性能上的显著\n提升，这种计算复杂度的增加在可接受范围内。\n4. 算法特性分析\nRLOO 算法在继承 REINFORCE 算法优点的同时，有效改进了其部分缺陷。它通过多样本构\n建基线的方式，降低了梯度估计的方差，提高了策略更新的稳定性和准确性，使得算法在复杂环\n境和大规模任务中表现更优。然而，RLOO 算法也并非完美无缺。在处理大规模样本时，其计算\n复杂度的增加可能会成为限制因素，需要消耗更多的计算资源和时间。此外，RLOO 算法对样本\n的独立性假设较为依赖，如果样本之间存在较强的相关性，可能会影响基线的有效性，进而影响\n算法性能。 在实际应用中， 需要根据具体问题的特点和资源情况， 合理选择是否使用RLOO 算法。\n6.2.6 GRPO\n图 6.2 GRPO 算法流程图（需要重画）",
        "大规模语言模型：从理论到实践 203\n假设在一次训练中，从策略 πθ(a|s) 中采样得到 k 个独立同分布的样本 y(1), · · · , y(k)\ni.i.d\n∼\nπθ(.|x)，对于每个样本 y(i)，其对应的奖励为 R(y(i), x)。RLOO 构建的基线为除 y(i) 之外的其他\nk − 1 个样本奖励的平均值，即 1\nk−1\nP\nj̸=i R(y(j), x)。\n基于此，RLOO 的策略梯度估计公式为：\n1\nk\nkX\ni=1\n2\n4R\n\u0000\ny(i), x\n\u0001\n− 1\nk − 1\nX\nj̸=i\nR\n\u0000\ny(j), x\n\u0001\n3\n5 ∇ log π\n\u0000\ny(i)|x\n\u0001\n(6.30)\n这个公式的含义是，对每个样本的奖励减去用其他样本构建的基线，再乘以该样本动作概率的对\n数梯度，最后对所有样本的结果进行平均，以此来估计策略梯度。\n2. 算法步骤\nRLOO 算法的实现步骤在 REINFORCE 算法基础上有所扩展：\n(1) 初始化策略参数：同 REINFORCE 算法，随机初始化策略网络参数 θ。\n(2) 采样多组轨迹： 使用当前策略πθ(a|s) 与环境交互， 每次收集k 条轨...",
        "大规模语言模型：从理论到实践 199\n因此，为了估计当前动作价值，不必采样未来的很多步，而只采样一步。对于一步之后的很\n多步结果，则使用状态价值函数进行估计，即\nQ(st, at) = rt + γV (st+1)\n假设 V (st) 是无偏的，那么动作价值也是无偏的，即：\nE [rt + γV (st+1)] = E\n\"\nrt + γE\n\" TX\nt′=t+1\nγt′−t−1rt′\n##\n通过展开，我们得到：\nE\n\"\nrt + γ\nTX\nt′=t+1\nγt′−t−1rt′\n#\n= E\n\"\nrt +\nTX\nt′=t+1\nγt′−trt′\n#\n= E\n\" TX\nt′=t\nγt′−trt′\n#\n前面使用了 Vϕ(st) 来近似 V (st)，这可能导致 rt + γVϕ(st+1) 有较高的偏差，尽管其方差较低。\n类似地，可以采样 k 步奖励，即\nQk(st, at) = rt + γrt+1 + · · · + γk−1rt+k−1 + γkV (st+k)\n随着 k 的增大，这个结果逐渐趋向于蒙特卡洛方法。因此，从蒙特卡洛方法到时序差分方法，方\n差逐渐减小，偏差逐渐增大。k 步优势可以...",
        "204 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n(1) 方差降低效果：REINFORCE 算法使用简单基线 （如移动平均基线） ， 在降低方差方面效果有\n限。 而RLOO 通过利用多个样本构建动态基线， 能更有效地降低梯度估计的方差。 例如， 在\n实验中，RLOO 在相同训练条件下， 其奖励方差明显低于REINFORCE 算法， 这使得RLOO\n在优化过程中更加稳定，能够更快地收敛到较优的策略。\n(2) 样本利用效率：REINFORCE 算法在更新策略时，每个样本主要用于自身的梯度计算，样本\n之间的信息利用不足。RLOO 则充分利用了多个样本之间的关系， 每个样本不仅用于自身的\n梯度计算， 还参与构建其他样本的基线， 大大提高了样本的利用效率。实验表明， 在相同采\n样预算下，RLOO 能够实现更好的优化效果，如在多个数据集和模型上的实验显示，RLOO\n在胜率和奖励优化方面均优于 REINFORCE 算法。\n(3) 计算复杂度： 虽然RLOO 在样本利用和方差降低上具有优势， 但它的计算复杂度相对REIN-\nFORCE 算法有所增加。在构建基线时，RLOO 需要对每个样...",
        "98 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n倍的速度发展。模型参数规模增大的同时，对训练数据量的要求也呈指数级增长，这更加剧了对\n算力的需求。然而，近几年，CPU 的算力增加已经远低于摩尔定律（Moore’s Law） ，虽然计算加\n速设备（如 GPU、TPU 等）为机器学习模型提供了大量的算力，但是其增长速度仍然没有突破每\n18 个月翻倍的摩尔定律。只有通过分布式训练系统才可以匹配模型不断增长的算力需求，满足机\n器学习模型的发展需要。\n图 4.1 单个计算设备和多个计算设备的示例\n图 4.2 机器学习模型参数量增长和计算硬件的算力增长对比 [167]\n分布式训练的总体目标就是加快总的训练速度，减少模型训练的总体时间。总训练速度可以\n用式（4.1）简略估计：\n总训练速度 ∝ 单设备计算速度 × 计算设备总量 × 多设备加速比 (4.1)\n其中，单设备计算速度主要由单块计算加速芯片的运算速度和数据 I/O 能力决定，对单设备训练"
      ],
      "response_time": 1.329493522644043,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.01654081768851188,
        "semantic_similarity": 0.7624920606613159
      }
    },
    {
      "sample_id": "0086",
      "question": "强化学习算法对模型性能优化的有效性是谁证明的？",
      "ground_truth": "Diamond 等",
      "predicted_answer": "Diamond等人证明了强化学习算法对模型性能优化的有效性[1]。",
      "ground_context": [
        "Diamond 等，也展现出强大的推理能力，证明了强化学习算法对模型性能优化的有效性。"
      ],
      "retrieved_contexts": [
        "Diamond 等，也展现出强大的推理能力，证明了强化学习算法对模型性能优化的有效性。\n• 自我进化过程： 训练过程中， 模型的思考时间和生成回答的长度不断增加， 这并非外部调整\n所致， 而是模型在强化学习环境中自我改进的结果。 随着测试时计算量的增加， 模型自发出\n现复杂行为， 如反思先前步骤、 探索多种解题方法等， 显著提升了推理能力， 使其能够处理\n更具挑战性的任务。\n• 顿悟时刻： 训练过程中出现 “aha moment” ， 模型在中间版本学会重新评估初始解题方法， 分\n配更多思考时间， 这一行为不仅体现了模型推理能力的提升， 也展示了强化学习可带来意想\n不到的复杂结果，凸显了强化学习在激发模型智能方面的潜力。",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "找食物、住宿还是娱乐。\n现代强化学习之父 Richard Sutton 在 《苦涩的教训 （The Bitter Lesson） 》 中指出， 过去70 年人\n工智能研究领域最重要的一堂课是，只有通用的、可规模化扩展的方法才是最终有效的，而且优\n势巨大。 因此， 结合OpenAI 的研究实践， 强化学习在大语言模型中的优势可重新归纳为以下三个\n维度：\n（1）摆脱局部最优束缚的全局优化能力。监督学习依赖词元级精确标注，本质上将人类先验",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "（Chain-of-Thought） 的决策序列， 在数学证明、 代码生成等场景中展现出超越监督学习的推理能力。\n相较于传统监督学习， 强化学习框架具有显著优势：在RLHF 范式下， 模型通过生成-反馈的\n闭环机制持续优化，摆脱对标准答案的绝对依赖；在深度推理场景中，强化学习能自主探索最优\n推理路径，通过价值函数估计引导模型突破局部最优解。两类方法都强调对生成文本的整体质量\n把控，前者侧重人类价值对齐，后者专注复杂问题求解，共同构成大语言模型能力进化的核心驱\n动力。\n本章将系统阐述基于人类反馈的强化学习技术体系，解析奖励模型构建、策略优化算法等关\n键组件。同时深入探讨强化学习在深度推理任务中的创新应用，包括思维链强化、过程奖励设计\n等前沿方法。最后通过 verl 实践案例，展示强化学习技术在大语言模型训练中的工程实现与效果\n验证。\n6.1 强化学习概述\n强化学习（Reinforcement Learning，RL）研究的是智能体与环境交互的问题，其目标是使智\n能体在复杂且不确定的环境中最大化 奖励。强化学习基本框架如图6.1 所示，主要由两部分组成："
      ],
      "response_time": 0.7054364681243896,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.6666666666666666,
        "rouge2": 0.0,
        "rougeL": 0.6666666666666666,
        "bleu": 0.034430441335820114,
        "semantic_similarity": 0.5169724822044373
      }
    },
    {
      "sample_id": "0087",
      "question": "DeepSeek-R1 在 AIME 2024 上的 Pass@1 得分是多少？",
      "ground_truth": "79.8%",
      "predicted_answer": "DeepSeek-R1 在 AIME 2024 上的 Pass@1 得分为 79.8%[1]。",
      "ground_context": [
        "DeepSeek-R1 在多个推理任务中表现出色，在 AIME 2024 上 Pass@1 得分达到 79.8%，略超 OpenAI-o1-1217。"
      ],
      "retrieved_contexts": [
        "动了相关领域的研究发展。\n4.2 模型性能卓越 DeepSeek-R1 在多个推理任务中表现出色， 在AIME 2024 上 Pass@1 得分达到\n79.8%，略超 OpenAI-o1-1217；在 MA TH-500上得分高达 97.3%，与 OpenAI-o1-1217 相当且远超\n其他模型。在编码相关任务中，于 Codeforces 竞赛中获得 2029 Elo 评级，超越 96.3% 的人类参与\n者；在知识类基准测试如 MMLU、GPQA Diamond 等任务中，也取得了优异成绩，展现出强大的\n知识掌握和推理应用能力。\n6.3.2 Kimi k1.5\n基于下一个 token 预测的语言模型预训练遵循缩放定律，即按比例增加模型参数和数据规模\n可提升模型智能程度。然而，这种方法严重依赖高质量训练数据的数量。在实际应用中，可用的\n高质量数据往往有限，这限制了模型性能的进一步提升。将强化学习与大语言模型相结合，有望",
        "出的同时，更符合用户需求和安全标准。\n3. 蒸馏：赋予小模型推理能力\n为使更小的模型具备类似 DeepSeek-R1 的推理能力，使用在 DeepSeek-R1 训练过程中收集的\n800k 样本，对 Qwen 和 Llama 等开源模型进行直接微调。实验发现，这种简单的蒸馏方法能显著\n提升小模型的推理能力。在实验中， 选择Qwen2.5-Math-1.5B、Qwen2.5-Math-7B 等多种模型作为\n基础模型，仅对蒸馏模型进行 SFT，未引入 RL 阶段，以突出蒸馏技术的有效性，后续 RL 阶段\n的探索留给研究社区。 结果显示， 蒸馏后的小模型在多个推理基准测试中表现优异， 如DeepSeek-\nR1-Distill-Qwen-7B 在 AIME 2024 上的成绩超越了部分强大的基线模型。\n4. 总结\n4.1 强化学习训练创新 DeepSeek-R1-Zero 首次验证了大语言模型的推理能力可通过纯强化学习\n激发，无需监督微调作为前期步骤。这种创新训练方式使模型能够自主探索思维链以解决复杂问\n题， 展现出自我验证、 反思和生成长思维链等能力， 为大语言模型推理能力提升开辟了新路径...",
        "Hunyuan-Large 2024 年 11 月 3890 亿 基础模型和对话模型 7.0 万亿个词元\nDeepSeek-V3 2024 年 12 月 6710 亿 对话模型 14.8 万亿个词元\nPhi-4 2024 年 12 月 140 亿 对话模型 10.0 万亿个词元\nDeepSeek-R1 2025 年 1 月 6710 亿 推理模型 14.8 万亿个词元",
        "晰呈现。\n不使用结果或过程神经奖励模型，因其在大规模强化学习中可能出现奖励黑客问题，且重新训练\n奖励模型会增加计算资源需求并使训练流程复杂化。\n1.3 训练模板 设计简单训练模板， 要求DeepSeek-R1-Zero 先产生推理过程， 再给出最终答案。 模\n板为：用户提出问题，助手先在脑海中思考推理过程，然后提供答案，推理过程和答案分别包含\n在 <think> </think> 和 <answer> </answer> 标签内，训练时 prompt 会被具体推理问题替换。通过\n这种模板，在避免内容特定偏差的同时，引导模型遵循指定结构进行推理，便于观察模型在强化\n学习过程中的自然发展。\n1.4 性能、自我进化过程与顿悟时刻\n• 性能： 在AIME 2024 基准测试中，DeepSeek-R1-Zero 的平均 pass@1 分数从初始的 15.6% 显\n著提升至 71.0%，达到与 OpenAI-o1-0912 相当的性能水平。使用多数投票后，分数进一步\n提升至 86.7%，超过 OpenAI-o1-0912。在其他推理相关基准测试中，如 MA TH-500、GPQA\nDiamond ...",
        "大规模语言模型：从理论到实践 439\nAIME（American Invitational Mathematics Examination， 美国邀请数学竞赛） 是一个以高挑战性著\n称的数学竞赛基准， 专为测试高中生的高级数学问题解决能力而设计。AIME 是继 AMC（American\nMathematics Competitions ，美国数学竞赛）之后的高级阶段考试，只有在 AMC 中表现优异的学\n生才有资格参加。其题目难度较高，涵盖了广泛的数学领域，包括代数、几何、数论和组合数学。\nAIME 的问题设置独具特色，旨在评估学生的深度数学思考能力、逻辑推理能力以及精确的计算\n能力。与许多其他数学竞赛不同，AIME 的试题通常要求考生提供一个具体的整数答案，而不是\n选择题形式。这种设计不仅考验了考生的数学知识，还挑战了他们在解题过程中保持细致和准确\n的能力。由于 AIME 题目难度较大，考生需要具备扎实的数学基础，同时还需要灵活运用多种数\n学思想来解决问题。比赛的目的是培养学生的创造性思维，锻炼他们面对复杂问题时的分析能力\n和解决能力。也正因如此，AIME 在全球范围内都备受关注，成为..."
      ],
      "response_time": 1.0237619876861572,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.3636363636363636,
        "rouge2": 0.2222222222222222,
        "rougeL": 0.3636363636363636,
        "bleu": 0.014238348381961592,
        "semantic_similarity": 0.5149210095405579
      }
    },
    {
      "sample_id": "0088",
      "question": "环境会如何根据智能体的行为给予反馈？",
      "ground_truth": "通常以奖励的形式。",
      "predicted_answer": "环境会根据智能体采取的动作给出下一个状态及当前动作带来的奖励[1]。",
      "ground_context": [
        "环境会根据智能体的行为给予反馈，通常以奖励的形式。"
      ],
      "retrieved_contexts": [
        "大规模语言模型：从理论到实践 191\n智能体和环境。 在强化学习过程中， 智能体与环境不断交互。 智能体在环境中获取某个状态后， 会\n根据该状态输出一个动作， 也称为决策。 动作会在环境中执行， 环境会根据智能体采取的动作， 给\n出下一个状态及当前动作带来的奖励。智能体的目标就是尽可能多地从环境中获取奖励。本节将\n介绍强化学习的基本概念、强化学习与有监督学习的区别，以及在大语言模型中基于人类反馈的\n强化学习流程。\n图 6.1 强化学习基本框架\n在现实生活中，经常会遇到需要通过探索和试错来学习的情境。例如，孩子学会骑自行车的\n过程或是教宠物狗如何玩飞盘。宠物狗一开始对如何抓飞盘一无所知，但每当它成功抓住飞盘时，\n都可以给予它一定的奖励。这种通过与环境交互，根据反馈来学习最佳行为的过程正是强化学习\n的核心思想。通过宠物狗学习抓飞盘的例子，可以引出一些强化学习中的基本概念。\n（1）智能体与环境：在宠物狗学习抓飞盘的场景中，宠物狗就是一个智能体（Agent），它做\n出决策（Decision）并执行动作。它所在的场景， 包括飞盘的飞行轨迹和速度， 以及其他可能的因\n素，构成了环境（Environ...",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "192 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n6.1.1 强化学习基础概念\n智能体与环境的不断交互过程中，会获得很多观测 oi。针对每一个观测，智能体会采取一个\n动作 ai，也会得到一个奖励 ri。可以定义历史 Ht 是观测、动作、奖励的序列：\nHt = o1, a1, r1, o2, a2, r2, · · · , ot, at, rt (6.1)\n由于智能体在采取当前动作时会依赖它之前得到的历史， 因此可以把环境整体状态St 看作关于历\n史的函数：\nSt = f (Ht) (6.2)\n当智能体能够观察到环境的所有状态时，称环境是完全可观测的（ Fully Observed） ，这时观测ot\n等于 St。当智能体只能看到部分观测时，称环境是部分可观测的（Partially Observed） ，这时观测\n是对状态的部分描述。整个状态空间使用 S 表示。\n在给定的环境中， 有效动作的集合经常被称为动作空间（Action Space）， 使用A 表示。例如\n围棋（Go）这样的环境具有离散动作空间（Discrete Action Space），智能体的动作数量在这个空\n...",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "能够根据环境反馈和人类反馈动态调整操作策略，从而不断优化工具的使用方法。\n在实际操作中，环境反馈通常通过智能体与外部环境交互后所获得的结果进行优化；具体而\n言，结果反馈用于评估智能体一系列动作的整体效果，而中间反馈则着重考察每一步操作的即时\n表现。例如，在 WebShop[393] 场景中，智能体通过对比其购买行为与人类购买行为之间的相似性\n来获得结果反馈，从而评估其表现的有效性。在此基础上，人类反馈强化学习通过模拟人类奖励\n机制，结合强化学习算法优化智能体的策略，以提升其决策能力和执行效果。同时，智能体会将\n每次尝试的结果系统化地记录下来，构建经验库。这一过程不仅使智能体能够积累丰富的操作经\n验，还能逐步提升其对工具的使用熟练度和操作效率。\n探索学习的关键在于通过持续的试探与调整，使智能体在动态环境中不断完善其工具使用能\n力。这种方法不仅赋予智能体更强的适应性与自主性，还为其在多变任务场景中的高效表现提供\n了坚实的技术支持。\n当前研究的重点在于如何通过整合多种学习策略来优化模型性能，从而全面提升大模型智能\n体的表现能力。例如，将示范学习的精确性与探索学习的灵活性相结合，可以显著增强模..."
      ],
      "response_time": 0.7470507621765137,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.015848207591271795,
        "semantic_similarity": 0.6358858346939087
      }
    },
    {
      "sample_id": "0089",
      "question": "面向推理的强化学习采用了什么样的大规模强化学习训练过程？",
      "ground_truth": "采用与 DeepSeek-R1-Zero 相同的大规模强化学习训练过程。",
      "predicted_answer": "面向推理的强化学习，如DeepSeek团队的研究，采用了基于GRPO算法的大规模强化学习训练过程[3]。该算法舍弃了传统Actor-Critic范式中的critic模型，通过从一组得分估计基线来优化策略模型，从而提高了强化学习的效率[3]。",
      "ground_context": [
        "在冷启动微调后，采用与 DeepSeek-R1-Zero 相同的大规模强化学习训练过程，聚焦于编码、数学、科学和逻辑推理等推理密集型任务。"
      ],
      "retrieved_contexts": [
        "（Chain-of-Thought） 的决策序列， 在数学证明、 代码生成等场景中展现出超越监督学习的推理能力。\n相较于传统监督学习， 强化学习框架具有显著优势：在RLHF 范式下， 模型通过生成-反馈的\n闭环机制持续优化，摆脱对标准答案的绝对依赖；在深度推理场景中，强化学习能自主探索最优\n推理路径，通过价值函数估计引导模型突破局部最优解。两类方法都强调对生成文本的整体质量\n把控，前者侧重人类价值对齐，后者专注复杂问题求解，共同构成大语言模型能力进化的核心驱\n动力。\n本章将系统阐述基于人类反馈的强化学习技术体系，解析奖励模型构建、策略优化算法等关\n键组件。同时深入探讨强化学习在深度推理任务中的创新应用，包括思维链强化、过程奖励设计\n等前沿方法。最后通过 verl 实践案例，展示强化学习技术在大语言模型训练中的工程实现与效果\n验证。\n6.1 强化学习概述\n强化学习（Reinforcement Learning，RL）研究的是智能体与环境交互的问题，其目标是使智\n能体在复杂且不确定的环境中最大化 奖励。强化学习基本框架如图6.1 所示，主要由两部分组成：",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "大规模语言模型：从理论到实践 207\n模型在不同场景高效运用推理能力， 仍是学界和业界面临的挑战。 此前研究尝试了多种方法， 如基\n于过程的奖励模型、 强化学习以及蒙特卡洛树搜索和波束搜索等搜索算法， 但均未达到与OpenAI\no 系列模型相媲美的通用推理性能。在此背景下，DeepSeek 团队开展了基于纯强化学习提升模型\n推理能力的探索。\n1. DeepSeek-R1-Zero：基于基座模型的强化学习\n1.1 强化学习算法 DeepSeek 的研究人员采用 GRPO 算法进行强化学习， 该算法舍弃了传统Actor-\nCritic 范式中与策略模型规模相当的 critic 模型，通过从一组得分估计基线来优化策略模型。通过\n这种方式，能够提高强化学习的效率，有利于大规模强化学习的开展。\n1.2 奖励建模 采用基于规则的奖励系统，包含两种奖励类型：\n• 准确性奖励： 用于评估模型响应的正确性。 对于有确定性答案的数学问题， 要求模型按指定\n格式输出最终答案以便验证；对于 LeetCode 编程问题，利用编译器根据预定义测试用例生\n成反馈。\n• 格式奖励：促使模型将思考过程置于‘<think...",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "414 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n图 11.5 “红队”测试整体框架 [563]\n测试用例作为训练数据， 以最大似然估计损失为目标进行学习。 随机抽取90% 的测试用例组成训\n练集， 剩余的测试用例用于验证。 通过一次训练周期来学习pr(x)， 以保持测试用例的多样性并避\n免过拟合。\n（4） 强化学习： 使用强化学习来最大化有害性期望Epr(x)[r(x, y)]。 使用Advantage Actor-Critic\n（A2C）[564] 训练“红队”大语言模型pr(x)。 通过使用有监督学习得到的训练模型进行初始化热启\n动 pr(x)。为了防止强化学习塌陷到单个高奖励，还添加了损失项，使用当前 pr(x) 与初始化分布\n之间的 KL 散度。 最终损失是KL 散度惩罚项和 A2C 损失的线性组合， 使用α ∈ [0, 1] 进行两项之\n间的加权。\n11.2.3 垂直领域评估\n前面几节重点介绍了评估大语言模型整体能力的评估体系。本节将对垂直领域和重点能力的\n细粒度评估展开介绍，主要包括复杂推理、环境交互、特定领域。\n1. 复杂推理\n复杂推理（Complex ..."
      ],
      "response_time": 1.848379373550415,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.2,
        "rouge2": 0.0,
        "rougeL": 0.2,
        "bleu": 0.08048862002869049,
        "semantic_similarity": 0.7039951682090759
      }
    },
    {
      "sample_id": "0090",
      "question": "GRPO算法如何在平衡策略的探索与利用的同时优化目标函数？",
      "ground_truth": "GRPO通过对目标函数的优化，能够利用组内奖励信息，同时平衡策略的探索与利用，从而实现高效稳定的训练。",
      "predicted_answer": "GRPO算法通过以下方式平衡策略的探索与利用，从而优化目标函数：\n\n1. **利用组内奖励信息**：从旧策略中抽取多个输出形成组，并利用组内的奖励信息来计算优势值，这有助于更有效地估计基线，减少每个样本独立计算基线的成本[1]。\n   \n2. **优化目标函数设计**：GRPO的目标函数 \\( J_{\\text{GRPO}}(\\theta) \\) 设计得既能最大化策略的期望奖励，又能控制策略变化的幅度，以保持训练的稳定性[2][3]。\n\n3. **计算优势值**：对于每个输出和时间步，计算优势值 \\( \\hat{A}_{i,t} \\)，该值反映了在特定时间点采取某一行动相较于平均情况下的优势[5]。\n\n综上所述，通过这些机制，GRPO能够在利用组内奖励信息的同时平衡策略的探索与利用，实现高效且稳定的训练过程。",
      "ground_context": [
        "优化目标是最大化轨迹的期望回报 J(θ)，即：J(θ) = Eτ ∼P (τ ;θ) [R(τ )]",
        "通过对这个目标函数的优化，GRPO 能够在利用组内奖励信息的同时，平衡策略的探索与利用，实现高效稳定的训练。"
      ],
      "retrieved_contexts": [
        "大规模语言模型：从理论到实践 205\nGroup Relative Policy Optimization （GRPO）是一种基于近端策略优化算法改进而来的优化算\n法， 旨在解决传统PPO 在计算资源和训练稳定性方面的问题。 它通过创新的组奖励机制来估计基\n线，在不依赖独立价值模型的情况下实现高效训练，尤其适用于大型模型的优化。\n1. 算法概述\n传统的近端策略优化算法在训练过程中依赖独立的价值模型来估计奖励和减少方差。 然而， 这\n种方式在处理大型模型时会带来较高的计算成本和内存消耗。 GRPO 则另辟蹊径，它不再使用独\n立的价值模型，而是通过组奖励来估计基线。具体来说， GRPO 从旧策略中抽取多个输出（形成\n组） ， 利用组内奖励信息计算优势值， 以此优化策略。 这种方法避免了对每个样本都进行独立基线\n计算，大大减少了训练资源的消耗，在提升计算效率的同时，增强了训练过程的稳定性。\n2. 算法原理\nGRPO 的核心在于其优化目标函数的设计。目标函数 JGRPO(θ) 旨在最大化策略的期望奖励，\n同时控制策略的变化幅度，确保训练的稳定性：\nJGRPO(θ) =Eq∼P (Q),{oi}...",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "通过对这个目标函数的优化，GRPO 能够在利用组内奖励信息的同时，平衡策略的探索与利\n用，实现高效稳定的训练。\n3. 算法步骤\n如图6.2所示，GRPO 算法实施的流程如下：\n(1) 初始化策略参数：随机初始化当前策略模型 πθ 的参数 θ 以及旧策略模型 πθold 的参数（通常\n初始值与 πθ 相同） 。\n(2) 抽取组样本：从分布 P (Q) 中采样问题 q，然后根据旧策略 πθold(O|q) 为每个问题 q 抽取 G\n个输出 {oi}G\ni=1。\n(3) 计算优势值和目标函数： 对于每个输出oi 的每个时间步 t， 计算优势值 ˆAi,t ， 并根据目标函",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "i=1∼πθold (O|q)[ 1\nG\nGX\ni=1\n1\n|oi|\n|oi|X\nt=1\n[min( πθ(oi,t|q, oi,<t)\nπθold(oi,t|q, oi,<t)\nˆAi,t,\nclip\n\u0012 πθ(oi,t|q, oi,<t)\nπθold(oi,t|q, oi,<t) , 1 − ϵ, 1 + ϵ\n\u0013\nˆAi,t)] − βDKL [πθ||πref]]\n(6.34)\n在这个公式中 πθ 代表当前正在优化的策略模型，其参数为 θ，πθold 是旧的策略模型，用于提供参\n考和对比。G 表示组大小，即从旧策略 πθold 中抽取的多个输出 oi 的数量。每个 oi 都是一个完整\n的输出序列，|oi| 表示序列 oi 的长度。 ˆAi,t 是基于组内奖励计算得到的优势值，它衡量了在时间\n步 t 采取动作 oi,t 相对于平均水平的优势程度，用于指导策略的更新。ϵ 和 β 是超参数。ϵ 用于控\n制梯度剪切，防止策略更新幅度过大导致不稳定，β 则控制 KL 散度 DKL 的权重，DKL [πθ||πref]\n用于约束当前策略 πθ 和参考策略 πref 之间的差异，确保策略不会偏..."
      ],
      "response_time": 4.967054843902588,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 0.3333333333333333
      },
      "answer_quality_metrics": {
        "rouge1": 0.10526315789473684,
        "rouge2": 0.0,
        "rougeL": 0.10526315789473684,
        "bleu": 0.060763866797855184,
        "semantic_similarity": 0.8857806324958801
      }
    },
    {
      "sample_id": "0091",
      "question": "在推理密集型任务的强化学习训练中，RLOO算法相比REINFORCE算法在方差降低和样本利用效率方面表现如何？",
      "ground_truth": "RLOO算法通过构建动态基线，能够更有效地降低梯度估计的方差，从而在优化过程中更加稳定，收敛更快。而在样本利用效率方面，RLOO充分利用多个样本之间的关系，使得每个样本不仅用于自身的梯度计算，还参与构建其他样本的基线，这大大提高了样本的利用效率。相比之下，REINFORCE算法的样本利用效率有限，主要用于自身的梯度计算。",
      "predicted_answer": "在推理密集型任务的强化学习训练中，RLOO算法相比REINFORCE算法表现出以下特点：\n\n- 方差降低方面：RLOO通过利用多个样本构建动态基线，能更有效地降低梯度估计的方差。例如，在实验中，RLOO在相同训练条件下，其奖励方差明显低于REINFORCE算法，这使得RLOO在优化过程中更加稳定，能够更快地收敛到较优的策略[1]。\n- 样本利用效率方面：RLOO充分利用了多个样本之间的关系，每个样本不仅用于自身的梯度计算，还参与构建其他样本的基线，大大提高了样本的利用效率。实验表明，在相同采样预算下，RLOO能够在胜率和奖励优化方面均优于REINFORCE算法[1][2]。",
      "ground_context": [
        "面向推理的强化学习...聚焦于编码、数学、科学和逻辑推理等推理密集型任务。",
        "RLOO通过利用多个样本构建动态基线，能更有效地降低梯度估计的方差...大大提高了样本的利用效率。"
      ],
      "retrieved_contexts": [
        "204 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n(1) 方差降低效果：REINFORCE 算法使用简单基线 （如移动平均基线） ， 在降低方差方面效果有\n限。 而RLOO 通过利用多个样本构建动态基线， 能更有效地降低梯度估计的方差。 例如， 在\n实验中，RLOO 在相同训练条件下， 其奖励方差明显低于REINFORCE 算法， 这使得RLOO\n在优化过程中更加稳定，能够更快地收敛到较优的策略。\n(2) 样本利用效率：REINFORCE 算法在更新策略时，每个样本主要用于自身的梯度计算，样本\n之间的信息利用不足。RLOO 则充分利用了多个样本之间的关系， 每个样本不仅用于自身的\n梯度计算， 还参与构建其他样本的基线， 大大提高了样本的利用效率。实验表明， 在相同采\n样预算下，RLOO 能够实现更好的优化效果，如在多个数据集和模型上的实验显示，RLOO\n在胜率和奖励优化方面均优于 REINFORCE 算法。\n(3) 计算复杂度： 虽然RLOO 在样本利用和方差降低上具有优势， 但它的计算复杂度相对REIN-\nFORCE 算法有所增加。在构建基线时，RLOO 需要对每个样...",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "随着样本数量 k 的增加， 计算量会相应增大。 不过， 在实际应用中， 由于其在性能上的显著\n提升，这种计算复杂度的增加在可接受范围内。\n4. 算法特性分析\nRLOO 算法在继承 REINFORCE 算法优点的同时，有效改进了其部分缺陷。它通过多样本构\n建基线的方式，降低了梯度估计的方差，提高了策略更新的稳定性和准确性，使得算法在复杂环\n境和大规模任务中表现更优。然而，RLOO 算法也并非完美无缺。在处理大规模样本时，其计算\n复杂度的增加可能会成为限制因素，需要消耗更多的计算资源和时间。此外，RLOO 算法对样本\n的独立性假设较为依赖，如果样本之间存在较强的相关性，可能会影响基线的有效性，进而影响\n算法性能。 在实际应用中， 需要根据具体问题的特点和资源情况， 合理选择是否使用RLOO 算法。\n6.2.6 GRPO\n图 6.2 GRPO 算法流程图（需要重画）",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "θn+1 = arg max\nθ\nJPPO(θn)\n7: 通过最小化均方误差来更新价值函数：\nϕn+1 = arg min\nϕ\nL(ϕn)\n8: end for\n6.2.5 RLOO\nREINFORCE Leave-One-Out（RLOO）算法是在 REINFORCE 算法基础上发展而来的一种改\n进算法，它主要针对 REINFORCE 算法梯度估计方差较高的问题，通过利用多个在线样本构建更\n有效的基线来降低方差，从而提升算法性能。\n1. 算法原理\nRLOO 的核心在于改进基线的构建方式。在 REINFORCE 算法中，通常使用简单的移动平均\n基线，这种基线在处理复杂环境和多样本情况时存在一定局限性。RLOO 则利用每次采样得到的\n多个样本之间的关系，为每个样本单独构建基线。"
      ],
      "response_time": 4.340529680252075,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.4,
        "rouge2": 0.3076923076923077,
        "rougeL": 0.4,
        "bleu": 0.29708577274916015,
        "semantic_similarity": 0.8899900317192078
      }
    },
    {
      "sample_id": "0092",
      "question": "在策略梯度算法中，如何通过剪切机制解决高方差问题以提高优化的稳定性？",
      "ground_truth": "PPO算法通过引入剪切机制，限制重要性权重在特定范围内，以避免优化不稳定，从而解决高方差问题。",
      "predicted_answer": "在策略梯度算法中，通过PPO（Proximal Policy Optimization）算法引入了剪切机制来解决高方差问题并提高优化的稳定性。具体来说，PPO算法通过Clip函数裁剪重要性权重的大小，将其限制在一个特定范围内，从而避免权重出现极端值导致的高方差问题。该范围由超参数ε定义，一般可以设置为0.1或0.2，确保重要性权重在\\(1-\\epsilon\\)和\\(1+\\epsilon\\)之间[1][5]。这样可以减少优化过程中的波动，提升训练的稳定性和效率。",
      "ground_context": [
        "基于蒙特卡洛采样的 REINFORCE 方法作为经典的策略梯度算法，存在以下显著缺陷：首先，其依赖完整轨迹采样的蒙特卡洛特性导致梯度估计方差过高，这不仅会显著延缓收敛速度，还容易引发策略更新方向的剧烈波动，造成训练过程的不稳定性。",
        "PPO算法引入了剪切机制，通过将权重限制在特定范围内来避免优化不稳定，即：JPPO(θ) = E(s,a)∼πθ′ (a|s) clip(πθ(a|s)/πθ′ (a|s), 1 − ε, 1 + ε) A(s, a)。"
      ],
      "retrieved_contexts": [
        "202 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n前面提到，重要性采样需要保证两个策略分布相似，否则高方差会导致优化不稳定。因此， PPO\n算法引入了剪切机制，通过将权重限制在特定范围内来避免优化不稳定，即：\nJPPO(θ) = E(s,a)∼πθ′ (a|s)\n\u0014\nclip\n\u0012 πθ(a|s)\nπθ′ (a|s) , 1 − ε, 1 + ε\n\u0013\nA(s, a)\n\u0015\n(6.29)\n其中，ε 是超参数（例如可以设置为 0.1 或 0.2） 。Clip 函数裁剪重要性权重的大小，限制权重在\n1 − ε 和 1 + ε 之间。\n2. 算法流程\n综合上面的推导过程，我们可以得到 PPO 算法的流程，如代码 6.1 所示。\n代码 6.1: PPO 算法的流程\n1: 输入: 初始策略参数 θ0，初始价值函数参数 ϕ0\n2: for n = 0, 1, 2, · · · do\n3: 收集轨迹集合 Dn = {τi}，通过在环境中执行策略 πθn\n4: 针对每条轨迹计算回报 Rt\n5: 基于当前的价值函数 Vϕn ，使用广义优势估计方法计算优势 At\n6: 通过最小化策略梯度损失函数...",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "大规模语言模型：从理论到实践 201\n此，策略梯度方法需要不断地在环境交互中学习，训练效率较低。\n注意，在策略梯度方法中，同一个智能体既负责与环境交互，也负责策略参数更新，这种训\n练方法被称为同策略（On-Policy）训练方法。相反，异策略（Off-Policy）训练方法将这两个职能\n分开，即固定一个智能体与环境交互而不更新，另一个智能体则只负责从采集的数据中学习更新\n参数。这种方式可以重复利用历史数据。然而，由于两个智能体的分布不同，直接更新会导致不\n稳定的训练。 一种思路是调整这两个分布使它们保持一致，重要性采样（Importance Sampling） 就\n是在这种思路下的重要技术。\n1. 算法原理\n假设我们希望计算期望 Ex∼P (x)[f (x)]， 但采样数据来自另一个分布Q(x)， 可以通过设置采样\n数据的权重来修正结果：\nEx∼P (x)[f (x)] = Ex∼Q(x)\n\u0014 P (x)\nQ(x) f (x)\n\u0015\n(6.25)\n从 P 中每次采样一个 xi 并计算 f (xi)，都需要乘上一个重要性权重 P (xi)\nQ(xi) 来修正这两个分布\n的差异，这种方法被称...",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "从分布 P 时的期望。\n不过，两个分布的差异不能过大，否则会导致以下问题：\n(1) 高方差：当分布差异较大时，权重 P (x)\nQ(x) 可能出现极端值，导致估计的期望值方差增大。\n(2) 偏差：为了解决高方差问题，通常需要对权重进行裁剪或限制，这可能引入偏差。\n假设用于与环境交互的智能体策略为 θ′， 用于学习的智能体策略为θ， 应用重要性采样后， 可\n以将策略梯度公式改为异策略的形式，即：\n∇θJ(θ) = E(s,a)∼πθ(a|s) [A(s, a)∇θ log πθ(a|s)]\n= E(s,a)∼πθ′ (a|s)\n\u0014 pθ(s, a)\npθ′ (s, a) A(s, a)∇θ log πθ(a|s)\n\u0015 (6.26)\n其中，pθ(s, a) = πθ(a|s)p(s) 表示状态-动作对出现的概率，状态的概率被认为与策略无关，以便\n进行优化。因此，最终的策略梯度为：\n∇θJ(θ) = E(s,a)∼πθ′ (a|s)\n\u0014 πθ(a|s)\nπθ′ (a|s) A(s, a)∇θ log πθ(a|s)\n\u0015\n(6.27)\n从上述梯度形式反推 PPO 的目标函数为：\nJ(θ) = ..."
      ],
      "response_time": 3.3012568950653076,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.11764705882352941,
        "rouge2": 0.0,
        "rougeL": 0.11764705882352941,
        "bleu": 0.074051793700103,
        "semantic_similarity": 0.8882070183753967
      }
    },
    {
      "sample_id": "0093",
      "question": "ppo_mini_batch_size 设置为多少？",
      "ground_truth": "64",
      "predicted_answer": "根据提供的文档，我无法回答这个问题。文档中未提到 \"ppo_mini_batch_size\" 这个参数。不过，文档中提到了一个类似的参数 \"critic.ppo_micro_batch_size_per_gpu\" 被设置为了 4。这可能是类似的概念，但不能确定是否就是所求的参数。[5]",
      "ground_context": [
        "这个参数设置为 64，意味着每次从训练集中选取 64 个样本组成一个小批次，用于计算梯度和更新演员模型的参数。"
      ],
      "retrieved_contexts": [
        "\"offload_param\": {\n\"device\": device\n},\n\"memory_efficient_linear\": False\n}\nreturn {\n\"train_batch_size\": GLOBAL_BATCH_SIZE,\n\"train_micro_batch_size_per_gpu\": MICRO_BATCH_SIZE,\n\"steps_per_print\": 10,\n\"zero_optimization\": zero_opt_dict,\n\"fp16\": {\n\"enabled\": True\n},\n\"gradient_clipping\": 1.0,\n\"prescale_gradients\": False,\n\"wall_clock_breakdown\": False\n}",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "避免因数据量过大导致的显存溢出问题。 log_prob_micro_batch_size_per_gpu 表示每个 GPU\n上用于计算对数概率的微批次大小。在强化学习中，对数概率用于计算策略梯度，它反映了模型\n在当前策略下采取某个动作的概率。将这个参数设置为 8，即每个 GPU 在计算对数概率时，每次\n处理 8 个样本的微批次，这样可以优化计算过程，提高训练效率。",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "actor_rollout_ref.rollout.log_prob_micro_batch_size_per_gpu=8\nactor_rollout_ref.rollout.tensor_model_parallel_size=1\nactor_rollout_ref.rollout.gpu_memory_utilization=0.4\nactor_rollout_ref.ref.log_prob_micro_batch_size_per_gpu=4\ncritic.optim.lr=1e-5\ncritic.model.path=Qwen/Qwen2.5-0.5B-Instruct\ncritic.ppo_micro_batch_size_per_gpu=4\nalgorithm.kl_ctrl.kl_coef=0.001\ntrainer.logger=['console']\n+trainer.val_before_train=False\ntrainer.default_hdfs_dir=null\ntrainer.n_gpus_per_node=1\ntrainer.nnodes=1\ntrai..."
      ],
      "response_time": 1.8669261932373047,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0.0,
        "bleu": 0.0,
        "semantic_similarity": 0.29890426993370056
      }
    },
    {
      "sample_id": "0094",
      "question": "在模仿学习中，训练数据包含了什么？",
      "ground_truth": "输入和相应的期望输出，即专家生成的正确答案。",
      "predicted_answer": "根据提供的文档，我无法回答这个问题。文档中讨论的是指令微调的数据构成，而非模仿学习中的训练数据构成。",
      "ground_context": [
        "在模仿学习中，训练数据包含了输入和相应的期望输出，即专家生成的正确答案。"
      ],
      "retrieved_contexts": [
        "练数据对预训练模型进行微调操作。从训练方式的角度来看，指令微调与预训练大体上较为相似，\n不过指令微调的目标函数往往只是针对输出部分来计算损失。\n本节将从指令微调数据构造、数据评估和影响、指令微调训练策略以及开源指令微调数据等\n方面进行介绍。\n5.1.1 指令微调数据\n指令微调数据通常由文本对构成，包含“指令输入”与“答案输出”两个关键部分。 “指令输\n入” ， 是指人们向模型提出的各类请求， 包含定义精准、 清晰的指令或者提示信息， 其核心作用在\n于详细阐释任务的目标究竟是什么，以及明确规定输出需要满足的各项要求。指令涵盖的范畴极\n为广泛，包括问题回答、信息分类、内容总结、文本改写等。 “答案输出” ，则是期望模型依据所",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "据集，会对大语言模型在问答任务上的表现产生怎样的影响？ （3）不同的大语言模型在指令微调\n阶段，对于数据的需求方面存在着怎样的差异呢？\n针对上述问题， 文献[224] 给出了详细的分析。 研究人员使用了ENTITYQUESTIONS[225]， 这\n是一个包含维基百科上 24 个不同话题知识的问答数据集。选择了其中 12 个与地点相关的原始训\n练集作为训练数据， 将它们对应的测试集作为测试集， 并将剩余12 个话题的测试集作为领域外测\n试集。通过设计的多模板补全机制，能够可靠地评估大语言模型对不同知识的记忆程度。利用该\n机制，根据其知识记忆水平将训练和测试集均进行了 5 个级别的划分。\n文献 [224] 中将训练数据划分为六个不同的数据量级别，从 60 个样本到完整数据集不等，并\n通过从 12 个话题中均匀抽样来构建训练集。实验结果表明，仅需 60 个训练样本的指令微调，就\n足以使大语言模型高效执行问答任务，并展现出强大的泛化能力。如图5.4所示。无论基础模型或\n记忆水平如何，大语言模型在使用较少训练样本时的表现优于使用 960 个或全部样本。增加训练",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "164 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n合顺序训练：首先在领域数据集（代码、数学）上应用多任务学习，然后在通用能力数据集上进\n行指令微调； （4）双阶段混合微调：首先在领域数据集（代码、数学）上应用多任务学习，然后\n使用少量领域数据混合全量通用数据再进行指令微调。实验结果如表所示。\n表 5.1 不同指令微调策略准确率对比 [226]\n方法 LLaMA - 7B LLaMA - 33B\nGSM8K HumanEval MT-Bench GSM8K HumanEval MT-Bench\n仅通用数据 11.10% 10.42% 5.88% 26.06% 24.30% 6.63%\n仅数学数据 49.10% 6.71% 2.53% 57.91% 15.5% 3.18%\n仅编程数据 4.51% 18.40% 4.30% 6.06% 26.82% 4.18%\n多任务学习 47.53% 14.63% 5.76% 56.69% 18.9% 6.07%\n顺序训练 31.39% 15.85% 5.72% 47.27% 24.80% 6.73%\n混合顺序 32.60% 15.24% ..."
      ],
      "response_time": 0.9369261264801025,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.012796221854994943,
        "semantic_similarity": 0.4654918611049652
      }
    },
    {
      "sample_id": "0095",
      "question": "Drm 表示什么？",
      "ground_truth": "训练数据集的经验分布。",
      "predicted_answer": "Drm 表示训练数据集的经验分布 [1]。",
      "ground_context": [
        "其中 Drm 表示训练数据集的经验分布。"
      ],
      "retrieved_contexts": [
        "L (ψ) = −λE(x,yw,yl)∼Drm [log σ (r (x, yw) − r (x, yl))] − βrmE(x,yw)∼Drm [log (r′ (x, yw))] (6.36)\n其中 Drm 表示训练数据集的经验分布。r′ 是与 r 相同的模型， 只有顶层的线性层与r 有所不同， 该\n线性层的维度与词汇表的大小相对应。 在r′ 模型中，r′ (x, yw) 表示在给定输入提示 x 和首选输出\nyw 的条件下的似然概率，这个似然概率表达了模型生成给定输出的可能性。\n另外，还可以引入一个附加项到奖励函数中，该附加项基于学习得到的强化学习策略 πRL\nϕ 与\n初始监督模型 πSFT 之间的 Kullback-Leibler（KL） 散度， 从而引入了一种惩罚机制。 总奖励可以根\n据文献 [250] 通过如下方式表达：\nrtotal = r (x, y) − ηKL\n\u0000\nπRL\nϕ (y|x) , πSFT (y|x)\n\u0001\n(6.37)\n其中 η 代表 KL 奖励系数， 用于调整KL 惩罚的强度。 这个KL 散度项在这里发挥着两个重要的作\n用。首先，它作为一个熵奖励，促进...",
        "(9.7)\n其中，ˆst 表示临时答案，st 是语言模型的输出。 接受ˆst 的条件是其内部的所有词元都必须具有大\n于或等于阈值 τ 的关联概率。如果不满足这一条件，系统就会转而生成新的答案。\nRAG 系统还可以通过大语言模型直接进行判断（LLM Judge） 。这一方式包括两种主要方法：\n第一种方法利用 LLM 的上下文学习能力，通过精心设计的提示来进行决策。这种方法的优势在\n于无需对模型进行额外的微调，但其判断结果的准确性通常依赖于 LLM 对提示的理解程度。第\n二种方法通过对 LLM 进行微调， 使其生成特定的触发标记， 来直接控制模型的行为。 例如， 借助",
        "（1）指令微调：使用精选的人类回答来微调预训练语言模型以应对各种查询。\n（2）奖励模型微调：使用一个包含人类对同一查询的多个答案打分的数据集来训练一个独立\n的奖励模型。\n（3）基于人类反馈的强化学习（Reinforcement Learning from Human Feedback ，RLHF）训练：\n利用近端策略优化（Proximal Policy Optimization，PPO） 算法， 根据奖励模型的奖励反馈进一步微\n调 SFT 模型。\n本节只针对步骤 （1） 指令微调的实践进行介绍， 对于奖励模型微调和RLHF 训练的实践会在\n后续对应章节中详细介绍。\n图 5.10 基于 DeepSpeed-Chat 训练类 ChatGPT 对话模型的三个步骤 [244]\nDeepSpeed-Chat 具有以下三大核心功能。",
        "得如何理解人类所使用的不太精确、 可能有歧义甚至有些混乱的语言。语言模型（Language Model，\nLM） 的目标就是对自然语言的概率分布建模。 词汇表V 上的语言模型， 由函数P (w1w2 · · · wm) 表\n示， 可以形式化地构建为词序列w1w2 · · · wm 的概率分布， 表示词序列w1w2 · · · wm 作为一个句子\n出现的可能性的大小。 由于联合概率P (w1w2 · · · wm) 的参数量巨大， 因此直接计算P (w1w2 · · · wm)\n非常困难[3]。 《现代汉语词典》 （第7 版）包含约 7 万词，句子长度按照 20 个词计算，语言模型的\n参数量达到 7.9792 × 1096 的天文数字。在中文的书面语中，超过 100 个词的句子并不罕见，如果\n要将所有可能性都纳入考虑，则语言模型的复杂度会进一步增加，以目前的计算手段无法进行存\n储和运算。\n为了减小 P (w1w2 · · · wm) 模型的参数空间，可以利用句子序列（通常是从左至右）的生成过\n程将其进行分解，使用链式法则可以得到",
        "大规模语言模型：从理论到实践 179\n（1） 易用的类ChatGPT 模型的训练和强化推理： 只需要一个脚本就可以实现多个训练步骤， 包\n括使用 HuggingFace 预训练的模型，使用 InstructGPT 训练的所有三个步骤构建类 ChatGPT 模型。\n此外，还提供了一个易于使用的推理 API，用于用户在模型训练后进行对话式交互性测试。\n（2）DeepSpeed-RLHF 管道：DeepSpeed-RLHF 复现了 InstructGPT[24] 论文中的训练模式，包\n括指令微调、奖励模型微调和基于人类反馈的强化学习三个步骤。此外，还提供了数据抽象和混\n合功能，以支持用户使用多个不同来源的数据源进行训练。\n（3）DeepSpeed-RLHF 系统： 将DeepSpeed 的训练能力 （Training Engine） 和推理能力 （Inference\nEngine）整合到统一的混合引擎（DeepSpeed Hybrid Engine ，DeepSpeed-HE）中用于 RLHF 训练。\nDeepSpeed-HE 能够无缝地在推理和训练模式之间切换，使其能够利用来自 DeepS..."
      ],
      "response_time": 0.6339888572692871,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.43361890903486755,
        "semantic_similarity": 0.78663170337677
      }
    },
    {
      "sample_id": "0096",
      "question": "Kimi k1.5 在 AIME 上达到了多少分？",
      "ground_truth": "77.5 分",
      "predicted_answer": "Kimi k1.5 在 AIME 上达到了 79.8 分[1]。",
      "ground_context": [
        "出色的推理性能：在长思维链模式下，Kimi k1.5 在多个基准测试和模态中达到了 SOTA 模型 OpenAI o1 正式版的水平，如在 AIME 上达到 77.5 分，MA TH 500上达到 96.2 分，在 Codeforces"
      ],
      "retrieved_contexts": [
        "OpenAI o1 正式版的水平，如在 AIME 上达到 77.5 分，MA TH 500上达到 96.2 分，在 Codeforces\n上达到 94 百分位，在 MathVista 上达到 74.9 分。在短思维链模式下，其数学、代码、视觉多模态\n和通用能力也表现出色，大幅超越现有短链思维模型，如在 AIME 上达到 60.8 分，MA TH500上\n达到 94.6 分，LiveCodeBench 上达到 47.3 分，提升幅度高达 550%。\n创新的技术架构通过长上下文扩展和改进的策略优化，Kimi k1.5 建立了一个简单有效的 RL\n框架， 无需依赖复杂技术如蒙特卡罗树搜索、 价值函数和过程奖励模型， 即可实现强大的性能。 同\n时，Long2short 方法有效提升了短 CoT 模型的性能，提高了模型的 token 效率。\n高效的数据处理与训练精心设计的数据工程确保了训练数据的高质量和多样性，多模态数据\n的有效整合使模型能够更好地理解和处理不同类型的信息。优化的训练架构和算法，如部分回放\n技术、混合部署框架等，提高了训练效率和系统的可扩展性。\n6.4 基于人类反馈的强化学习\n6...",
        "210 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n解决数据受限的问题。大语言模型可通过强化学习中的奖励机制，学习如何探索不同的推理路径，\n从而扩大训练数据的范围。但此前的相关研究成果未达到理想的竞争效果，Kimi k1.5 旨在探索一\n条更有效的技术路线。\n1. 技术路线\n强化学习提示数据集构建： RL 提示数据集的质量和多样性对强化学习的有效性至关重要。\nKimi k1.5 定义了高质量 RL 提示集的三个关键属性：\n多样覆盖：提示应涵盖 STEM、编程和一般推理等广泛学科，以增强模型的适应性和跨领域\n应用能力。为此，采用自动过滤器选择需要丰富推理且易于评估的问题，数据集来源广泛，包括\n不同领域的问题以及纯文本和图像 - 文本问答数据。\n平衡难度：提示集应包含不同难度级别的问题，以促进模型的逐步学习并防止过度拟合。利\n用模型自身能力自适应评估提示难度， 通过SFT 模型多次生成答案计算通过率作为难度代理指标，\n并开发标签系统按领域和学科分类提示，实现难度平衡。\n准确评估能力：提示应能被验证者客观可靠地评估，确保基于正确推理衡量模型性能。为避\n免奖励操纵，排除易出现验证...",
        "免奖励操纵，排除易出现验证错误的问题类型，并通过特定方法识别和移除易被操纵的提示。\n此外，为提高模型的图像推理能力，数据还来源于现实世界数据、合成视觉推理数据和文本\n渲染数据这三个类别。\n预训练数据集的构建与处理： Kimi k1.5 的预训练数据集涵盖英语、 中文、 代码、 数学与推理\n以及知识数据五个领域，以确保数据多样性。为保证数据高质量，采用多种清洗方法：\n针对英语和中文文本数据，建立多维质量过滤框架，包括基于规则的过滤、基于 FastText 的\n分类、基于嵌入的相似性分析和基于大模型的质量评估，最后通过动态采样率对不同质量的文档\n进行处理。\n对于代码数据， 对纯代码数据和文本-代码交错数据分别进行处理， 前者遵循BigCode 方法进\n行预处理和采样调整，后者采用基于句向量的方法召回高质量数据。数学与推理数据通过开发专\n门的数据清洗程序和 OCR 模型， 以及两阶段数据清洗过程， 提高数据质量。 知识数据通过精心策\n划，利用内部语言模型添加多维标签，并实施复杂的过滤和采样管道，优化数据组成。\n微调数据集的构建： Kimi k1.5 的 SFT 数据集包含约 100 万个文本...",
        "动了相关领域的研究发展。\n4.2 模型性能卓越 DeepSeek-R1 在多个推理任务中表现出色， 在AIME 2024 上 Pass@1 得分达到\n79.8%，略超 OpenAI-o1-1217；在 MA TH-500上得分高达 97.3%，与 OpenAI-o1-1217 相当且远超\n其他模型。在编码相关任务中，于 Codeforces 竞赛中获得 2029 Elo 评级，超越 96.3% 的人类参与\n者；在知识类基准测试如 MMLU、GPQA Diamond 等任务中，也取得了优异成绩，展现出强大的\n知识掌握和推理应用能力。\n6.3.2 Kimi k1.5\n基于下一个 token 预测的语言模型预训练遵循缩放定律，即按比例增加模型参数和数据规模\n可提升模型智能程度。然而，这种方法严重依赖高质量训练数据的数量。在实际应用中，可用的\n高质量数据往往有限，这限制了模型性能的进一步提升。将强化学习与大语言模型相结合，有望",
        "作为负样本（包括错误长响应和正确但超长响应） ，形成成对偏好数据用于DPO 训练，DPO 细节\n参见下文 [245]。\n4. 长到短强化学习：在标准 RL 训练后，选择性能和 token 效率平衡最佳的模型作为基础模\n型，进行单独的长到短 RL 训练，应用长度惩罚方案惩罚超长响应。\n改进的策略优化：Kimi k1.5 推导出带有长推理链的强化学习公式， 并采用在线镜像下降的变\n体进行策略优化。该算法通过以下方式进一步改进：\n1. 采样策略：采用课程采样和优先级采样策略。课程采样从简单任务开始训练，逐渐过渡到\n困难任务，利用数据的难度标签提高训练效率；优先级采样跟踪每个问题的成功率，按比例采样\n问题，使模型专注于薄弱领域。\n2. 长度惩罚：针对 RL 训练期间模型响应长度增加的问题，引入长度奖励限制 token 长度增\n长。在正确答案中提倡简短回答并惩罚较长回答，对错误答案的长回答明确惩罚。为缓解长度惩\n罚在训练初期对训练速度的影响，采用逐渐增加长度惩罚的方式。\n3. 训练架构及工程框架\nKimi k1.5 模型的训练分为三个阶段：\n1. 视觉语言预训练阶段：模型最初仅在语言数据上训练，..."
      ],
      "response_time": 0.8030974864959717,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.22222222222222224,
        "rouge2": 0.0,
        "rougeL": 0.22222222222222224,
        "bleu": 0.018724372764461875,
        "semantic_similarity": 0.6392924189567566
      }
    },
    {
      "sample_id": "0097",
      "question": "ImageBind可以支持哪些数据的编码？",
      "ground_truth": "图像、文本、音频、深度、热成像和惯性测量单元（Inertial Measurement Unit，IMU）等多种数据的编码。",
      "predicted_answer": "ImageBind可以支持文本、图像/视频、音频、深度信息、热成像数据和运动传感器数据的编码。[3]",
      "ground_context": [
        "ImageBind[256] 则可以支持图像、 文本、 音频、 深度、 热成像和惯性测量单元 （Inertial Measurement Unit，IMU） 等多种数据的编码。"
      ],
      "retrieved_contexts": [
        "230 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n能够响应多种模态的输入， 包括音频、3D 点云、 视频及其嵌入空间。此外，ImageBind-LLM 还通\n过仅进行图像-文本对齐训练， 实现了多模态的指令跟随能力。 在训练过程中，ImageBind-LLM 采\n用一个可学习的绑定网络（Bind Network） ，将LLaMA 与 ImageBind 图像编码器的嵌入空间对齐。\n然后，绑定网络转换后的图像特征被添加到 LLaMA 所有层的词语 Token 中，从而通过一种无注\n意力且零初始化的门控机制逐步注入视觉指令。\n4. KOSMOS 系列\nKOSMOS 是微软开发的一系列多模态大语言模型，将语言模型原生支持多模态数据作为目\n标。 通过结合语言理解与视觉感知能力， 为多模态学习提供了另外的解决方案。KOSMOS-1[258] 从\n预训练阶段开始之初，便引入多模态数据，支持文本、图像和语音输入，原生具备处理多模态信\n息的能力。 因此，KOSMOS-1 能够同时胜任语言任务、 感知-语言任务和视觉任务， 包括视觉对话、\nOCR、 简单数学方程求解以及带描述的零样本图像...",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "如，模型可以回答与图像相关的问题，或根据视觉线索指导机器人完成特定任务。 PaLM-E 拥有\n5620 亿个参数，在多模态推理与迁移学习方面表现卓越，能够在一系列具身任务中高效表现，而\n无需针对特定任务进行微调。这一特性使得 PaLM-E 成为人工智能研究与机器人实际应用中的一\n项多功能工具。\n3. ImageBind\nMeta 发布的 ImageBind[256] 是一个多模态对齐模型，旨在通过整合六种不同类型的数据（文\n本、 图像/视频、 音频、 深度信息、 热成像数据和运动传感器数据） 来创建一个统一的嵌入空间， 设\n计使得模型能够处理和理解来自多种感官的信息。与传统模型不同， ImageBind 不要求所有模态\n同时存在于同一数据集中，而是利用图像的固有链接性质，实现跨模态的对齐和理解，这为生成\n更复杂的虚拟环境提供了可能性。\nImageBind-LLM[257] 是基于 ImageBind 的多模态大语言模型，它使用 ImageBind 的联合嵌入\n空间来处理多模态数据。与现有主要专注于语言和图像视觉的大语言模型不同， ImageBind-LLM",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "元 （Entry） 。 在具体实现上，SEED 将尺寸为 224 × 224 的 RGB 图像分解为 16 × 16 的小块 （Patch） ，\n经过编码后将这些小块转换为量化的码元序列。 这些码元与预训练的unCLIP Stable Diffusion 模型\n的编码空间对齐，最终通过 UNet 解码器将码元序列恢复为原始图像。\nSpeechTokenizer[303] 则应用于 AnyGPT 作为语音分词器。SpeechTokenizer 的内部结构包含 8\n个码本， 每个码本包含1024 个词元表示。 其架构基于编码器-解码器， 并结合残差向量量化 （RVQ） ，\n能够将单通道音频序列压缩为离散的矩阵表示。 下采样后的帧率为50Hz， 语音分词器通过结合语\n义损失和重建损失，将语音信息解耦为语义信息和副语言学信息。具体来说，10 秒的音频会被转"
      ],
      "response_time": 0.9849448204040527,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0.0,
        "bleu": 0.23793665482062612,
        "semantic_similarity": 0.6652535200119019
      }
    },
    {
      "sample_id": "0098",
      "question": "整个系统通过什么实现多角色的协同训练？",
      "ground_truth": "资源池管理",
      "predicted_answer": "整个系统通过资源池管理实现多角色（Actor/Critic/Ref Policy 等）的协同训练[1]。",
      "ground_context": [
        "整个系统通过资源池管理实现多角色 （Actor/Critic/Ref Policy 等） 的协同训练， 具有分布式训练、弹性资源调度和可扩展的架构设计等特点。"
      ],
      "retrieved_contexts": [
        "大规模语言模型：从理论到实践 221\n键组件。 整个系统通过资源池管理实现多角色 （Actor/Critic/Ref Policy 等） 的协同训练， 具有分布\n式训练、弹性资源调度和可扩展的架构设计等特点。",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "能够根据环境反馈和人类反馈动态调整操作策略，从而不断优化工具的使用方法。\n在实际操作中，环境反馈通常通过智能体与外部环境交互后所获得的结果进行优化；具体而\n言，结果反馈用于评估智能体一系列动作的整体效果，而中间反馈则着重考察每一步操作的即时\n表现。例如，在 WebShop[393] 场景中，智能体通过对比其购买行为与人类购买行为之间的相似性\n来获得结果反馈，从而评估其表现的有效性。在此基础上，人类反馈强化学习通过模拟人类奖励\n机制，结合强化学习算法优化智能体的策略，以提升其决策能力和执行效果。同时，智能体会将\n每次尝试的结果系统化地记录下来，构建经验库。这一过程不仅使智能体能够积累丰富的操作经\n验，还能逐步提升其对工具的使用熟练度和操作效率。\n探索学习的关键在于通过持续的试探与调整，使智能体在动态环境中不断完善其工具使用能\n力。这种方法不仅赋予智能体更强的适应性与自主性，还为其在多变任务场景中的高效表现提供\n了坚实的技术支持。\n当前研究的重点在于如何通过整合多种学习策略来优化模型性能，从而全面提升大模型智能\n体的表现能力。例如，将示范学习的精确性与探索学习的灵活性相结合，可以显著增强模...",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "大规模语言模型：从理论到实践 99\n效率进行优化，主要的技术手段有混合精度训练、算子融合、梯度累加等；在分布式训练系统中，\n随着计算设备数量的增加，理论上峰值计算速度会增加，然而受通信效率的影响，计算设备数量\n增多会造成加速比急速降低；多设备加速比是由计算和通信效率决定的，需要结合算法和网络拓\n扑结构进行优化，分布式训练并行策略的主要目标就是提升分布式训练系统中的多设备加速比。\n大语言模型的参数量和所使用的数据量都非常大，因此都采用了分布式训练架构完成训练。\n文献 [13] 仅在 GPT-3 的训练过程中提到全部使用 NVIDIA V100 GPU ，文献 [29] 介绍了 OPT 使\n用 992 块 NVIDIA A100 80GB GPU ，采用全分片数据并行（Fully Sharded Data Parallel ）[168] 以及\nMegatron-LM 张量并行（Tensor Parallelism）[169]，整体训练时间近两个月。BLOOM[31] 模型的研\n究人员则公开了更多在硬件和所采用的系统架构方面的细节。该模型的训练一共花费了 3.5 个月，\n使用 48 个计算节..."
      ],
      "response_time": 0.8994500637054443,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.05022135506220879,
        "semantic_similarity": 0.6665575504302979
      }
    },
    {
      "sample_id": "0099",
      "question": "如何初始化训练器的工作者？",
      "ground_truth": "trainer.init_workers()",
      "predicted_answer": "在初始化训练器的工作者时，通常会在训练脚本中调用 `trainer.init_workers()` 方法来完成此操作。虽然具体的细节未详细列出，但这是初始化训练器工作者的标准做法之一[1]。 \n\n参考文献：\n[1]",
      "ground_context": [
        "trainer.init_workers()"
      ],
      "retrieved_contexts": [
        "reward_fn=reward_fn,\nval_reward_fn=val_reward_fn)\ntrainer.init_workers()\ntrainer.fit()\nif __name__ == '__main__':\nmain()",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "142 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n5. DeepSpeed 初始化\n设置 DeepSpeed 的配置参数后， 可以利用DeepSpeed 进行模型训练的初始化， 初始化流程如下。\n（1）确定运行的设备：首先，检查代码是否有指定的本地 GPU（通过 args.local_rank） 。如果\n没有指定，则程序默认使用 CUDA 设备。否则，它会为进程设置指定的 GPU。\n（2） 初始化分布式后端： 在分布式训练中， 使用deepspeed.init_distributed() 函数实现每个进程\n与其他进程的同步，初始化分布式环境。\n（3）获取当前进程的全局排序：在分布式训练中，使用torch.distributed.get_rank() 函数获得每\n个进程的唯一排序或 ID。\n（4）设置 DeepSpeed 配置：根据用户参数（如是否进行 offload、使用哪个 Zero Stage 等）构\n建一个 DeepSpeed 配置字典，来决定训练设置。\n（5） 同步所有工作进程： 使用torch.distributed.barrier() 确保在进一步的初始化之...",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "--max_seq_len 512\n--learning_rate 9.65e-6\n--weight_decay 0.\n--num_train_epochs 2\n--gradient_accumulation_steps 1\n--lr_scheduler_type cosine\n--num_warmup_steps 0\n--seed 1234\n--gradient_checkpointing\n--zero_stage $ZERO_STAGE\n--deepspeed\n--output_dir $OUTPUT\n&> $OUTPUT/training.log\n5.4.4 模型训练\n数据预处理和自定义模型的修改都完成后，就可以正式进行训练了。进入步骤一指令微调的\n路径 training/step1_supervised_finetuning 下，把上述构造的训练脚本放置到 training/"
      ],
      "response_time": 1.498234510421753,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.7499999999999999,
        "rouge2": 0.6666666666666666,
        "rougeL": 0.7499999999999999,
        "bleu": 0.09880177230676102,
        "semantic_similarity": 0.7695378065109253
      }
    },
    {
      "sample_id": "0100",
      "question": "强化学习的核心在于实现什么?",
      "ground_truth": "智能体策略与价值函数的优化，从而提升性能与稳定性。",
      "predicted_answer": "强化学习的核心在于通过一系列精心设计的计算和控制机制，实现智能体策略与价值函数的优化，从而提升性能与稳定性[5]。",
      "ground_context": [
        "强化学习的核心在于通过一系列精心设计的计算和控制机制，实现智能体策略与价值函数的优化，从而提升性能与稳定性。"
      ],
      "retrieved_contexts": [
        "（Chain-of-Thought） 的决策序列， 在数学证明、 代码生成等场景中展现出超越监督学习的推理能力。\n相较于传统监督学习， 强化学习框架具有显著优势：在RLHF 范式下， 模型通过生成-反馈的\n闭环机制持续优化，摆脱对标准答案的绝对依赖；在深度推理场景中，强化学习能自主探索最优\n推理路径，通过价值函数估计引导模型突破局部最优解。两类方法都强调对生成文本的整体质量\n把控，前者侧重人类价值对齐，后者专注复杂问题求解，共同构成大语言模型能力进化的核心驱\n动力。\n本章将系统阐述基于人类反馈的强化学习技术体系，解析奖励模型构建、策略优化算法等关\n键组件。同时深入探讨强化学习在深度推理任务中的创新应用，包括思维链强化、过程奖励设计\n等前沿方法。最后通过 verl 实践案例，展示强化学习技术在大语言模型训练中的工程实现与效果\n验证。\n6.1 强化学习概述\n强化学习（Reinforcement Learning，RL）研究的是智能体与环境交互的问题，其目标是使智\n能体在复杂且不确定的环境中最大化 奖励。强化学习基本框架如图6.1 所示，主要由两部分组成：",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "找食物、住宿还是娱乐。\n现代强化学习之父 Richard Sutton 在 《苦涩的教训 （The Bitter Lesson） 》 中指出， 过去70 年人\n工智能研究领域最重要的一堂课是，只有通用的、可规模化扩展的方法才是最终有效的，而且优\n势巨大。 因此， 结合OpenAI 的研究实践， 强化学习在大语言模型中的优势可重新归纳为以下三个\n维度：\n（1）摆脱局部最优束缚的全局优化能力。监督学习依赖词元级精确标注，本质上将人类先验",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "大规模语言模型：从理论到实践 225\n3.1 优势函数与回报计算 强化学习的核心在于通过一系列精心设计的计算和控制机制，实现智\n能体策略与价值函数的优化，从而提升性能与稳定性。因此 verl 的核心算法模块涵盖多个关键功\n能模块。在系数控制上，有根据 KL 散度动态调整系数的 AdaptiveKLController 和系数固定不\n变的 FixedKLController，并通过 get_kl_controller 依据配置返回对应实例。优势函数与回\n报计算方面，包含计算广义优势估计和回报的 compute_gae_advantage_return，以及针对不同\n算法的优势函数计算方法， 如GRPO、REINFORCE++、ReMax 算法对应的优势计算函数。 奖励计\n算通过 compute_rewards 完成， 依据分数、 对数概率等计算最终奖励。 在损失计算上， 分别有利\n用裁剪技巧限制更新幅度的策略损失计算 comput_policy_loss、保持策略多样性的熵损失计算\ncompute_entropy_loss、 防止过拟合的价值损失计算 compute_value_loss..."
      ],
      "response_time": 1.013418436050415,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.40236057392677443,
        "semantic_similarity": 0.7757806777954102
      }
    },
    {
      "sample_id": "0101",
      "question": "在多模态学习中，KOSMOS-1如何结合语言理解与视觉感知能力，而ImageBind-LLM的训练方法又是如何支持多模态指令跟随能力的？",
      "ground_truth": "KOSMOS-1通过原生支持多模态数据，实现语言理解和视觉感知的结合，能够处理文本、图像和语音输入，从而胜任语言任务、感知-语言任务和视觉任务。而ImageBind-LLM则通过图像-文本对齐训练，利用一个可学习的绑定网络将LLaMA与ImageBind图像编码器的嵌入空间对齐，实现多模态的指令跟随能力。",
      "predicted_answer": "KOSMOS-1 从预训练阶段开始就引入多模态数据，支持文本、图像和语音输入，因此它原生具备处理多模态信息的能力，能够同时胜任语言任务、感知-语言任务和视觉任务，包括视觉对话、OCR、简单数学方程求解以及带描述的零样本图像分类等[3][5]。\n\nImageBind-LLM 通过仅进行图像-文本对齐训练实现了多模态的指令跟随能力。在训练过程中，采用了一个可学习的绑定网络（Bind Network），将 LLaMA 与 ImageBind 图像编码器的嵌入空间对齐。绑定网络转换后的图像特征被添加到 LLaMA 所有层的词语 Token 中，从而通过一种无注意且零初始化的门控机制逐步注入视觉指令[1]。",
      "ground_context": [
        "KOSMOS 是微软开发的一系列多模态大语言模型，将语言模型原生支持多模态数据作为目标。",
        "KOSMOS-1 从预训练阶段开始之初，便引入多模态数据，支持文本、图像和语音输入。",
        "ImageBind-LLM 通过仅进行图像-文本对齐训练，实现了多模态的指令跟随能力。",
        "在训练过程中，ImageBind-LLM 采用一个可学习的绑定网络，将LLaMA 与 ImageBind 图像编码器的嵌入空间对齐。"
      ],
      "retrieved_contexts": [
        "230 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n能够响应多种模态的输入， 包括音频、3D 点云、 视频及其嵌入空间。此外，ImageBind-LLM 还通\n过仅进行图像-文本对齐训练， 实现了多模态的指令跟随能力。 在训练过程中，ImageBind-LLM 采\n用一个可学习的绑定网络（Bind Network） ，将LLaMA 与 ImageBind 图像编码器的嵌入空间对齐。\n然后，绑定网络转换后的图像特征被添加到 LLaMA 所有层的词语 Token 中，从而通过一种无注\n意力且零初始化的门控机制逐步注入视觉指令。\n4. KOSMOS 系列\nKOSMOS 是微软开发的一系列多模态大语言模型，将语言模型原生支持多模态数据作为目\n标。 通过结合语言理解与视觉感知能力， 为多模态学习提供了另外的解决方案。KOSMOS-1[258] 从\n预训练阶段开始之初，便引入多模态数据，支持文本、图像和语音输入，原生具备处理多模态信\n息的能力。 因此，KOSMOS-1 能够同时胜任语言任务、 感知-语言任务和视觉任务， 包括视觉对话、\nOCR、 简单数学方程求解以及带描述的零样本图像...",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "OCR、 简单数学方程求解以及带描述的零样本图像分类等。KOSMOS-1 的训练是在大规模的多模\n态语料库上进行的， 包括单模态数据 （例如文本语料库） 、 跨模态配对数据 （例如图像-字幕对） 以\n及交错的多模态数据（例如包含任意交错图像和文本的文档） 。\nKOSMOS-2[259] 采用了与 KOSMOS-1 相同的模型架构， 引入了基于语义和描述的视觉定位任\n务， 使得模型能够更准确地将文本与视觉对象连接， 并实现细粒度的对象级交互。 为了训练Kosmos-\n2， 研究团队构建了GRIT（Grounded Image-Text pairs） 数据集， 包含大量图像和文本对。 这个数据\n集通过将图像中的物体与相应文本描述进行精确匹配，极大地丰富了模型的训练数据，提高了其\n在多模态任务中的表现，尤其在文本密集图像任务中表现出色，能够生成结构化 Markdown 文本。\nKOSMOS-2.5[260] 结合基于 ViT（Vision Transformer）[261] 的视觉编码器和 Transformer 结构的\n解码器， 通过重采样模块进行连接， 实现了高效的多模态数据处理。 这种...",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "解码器， 通过重采样模块进行连接， 实现了高效的多模态数据处理。 这种统一的模型接口简化了下\n游任务训练，并提升了模型的指令执行能力。KOSMOS-2.5 能够处理文本与图像协作的复杂任务，\n例如生成具有空间感知的文本块或以 Markdown 格式生成结构化文本输出。同时， KOSMOS-2.5\n在文本密集图像的理解上表现优异，支持信息提取、布局分析、视觉问答、截图理解以及用户界\n面自动化等多种任务。\n5. 开源模型\nLLaV A（Large Language and Vision Assistant）[262] 是开源的多模态大语言模型， 通过端到端训\n练方式，将视觉编码器（如 CLIP 的 ViT-L/14）与大语言模型（如 LLaMA、Vicuna）相结合，实\n现了对多模态指令的深刻理解与执行。其架构主要包括三部分： （1）视觉编码器负责提取输入图\n像的特征； （2）语言模型用于理解用户的语言指令并生成响应； （3）跨模态连接器（通常是线性\n层） 将视觉特征与语言模型的输入对齐， 从而实现跨模态信息的融合。 这种设计使得LLaV A能够\n高效处理和理解复杂的多模态任务。\nMini..."
      ],
      "response_time": 4.446990966796875,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.6,
        "rouge2": 0.33333333333333337,
        "rougeL": 0.6,
        "bleu": 0.2767855117916883,
        "semantic_similarity": 0.9415338039398193
      }
    },
    {
      "sample_id": "0102",
      "question": "在评估模型回答的质量时，如何影响答案提取和人类偏好数据集的设计？",
      "ground_truth": "在评估模型回答的质量时，答案提取方法（如'严格'或'灵活'）会影响最终得分，而人类偏好数据集的设计则通过收集对话的选择与拒绝，确保数据集中的每个配对比较具有相同的权重。使用更有害的回应可能导致对话朝着更有害的方向发展。",
      "predicted_answer": "在评估模型回答的质量时，通过设计人类偏好数据集可以影响答案提取。具体来说，在数据收集阶段，可以选择将更具危害性的回应纳入数据集中[1]，这可能导致最终的数据集偏向于更有害的回答。相反地，如果选择将更具有益性的回应纳入，则可能倾向于更有益的答案[1]。因此，人类偏好数据集的设计直接影响了模型回答的质量评估方向。",
      "ground_context": [
        "answer = extract_solution(solution_str=solution_str, method=method)",
        "收集数据的时候，... 只把数据集中的每个配对比较都当作二选一，并且权重相同"
      ],
      "retrieved_contexts": [
        "216 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n然后选择模型提供的更有害的回应。聊天交互工具允许众包工作者表达他们对两个模型回答偏好\n的强度。只有在偏好强度高于最弱选项时，才会把这些配对比较放入数据集中。收集数据的时候，\n不会记录偏好强度，只把数据集中的每个配对比较都当作二选一，并且权重相同（数据集中不包\n括平局的情况） 。需要注意的是，这意味着“有用性”数据集往往会把对话推向更有益的方向，而\n“无害性”数据集中，人们的回应会让对话朝着更有害的方向发展。表 6.1 给出了人类偏好数据集\n的例子，包括中文和英文，针对同样的问题，包含 chosen 和 rejected，其中 chosen 代表人类更偏\n好的回复，rejected 则反之。\n表 6.1 人类偏好数据集的例子\n选择 拒绝\nHuman: Depression is just temporary bad mood, isn’t it?\nWhat should be taken into consideration in daily life when\ndealing with depression? Assis...",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "218 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n需要注意的是，对比部分仅有训练和验证划分，而轴向部分仅有测试和验证划分。\nWebGPT[25] 使用人类反馈训练了一个奖励模型，来指导模型提升长文档问答能力，使其与人\n类的偏好相符。该数据集包含在 WebGPT 项目结束时被标记为适合奖励建模的所有对比数据，总\n计 1.9 万条数据。\nAnthropic 的 HH-RLHF 数据集主要分为两大部分。第一部分是关于有用性和无害性的人类偏\n好数据， 共计17 万条。 这些数据的目标是为强化学习的训练提供奖励模型， 但并不适合直接用于\n对话模型的训练，因为这样可能会导致模型产生不良行为。第二部分是由人类生成并注释的红队\n测试对话。这部分数据可以帮助我们了解如何对模型进行更深入的鲁棒性测试，并发现哪些攻击\n方式更有可能成功。\nStanford Human Preferences （SHP）数据集包含 38.5 万条来自 18 个不同领域的问题和指令，\n覆盖了从烹饪到法律建议的多个话题。这些数据衡量了人们对哪个答案更有帮助的偏好，旨在为\nRLHF 奖励模型和自然语言生成评估模型提供...",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "214 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n有太大的变化，从而维持模型的稳定性。\n6.4.2 奖励模型\n基于人类反馈训练的奖励模型可以很好地学习人类的偏好。理论上，可以通过强化学习使用\n人类标注的反馈数据直接对模型进行微调建模。然而，由于工作量和时间的限制，针对每次优化\n迭代，人类很难提供足够的反馈。更为有效的方法是构建奖励模型，模拟人类的评估过程。奖励\n模型在强化学习中起着至关重要的作用， 它决定了智能体如何从与环境的交互中学习并优化策略，\n以实现预定的任务目标。本节将从数据收集、模型训练和开源数据三个方面介绍大语言模型奖励\n模型的实现。\n1. 数据收集\n针对文献 [24] 所提出的大语言模型应该满足的3H 原则，如何构建用于训练奖励模型的数据是\n奖励模型训练的基础。本节介绍的奖励模型数据收集细节主要依据Anthropic 团队在文献 [247] 中\n介绍的 HH-RLFH 数据集构建过程。主要针对有用性和无害性，分别收集了不同人类偏好数据集。\n（1）有用性： 有用性意味着模型应当遵循指令； 它不仅要遵循指令， 还要从少量的示例提示或\n其他可解释的模式中推断出意..."
      ],
      "response_time": 2.2935714721679688,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.16614228746995247,
        "semantic_similarity": 0.9204058647155762
      }
    },
    {
      "sample_id": "0103",
      "question": "在深度学习中，如何结合强化学习和掩码预测方法来优化模型的性能？",
      "ground_truth": "强化学习中的PPO（Proximal Policy Optimization）方法可以通过特征转换和损失函数优化来提升模型性能，而掩码预测则通过自编码器的方式在视觉语言模型中增强对图像和文本的理解。这两者结合可以提升模型对缺失信息的恢复能力。",
      "predicted_answer": "根据提供的文档，我无法直接回答如何结合强化学习和掩码预测方法来优化模型的性能。文档中提到了强化学习算法GRPO在DeepSeek-R1-Zero模型中的应用[5]，以及掩码预测在视觉语言模型中的作用[1]，但是并没有提供关于两者结合的具体方法。",
      "ground_context": [
        "run_ppo(config)",
        "掩码（Masking）预测方法扮演着重要角色， 它本质上属于自编码器的一种特殊变体。"
      ],
      "retrieved_contexts": [
        "时，系统将各类特征转换为数值向量表示，并通过损失函数优化以使描述内容与图像特征在向量\n空间中相互接近。\n2. 掩码预测\n在深度学习领域， 掩码（Masking）预测方法扮演着重要角色， 它本质上属于自编码器的一种\n特殊变体[277]。 掩码预测在视觉语言模型中应用， 主要体现在两种训练模式上： 基于文本描述来恢\n复图像的缺失部分； 通过遮掩描述性词汇， 让模型从图像中提取并复现这些被遮蔽的语义信息， 如\n图7.3所示。",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "批量解码或并行解码）以降低内存访问成本。这些优化策略将在后续章节中进一步探讨。\n10.2 模型优化\n模型优化是提升大语言模型推理效率的重要手段，主要集中在优化模型结构和模型压缩两方\n面。 模型结构优化通过设计高效的模型结构直接提升效率， 包括高效FFN 设计、 注意力机制优化、\nMoE 架构设计、Transformer 代替架构设计等， 这些内容大部分都在本书第二章大语言模型基础部\n分进行了介绍。模型压缩则涵盖了多种技术， 旨在通过修改模型的数据表示（例如量化） 、 改变其\n架构（例如稀疏化、结构优化等）或者知识蒸馏来提高预训练模型的推理效率。",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "大规模语言模型：从理论到实践 207\n模型在不同场景高效运用推理能力， 仍是学界和业界面临的挑战。 此前研究尝试了多种方法， 如基\n于过程的奖励模型、 强化学习以及蒙特卡洛树搜索和波束搜索等搜索算法， 但均未达到与OpenAI\no 系列模型相媲美的通用推理性能。在此背景下，DeepSeek 团队开展了基于纯强化学习提升模型\n推理能力的探索。\n1. DeepSeek-R1-Zero：基于基座模型的强化学习\n1.1 强化学习算法 DeepSeek 的研究人员采用 GRPO 算法进行强化学习， 该算法舍弃了传统Actor-\nCritic 范式中与策略模型规模相当的 critic 模型，通过从一组得分估计基线来优化策略模型。通过\n这种方式，能够提高强化学习的效率，有利于大规模强化学习的开展。\n1.2 奖励建模 采用基于规则的奖励系统，包含两种奖励类型：\n• 准确性奖励： 用于评估模型响应的正确性。 对于有确定性答案的数学问题， 要求模型按指定\n格式输出最终答案以便验证；对于 LeetCode 编程问题，利用编译器根据预定义测试用例生\n成反馈。\n• 格式奖励：促使模型将思考过程置于‘<think..."
      ],
      "response_time": 1.886279582977295,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0.0,
        "bleu": 0.08806624130540115,
        "semantic_similarity": 0.8226406574249268
      }
    },
    {
      "sample_id": "0104",
      "question": "X-VLM 的训练依赖于哪些大规模标注数据集？",
      "ground_truth": "COCO、Visual Genome、SBU 和 Conceptual Captions",
      "predicted_answer": "X-VLM 的训练依赖于多个大规模标注数据集，包括 COCO[316]、Visual Genome[317]、SBU 和 Conceptual Captions[318]。",
      "ground_context": [
        "X-VLM 的训练依赖于多个大规模标注数据集，包括 COCO、Visual Genome、SBU 和 Conceptual Captions。"
      ],
      "retrieved_contexts": [
        "248 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n标注， 它有效地帮助模型理解复杂的视觉与文本关系。X-VLM 的训练依赖于多个大规模标注数据\n集，包括 COCO[316]、Visual Genome[317]、SBU 和 Conceptual Captions[318]，总计包含约 1600 万张\n图像。这些数据集丰富的标注信息为模型提供了大量高质量的视觉语义关联训练样本，使其在图\n文检索、视觉推理、视觉语义对齐以及图像描述等任务中均表现优异，超越了其他现有方法。这\n表明，边界框标注不仅能够提升模型的性能，还为复杂任务提供了更强的泛化能力。\n除了直接利用现成的标注数据集，一些研究者选择通过公开模型生成新的图文对数据集。例\n如，Kosmos-2[259] 使用网络爬取的数据构建了大规模图文对。其方法首先借助 spaCy 从文本中提\n取名词，然后通过基础模型 GLIP[319] 检测与这些名词相关的边界框。随后，使用 spaCy 从文本中\n进一步提取与名词对应的描述，生成能够与检测到的边界框匹配的图文对。这种方法显著扩展了\n标注数据的规模，为提升模型在视觉语义关联任务中的...",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "模型训练，避免模态间竞争导致的不稳定性。 Chameleon 在大量多样化的数据上进行预训练，包\n括文本、图像 - 文本对以及交错序列等。这种多样化的训练数据使模型能够学习丰富的多模态表\n示，显著提升了其泛化能力和对复杂多模态任务的适应性。\n4. 映射学习\nVLM 的训练通常面临显著的计算开销问题， 依赖庞大的计算资源和海量数据支持。 为解决这\n一问题，映射学习范式提出了一种高效的训练方法，即在现有的大语言模型和视觉特征提取模型\n的基础上进行二次训练，如图 7.5 所示。该方法通过利用开源的大语言模型，重点学习文本模态\n与图像模态之间的映射关系。通过构建这种映射，大语言模型能够适应视觉任务，同时显著降低\n对计算资源的需求。\nA photo Of a bird 大语言模型图像编码器映射网络\n图 7.5 视觉语言模型映射学习范式 [271]\nFrozen [284] 是一种将预训练大语言模型与视觉信息相融合的开创性方法。该方法设计了一种\n简洁高效的特征转换架构， 用于将图像特征映射到文本语义空间。 具体来说， 它采用NF-ResNet-50\n作为图像特征提取的基础模型，并训练了一个特征到语...",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "大规模语言模型：从理论到实践 219\n动相关领域的发展，也为开发者在大模型强化学习领域的创新提供了支持。本节将介绍使用 verl\n框架进行大模型中强化学习的实践。\n1. 训练脚本与参数配置\n以推理任务为例，我们按照官方教程选用 Qwen2.5-0.5B-Instruct 模型在 GSM8K 数据集上进\n行强化学习训练。下面是一些关键的 RL 训练参数：\n在近端策略优化算法中，ppo_mini_batch_size 表示小批次的大小。在训练过程中，我们并\n不会一次性使用整个训练集来更新模型参数，而是将训练集划分为多个小批次。这个参数设置\n为 64，意味着每次从训练集中选取 64 个样本组成一个小批次，用于计算梯度和更新演员模型\n的参数。通过使用小批次，可以减少内存的占用，并且在一定程度上提高训练的稳定性和效率。\nppo_micro_batch_size_per_gpu 指的是每个 GPU 上的微批次大小。在多 GPU 训练环境下，为\n了更高效地利用 GPU 资源， 会将小批次进一步划分为微批次。 这里设置为4， 表示每个GPU 每次\n处理 4 个样本的微批次。 这种细粒度的划分有助于在G..."
      ],
      "response_time": 1.3388943672180176,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.7058823529411764,
        "rouge2": 0.4,
        "rougeL": 0.7058823529411764,
        "bleu": 0.14957644445778928,
        "semantic_similarity": 0.7209202647209167
      }
    },
    {
      "sample_id": "0105",
      "question": "AnyGPT 能够处理哪些模态的输入与输出？",
      "ground_truth": "文本、语音、图像和音乐四种模态",
      "predicted_answer": "AnyGPT 能够处理文本、语音、图像和音乐这四种模态的输入与输出[1]。",
      "ground_context": [
        "AnyGPT 统一了文本、语音、图像和音乐四种模态，并实现了任意模态组合的相互转换，为多模态交互提供了一个"
      ],
      "retrieved_contexts": [
        "多模态大语言模型的架构种类繁多，其设计方式根据任务需求和输入输出的模态复杂性而有\n所不同。本节将重点介绍两种具有代表性的多模态模型：一是能够处理任意模态输入与输出的多\n模态大语言模型 AnyGPT[300]， 二是具有多视觉编码器融合架构的眸思 （MouSi）[301]。AnyGPT 通\n过统一的框架实现了跨模态的无缝交互，具备高度灵活的适应性，而眸思则通过集成多个视觉编\n码器，大幅增强了对复杂视觉信息的理解与生成能力。两者在多模态领域均展现出强大的性能和\n应用潜力。\n1. AnyGPT\nAnyGPT 将所有模态的数据转换为统一的离散化表示，并基于大语言模型采用的 Next Token\nPrediction 任务进行统一训练。 基于GPT 的原始架构以及多模态的离散化表示，AnyGPT 统一了文\n本、语音、图像和音乐四种模态，并实现了任意模态组合的相互转换，为多模态交互提供了一个",
        "242 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n统一的框架，如图 7.10 所示。\n语音解码器 图像解码器音乐解码器\n语音编码器 语音编码器音乐编码器\n语音文本图像 音乐\n图 7.10 AnyGPT 模型框架 [300]\nAnyGPT 所提出的统一的多模态生成框架由三个核心组成部分构成：多模态分词器、多模态\n大语言模型以及多模态生成器。具体来说，多模态分词器的作用是将连续的非文本模态数据转换\n为离散的 Token，并将这些 Token 组织成多模态交错序列。随后，大语言模型以 Next Token 预测\n损失为目标，在这些多模态序列上进行统一训练。在推理阶段，生成的多模态 Token 会通过对应\n的生成器解码回原始的模态表示。为了进一步提升生成结果的质量，还可以借助多模态增强模块\n对输出进行后处理，例如声音克隆或图像超分辨率等技术。\nAnyGPT 使用 SEED[302] 作为图像分词器。SEED 由 ViT 编码器、因果 Q-Former、VQ（V ector\nQuantization）码本、多层感知机以及 UNet 解码器组成，其内部码本（Codebook）包含 ...",
        "大语言模型：从理论到实践 243\n换为一个大小为 500 × 8 的矩阵，其中包含 500 × 1 的语义 Token 和 500 × 8 的声学 Token。\nAnyGPT 使用 Encodec[294] 作为音乐分词器。Encodec 内部包含 4 个码本，每个码本包含 2048\n个词元表示。 具体实现中， 使用一个在音乐数据上预训练的模型， 输入为32kHz 的单声道音频。 编\n码器将输入音频转换为嵌入向量， 随后通过残差向量量化 （RVQ） 进行量化， 使用4 个量化器， 每\n个量化器包含 2048 个码元，从而生成一个总数为 8192 的音乐 Token 表示。对于 5 秒长度的音频，\nEncodec 会将其量化为一个大小为 250 × 4 的码元矩阵。 为了适配语言模型的输入格式， 将这些码\n元按逐帧方式展平成一维序列，便于语言模型预测完整的音乐信息。\n为了将多模态的离散表示纳入预训练的大语言模型， AnyGPT 对模型进行了扩展，具体包括\n将每种模态的 Token 加入到词汇表中，并相应地扩展嵌入层和预测层。新加入的参数均采用随机\n初始化。 最终， 所有模态的Token ...",
        "元 （Entry） 。 在具体实现上，SEED 将尺寸为 224 × 224 的 RGB 图像分解为 16 × 16 的小块 （Patch） ，\n经过编码后将这些小块转换为量化的码元序列。 这些码元与预训练的unCLIP Stable Diffusion 模型\n的编码空间对齐，最终通过 UNet 解码器将码元序列恢复为原始图像。\nSpeechTokenizer[303] 则应用于 AnyGPT 作为语音分词器。SpeechTokenizer 的内部结构包含 8\n个码本， 每个码本包含1024 个词元表示。 其架构基于编码器-解码器， 并结合残差向量量化 （RVQ） ，\n能够将单通道音频序列压缩为离散的矩阵表示。 下采样后的帧率为50Hz， 语音分词器通过结合语\n义损失和重建损失，将语音信息解耦为语义信息和副语言学信息。具体来说，10 秒的音频会被转",
        "初始化。 最终， 所有模态的Token 组合形成了一个新的词汇表， 其大小等于所有模态的token 数之\n和。借助特定模态的分词器，能够将多模态数据压缩为离散的 Token 序列。语言模型在这些序列\n上执行 Next Token Prediction 任务进行训练，从而使核心的 LLM 能以自回归的方式自然地统一多\n模态感知、理解、推理和生成等任务。AnyGPT 使用 LLaMA-2 7B 的参数对大语言模型进行初始\n化，除了扩展嵌入矩阵和预测头外，语言模型的其余部分保持不变。\n使用大语言模型生成高质量的多模态数据是一项具有挑战性的任务，因为图像和音频的精确\n表示需要大量存储，导致序列长度显著增加，从而提高了语言模型的计算复杂度。为了解决这一\n问题，AnyGPT 提出了一种两阶段框架，用于高质量多模态数据生成，包括语义信息建模和感知\n信息建模。在语义层面，自回归语言模型生成融合且对齐的多模态 Token 序列；随后，非自回归\n模型将这些多模态语义 Token 转换为高保真的多模态内容，从而在性能和效率之间取得平衡。\n具体来说，在视觉语言建模中使用 SEED 标记，并通过扩散模型将其解码..."
      ],
      "response_time": 0.9700384140014648,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.30603689509300896,
        "semantic_similarity": 0.7557317018508911
      }
    },
    {
      "sample_id": "0106",
      "question": "MiniGPT-4 如何减少训练开销？",
      "ground_truth": "MiniGPT-4 将预训练的大语言模型和视觉编码器同时冻结，只需要单独训练线性投影层，使视觉特征和语言模型对齐。",
      "predicted_answer": "MiniGPT-4 通过利用现有的预训练大语言模型 Vicuna，并冻结其参数权重来降低计算开销[2][4]。",
      "ground_context": [
        "为了减少训练开销、避免全参数微调带来的潜在威胁， MiniGPT-4 将预训练的大语言模型和视觉编码器同时冻结，只需要单独训练线性投影层，使视觉特征和语言模型对齐。"
      ],
      "retrieved_contexts": [
        "在预训练的基础上， 研究人员使用精心构建的高质量图像-文本对对预训练的 MiniGPT-4 模型\n进行微调。在训练过程中，MiniGPT-4 同样要完成类似的文本描述生成任务，不过具体的任务指\n令不再固定， 而是来自一个更广泛的预定义指令集。例如， “详细描述此图像” 、 “你可以为我描述\n此图像的内容吗” ，或者“解释这张图像为什么有趣” 。微调训练只在训练数据集和文本提示上与\n预训练过程略微不同，在此不再介绍相关的代码实现。\n微调结果表明，MiniGPT-4 能够产生更加自然、 更加流畅的视觉问答反馈。 同时， 这一训练过\n程也是非常高效的，只需要 400 个训练步骤，批量大小为 12，使用单块 NVIDIA A100 80GB GPU\n训练 7 分钟即可完成。\n在微调完成后，研究人员发现 MiniGPT-4 具备其他各种有趣的能力，这是在 GPT-4 的演示中\n没有体现的，例如：通过观察诱人的食物照片，直接生成详细的食谱；识别图像中存在的问题并\n提供相应的解决方案；直接从图像中检索出有关人物、电影或绘画作品的事实信息。如图7.16 所\n示，用户希望 MiniGPT-4 指出输入的...",
        "258 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n这段代码实现了整个 MiniGPT-4 模型的前向传播过程， 包括图像和文本的编码、 提示处理、 多\n模态数据编码的连接， 以及最终损失的计算。通过在Conceptual Caption、SBU 等组合数据集上进\n行计算，即可获得预训练的 MiniGPT-4 模型。\n在第一轮训练完成后，MiniGPT-4 获得了关于图像的丰富知识，并且可以根据人类查询提供\n合理的描述。但是它在生成连贯的语句输出方面遇到了困难，例如，可能会产生重复的单词或句\n子、碎片化的句子或者完全不相关的内容。这样的问题降低了 MiniGPT-4 与人类进行真实交流时\n流畅的视觉对话能力。\n2. 高质量数据集构建\n研究人员注意到，预训练的 GPT-3 曾面临类似的问题。虽然在大量的语言数据集上做了预训\n练，但模型并不能直接生成符合用户意图的文本输出。 GPT-3 通过从人类反馈中进行指令微调和\n强化学习，产生了更加人性化的输出。借鉴这一点，研究人员期望预训练的 MiniGPT-4 也可以做\n到与用户意图对齐，增强模型的可用性。\n为此，研究人员精心构建了...",
        "大语言模型：从理论到实践 255\n7.4.2 MiniGPT-4 训练策略\n为了获得真正具备多模态能力的大语言模型，MiniGPT-4 提出了一种分为两阶段的训练方法。\n第一阶段，MiniGPT-4 在大量的图像-文本对数据上进行预训练，以获得基础的视觉语言知识。第\n二阶段，MiniGPT-4 使用数量更少但质量更高的图像-文本数据集进行微调，以进一步提高预训练\n模型的生成质量与综合表现。\n1. MiniGPT-4 预训练\n在预训练阶段，MiniGPT-4 希望从大量的图像-文本对中学习视觉语言知识，所以使用了来自\nConceptual Caption [318, 334]、SBU[335] 和 LAION[336] 的组合数据集进行模型预训练。以 Conceptual\nCaption 数据集为例，数据格式如图7.14 所示，包含基本的图像信息与对应的文本描述。",
        "大语言模型：从理论到实践 251\n7.4.1 MiniGPT-4 模型架构\nMiniGPT-4 期望将来自预训练视觉编码器的图像信息与大语言模型的文本信息对齐，它的模\n型架构如图7.13 所示，具体来说主要由三个部分构成：预训练的大语言模型 Vicuna[41]、预训练的\n视觉编码器，以及一个单一的线性投影层。\n图 7.13 MiniGPT-4 的模型架构 [252]\n1. Vicuna 模型\nVicuna 是一个基于解码器的大语言模型，它建立在 LLaMA[34] 的基础上，可以执行多种复杂\n语言任务。在 MiniGPT-4 中，它的主要任务是同时理解输入的文本与图像数据，对多个模态的信\n息具有感知理解能力， 生成符合指令的文本描述。 在具体的构建过程中，MiniGPT-4 并不从头开始\n训练大语言模型，而是直接利用现有的 Vicuna-13B 或 Vicuna-7B 版本，冻结所有的参数权重，降\n低计算开销。相关的预训练代码可以参考第 4 章和第 5 章的相关内容。\n2. 视觉编码器\n为了让大语言模型具备良好的视觉感知能力，MiniGPT-4 使用了与 BLIP-2[263] 相同...",
        "256 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n图 7.14 Conceptual Caption 数据集的格式\n在第一阶段的训练过程中，预训练的视觉编码器和大语言模型都被设置为冻结状态，只对单\n个线性投影层进行训练。预训练共进行了约 2 万步，批量大小为 256，覆盖了 500 万个图像-文本\n对，在 4 块 NVIDIA A100 80GB GPU 上训练了 10 小时。以下代码示例有助于读者更好地理解\nMiniGPT-4 的训练过程："
      ],
      "response_time": 1.0585196018218994,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.5714285714285715,
        "rouge2": 0.4,
        "rougeL": 0.5714285714285715,
        "bleu": 0.08303095690192591,
        "semantic_similarity": 0.767959713935852
      }
    },
    {
      "sample_id": "0107",
      "question": "语音到文本模态的转换是目前广泛采用的方法之一吗？",
      "ground_truth": "是的。",
      "predicted_answer": "是的，语音到文本模态的转换是目前广泛采用的方法之一[5]。这种方法充分考虑到了大语言模型主要为文本模态设计的特点，通过将语音模态信息投射到文本空间，实现了语音与文本模态的直接对齐。",
      "ground_context": [
        "语音到文本模态的转换是目前广泛采用的方法之一。"
      ],
      "retrieved_contexts": [
        "238 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\nS2T 模式实现简单，适用于纯语音到文本的转换任务，但是无法处理更复杂的多模态任务。\nST2T 是目前语音语言模型中最广泛采用的模式。 该模式支持同时输入语音和文本， 其中文本\n通常作为指令或任务提示。模型通过同时处理音频与文本模态的信息，融合两者的特征后生成最\n终的文本输出。这种模式不仅能够支持多任务学习，还能充分发挥大语言模型的强大能力，处理\n更广泛的任务，可以应用于语音翻译、语音情感分析等涉及音频和文本模态的任务。\nST2ST 是一种更高级的模式，模型在输入中结合语音和文本，并在输出中同时生成语音和文\n本。这种模式在解码阶段需要额外的语音合成模块（V ocoder）来生成语音输出。ST2ST 模式不仅\n能够完成基本的语音识别任务，还支持文本语音生成（Text-to-Speech，TTS） 、语音翻译及语音转\n换等复杂任务。\n2. 语音嵌入表示预训练\n语音嵌入表示预训练是一种通过在大规模语音数据上学习语音通用特征表示，进而提升下游\n语音任务性能的关键技术。近年来，基于不同模型架构的预训练方法逐渐成为研究热点，其中主\n...",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "240 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n……\n音频嵌入\n对齐的音频嵌入\n+文本编码器\n文本编码器\n文本输入\n音频输入\n音频嵌入空间\n转换\n大语言模型文本嵌入空间\n大语言模型输出\n…………\n文本嵌入\n对齐的音频嵌入\n文本嵌入\n连接\n……\n图 7.7 语音和文本表示融合架构直接投射方法 [285]\nToken 映射方法通过将语音特征转换为大语言模型可处理的文本 Token 实现模态转换[297]， 如\n图7.8所示。 具体而言， 语音特征经过投射器或转换器生成与文本Token 对应的表示， 这些符号随后\n与文本的 Token 序列结合，形成一个同时包含语音和文本信息的 Token 序列，并将其输入到 LLM\n中进行统一处理。 该方法不仅能够较好地保留语音特征信息， 还确保了LLM 在处理数据时的连续\n性和一致性。\n映射/转换\n文本词元切分文本输入\n音频词元\n+连接\n音频输入\n文本词元\n音频词元\n文本词元\n连接词元\n…………\n…………\n融合嵌入\n语言模型文本嵌入空间\n大语言模型输出\n…\n文本编码器\n…\n图 7.8 语音和文本表示融合架构 Token 映射方法 [28...",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "在此基础上引入了 LSTM 模块以增强序列分析能力，同时结合 Transformer 架构优化离散符号序\n列的建模能力，从而在多种语音任务中取得了显著的性能提升。\n3. 语音和文本表示融合架构\n获得语音模态信息后， 需要将其与文本模态信息集成， 以便大语言模型进行进行最终推理。 语\n音和文本表示融合主要有两个技术路线：语音模态表示转换到文本模态空间；语音和文本两个模\n态数据融合在同一空间联合表示。\n语音到文本模态的转换是目前广泛采用的方法之一。这种方法充分考虑到大语言模型主要是\n为文本模态设计的特点，通过将语音模态信息投射到文本空间，实现语音与文本模态的直接对齐，\n从而在最大程度上保留大语言模型的能力。 为了实现这一目标， 通常需要引入一个 “连接器” （Con-\nnector）或“投射器” （Projector）来将语音模态特征转换到文本模态特征空间。 在此过程中， 需尽\n量减少语音特征信息的损失，并保证模态转换的平滑性。目前，主要有以下两种实现方式：直接\n投射（Direct Projection）和 Token 映射（Token Mapping） 。\n直接投射方法通过连接器将语音特..."
      ],
      "response_time": 1.688422679901123,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 1.0,
        "mrr": 0.2
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.01288604499272648,
        "semantic_similarity": 0.5591537356376648
      }
    },
    {
      "sample_id": "0108",
      "question": "这张图片呈现了什么？",
      "ground_truth": "这是一张站着的鸟的照片。",
      "predicted_answer": "这张图片呈现了一只站立在湖边石柱上的鸟[A]。",
      "ground_context": [
        "This is a photo of a standing bird."
      ],
      "retrieved_contexts": [
        "246 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n质量提升数据剪枝数据平衡祛除冗余\nA photo of a bird   \nA photoof a dog\nyes\nno“A bird standing on a stone pillar in a lake”\nInstruction: Provide a description of what is presented in the image.\nAnswer: This is a photo of a standing bird.\nInstruction-tuningReinforcement Learning From Human FeedbackVLM: This image is a photo of a standing bird. VLM: Hi ,I am a robot. It is a photo of a dog.goodbad\nA black swan swimmingBaby swans in a poolUsing bounding boxesUsing negative captions\nA...",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "Y\n<latexit sha1_base64=\"17zqk6qKa6kdejpo5MShoZgkkWA=\">AAAB/XicbVC7SgNBFL0bXzG+opY2g0GwWnYlRBshaGMZwTwgWcLsZDYZMzO7zMwKYQn+gq32dmLrt9j6JU6SLTR64MLhnHs5lxMmnGnjeZ9OYWV1bX2juFna2t7Z3SvvH7R0nCpCmyTmseqEWFPOJG0aZjjtJIpiEXLaDsfXM7/9QJVmsbwzk4QGAg8lixjBxkqt5NJzq36/XPFcbw70l/g5qUCORr/81RvEJBVUGsKx1l3fS0yQYWUY4XRa6qWaJpiM8ZB2LZVYUB1k82+n6MQqAxTFyo40aK7+vMiw0HoiQrspsBnpZW8m/uuFYinZRBdBxmSSGirJIjhKOTIxmlWBBkxRYvjEEkwUs78jMsIKE2MLK9lS/OUK/pLWmevX3NpttVK/yuspwhEcwyn4cA51uIEGNIHAPTzBM7w4j86r8+a8L1YLTn5zCL/gfH...",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "x\n<latexit sha1_base64=\"IVqKn81Phb8FhcRP6SaEHpTS2es=\">AAAB+HicbVDLTgJBEOzFF+IL9ehlIzHxRHaNQY9ELx4hkYeBDZkdemHCzOxmZtYECV/gVe/ejFf/xqtf4gB7ULCSTipV3alOhQln2njel5NbW9/Y3MpvF3Z29/YPiodHTR2nimKDxjxW7ZBo5ExiwzDDsZ0oJCLk2ApHtzO/9YhKs1jem3GCgSADySJGibFS/aFXLHllbw53lfgZKUGGWq/43e3HNBUoDeVE647vJSaYEGUY5TgtdFONCaEjMsCOpZII1MFk/ujUPbNK341iZUcad67+vpgQofVYhHZTEDPUy95M/NcLxVKyia6DCZNJalDSRXCUctfE7qwFt88UUsPHlhCqmP3dpUOiCDW2q4ItxV+uYJU0L8p+pVypX5aqN1k9eTiBUzgHH66gCndQgwZQQHiGF3h1npw35935WKzmnOzmGP7A+fwB8QeTjg..."
      ],
      "response_time": 0.6785922050476074,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.04030833724870907,
        "semantic_similarity": 0.7922948598861694
      }
    },
    {
      "sample_id": "0109",
      "question": "DataComp 框架的目标是什么？",
      "ground_truth": "旨在构建能够在 38 项下游任务中表现卓越的图像-文本数据组合。",
      "predicted_answer": "DataComp 框架的目标是构建能够在 38 项下游任务中表现卓越的图像-文本数据组合[3][4]。",
      "ground_context": [
        "该框架基于标准化的CLIP 架构与预训练参数， 旨在构建能够在 38 项下游任务中表现卓越的图像-文本数据组合。"
      ],
      "retrieved_contexts": [
        "据集，DataComp 提出了从多样化设计的数据集中进行采样的策略。 具体而言， 采样方法主要分为\n基于文本和基于图像两种： 基于文本的采样方法保留与ImageNet 类别相关联的图文对描述； 而基\n于图像的采样则通过利用 OpenAI CLIP 的 ViT-L/14模型对图像进行编码， 并借助FAISS 工具将大\n规模噪声图像聚类为 100,000 个组，然后根据 ImageNet 训练样本的嵌入，选择与这些样本最相近\n的聚类，从而生成具有多样性的图像数据集。尽管这些方法能有效提升数据的多样性，但它们对\nImageNet 等语义数据集的依赖可能会引入类别偏倚，从而限制模型在新下游任务中的泛化能力。\n此外，MetaCLIP[314] 提出了另一种方法， 利用来自Wikipedia 和 WordNet 的 500,000 个查询作为元\n数据，构建覆盖广泛概念的预训练数据分布。通过“平衡采样”算法，MetaCLIP 限制每个查询的\n样本数量（最多 20,000 个） ，在概念的多样性与代表性之间寻求平衡，从而进一步提升模型的泛\n化能力。\n7.3.2 视觉语义关联\n视觉语义关联是多模态大语言...",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "A photo of a birdA photo of a dog\n数据\n视觉语义关联视觉语义对齐\n图 7.12 多模态大语言模型训练策略 [271]\n本节主要从数据处理、视觉语义关联、文本对齐等方面进行介绍。\n7.3.1 数据处理\n在多模态大语言模型的训练中，数据质量对模型性能起着至关重要的作用。高效的数据处理\n与筛选策略能够显著提升模型的学习效果及其在下游任务中的泛化能力。为评估基础数据集的质\n量， 研究团队提出了DataComp 框架[305]。 该框架基于标准化的CLIP 架构与预训练参数， 旨在构建\n能够在 38 项下游任务中表现卓越的图像-文本数据组合。DataComp 构建了一个包含 128 万至 128\n亿对图像-文本样本的噪声网络数据库，并系统性地探索了多种数据筛选策略。研究表明，剪枝优\n化是提升跨模态大语言模型效果的关键技术手段，为高效能模型的训练提供了重要支持。\n数据剪枝的方法可以分为三类：(1) 使用启发式方法去除低质量样本；(2) 基于预训练 VLM 的\n打分方法对图文对进行排序，丢弃对齐较差的样本；(3) 创建多样化且平衡的数据集。\n启发式方法可以分为单模态...",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "Deduplication） 的目标是删除连续相同的序列字符串。 使用后缀数组进行逐个词元间的对比， 并删\n除 50 个以上的连续相同词元序列。URL 冗余去除（URL Deduplication）的目标是删除具有相同\nURL 的文档。CommonCrawl 数据集中存在一定量的具有重复 URL 的文档，并且这些文档的内容\n通常是完全相同的。构建 RefinedWeb 数据集时，对 CommonCrawl 数据集中不同部分之间相同的\nURL 进行了去除。该阶段处理完成后的数据集称为 RefinedWeb，仅保留了原始数据的 11.67%。\n以上三个阶段所包含的各个任务的详细处理规则可以参考文献[60] 的附录部分。 此外， 文献[60]\n还利用三个阶段产生的数据分别训练 10 亿和 30 亿参数规模的模型，并使用零样本泛化能力对模\n型结果进行评测。 评测后发现，RefinedWeb 的效果远好于 RW-Raw 和 RW-Filtered。 这也在一定程\n度上说明高质量数据集对语言模型具有重要的影响。\n3.4.4 CulturaX\nCulturaX[159] 是一个可以用于预训练的多语言..."
      ],
      "response_time": 1.033179759979248,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.4,
        "rouge2": 0.0,
        "rougeL": 0.4,
        "bleu": 0.5543628311642381,
        "semantic_similarity": 0.843100905418396
      }
    },
    {
      "sample_id": "0110",
      "question": "什么方法将RLHF从文本领域适配到视觉语言任务？",
      "ground_truth": "事实增强 RLHF（Factually Augmented RLHF）",
      "predicted_answer": "创新方法——事实增强 RLHF（Factually Augmented RLHF）将RLHF从文本领域适配到视觉语言任务，通过在奖励模型中加入图像标题和真实多选题的额外事实信息，减少了奖励滥用问题。[3]",
      "ground_context": [
        "创新方法——事实增强 RLHF（Factually Augmented RLHF ） 。该方法将RLHF 从文本领域适配到视觉语言任务，通过在奖励模型中加入图像标题和真实多选题的额外事实信息，减少奖励滥用问题。"
      ],
      "retrieved_contexts": [
        "练。通过将预训练的 Vicuna 语言模型编码器与 CLIP ViT-L/14 视觉编码器的输出融合到相同的维\n度空间，LLaV A在合成指令跟随任务和 Science QA 基准测试中表现出显著的改进。LLaV A 1.5[320]\n在 LLaV A的基础上进一步优化了多模态文本对齐能力。其改进包括引入跨模态全连接多层感知\n机（MLP）层，并结合视觉问答（VQA）指令数据进行训练。LLaV A 1.5仅使用 60 万条图文对数\n据，在 8 张 A100 GPU 上约一天即可完成训练。LLaV A-NeXT (v1.6)[330] 在 LLaV A 1.5的基础上进\n行了多方面的改进，进一步推动了多模态文本对齐的性能。通过将全图和小图块的视觉特征分别\n输入视觉编码器，并将其拼接后处理，提高了图像分辨率的利用效率。优化了视觉指令调优数据\n集，新增了更好的视觉推理、OCR、世界知识和逻辑推理样本。\n由于高质量视觉指令调优数据的稀缺， LLaV A等模型可能在视觉和文本模态对齐上存在偏\n差， 甚至生成幻觉性输出。 为了解决这一问题，LLaV A-RLHF[262] 提出了基于人类反馈强化学习...",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "创新方法——事实增强 RLHF（Factually Augmented RLHF ） 。该方法将RLHF 从文本领域适配到\n视觉语言任务，通过在奖励模型中加入图像标题和真实多选题的额外事实信息，减少奖励滥用问\n题。LLaV A-RLHF还利用 GPT-4 生成的训练数据及人工编写的图文对进一步提升其通用能力。在\nLLaV A-Bench中，其性能达到了 GPT-4 的 94%，在专注于减少幻觉的 MMHAL-BENCH 中，相较\n基线模型提升了 60%。\n2. 富含文本信息的图像理解\n富含文本信息的图像（Text-rich Image，如电影海报、书籍封面、文档扫描等）不仅需要模型\n理解视觉内容，还需要解析其中包含的细粒度文本信息，并与视觉语义进行有效关联。传统的多\n模态大语言模型在处理这类任务时往往面临文本识别能力不足、分辨率限制以及上下文信息捕获\n不充分等问题。 为应对这些挑战， 近年来涌现出一系列创新方法和模型， 包括LLaV AR、Monkey、\nLumos 等，它们专注于提升文本丰富图像的理解能力。",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "大语言模型：从理论到实践 249\n加贴合人类需求。此外，多模态大模型处理文本丰富的图像理解面临特定挑战，相关领域也涌现\n出大量研究，推动了技术的持续发展。本节针对上述内容进行介绍。\n1. 多模态指令微调与 RLHF\n多模态指令微调通过在包含指令、 输入和期望响应的监督数据上对多模态文本对齐进行优化，\n从而提升模型理解和执行复杂指令的能力。与大规模的预训练数据集相比，指令微调数据集的规\n模通常较小， 其样本数量从几千到一百万不等[328]。 代表性的视觉语言模型如LLaV A、InstructBLIP\n和 OpenFlamingo[329] 均引入了指令微调技术，显著提升了多模态任务的表现。\nRLHF 则专注于通过人类反馈使模型输出更符合人类偏好。 具体来说，RLHF 首先通过训练一\n个奖励模型来评估模型响应的质量， 捕捉人类偏好的特征。 借助这一奖励模型，RLHF 能有效模拟\n人类偏好，从而减少对人工标注的依赖。随后，通过奖励模型对多模态大语言模型进行微调，使\n其生成的响应更加贴合人类期望。\nLLaV A[320] 通过指令微调提升了多模态对话能力，采用了 15 万条合成视觉指令样本进..."
      ],
      "response_time": 1.6172223091125488,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 0.3333333333333333
      },
      "answer_quality_metrics": {
        "rouge1": 0.8,
        "rouge2": 0.7499999999999999,
        "rougeL": 0.8,
        "bleu": 0.13772093428074758,
        "semantic_similarity": 0.7603427171707153
      }
    },
    {
      "sample_id": "0111",
      "question": "Qformer使用的基础模型是什么？",
      "ground_truth": "BertLMHeadModel",
      "predicted_answer": "Qformer使用的基础模型是BERT[1]。",
      "ground_context": [
        "Qformer = BertLMHeadModel(config=encoder_config)"
      ],
      "retrieved_contexts": [
        "大语言模型：从理论到实践 253\ndef init_Qformer(cls, num_query_token, vision_width, cross_attention_freq =2):\n#֥BERTଆ྘஥ᇂQ-Former\nencoder_config = BertConfig.from_pretrained(\"bert-base-uncased\")\n#؇\nencoder_config.encoder_width = vision_width\nencoder_config.query_length = num_query_token\n# ᄝBERTҭᇿၩ৯Ҫ\nencoder_config.add_cross_attention = True\nencoder_config.cross_attention_freq = cross_attention_freq\n#֥BERTଆ྘ቔູQ-Formerଆॶ\nQformer = BertLMHeadModel(config=encoder_config)\n#༢\nquery_tokens = nn.Parameter(\ntorch.zeros...",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "254 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n#ݖࣜࡼQ-Formerࡗ\nimg_f_dim؇\nllama_model.config.hidden_size؇\nself.llama_proj = nn.Linear(\nimg_f_dim, self.llama_model.config.hidden_size\n)\n#đMiniGPT-4ԩ৘ੀӱ\ndef encode_img(self, image):\ndevice = image.device\nwith self .maybe_autocast():\n#ྛщ઒đᄜ൐ႨLayerNormԩ৘\nimage_embeds = self.ln_vision(self.visual_encoder(image)).to(device)\n#֥ࢲײQ-Former\nif self .has_qformer:\n#ᇿၩ৯ဃ઒\nimage_atts = torch.ones(image_embeds.size()[:-1], dtype =torch.long).to(device)\n#؇\nquery_tokens = self.qu...",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "2. 大语言模型基础\n语言模型的核心目标是对自然语言的概率分布进行建模，这一任务在自然语言处理研究中占\n据重要地位，是其基础性工作之一。大量研究围绕这一目标，从不同角度展开了探索，包括 n 元\n语言模型（n-gram Language Models ） 、神经语言模型和预训练语言模型等。这些研究在不同发展\n阶段对自然语言处理任务产生了深远影响。 随着基于Transformer 架构的语言模型不断发展， 以及\n预训练-微调范式在各类自然语言处理任务中取得突破性成果，自 2020 年 OpenAI 发布 GPT-3 以\n来，大语言模型的研究逐步深入。尽管大语言模型参数规模庞大，并且通过有监督微调和强化学\n习可以完成众多任务，其理论基础仍然离不开对语言建模的核心研究。\n本章首先介绍 Transformer 结构， 并在此基础上讲解生成式预训练语言模型GPT、 大语言模型\n的网络结构、注意力机制优化及相关实践。关于 n 元语言模型、神经语言模型及其他预训练语言\n模型的内容，可参考《自然语言处理导论》第 6 章[4]。\n2.1 Transformer 结构\nTransformer 结构[44] ..."
      ],
      "response_time": 0.5321395397186279,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0.0,
        "bleu": 0.0,
        "semantic_similarity": 0.5156816244125366
      }
    },
    {
      "sample_id": "0112",
      "question": "在代码中，如何计算损失（loss）?",
      "ground_truth": "loss = outputs.loss",
      "predicted_answer": "在给定的代码片段中，损失（loss）是通过调用PyTorch的`F.cross_entropy`函数来计算的。具体来说，损失是在模型的预测值`preds`与目标值`results`之间计算得到的，并且指定了`ignore_index=target_pad`参数以忽略特定的目标索引。计算损失的具体步骤如下：\n\n```python\nloss = F.cross_entropy(preds.view(-1, preds.size(-1)), results, ignore_index = target_pad)\n```\n\n这里的`preds`是模型输出的未归一化的概率分布，而`results`是实际的目标标签。通过这种方式，可以有效地计算出交叉熵损失。[3]",
      "ground_context": [
        "loss = outputs.loss"
      ],
      "retrieved_contexts": [
        "112 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n图 4.13 多头自注意力机制的张量并行示意图 [169]\n分类网络最后一层一般会选用 Softmax 和 Cross_entropy 算子来计算交叉熵损失。如果类别数\n量非常大，则会导致单计算设备内存无法存储和计算 logit 矩阵。针对这一类算子，可以按照类别\n维度切分，同时通过中间结果通信，得到最终的全局交叉熵损失。首先计算的是 Softmax 值，公\n式如下：\nSoftmax(xi) = exi\nP\nj\nexj\n= exi−xmax\nP\nj\nexj −xmax\n= exi−xmax\nP\nN\nP\nj\nexj −xmax\n(4.4)\nxmax = max\np\n(max\nk\n(xk)) (4.5)\n其中，p 表示张量并行的设备号。 得到Softmax 计算结果之后， 同时对标签Target 按类别切分， 每\n个设备得到部分损失，最后进行一次通信，得到所有类别的损失。整个过程，只需要进行三次小\n量的通信，就可以完成交叉熵损失的计算。\nPyTorch 提供了细粒度张量级别的并行 API——DistributedTensor...",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "src = batch.English.transpose(0,1)\ntrg = batch.French.transpose(0,1)\n#ე\n#ყҩ\ntrg_input = trg[:, : -1]\n# ൫๭ყҩֆՍ\ntargets = trg[:, 1:].contiguous().view(-1)\n#ඔটᇅቔဃ઒\nsrc_mask, trg_mask = create_masks(src, trg_input)\npreds = model(src, trg_input, src_mask, trg_mask)\noptim.zero_grad()\nloss = F.cross_entropy(preds.view(-1, preds .size(-1)),\nresults, ignore_index =target_pad)\nloss.backward()\noptim.step()\ntotal_loss += loss.data[0]\nif (i + 1) % print_every == 0:\nloss_avg = total_loss / print_every\nprint(\"...",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "start = time.time()\nbatch = to_device(batch, device)\noutputs = model(**batch, use_cache =False)\nloss = outputs.loss\nif args.print_loss:\nprint(\nf\"Epoch: {epoch}, Step: {step},\nRank: {torch.distributed.get_rank()}, loss = {loss}\"\n)\nmodel.backward(loss)\nmodel.step()\nend = time.time()\nif torch.distributed.get_rank() == 0:\nprint_throughput(model.model, args, end - start,\nargs.global_rank)\nif args.output_dir is not None:\nprint_rank_0('saving the final model ... ' , args .global_rank)\nmodel = convert_l..."
      ],
      "response_time": 3.577355146408081,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 1.0,
        "mrr": 0.2
      },
      "answer_quality_metrics": {
        "rouge1": 0.1212121212121212,
        "rouge2": 0.0,
        "rougeL": 0.1212121212121212,
        "bleu": 0.005469906266499716,
        "semantic_similarity": 0.6039063930511475
      }
    },
    {
      "sample_id": "0113",
      "question": "MouSi模型如何结合不同视觉专家的信息，以更好地识别图像中的文本，并且如何通过ChatGPT提高文本描述的质量？",
      "ground_truth": "MouSi模型通过引入多个视觉专家（如CLIP、SAM和Layout Mv3专家）编码处理图像信息，并利用多视觉融合网络整合这些输出序列，以完成视觉问答和光学字符识别任务。同时，ChatGPT被用作自动化的文本质量评估者，检查生成的文本描述，修正其中的语义和语法错误，从而提高描述的质量。",
      "predicted_answer": "MouSi模型通过一个多视觉融合网络来结合不同视觉专家的信息，这个融合网络能够统一处理来自CLIP专家、SAM专家、LayoutLMv3专家以及其他专家的不同视觉标记，进而实现高效的多模态信息整合[3]。此外，MouSi模型采用了多补丁-单标记投影方法，通过压缩局部视觉信息，将多个补丁映射为单个标记，从而降低视觉标记数量，提高推理效率[5]。\n\n至于ChatGPT提高文本描述质量方面，题干并未提供相关信息。根据提供的文档，我无法回答ChatGPT如何提高文本描述质量的问题。",
      "ground_context": [
        "基于 MouSi 模型，当用户上传一张描绘风媒花授粉过程的图片并询问“哪些球果产生花粉？”时，该图片依次经过 CLIP 专家、SAM 专家、Layout Mv3 专家及其他专家的编码处理，产生多组不同的视觉标记。",
        "研究人员利用ChatGPT强大的语言理解和生成能力，让其作为一个自动化的文本质量评估者，对生成的5000个图像-文本对进行检查。"
      ],
      "retrieved_contexts": [
        "244 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n嵌入表示\n Poly-Expert 融合网络\nOCR专家分割专家 其他专家\nLayoutLMv3图像编码器SAM图像编码器 任意模态编码器\n视觉语言模型\n图片描述生成 OCR\n哪些松果产⽣花粉？\nCLIP图像编码器\n语义专家\n图片问答 其他\n图 7.11 眸思（MouSi）模型框架 [301]\n基于 MouSi 模型，当用户上传一张描绘风媒花授粉过程的图片并询问“哪些球果产生花粉？ ”\n时，该图片依次经过 CLIP 专家、SAM 专家、Layout Mv3 专家及其他专家的编码处理，产生多组\n不同的视觉标记。随后，一个多视觉融合网络压缩融合多通道视觉信息，并将其与视觉输入标记\n对齐。用户的问题通过大语言模型的嵌入层被处理成文本标记。最终，MouSi 通过对视觉语言标\n记进行处理，完成 VQA（视觉问答）和 OCR（光学字符识别）任务，从图片中识别答案文本， 生\n成正确答案“雄性球果产生花粉。 ”\n由于不同视觉专家的输出序列在维度和数量上往往存在差异，因此需要设计融合网络来统一\n处理这些输出。 为了更好地整合多专家信息，M...",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "具体来说，在视觉语言建模中使用 SEED 标记，并通过扩散模型将其解码为高质量图像。在\n语音生成任务中，采用 SoundStorm 模型生成声学 Token，随后将其解码为原始音频数据。对于音\n乐生成，使用 Encodec 标记以捕捉高频细节，并通过 Encodec 解码器将其重构为高保真的音频数\n据。 通过这种设计，AnyGPT 在显著减少语音序列长度的同时， 能够生成高质量的多模态数据， 从\n而在生成效果和计算效率之间实现了良好的平衡。\n2. 眸思（MouSi）\n当前的视觉语言模型经常遭遇单视觉编码器组件能力不足和视觉 Token 过长等挑战。这些挑\n战会限制模型准确理解繁复的视觉信息和过长的上下文信息。解决这些难题对于提高 VLM 的性\n能和可用性至关重要。\n为解决上述问题， 多模态大模型眸思 （MouSi）[301] 提出了使用多专家技术以协同各视觉编码\n器的能力， 这些能力包括图像文本匹配， 光学字符识别， 图像分割等。 该技术引入一个融合网络使\n得来自不同视觉专家的输出得到统一，同时弥合了视觉编码器和预训练 LLM 之间的差异。此外，\n还提出了二维可训练图像位置编码方法， ...",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "影融合网络和 Q-Former 融合网络。 然而， 在实际应用中， 多个视觉专家输出的大量视觉标记不仅\n增加了视觉语言模型的计算成本和内存使用率，还可能超过推理过程中最大序列长度的限制。为\n了解决这一问题，MouSi 模型提出了多补丁-单标记投影方法， 以按比例减少每个专家的输出标记\n数量。具体而言，由于图像信号具有局部性和稀疏性属性，用一个标记表示相邻的多个补丁是合\n理的。这种方法通过对局部视觉信息进行压缩，将多个补丁映射为单个标记，从而实现了多通道\n视觉信号的高效传输。通过多补丁-单标记投影，不仅有效降低了视觉信号传输的冗余，还减少了\n视觉大语言模型后续处理的计算成本，显著提高了推理效率，为多视觉专家的高效整合提供了切\n实可行的解决方案。\n尽管通过多补丁-单标记操作或在 Q-Former 中定义少量查询可以显著减少视觉标记的数量， 但"
      ],
      "response_time": 3.3450348377227783,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.5333333333333333,
        "rouge2": 0.3076923076923077,
        "rougeL": 0.5333333333333333,
        "bleu": 0.03584680336491316,
        "semantic_similarity": 0.8866490721702576
      }
    },
    {
      "sample_id": "0114",
      "question": "如何通过多补丁-单标记投影方法提高多模态大语言模型在视觉和语音处理中的效率？",
      "ground_truth": "多补丁-单标记投影方法能够有效降低视觉信号传输的冗余，并减少视觉大语言模型后续处理的计算成本，从而显著提高推理效率。与此同时，语音语言模型通过端到端架构直接学习音频特征与语言语义的映射关系，增强了模型在开放世界场景中的泛化能力。这两者结合可以实现视觉与语音模态的高效融合与处理。",
      "predicted_answer": "通过多补丁-单标记投影方法，MouSi 模型按比例减少了每个专家的输出标记数量，从而实现多通道视觉信号的高效传输[1]。这种方法通过对局部视觉信息进行压缩，将多个补丁映射为单个标记，有效降低了视觉信号传输的冗余，并减少了视觉大语言模型后续处理的计算成本，显著提高了推理效率[1]。此外，这种方法还能解决多视觉专家导致的超长序列问题，并减少位置编码的冗余分配，进一步优化视觉标记的处理效率，为多模态模型的可扩展性提供支持[5]。",
      "ground_context": [
        "MouSi 模型提出了多补丁-单标记投影方法， 以按比例减少每个专家的输出标记数量。",
        "语音语言模型通过端到端架构直接学习音频特征与语言语义的映射关系，从而增强了模型在开放世界场景中的泛化能力。"
      ],
      "retrieved_contexts": [
        "影融合网络和 Q-Former 融合网络。 然而， 在实际应用中， 多个视觉专家输出的大量视觉标记不仅\n增加了视觉语言模型的计算成本和内存使用率，还可能超过推理过程中最大序列长度的限制。为\n了解决这一问题，MouSi 模型提出了多补丁-单标记投影方法， 以按比例减少每个专家的输出标记\n数量。具体而言，由于图像信号具有局部性和稀疏性属性，用一个标记表示相邻的多个补丁是合\n理的。这种方法通过对局部视觉信息进行压缩，将多个补丁映射为单个标记，从而实现了多通道\n视觉信号的高效传输。通过多补丁-单标记投影，不仅有效降低了视觉信号传输的冗余，还减少了\n视觉大语言模型后续处理的计算成本，显著提高了推理效率，为多视觉专家的高效整合提供了切\n实可行的解决方案。\n尽管通过多补丁-单标记操作或在 Q-Former 中定义少量查询可以显著减少视觉标记的数量， 但",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "250 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\nLLaV AR[331] 针对多模态模型在理解图像中文本细节方面的不足，改进了视觉指令微调流程。\n通过引入包含大量文本的图像（如电影海报和书籍封面） ，该模型显著提升了文本细节处理能力。\n研究者使用 OCR 工具从 LAION 数据集中提取了 42.2 万张文本丰富的图像， 并结合GPT-4 生成了\n1.6 万条基于这些图像的对话数据， 每条数据包含多个问答对。 将这些新生成的数据与现有的多模\n态指令跟随数据结合后，LLaV AR显著改进了 LLaV A模型的能力， 在文本相关的视觉问答数据集\n上准确率提高了 20%，并在自然图像任务中取得了轻微提升。这表明，针对文本丰富图像的指令\n微调能够显著增强模型的文本理解和语义对齐能力。\n当前大多数多模态模型的输入图像分辨率限制在 224 × 224 像素，这是其视觉编码器架构的\n默认输入大小。这种限制导致模型在处理需要高分辨率和细节分析的文本任务时表现不佳。例如\n场景文本中心的 VQA（Scene Text-Centric VQA） 、 面向文档的VQA（Document-Or...",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "大语言模型：从理论到实践 245\n在推理过程中，视觉标记对位置编码的占用仍然是一个不可忽视的问题。事实上，视觉标记的长\n度通常比文本标记高出 500 倍以上，在具有位置感知的视觉语言模型中，这会消耗大量的位置嵌\n入资源。鉴于视觉专家本身已经包含位置编码信息，为每个视觉标记再次分配视觉大语言模型的\n位置嵌入显得冗余且低效。为了解决这一问题，MouSi 模型提出了一种二维可训练图像位置编码\n方法，通过直接在视觉标记中引入可训练的二维位置编码，避免了对视觉大语言模型位置嵌入的\n额外占用。这种方法不仅有效解决了多视觉专家导致的超长序列问题，还减少了位置编码的冗余\n分配，从而优化了视觉标记的处理效率，为多模态模型的可扩展性提供了重要支持。\n7.3 多模态大语言模型训练策略\n深度神经网络缩放法则（Scaling Law）为多模态大语言模型的训练策略提供了重要参考。以\n往业界普遍采用增加计算资源和模型规模的方式来提升性能，然而，根据文献 [304] 的研究成果，\n优化数据处理环节亦可带来突破性进展。以 CLIP 为例，其采用 4 亿张图像进行训练，开源版本\nOpenCLIP[305] 则需数百卡 G..."
      ],
      "response_time": 3.340190887451172,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.1945277033237522,
        "semantic_similarity": 0.8262566328048706
      }
    },
    {
      "sample_id": "0115",
      "question": "在直接投射方法中，如何将语音特征与文本模态结合以生成输出，并与S2T模式有何不同？",
      "ground_truth": "直接投射方法通过连接器将语音特征映射到大语言模型的文本模态嵌入空间，生成的语音嵌入向量与输入文本的嵌入向量拼接，形成一个融合语音和文本信息的新嵌入向量。而S2T模式则以语音作为输入，通常用于自动语音识别任务，不需要文本编码器，而是通过音频编码器提取特征并生成文本输出。",
      "predicted_answer": "在直接投射方法中，首先通过连接器将语音特征映射到大语言模型的文本模态嵌入空间[1]，然后经过编码器提取生成包含语音信息的特征张量，再通过投影器将该张量转化为与文本模态对齐的嵌入向量[1]。这些嵌入向量与输入文本的嵌入向量拼接，形成融合了语音和文本信息的新嵌入向量，并将其输入到大语言模型中进行处理[1]。\n\n相比之下，S2T (Speech to Text) 模式仅用于纯语音到文本的转换任务，不支持多模态处理[3]。而直接投射方法则可以在处理语音的同时，利用文本模态的信息，从而支持更广泛的多模态任务[1][3]。",
      "ground_context": [
        "直接投射方法通过连接器将语音特征映射到大语言模型的文本模态嵌入空间，生成的语音嵌入向量与输入文本的嵌入向量拼接，形成一个融合语音和文本信息的新嵌入向量。",
        "S2T 是最基础的模式，模型以语音作为输入，并生成对应的文本输出。这种模式通常用于自动语音识别（Automatic Speech Recognition，ASR）任务。"
      ],
      "retrieved_contexts": [
        "直接投射方法通过连接器将语音特征映射到大语言模型的文本模态嵌入空间[295, 296]， 如图7.7所\n示。语音特征经过编码器提取，生成包含语音信息的特征张量。该张量随后通过投射器转化为与\n文本模态对齐的嵌入向量。生成的语音嵌入向量与输入文本的嵌入向量拼接，形成一个融合语音\n和文本信息的新嵌入向量，并将其输入到大语言模型中进行处理。此外，一些研究者采用隐式投\n射方式，通过调整原始编码器的参数，在训练过程中直接完成语音到文本模态的映射，无需额外\n的连接器。",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "238 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\nS2T 模式实现简单，适用于纯语音到文本的转换任务，但是无法处理更复杂的多模态任务。\nST2T 是目前语音语言模型中最广泛采用的模式。 该模式支持同时输入语音和文本， 其中文本\n通常作为指令或任务提示。模型通过同时处理音频与文本模态的信息，融合两者的特征后生成最\n终的文本输出。这种模式不仅能够支持多任务学习，还能充分发挥大语言模型的强大能力，处理\n更广泛的任务，可以应用于语音翻译、语音情感分析等涉及音频和文本模态的任务。\nST2ST 是一种更高级的模式，模型在输入中结合语音和文本，并在输出中同时生成语音和文\n本。这种模式在解码阶段需要额外的语音合成模块（V ocoder）来生成语音输出。ST2ST 模式不仅\n能够完成基本的语音识别任务，还支持文本语音生成（Text-to-Speech，TTS） 、语音翻译及语音转\n换等复杂任务。\n2. 语音嵌入表示预训练\n语音嵌入表示预训练是一种通过在大规模语音数据上学习语音通用特征表示，进而提升下游\n语音任务性能的关键技术。近年来，基于不同模型架构的预训练方法逐渐成为研究热点，其中主\n...",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "240 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n……\n音频嵌入\n对齐的音频嵌入\n+文本编码器\n文本编码器\n文本输入\n音频输入\n音频嵌入空间\n转换\n大语言模型文本嵌入空间\n大语言模型输出\n…………\n文本嵌入\n对齐的音频嵌入\n文本嵌入\n连接\n……\n图 7.7 语音和文本表示融合架构直接投射方法 [285]\nToken 映射方法通过将语音特征转换为大语言模型可处理的文本 Token 实现模态转换[297]， 如\n图7.8所示。 具体而言， 语音特征经过投射器或转换器生成与文本Token 对应的表示， 这些符号随后\n与文本的 Token 序列结合，形成一个同时包含语音和文本信息的 Token 序列，并将其输入到 LLM\n中进行统一处理。 该方法不仅能够较好地保留语音特征信息， 还确保了LLM 在处理数据时的连续\n性和一致性。\n映射/转换\n文本词元切分文本输入\n音频词元\n+连接\n音频输入\n文本词元\n音频词元\n文本词元\n连接词元\n…………\n…………\n融合嵌入\n语言模型文本嵌入空间\n大语言模型输出\n…\n文本编码器\n…\n图 7.8 语音和文本表示融合架构 Token 映射方法 [28..."
      ],
      "response_time": 4.3073296546936035,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.18181818181818182,
        "rouge2": 0.0,
        "rougeL": 0.18181818181818182,
        "bleu": 0.2347694448061374,
        "semantic_similarity": 0.9166766405105591
      }
    },
    {
      "sample_id": "0116",
      "question": "思维链提示方式的目的是什么？",
      "ground_truth": "提升模型的推理能力。",
      "predicted_answer": "思维链提示方式的目的是引导模型在回答问题之前先输出推理过程，从而提升其在推理任务上的表现[1][2]。这种方法可以让模型不仅输出最终结果，还展示中间步骤，增强模型的推理能力[1][3]。",
      "ground_context": [
        "思维链（Chain-of-Thought，CoT）提示方式...使得模型不仅输出最终结果，还输出中间步骤，从而提升模型的推理能力。"
      ],
      "retrieved_contexts": [
        "276 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n个问题，Google Brain 的研究人员提出了思维链（Chain-of-Thought，CoT） 提示方式[395]， 除了将问\n题输入模型， 还将类似题目的解题思路和步骤输入模型， 使得模型不仅输出最终结果， 还输出中间\n步骤， 从而提升模型的推理能力。 研究人员甚至提出了零样本思维链（Zero-shot Chain-of-Thought，\nZero-shot CoT） 提示方式， 只需要简单地告知模型 “让我们一步一步思考 （Let’s think step by step） ”\n[397]，模型就能够自动输出中间步骤。\n思维链提示方式如图8.3 所示， 标准少样本提示 （Standard Few-shot Prompting） 技术在给模型\n的输入里面提供了 k 个 [问题，答案] 对，以及当前问题，由模型输出答案。而思维链提示在给模\n型的输入里面提供了 k 个 [问题，思维链，提示] 元组及当前问题，引导模型在回答问题之前先输\n出推理过程。 可以看到在标准少样本提示下， 模型通常直接给出答案， 但是由于缺少推...",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "对环境和任务有深刻理解， 从而生成并优化任务执行计划， 制定合理的行动步骤以实现既定目标。\n然而，仅仅通过扩大语言模型的规模，并不能显著提升推理（Reasoning）能力，如常识推理、逻\n辑推理、数学推理等。通过示例（Demonstration）或者明确指导模型在面对问题时如何逐步思考，\n促使模型在得出最终答案之前生成中间的推理步骤，可以显著提升其在推理任务上的表现。这种\n方法被称为思维链提示（Chain-of-Thought Prompting）[395]。同样地，面对复杂任务或问题时，大\n语言模型可以展现出良好的规划 （Planning） 能力。 通过引导模型首先将复杂的问题分解为多个较\n为简单的子问题，然后逐一解决这些子问题，可使模型得出最终答案，这种策略被称为由少至多\n提示[396]。 本节将重点介绍如何利用思维链提示和由少至多提示这两种方式， 提升大语言模型的推\n理规划能力。\n1. 思维链提示\n语言模型在推理能力方面的表现一直未能令人满意，一些研究人员认为这可能是因为此前的\n模式是直接让模型输出结果，而忽略了其中的思考过程。人类在解决包括数学应用题在内的、涉\n及多步推理的问题...",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "j ， 则构\n建提示输入 [Q : q(i)\nj , A : [ P ]]，其中 [P ] 是一个单一提示“让我们一步一步思考” 。将这个提示输\n入使用 Zero-Shot CoT[397] 的大语言模型中，得到由解释 r(i)\nj 和提取的答案 a(i)\nj 组成的推理链。最\n终得到范例 d(i)\nj = [Q : q(i)\nj , A : r(i)\nj ◦ a(i)\nj ]。如果 r(i)\nj 中的推理步骤小于 5 步，并且 q(i)\nj 中的词元\n小于 60 个，则将 d(i)\nj 纳入 d(i)。\n此外， 一些研究人员提出了对思维链提示的改进方法， 例如从训练样本中选取推理最复杂的样\n本 来 形 成 示 例 样 本， 被 称 为Complex-CoT[400]。 也 有 研 究 人 员 指 出 可 以 从 问 题 角 度 考 虑 优\n化思维链提示，通过将复杂的、模糊的、低质量的问题优化为模型更易理解的、高质量的问题，\n进一步提升思维链提示的性能，这一方法被称为 Self-Polish[401]。"
      ],
      "response_time": 1.5580408573150635,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.04117516541350685,
        "semantic_similarity": 0.6706714630126953
      }
    },
    {
      "sample_id": "0117",
      "question": "大模型智能体的核心能力包括哪些？",
      "ground_truth": "感知、规划、记忆以及工具使用",
      "predicted_answer": "大模型智能体的核心能力包括感知、规划、记忆以及工具使用[1]。这些能力使得大模型智能体能够弥补传统大模型无法与外部世界交互的局限性。",
      "ground_context": [
        "大模型智能体的核心能力涵盖了感知、规划、记忆以及工具使用，这些能力使其能够弥补传统大模型无法与外部世界交互的局限性。"
      ],
      "retrieved_contexts": [
        "知环境中的适应能力；而教程学习与示范学习的结合，则能够为模型理解复杂工具操作提供双重\n支持。 这种多策略融合不仅提升了模型的学习效率， 还为处理更复杂的多工具任务开辟了新路径。\n8.3 大模型智能体训练\n大模型智能体的核心能力涵盖了感知、规划、记忆以及工具使用，这些能力使其能够弥补传\n统大模型无法与外部世界交互的局限性。然而，大语言模型在最初的设计中并不具备这些核心能\n力。大语言模型主要依赖于大规模的文本数据训练，擅长语言生成和理解，但无法直接使用外部",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "8. 大模型智能体\n一直以来， 实现通用类人智能都是人类不懈追求的目标，智能体， 也是在该背景下提出的。 早\n期的智能体主要是基于强化学习实现的，不仅计算成本高，需要用大量的数据训练，而且难以实\n现知识迁移。随着大模型的发展，其在诸多领域展现出惊人的语义处理能力，能够快速生成文本、\n回答问题， 甚至完成一些复杂的知识推理任务。 研究人员开始思考如何将大模型与智能体结合， 从\n而突破大模型本身不具备与外部世界联系， 无法感知外部环境以及调用外部工具的问题。 同时， 智\n能体借助大模型强大的多模态理解与生成优势，可以快速处理信息、规划行动。智能体与大模型\n结合展现出了强大的能力，因此近年来大模型智能体受到了越来越多的关注并在很多应用领域取\n得了很好的实践结果。\n本章将重点介绍智能体的发展、大语言模型智能体架构，最后以 LangChain 为例介绍大语言\n模型智能体实践。\n8.1 智能体基础\n“智能体” （Agent）也称为智能代理，这一概念源远流长，其历史渊源可上溯至亚里士多德、\n休谟等先哲的相关论述。 从哲学维度剖析， “智能体”意指具备行动潜能的实体， 而“代理”一词，\n则侧重于对这种...",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "度学习的兴起，深度强化学习应运而生，它融合了强化学习与深度神经网络技术 [358, 359]，促使智\n能体具备从高维输入数据中学习复杂策略的能力，得以在未知环境里自如探索、自主学习，进而\n在从电子游戏竞技到机器人操控等诸多领域广泛渗透， 产生了AlphaGo[360]、DQN[361] 等一系列重\n要成果。但是，强化学习智能体依旧面临着训练周期冗长、采样效率欠佳以及稳定性不足等棘手\n难题，尤其在错综复杂的现实世界场景应用中，这些短板更加凸显[355]。\n2023 以来，大模型异军突起，其所展现出的惊人的能力引发广泛瞩目，基于大模型构建的智\n能体也日益备受瞩目[362–365]。 大模型智能体具有感知、 决策、 行动和记忆的能力， 通过感知模块捕\n获周围环境的信息，利用大模型进行推理和决策，通过执行器实施具体行动，同时还能存储和管\n理记忆， 以支持持续学习和适应动态环境[366]。 大模型智能体将大模型作为智能体的核心中枢， 即\n大脑或控制器的关键构成要素，同时借助多模态感知、工具运用等策略，全方位拓展智能体的感\n知范畴与行动边界。凭借思维链、问题分解等技术手段，大模型智能体得以彰显出可..."
      ],
      "response_time": 1.200521469116211,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.16636035585327164,
        "semantic_similarity": 0.5692927837371826
      }
    },
    {
      "sample_id": "0118",
      "question": "智能体的概念核心聚焦于什么？",
      "ground_truth": "个体的自主性",
      "predicted_answer": "智能体概念的核心聚焦于个体的自主性，即赋予其实现自主意志、进行抉择判断及付诸行动的能力，使其能够摆脱单纯被动回应外部刺激的模式。[2][4]",
      "ground_context": [
        "尤为关键的是，智能体概念的核心聚焦于个体的自主性，即赋予其运用意志、抉择判断以及付诸行动的能力，使之摆脱了单纯被动回应外部刺激的模式。"
      ],
      "retrieved_contexts": [
        "8 大模型智能体 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 261\n8.1 智能体基础 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 261\n\u0019\u000f\u0012\u000f\u0012ᅚ৥ൎ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ...",
        "则侧重于对这种行动潜能的施行与展现[337]。 智能体的范畴颇为广泛， 既涵盖人类个体， 亦囊括物\n理世界以及虚拟空间中的其他各类实体。尤为关键的是，智能体概念的核心聚焦于个体的自主性，\n即赋予其运用意志、抉择判断以及付诸行动的能力，使之摆脱了单纯被动回应外部刺激的模式。\n本节将从智能体的发展历史和大模型智能体应用范式角度介绍智能体发展的大体历程以及大\n模型智能体在实际应用中的具体范式。\n8.1.1 智能体发展历史\n自 20 世纪 80 年代中后期起， 人工智能研究人员开展了智能体相关研究[338–341]。 与此同时， 智\n能体的内涵也历经演变，与哲学意义上的智能体逐渐有所区别。就人工智能范畴而言，智能体本\n质上是一种计算实体[342, 343]。由于哲学范畴内容关于智能体的定义涉及意识、欲望等概念，这些",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "262 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n对于计算实体来说很难定义和度量 [344]，我们所能直接观测到的仅仅是计算实体的外在行为表现。\n因而，包括艾伦・图灵在内的诸多人工智能研究者提议，暂且搁置有关智能体是否“真正”在思\n考，又或者是否真正持有“思想”这类问题的探讨 [345]。研究人员转而采用诸如自主性、响应性、\n主动性以及社交性等其他特性， 用以辅助阐释智能体[342]。 从根本上来说， 人工智能领域的智能体\n与哲学意义层面的智能体并非同一概念，人工智能领域的智能体是智能体哲学概念于人工智能语\n境下的具象化呈现。\n自 20 世纪 90 年代开始，人工智能领域智能体研究开始更快速发展，从整体上看智能体技术\n的发展与人工智能发展紧密相关，可以粗略地划分为以下三个阶段：符号智能体、基于强化学习\n的智能体、以及基于大模型的智能体。\n在人工智能发展的早期阶段， 符号智能体扮演着关键角色， 主要关注转导、 表征和推理问题[346]。\n具体而言，转导问题侧重于将来自环境的低层次感知数据，诸如传感器读取的数据，转换为高层\n次的符号表示；表征和推理问题，则聚焦选择和设计适当...",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n..."
      ],
      "response_time": 1.2748727798461914,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 0.5
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.02889298856690844,
        "semantic_similarity": 0.6084879636764526
      }
    },
    {
      "sample_id": "0119",
      "question": "ToolLLM 提出了什么方法来提升开源大模型的工具使用能力？",
      "ground_truth": "ToolLLM 提出了通过构建 ToolBench 数据集，为3000 余种工具（涵盖16000多个 API）自动生成任务指令，并利用深度优先搜索算法自动化构建解决方案路径，从而对开源大模型进行微调，显著提升其基于教程学习的工具使用能力。",
      "predicted_answer": "ToolLLM 提出了一种通过构建 ToolBench 数据集并利用深度优先搜索算法自动化构建解决方案路径的方法，对开源大模型进行微调，以显著提升其基于教程学习的工具使用能力。此外，该方法还通过 API 检索器推荐最适合的 API，以进一步优化工具选择与操作过程。[3]",
      "ground_context": [
        "ToolLLM 提出了通过构建 ToolBench 数据集，为3000 余种工具（涵盖16000多个 API）自动生成任务指令，并利用深度优先搜索算法自动化构建解决方案路径，从而对开源大模型进行微调，显著提升其基于教程学习的工具使用能力。"
      ],
      "retrieved_contexts": [
        "大语言模型：从理论到实践 271\n成教程学习任务，现有的开源大模型却因其上下文理解能力的不足，难以通过教程学习有效掌握\n工具使用技能。\n针对这一问题，ToolLLM[392] 提出了通过构建 ToolBench 数据集， 为3000 余种工具 （涵盖16000\n多个 API）自动生成任务指令，并利用深度优先搜索算法自动化构建解决方案路径，从而对开源\n大模型进行微调，显著提升其基于教程学习的工具使用能力。此外，该方法还通过 API 检索器推\n荐最适合的 API，以进一步优化工具选择与操作过程，成功解决了开源大模型在依赖工具手册提\n示语句进行学习时效果受限的问题。\n教程学习的显著优势在于其系统性与全面性。大模型能够通过详细的文档深入学习工具的功\n能与操作方法，从而赋予智能体更为全面且强大的工具使用能力。这种学习方式不仅能够帮助智\n能体高效掌握工具，还为其在复杂任务场景中的灵活应用奠定了坚实基础。\n3. 探索学习\n探索学习 （Exploratory Learning） 是一种通过自主尝试与实验来掌握工具使用的方法。 在这一\n过程中，智能体通过自主探索和反复试验，逐步学习工具的操作技巧及其最佳...",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "272 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n工具，也不能很好对任务进行多步骤的规划。同时大语言模型构建之初也没有考虑记忆和使用用\n户全部对话历史。为了弥补这些不足，研究者们开始系统地研究如何提升大语言模型解决上述问\n题的能力。本节将重点介绍大语言模型工具使用能力提升、推理规划能力提升以及长期记忆构建\n与应用的策略方法。\n8.3.1 工具学习\n大模型工具学习 （Tool Learning） 是指通过让大语言模型学会使用各种工具的调用方式， 进而\n利用合适的工具去实现特定的功能需求。 例如， 用户输入“请告诉我上海今天的天气。 ”具备工具\n使用能力的大语言模型会给出如下响应：\n1. ്љ಩ༀো྘ğ ฿గҰ࿘಩ༀb\n2.Ⴈ฿గ APIğႨ฿గ APIༀ APIčೂ WeatherMapෂҰ࿘ҕඔĎ b\nresponse = requests.get(\"https://api.weathermap/data/2.5/weather\",\nparams={\n\"q\": \"Shanghai\",\n\"date\": \"2025-1-6\",\n\"appid\": \"your_api_key\"...",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "型的参数量巨大，在不同任务上都进行微调需要消耗大量的计算资源，因此预训练微调范式不再\n适用于大语言模型。 研究人员发现， 通过语境学习（In-Context Learning，ICL） 等方法， 直接使用\n大语言模型，就可以在很多任务的少样本场景中取得很好的效果。此后，研究人员提出了面向大\n语言模型的提示词（Prompt）学习方法，以及模型即服务范式（Model as a Service ，MaaS） 、指令\n微调（Instruction Tuning）等方法， 在不同任务中都取得了很好的效果。 与此同时，Google、Meta、\nBigScience、 百度、 华为等公司和研究机构纷纷发布了PaLM[14]、LaMDA[15]、T0[16] 等不同大语言\n模型。2022 年年底 ChatGPT 的出现， 将大语言模型的能力进行了充分的展现， 也引发了大语言模\n型研究的热潮。\nKaplan 等人在文献 [17] 中提出了缩放法则（Scaling Laws），指出模型的性能依赖于模型的规\n模， 包括参数量、 数据集大小和计算量， 模型的效果会随着三者的指数增加而平稳提升。 如图1.1 所..."
      ],
      "response_time": 2.085808753967285,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.6,
        "rouge2": 0.25,
        "rougeL": 0.6,
        "bleu": 0.43156216058044783,
        "semantic_similarity": 0.9274711608886719
      }
    },
    {
      "sample_id": "0120",
      "question": "感知模块负责从环境中获取哪些形式的信息？",
      "ground_truth": "文本、视觉、听觉等多种形式的信息",
      "predicted_answer": "感知模块负责从环境中获取文本、视觉、听觉等多种形式的信息[3]。",
      "ground_context": [
        "感知模块负责从环境中获取文本、 视觉、 听觉等多种形式的信息, 并将其传递给其他模块进行处理。"
      ],
      "retrieved_contexts": [
        "视觉感知方面的性能。\n在音频感知方面，声音信息是外界环境中不可或缺的重要组成部分，为大模型智能体赋予听\n觉感知能力， 能够显著增强其对交互内容、 环境状况乃至潜在危险的感知能力。 目前， 已有多种针\n对音频处理的模型和方法被开发， 但这些模型通常仅在特定任务中表现优异[288, 387, 388]。 鉴于大模\n型智能体在工具使用方面的强大能力，研究人员提出了一种直观的方案，即通过将大模型作为控\n制中心，级联调用现有的工具集或模型库以感知音频信息，从而实现多模态感知的高效融合。然\n而，与视觉感知类似，这种通过外部模型进行听觉感知的方法仍存在信息丢失的隐患。因此，如\n何将听觉感知能力直接融入大模型体系，成为当前亟待解决的重要研究课题。\n此外，感知模块的发展还应涵盖其他潜在的输入形式，如触觉、嗅觉等，以进一步拓展大模\n型智能体的感知能力。未来的智能体可能具备更加丰富的感官系统，能够像人类一样感知并理解\n多样化的现实世界信息。例如，通过配备特定的触觉和嗅觉器官，智能体可以在与物体交互时获\n取更为详尽的信息；同时，其还能够对环境中的温度、湿度、光照强度等要素进行精准感知，从\n而实现适应性行动。总...",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "266 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n观察天空，你觉得明天会下⾬吗？如果会下⾬的话，请给我⼀把⾬伞。\n环境\n感知模块\n从⽬前的天⽓情况和⽹上的天⽓预报来看，明天很可能会下⾬。这是你要的⾬伞。\n推理规划\n记忆模块\n⻓期记忆\n短期记忆\n输⼊\n智能体\n调⽤API …\n⼯具使⽤模块示范学习教程学习探索学习\n规划模块制定决策\n图 8.2 智能体框架结构 [370]\n8.2.1 感知模块\n感知模块负责从环境中获取文本、 视觉、 听觉等多种形式的信息, 并将其传递给其他模块进行\n处理。多模态感知能力对于大模型智能体的发展至关重要。通过整合这些多样化的输入，智能体\n能够深入理解其所处的环境，做出更明智的决策，在复杂多变的任务中发挥出色表现。赋予大模\n型智能体多模态感知能力已成为一个重要的研究方向，除了常见的输入形式之外，触觉反馈、手\n势以及 3D 雷达等其他潜在输入也可以丰富智能体的感知范围，使其在复杂环境中保持灵活、全\n面的感知能力。\n文本作为人类与世界交互的核心载体， 在大模型智能体的发展中扮演着重要角色。 同时， 文本作\n为承载数据、 信息和知识的主要媒介， 也是人机...",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "而实现适应性行动。总体而言，感知模块的多模态扩展不仅能够帮助智能体更全面地理解并适应\n外部环境，还将显著提升其在复杂任务中的执行能力。未来研究的核心将聚焦于赋予大模型更强\n的多模态理解能力，以进一步增强其感知与决策水平。这一领域的突破将为大模型智能体的全面\n发展奠定重要基础。\n8.2.2 规划模块\n规划模块是大模型智能体的核心，其主要职责是通过对环境与任务的深刻理解，生成并优化\n任务执行计划，制定合理的行动步骤以实现既定目标。研究表明，大模型的推理与规划能力随着\n模型参数规模和训练数据量的增加呈现出显著的阶跃式提升 [62]。尤其是在模型参数量达到数百\n亿级别时，即使缺乏直接与任务相关的数据，大模型也能够通过在输入提示中加入包含任务中间\n推理步骤的示例，或通过引导模型逐步输出推理过程，逐步构建任务的解决方案。将大模型作为\n规划模块的核心，充分发挥其强大的推理能力和丰富的知识库，可以在复杂且动态的环境中实现"
      ],
      "response_time": 0.7520277500152588,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.401577332834242,
        "semantic_similarity": 0.7377021312713623
      }
    },
    {
      "sample_id": "0121",
      "question": "无反馈规划指的是什么？",
      "ground_truth": "无反馈规划（Planning without feedback）指在规划阶段一次性生成完整的任务和子任务拆分计划，并严格按照该计划逐步执行，而不根据外界变化进行实时调整。",
      "predicted_answer": "无反馈规划是指在规划阶段一次性生成完整的任务和子任务拆分计划，并严格按照该计划逐步执行，而不根据外界变化进行实时调整。在这种模式下，大模型智能体会在任务开始前，根据当前环境和任务要求生成一个完整的执行方案，并在执行过程中始终遵循初始计划。无反馈规划的主要优势在于其执行效率较高，适用于环境相对稳定、变化较少的任务场景。[1][3]",
      "ground_context": [
        "无反馈规划（Planning without feedback）指在规划阶段一次性生成完整的任务和子任务拆分计划，并严格按照该计划逐步执行，而不根据外界变化进行实时调整。"
      ],
      "retrieved_contexts": [
        "268 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n快速决策，并灵活应对各种变化。目前，这一领域的研究主要集中于无反馈规划与带反馈规划两\n大方向，为探索大模型在规划能力上的潜力提供了重要的研究路径。\n1. 无反馈规划\n无反馈规划（Planning without feedback ） 指在规划阶段一次性生成完整的任务和子任务拆分\n计划，并严格按照该计划逐步执行，而不根据外界变化进行实时调整。在这种模式下，大模型智\n能体会在任务开始前，根据当前环境和任务要求生成一个完整的执行方案，并在执行过程中始终\n遵循初始计划。无反馈规划的主要优势在于其执行效率较高，适用于环境相对稳定、变化较少的\n任务场景。 例如， 在文档生成任务中， 智能体可以根据预先设定的主题、 段落结构和内容要求， 生\n成包含所有预定义内容的完整文章，并在生成过程中不因外部反馈而修改文章内容。目前，无反\n馈规划的典型方法是将思维链推理技术扩展至智能体领域[389]。 在这种方法中， 大模型智能体能够\n利用思维链推理技术预先生成完成任务所需的所有子任务拆分计划，并为每个子任务设计相应的\n执行动作，以便在真实环境中逐...",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "在实际应用中，通常将无反馈规划与带反馈规划相结合，以兼顾效率与灵活性。例如，在自\n主配送系统中，可以首先利用无反馈规划生成初步的配送路线，并在实际执行过程中通过带反馈\n规划进行实时调整，以应对突发情况和动态变化。通过融合无反馈规划的高效性与带反馈规划的\n适应性，规划模块赋予了大模型自主智能体灵活且高效的决策能力，使其能够在多样化的任务环\n境中表现出色，从而完成复杂的任务目标。",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "际环境中面临执行困难或效果不佳的问题，特别是在忽略外部数据变化的情况下，智能体可能无\n法有效应对突发事件或异常情况。\n2. 带反馈规划\n带反馈规划（Planning with feedback）是一种更为复杂且灵活的规划方式， 智能体在执行任务\n的过程中能够持续获取环境反馈或监控环境变化，并基于反馈信息动态调整行动计划。在这一模\n式下，智能体不仅会在任务开始前制定初步的执行计划，还能够在任务执行过程中实时监测环境\n变化和任务进展，依据实际情况不断优化和修正计划。带反馈规划强调智能体与环境的交互，通\n过不断更新计划以确保任务的顺利完成。其显著优势在于高度的适应性与灵活性，尤其适用于环\n境复杂且变化频繁的任务场景。ReAct[390] 方法是大模型智能体带反馈规划方法的经典方法， 其核\n心在于将任务执行过程与推理规划过程相结合。在任务执行的每一步中，大模型智能体依据已完\n成的子任务和获得的环境反馈，动态生成当前步骤的子任务及相应的执行动作，并将其在真实环\n境中执行。完成后，环境反馈会被传递回智能体，用于下一步的任务规划。通过这一反复循环的\n过程，ReAct 方法使大模型智能体能够根据环境反..."
      ],
      "response_time": 2.3580174446105957,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0.0,
        "bleu": 0.28487934394816555,
        "semantic_similarity": 0.8862783312797546
      }
    },
    {
      "sample_id": "0122",
      "question": "大语言模型在视觉感知领域的表现如何？",
      "ground_truth": "尽管大语言模型在理解和处理多轮对话方面展现了卓越的性能，但仍然无法处理视觉模态信息。",
      "predicted_answer": "尽管大语言模型在理解和处理多轮对话方面表现出色[65]，但在视觉模态信息处理上仍显不足。将视觉输入转化为文本描述虽能提高模型的可解释性和节省计算资源，但也可能导致信息损失[255]。因此，研究人员尝试结合视觉编码器和大语言模型，通过加入可学习的接口层来改善大语言模型对视觉信息的感知能力[387]。这样不仅能减轻模型的学习负担，还能显著提升其在视觉感知方面的性能。[1][2][3]",
      "ground_context": [
        "尽管大语言模型在理解和处理多轮对话方面展现了卓越的性能，但仍然无法处理视觉模态信息。"
      ],
      "retrieved_contexts": [
        "大语言模型：从理论到实践 267\n陌生场景下，提升智能体的文本感知能力显得尤为重要。\n在视觉感知领域，尽管大语言模型在理解和处理多轮对话方面展现了卓越的性能[65]，但仍然\n无法处理视觉模态信息。 视觉输入通常包含丰富的环境信息， 例如物体的属性、 空间关系以及场景\n布局。将视觉信息与其他模态数据相结合，能够使智能体对外部环境的理解更加全面且精准 [255]。\n为了赋予智能体理解视觉信息的能力，一种直接的方法是将视觉输入通过图像描述生成技术转换\n为对应的文本描述[386]。 这种方法的优点在于其高度的可解释性， 并且无需为生成描述进行额外训\n练，从而显著节约计算资源。然而，此方法在转换过程中可能会丢失大量潜在信息，导致视觉信\n息的表达不完整。为解决上述问题，研究人员尝试将大语言模型与视觉编码器相结合，并通过增\n加一个可学习的接口层来对齐视觉编码与大模型的语言理解能力，从而增强大模型对视觉信息的\n感知能力[387]。 这一方法有效降低了大模型在学习视觉语言对齐任务中的负担， 并显著提升了其在\n视觉感知方面的性能。\n在音频感知方面，声音信息是外界环境中不可或缺的重要组成部分，为大模型智能体...",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "大语言模型：从理论到实践 233\n7.2 大语言模型与多模态融合架构\n近年来， 随着基于Transformer 架构的算法取得了显著进展， 视觉语言模型、 音频语言模型都\n有了很大的发展， 模型架构也多种多样， 包括双编码器架构、 融合架构和编码器-解码器架构等。 这\n些架构不断演化并结合新的技术，例如混合模态注意力机制、对比学习、强化学习等，进一步提\n升了模型的性能和适应能力。\n本节将围绕多模态大语言模型的架构展开介绍，分别探讨视觉语言模型架构、语音语言模型\n架构以及多模态大语言模型架构。\n7.2.1 视觉语言模型架构\n视觉语言模型（Vision-Language Models，VLM）是一类旨在结合计算机视觉与自然语言处理\n能力的模型， 近年来借助基于Transformer 的技术取得了显著进展。 这些模型的训练方法可以分为\n四种主要范式： 对比学习、 掩码预测、 生成式学习以及映射学习。 对比学习通过正负样本对的表示\n相似性与差异性训练模型；掩码预测则通过遮掩图像或文本的部分信息，训练模型进行重建；生\n成式视觉语言模型则专注于生成图像或文本，但因其复杂性通常需要更高的计算资源；映...",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "大语言模型：从理论到实践 245\n在推理过程中，视觉标记对位置编码的占用仍然是一个不可忽视的问题。事实上，视觉标记的长\n度通常比文本标记高出 500 倍以上，在具有位置感知的视觉语言模型中，这会消耗大量的位置嵌\n入资源。鉴于视觉专家本身已经包含位置编码信息，为每个视觉标记再次分配视觉大语言模型的\n位置嵌入显得冗余且低效。为了解决这一问题，MouSi 模型提出了一种二维可训练图像位置编码\n方法，通过直接在视觉标记中引入可训练的二维位置编码，避免了对视觉大语言模型位置嵌入的\n额外占用。这种方法不仅有效解决了多视觉专家导致的超长序列问题，还减少了位置编码的冗余\n分配，从而优化了视觉标记的处理效率，为多模态模型的可扩展性提供了重要支持。\n7.3 多模态大语言模型训练策略\n深度神经网络缩放法则（Scaling Law）为多模态大语言模型的训练策略提供了重要参考。以\n往业界普遍采用增加计算资源和模型规模的方式来提升性能，然而，根据文献 [304] 的研究成果，\n优化数据处理环节亦可带来突破性进展。以 CLIP 为例，其采用 4 亿张图像进行训练，开源版本\nOpenCLIP[305] 则需数百卡 G..."
      ],
      "response_time": 3.016590118408203,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.1008608401270007,
        "semantic_similarity": 0.7839244604110718
      }
    },
    {
      "sample_id": "0123",
      "question": "最终构建了多少条轨迹？",
      "ground_truth": "1,866 条轨迹。",
      "predicted_answer": "AgentInstruct 数据集包含 1,866 条经过严格验证的交互轨迹。[3]",
      "ground_context": [
        "最终构建了 1,866 条轨迹。"
      ],
      "retrieved_contexts": [
        "大语言模型：从理论到实践 281\n轨迹交互任务。在评估方法上，采用了 1-shot 评估策略，主要是为了满足智能体任务中对输出格\n式精确性的严格要求。对于每个任务，均提供来自训练集的完整交互过程作为示例。轨迹交互过\n程主要包括两个阶段。首先，向模型提供任务描述及一个成功的 1-shot 示例，以帮助其理解任务\n要求。随后进入正式交互阶段，向模型输入当前指令和必要的上下文信息。模型基于这些信息及\n此前的反馈内容，生成“思考” （Thought）并采取相应的行动。环境则根据模型的操作提供反馈，\n反馈内容可能包括状态变化或新的信息。 上述过程循环进行， 直至模型完成任务目标或达到Token\n限制。 若模型连续三次生成相同的输出， 则被视为重复性失败。 若模型输出的格式不符合要求， 则\n通过 BLEU 指标将其与所有可能的操作选项进行比较，并选择最接近的选项作为该步骤的操作。\n在涉及真实场景的智能体任务中，由于任务复杂性较高，即便是 GPT-4 在此类任务上的表现\n也未能达到预期。为了确保数据质量， AgentInstruct 数据集构造过程中还对其交互轨迹进行了严\n格的过滤。每条交互轨迹都会...",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "280 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n图 8.6 AgentTuning 方法框架 [402]\nAgentInstruct 数据集包含 1,866 条经过严格验证的交互轨迹。这些轨迹包含高质量的逐步推\n理过程（即 Chain-of-Thought） ，并涉及六种不同的智能体任务，包括AlfWorld[403]、WebShop[393]、\nMind2Web[404]、 知识图谱、 操作系统和数据库。 对于每个智能体任务，AgentInstruct 的构建包括三\n个主要阶段：指令构造、轨迹交互以及轨迹过滤。\n对于 AlfWorld、WebShop、Mind2Web 以及知识图谱等已有训练集的任务，AgentInstruct 直接\n利用其训练数据，依次完成轨迹交互和轨迹过滤两个阶段。对于缺乏训练集的任务（如操作系统\n和数据库） ， 则采用任务推导 （Task Derivation） 和自指令生成 （Self-Instruct）[405] 的方法构建相应\n的指令，以确保数据的完整性与多样性。\n在数据库任务的指令构建过程中， 使用了BIRD[406] 数据集作为基础，...",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "大语言模型：从理论到实践 273\n（3）解决轨迹标注：为每条生成的指令，通过 ChatGPT 的函数调用功能标注有效的解决路径\n（即多步 API 调用序列） 。使用深度优先搜索决策树（DFSDT）扩展搜索空间，允许模型探索多个\n推理路径， 并在必要时放弃当前路径以扩展新节点。 相比传统方法，DFSDT 有效解决了推理错误\n传播和探索不足的问题，最终生成了 126,486 条高质量的指令-解决路径对，为模型训练提供了丰\n富的数据支持。\nToolLLaMA-2-7B[392] 是通过使用上述方法构建的包含 12.6 万条数据的大规模数据集， 在LLaMA-\n2 模型上进行的有监督微调。然而，这种基于大规模数据集的微调方法往往忽略了工具使用中的\n任务特定特征， 从而导致模型性能的瓶颈。 即使经过如此大量的数据训练，ToolLLaMA-2-7B 的工\n具调用效果也仅能达到 GPT-4 的 80% 左右。\n文献 [394] 指出， 当前用于工具学习的数据集大多通过GPT-4 等模型自动构建， 数据中存在不\n小比例的错误。 例如，RoTLLaMA 的训练集包含 12,247 条由 GPT-4 生成..."
      ],
      "response_time": 0.8567652702331543,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.6666666666666666,
        "rouge2": 0.5,
        "rougeL": 0.6666666666666666,
        "bleu": 0.15989214998944032,
        "semantic_similarity": 0.7214756011962891
      }
    },
    {
      "sample_id": "0124",
      "question": "MemoryBank 的核心组件之一是什么？",
      "ground_truth": "记忆存储（Memory Stroage）是 MemoryBank 的核心组件之一，存储了丰富的信息，包含日常对话记录、过去的事件总结和用户个性评估的演变， 从而构建了一个动态的多层次记忆全景图。",
      "predicted_answer": "MemoryBank 的核心组件之一是记忆存储（Memory Stroage），它存储了丰富的信息，包括日常对话记录、过去的事件总结和用户个性评估的演变。[1][3]",
      "ground_context": [
        "记忆存储（Memory Stroage）是 MemoryBank 的核心组件之一，存储了丰富的信息，包含日常对话记录、 过去的事件总结和用户个性评估的演变， 从而构建了一个动态的多层次记忆全景图。"
      ],
      "retrieved_contexts": [
        "记忆增强提示词\n记忆检索\nSiliconFriend事件摘要用户画像相关记忆\n元提示词\n历史\n检索词\nTomorrow is my GF s’birthdayYou should prepare gifts......SendDo you remember the gifts she like? \n图 8.7 MemoryBank 方法框架 [407]\n记忆存储（Memory Stroage）是 MemoryBank 的核心组件之一，存储了丰富的信息，包含日\n常对话记录、 过去的事件总结和用户个性评估的演变， 从而构建了一个动态的多层次记忆全景图。\n通过按时间顺序记录多轮对话并添加时间戳，构建了有序的交互历史。这种细致的记录不仅支持\n精确的记忆检索，还为后续记忆更新提供了详细索引。\nMemoryBank 借鉴了人类记忆的复杂性， 不仅简单存储， 还对对话进行提炼， 生成每日事件总\n结，并进一步凝练为全局总结，形成层次化的记忆结构，为用户交互和重要事件提供鸟瞰式视角。\n具体来说，以之前的每日对话或每日事件为输入，要求大语言模型使用提示“ଽಸ [߅ؓ/൙\nࡱ]ྐ༏” 来总结每日事件或全局事...",
        "ࡱ]ྐ༏” 来总结每日事件或全局事件。 此外， 还专注于用户个性理解， 通过长期\n交互不断评估和更新个性洞察， 最终形成对用户个性的全局理解。 这种多层次方法使AI 伴侣能够\n学习、适应并根据用户独特特质定制响应，从而显著提升用户体验。以每日对话或个性分析为输\n入，要求大语言模型使用提示： “౦࿂b”或“ၛ༯൞၂\nb”来分析\n用户。\nMemoryBank 所采用的记忆检索机制类似于知识检索任务，具体实现上采用了一种类似稠密\n篇章检索 （Dense Passage Retrieval, DPR） 的双塔稠密检索模型[408]。 每次对话及其对应的总结均被\n视为一个记忆片段 m， 并通过编码器模型E(·) 进行预编码， 生成该片段的上下文向量化表示hm。\n整个记忆存储 M 被预编码为 M = {h0\nm, h1\nm, . . . , h|M |\nm }。随后，这些向量表示通过 FAISS 方法[409]\n进行索引， 以实现高效的检索操作。 在实际检索过程中， 当前对话的上下文c 同样通过编码器 E(·)",
        "式，例如文本文件或结构化数据库，并通过检索机制与反思机制进行访问与更新。外部记忆库可\n以采用向量数据库或可读写的神经网络记忆库等模式，模型能够动态地获取或更新所需知识。其\n中，检索增强生成（Retrieval-Augmented Generation, RAG ）是一种典型方法，将检索与生成有机\n结合，适用于知识动态变化的场景。然而，该方法在应用中仍面临检索效率和记忆库质量的挑战，\n这对系统性能具有重要影响。\n文献 [407] 提出了 MemoryBank 方法， 允许模型调用相关记忆， 通过持续的记忆更新不断进化，\n通过综合之前交互的信息，随着时间的推移理解和适应用户个性。MemoryBank 框架如图8.7所示，\n它由记忆存储、记忆检索以及记忆更新模块组成，每次用户输入的提示词会与记忆模块检索结果\n一起构成记忆增强的提示词。记忆存储作为主要的数据存储库，保存了对话的详细记录、事件总\n结和用户个性评估。记忆检索允许根据上下文进行记忆回忆。记忆更新受到艾宾浩斯遗忘曲线理\n论（Ebbinghaus Forgetting Curve Theory ）的启发，改理论认为遗忘在学习之后立即开始...",
        "282 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n忘的进程并不是均匀的，最初遗忘速度很快，以后逐渐缓慢。根据时间的流逝，帮助AI 记住、有\n选择地忘记和加强记忆。MemoryBank 具有较好灵活性，可以适应开源和闭源的大语言模型，支\n持中英双语，并且可以与遗忘机制一起使用。\nMemoryBankPast ConversationsConversations on date 04-28:\nConversations on date 04-29:.........\nEvent SummeryBook and gifts recommendationExperience of visiting parksImproving drawing skills\nUser Portraitopen-minded, curious, and receptive to advice \n记忆存储\n记忆更新Memory Strength UpdatingEbbinghausForgetting Curve\n记忆增强提示词\n记忆检索\nSiliconFriend事件摘要用户画像相关记忆\n元提示词...",
        "大语言模型：从理论到实践 269\n8.2.3 记忆模块\n记忆模块是在大模型智能体中承担着管理与操作智能体记忆的核心功能，包括对长短期记忆\n的存储、读取、处理以及反思等任务。该模块不仅负责存储历史数据与经验，还能够高效提取和\n更新信息，从而实现长期记忆与短期记忆之间的有机交互。通过记忆模块的支持，智能体在处理\n连续性任务时能够保持上下文的连贯性，并基于以往经验做出更加准确的判断与决策。\n1. 记忆模型\n大模型智能体所采用的记忆模型包括长期记忆和短期记忆两部分。它们各自有不同的功能和\n实现方式，但都依赖于大模型的强大计算和理解能力。\n短期记忆通常通过将记忆内容以提示语句的形式嵌入大模型输入的上下文中，借助大模型的\n上下文理解能力来实现。 包括存储和使用两个部分： （1） 存储： 在任务执行过程中， 关键的上下文\n信息与事件会被实时记录，形成短期记忆内容； （2）使用：在后续任务中，这些记忆内容会作为\n提示语句输入至大模型的上下文中，帮助模型基于提示进行推理与决策。例如，将前几步的操作\n结果及重要的环境信息作为输入内容，支持模型在接下来的步骤中做出更加合理的判断与选择。\n长期记忆通过构建记忆..."
      ],
      "response_time": 1.257094383239746,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.7499999999999999,
        "rouge2": 0.3333333333333333,
        "rougeL": 0.5,
        "bleu": 0.5787893865939253,
        "semantic_similarity": 0.9094467163085938
      }
    },
    {
      "sample_id": "0125",
      "question": "大模型智能体是如何解决早期基于强化学习的智能体面临的哪些问题的？",
      "ground_truth": "大模型智能体通过融合大模型的强大多模态理解与生成能力，突破了早期智能体在知识迁移、计算成本高和大量数据训练等方面的限制，同时也解决了大模型本身无法感知外部环境和调用外部工具的问题。",
      "predicted_answer": "大模型智能体解决了早期基于强化学习的智能体面临的一些关键问题，包括计算成本高[5]、需要大量数据训练[5]以及难以实现知识迁移[5]。此外，大模型智能体还克服了传统大模型无法直接与外部世界交互、无法感知外部环境以及调用外部工具的限制[5]。",
      "ground_context": [
        "大模型智能体具有感知、决策、行动和记忆的能力，...以支持持续学习和适应动态环境。",
        "随着大模型的发展，其在诸多领域展现出惊人的语义处理能力，能够快速生成文本、回答问题，甚至完成一些复杂的知识推理任务。"
      ],
      "retrieved_contexts": [
        "度学习的兴起，深度强化学习应运而生，它融合了强化学习与深度神经网络技术 [358, 359]，促使智\n能体具备从高维输入数据中学习复杂策略的能力，得以在未知环境里自如探索、自主学习，进而\n在从电子游戏竞技到机器人操控等诸多领域广泛渗透， 产生了AlphaGo[360]、DQN[361] 等一系列重\n要成果。但是，强化学习智能体依旧面临着训练周期冗长、采样效率欠佳以及稳定性不足等棘手\n难题，尤其在错综复杂的现实世界场景应用中，这些短板更加凸显[355]。\n2023 以来，大模型异军突起，其所展现出的惊人的能力引发广泛瞩目，基于大模型构建的智\n能体也日益备受瞩目[362–365]。 大模型智能体具有感知、 决策、 行动和记忆的能力， 通过感知模块捕\n获周围环境的信息，利用大模型进行推理和决策，通过执行器实施具体行动，同时还能存储和管\n理记忆， 以支持持续学习和适应动态环境[366]。 大模型智能体将大模型作为智能体的核心中枢， 即\n大脑或控制器的关键构成要素，同时借助多模态感知、工具运用等策略，全方位拓展智能体的感\n知范畴与行动边界。凭借思维链、问题分解等技术手段，大模型智能体得以彰显出可...",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "知环境中的适应能力；而教程学习与示范学习的结合，则能够为模型理解复杂工具操作提供双重\n支持。 这种多策略融合不仅提升了模型的学习效率， 还为处理更复杂的多工具任务开辟了新路径。\n8.3 大模型智能体训练\n大模型智能体的核心能力涵盖了感知、规划、记忆以及工具使用，这些能力使其能够弥补传\n统大模型无法与外部世界交互的局限性。然而，大语言模型在最初的设计中并不具备这些核心能\n力。大语言模型主要依赖于大规模的文本数据训练，擅长语言生成和理解，但无法直接使用外部",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "8. 大模型智能体\n一直以来， 实现通用类人智能都是人类不懈追求的目标，智能体， 也是在该背景下提出的。 早\n期的智能体主要是基于强化学习实现的，不仅计算成本高，需要用大量的数据训练，而且难以实\n现知识迁移。随着大模型的发展，其在诸多领域展现出惊人的语义处理能力，能够快速生成文本、\n回答问题， 甚至完成一些复杂的知识推理任务。 研究人员开始思考如何将大模型与智能体结合， 从\n而突破大模型本身不具备与外部世界联系， 无法感知外部环境以及调用外部工具的问题。 同时， 智\n能体借助大模型强大的多模态理解与生成优势，可以快速处理信息、规划行动。智能体与大模型\n结合展现出了强大的能力，因此近年来大模型智能体受到了越来越多的关注并在很多应用领域取\n得了很好的实践结果。\n本章将重点介绍智能体的发展、大语言模型智能体架构，最后以 LangChain 为例介绍大语言\n模型智能体实践。\n8.1 智能体基础\n“智能体” （Agent）也称为智能代理，这一概念源远流长，其历史渊源可上溯至亚里士多德、\n休谟等先哲的相关论述。 从哲学维度剖析， “智能体”意指具备行动潜能的实体， 而“代理”一词，\n则侧重于对这种..."
      ],
      "response_time": 2.039973735809326,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.1304037435309018,
        "semantic_similarity": 0.8860576152801514
      }
    },
    {
      "sample_id": "0126",
      "question": "如何结合视觉信息和思维链提示来提升大语言模型在多模态感知中的表现？",
      "ground_truth": "结合视觉信息和思维链提示可以显著提升大语言模型在多模态感知中的表现。通过将视觉输入转化为文本描述，能够为智能体提供更全面的环境理解，而思维链提示则能提升推理过程的准确性和可解释性。具体来说，研究人员通过将视觉编码与语言理解对齐的方法增强了模型对视觉信息的感知能力，同时利用思维链提示降低推理难度，提高最终结果的准确率。",
      "predicted_answer": "为了提升大语言模型在多模态感知中的表现，可以通过结合视觉信息和思维链提示来进行[2][5]。具体而言，在视觉感知领域，可以采用将视觉输入转化为文本描述的方法[386]，并通过增加一个可学习的接口层来增强大模型对视觉信息的理解[255]。同时，使用思维链提示，即展示模型逐步推理的过程，可以帮助模型更好地理解和处理涉及视觉信息的任务[395]。这样，模型不仅能够理解视觉信息，还能清晰地展示其推理过程，从而提高整体的表现。",
      "ground_context": [
        "将视觉信息与其他模态数据相结合，能够使智能体对外部环境的理解更加全面且精准。",
        "通过实验发现，使用由不同人员编写的符号推理示例在准确率上存在高达 28.2% 的差异，而改变范例的顺序在大多数任务中则只产生了不到 2% 的变化。"
      ],
      "retrieved_contexts": [
        "对环境和任务有深刻理解， 从而生成并优化任务执行计划， 制定合理的行动步骤以实现既定目标。\n然而，仅仅通过扩大语言模型的规模，并不能显著提升推理（Reasoning）能力，如常识推理、逻\n辑推理、数学推理等。通过示例（Demonstration）或者明确指导模型在面对问题时如何逐步思考，\n促使模型在得出最终答案之前生成中间的推理步骤，可以显著提升其在推理任务上的表现。这种\n方法被称为思维链提示（Chain-of-Thought Prompting）[395]。同样地，面对复杂任务或问题时，大\n语言模型可以展现出良好的规划 （Planning） 能力。 通过引导模型首先将复杂的问题分解为多个较\n为简单的子问题，然后逐一解决这些子问题，可使模型得出最终答案，这种策略被称为由少至多\n提示[396]。 本节将重点介绍如何利用思维链提示和由少至多提示这两种方式， 提升大语言模型的推\n理规划能力。\n1. 思维链提示\n语言模型在推理能力方面的表现一直未能令人满意，一些研究人员认为这可能是因为此前的\n模式是直接让模型输出结果，而忽略了其中的思考过程。人类在解决包括数学应用题在内的、涉\n及多步推理的问题...",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "大语言模型：从理论到实践 267\n陌生场景下，提升智能体的文本感知能力显得尤为重要。\n在视觉感知领域，尽管大语言模型在理解和处理多轮对话方面展现了卓越的性能[65]，但仍然\n无法处理视觉模态信息。 视觉输入通常包含丰富的环境信息， 例如物体的属性、 空间关系以及场景\n布局。将视觉信息与其他模态数据相结合，能够使智能体对外部环境的理解更加全面且精准 [255]。\n为了赋予智能体理解视觉信息的能力，一种直接的方法是将视觉输入通过图像描述生成技术转换\n为对应的文本描述[386]。 这种方法的优点在于其高度的可解释性， 并且无需为生成描述进行额外训\n练，从而显著节约计算资源。然而，此方法在转换过程中可能会丢失大量潜在信息，导致视觉信\n息的表达不完整。为解决上述问题，研究人员尝试将大语言模型与视觉编码器相结合，并通过增\n加一个可学习的接口层来对齐视觉编码与大模型的语言理解能力，从而增强大模型对视觉信息的\n感知能力[387]。 这一方法有效降低了大模型在学习视觉语言对齐任务中的负担， 并显著提升了其在\n视觉感知方面的性能。\n在音频感知方面，声音信息是外界环境中不可或缺的重要组成部分，为大模型智能体...",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "7.3.2 视觉语义关联\n视觉语义关联是多模态大语言模型和生成模型研究中的一项核心挑战。其主要目标是解决模\n型对文本提示理解不充分的问题，这种不足可能导致模型忽略提示中的某些关键信息，或错误生\n成不存在的内容。模型在处理视觉与文本的关联时，需要克服诸多复杂性，例如物体的空间位置\n关系（如左右位置） 、否定表达、计数能力，以及属性理解（如颜色和纹理） 。虽然目前尚无单一\n的方法能够完全解决这些问题，但研究者提出了一些行之有效的策略来提升模型的视觉语义关联\n能力。本节将重点介绍两种常用的改进方法：基于边界框标注和负样本生成方法。\n1. 基于边界框标注\n基于边界框标注是一种直接且高效的方式，用于增强视觉语义关联能力。例如， X-VLM[315]\n模型通过结合边界框回归与交并比 (IoU) 损失， 成功实现了视觉概念的精确定位， 并将这些概念与\n对应的文本描述对齐。通过明确标注图像中物体的位置及其相关描述，该模型能够更精准地将文\n本提示与正确的视觉线索关联，从而显著提升语义理解能力。这种方法的核心在于细粒度的视觉"
      ],
      "response_time": 3.1523163318634033,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.2331072068406811,
        "semantic_similarity": 0.939875602722168
      }
    },
    {
      "sample_id": "0127",
      "question": "在智能体如何结合长期记忆和视觉感知能力来提升其任务执行效率与环境理解能力上，有哪些关键点？",
      "ground_truth": "智能体通过长期记忆构建记忆库实现知识的持久化存储与高效调用，并在执行任务中系统化地存储经验和知识。当需要处理陌生场景时，结合视觉信息能够提升智能体的文本感知能力，而将视觉输入转换为文本描述的技术可以帮助智能体理解环境信息。通过将长期记忆与视觉感知结合，智能体能够更全面地理解外部环境，从而提升任务的执行效率和准确性。",
      "predicted_answer": "根据提供的文档，我无法回答这个问题。文档中未提及智能体如何结合长期记忆和视觉感知能力来提升其任务执行效率与环境理解能力的相关信息。",
      "ground_context": [
        "长期记忆通过构建记忆库来实现管理和检索，支持知识的持久化存储与高效调用。",
        "在陌生场景下，提升智能体的文本感知能力显得尤为重要。"
      ],
      "retrieved_contexts": [
        "长期记忆通过构建记忆库来实现管理和检索，支持知识的持久化存储与高效调用。包括构建\n和检索两个部分： （1）构建：在长期任务的执行过程中，智能体会将累积的经验、知识以及数据\n系统化地存储至记忆库中。记忆库的形式可以包括向量数据库、知识图谱等。 （2）检索：智能体\n需要获取以往的经验或相关知识时，可以通过查询记忆库进行检索，并将检索到的记忆内容作为\n大模型的输入，与当前任务需求结合后进行处理。例如，在面对类似问题时，智能体能够检索到\n此前解决相似问题的经验，从而显著提升问题解决的效率与准确性。\n2. 记忆操作\n智能体的记忆操作则包括写入、读取和反思等多个环节，这些操作旨在确保智能体能够高效\n地管理和利用其记忆资源，从而提升任务执行能力与智能化水平。\n记忆写入指将新的信息或经验存储到记忆模块中。在短期记忆中，写入的方式通常是将新的\n文本信息直接插入到上下文中，而在长期记忆中，则需要将信息存储到记忆库中，并对其进行索\n引与标记，以便后续检索使用。例如，当智能体完成某项任务后，可以将任务的执行过程及其结\n果记录为参考数据，供未来使用。通过不断积累经验，智能体能够逐步优化其能力，实现更高水\n平的智...",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "大语言模型：从理论到实践 269\n8.2.3 记忆模块\n记忆模块是在大模型智能体中承担着管理与操作智能体记忆的核心功能，包括对长短期记忆\n的存储、读取、处理以及反思等任务。该模块不仅负责存储历史数据与经验，还能够高效提取和\n更新信息，从而实现长期记忆与短期记忆之间的有机交互。通过记忆模块的支持，智能体在处理\n连续性任务时能够保持上下文的连贯性，并基于以往经验做出更加准确的判断与决策。\n1. 记忆模型\n大模型智能体所采用的记忆模型包括长期记忆和短期记忆两部分。它们各自有不同的功能和\n实现方式，但都依赖于大模型的强大计算和理解能力。\n短期记忆通常通过将记忆内容以提示语句的形式嵌入大模型输入的上下文中，借助大模型的\n上下文理解能力来实现。 包括存储和使用两个部分： （1） 存储： 在任务执行过程中， 关键的上下文\n信息与事件会被实时记录，形成短期记忆内容； （2）使用：在后续任务中，这些记忆内容会作为\n提示语句输入至大模型的上下文中，帮助模型基于提示进行推理与决策。例如，将前几步的操作\n结果及重要的环境信息作为输入内容，支持模型在接下来的步骤中做出更加合理的判断与选择。\n长期记忆通过构建记忆...",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "平的智能表现。\n记忆读取是指从记忆模块中提取与当前任务相关的信息，以支持任务的完成。在短期记忆中，\n读取操作通常是直接从上下文中提取提示信息并加以使用；而在长期记忆中，则通过检索记忆库\n来获得相关内容，通常通过匹配任务需求与记忆信息的方式完成。例如，当处理一个复杂问题时，\n智能体可以从长期记忆库中提取具有参考价值的解决方案，从而提供精准的建议或策略，提升任\n务处理的效率与质量。\n记忆反思是智能体对已存储记忆进行回顾与分析的一种机制，旨在进一步优化其行为策略与"
      ],
      "response_time": 1.1748783588409424,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.009765679646240958,
        "semantic_similarity": 0.806869626045227
      }
    },
    {
      "sample_id": "0128",
      "question": "如何完成角色扮演的交互过程？",
      "ground_truth": "可以直接调用 step() 函数完成角色扮演的交互过程，获取两个智能体的新一轮输出。",
      "predicted_answer": "要完成角色扮演的交互过程，可以按照以下步骤操作：\n\n1. 使用 `role_play_session.step()` 函数来执行角色扮演的交互过程，获取两个智能体的新一轮输出 [1]。\n2. 在每次调用 `step()` 函数后检查 `assistant_response` 和 `user_response` 是否标记为终止状态 [1]。\n3. 如果任一智能体的响应被标记为终止，则打印相应的终止原因并退出循环 [1]。\n\n具体代码示例如下：\n```python\nn = 0\nchat_turn_limit = 100  # 假设最大轮数为100\nwhile n < chat_turn_limit:  # 限制对话轮数\n    assistant_response, user_response = role_play_session.step(input_assistant_msg)  # 执行一步交互\n    if assistant_response.terminated:\n        print(f\"AI Assistant terminated. Reason: {assistant_response.info['termination_reasons']}\")  # 打印终止原因\n        break\n    if user_response.terminated:\n        print(f\"AI User terminated. Reas...\")\n```\n\n参考文献：\n[1]",
      "ground_context": [
        "基于 RolePlaying 的结构，可以直接调用 step() 函数完成角色扮演的交互过程，获取两个智能体的新一轮输出："
      ],
      "retrieved_contexts": [
        "大语言模型：从理论到实践 289\n基于 RolePlaying 的结构，可以直接调用 step() 函数完成角色扮演的交互过程，获取两个智能\n体的新一轮输出：\nassistant_response, user_response = role_play_session.step(input_assistant_msg)\n角色扮演的完整对话实现如下：\nwhile n < chat_turn_limit: #ս੽Ցཋᇅ\n#ྍ၂੽ൻԛ\nassistant_response, user_response = role_play_session.step(input_assistant_msg)\n#߅ؓ\nif assistant_response.terminated:\nprint(\"AI Assistant terminated. \"\nf\"Reason: {assistant_response.info['termination_reasons']}.\")\nbreak\nif user_response.terminated:\nprint(\"AI User terminated. \"\nf\"Reas...",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "为例，最简单的交互实现可大致分为以下步骤。\n（1）对于每个任务，用户首先描述任务的基本需求：\nquestion = \"Jimmy has $2 more than twice the money Ethel has.\nIf Ethal has $8, how much money is Jimmy having?\" #ิԛ໙ี\nagent_contexts = [[{\"role\": \"user\", \"content\": \"\"\"Can you solve the following math\nproblem? {} Explain your reasoning.\nYour final answer should be a single\nnumerical number, in the form\nboxed{{answer}}, at the end of your\nresponse.\"\"\".format(question)}]\nfor agent in range(agents)] #ᄯൻೆิൕ\n（2）每个智能体按一定顺序依次发言：\nfor i, agent_context in enu...",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "times 2 = $16.\nAdding $2 to this, we get that Jimmy has $16 + $2 = $\n boxed{18}$.\n2. 角色扮演\n角色扮演（Role-Playing）是指在事先设计的情景中自然地扮演某个角色。通过构造特定的提\n示，大语言模型有能力扮演不同的角色——无论是一个五年级的小学生，还是一个计算机领域的\n专家。令人意想不到的是，扮演特定角色的大语言模型能够激发其内部独特的领域知识，产生比\n没有指定角色时更好的答案。角色扮演在赋予智能体个体优势和专业技能的同时，更在多个智能\n体的协作交流中体现出了极大的价值，大大提高了多智能体系统的问题解决效率。\nCAMEL 是角色扮演的经典应用实例，该框架实现了两个智能体的交互，其中一个智能体作\n为用户，另一个智能体作为助手。此外， CAMEL 中还允许用户自由选择是否需要设置任务明确\n智能体与评论智能体，任务明确智能体专门负责将人类给出的初始任务提示细致化，评论智能体\n则负责评价交互的内容， 一方面引导交互向正确的方向进行， 另一方面判定任务目标是否已达成。\nCAMEL 中定义了一个 RoleP..."
      ],
      "response_time": 5.668363094329834,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.02985074626865672,
        "rouge2": 0.0,
        "rougeL": 0.02985074626865672,
        "bleu": 0.058232694180596296,
        "semantic_similarity": 0.7945091128349304
      }
    },
    {
      "sample_id": "0129",
      "question": "如何构建消息？",
      "ground_truth": "message = construct_message(agent_contexts_other, question, 2*round - 1 )",
      "predicted_answer": "根据提供的文档，我无法回答这个问题。",
      "ground_context": [
        "message = construct_message(agent_contexts_other, question, 2*round - 1 )"
      ],
      "retrieved_contexts": [
        "建主要通过以下两个基本操作实现。\n（1）提供更全面的描述： 为了使预训练的MiniGPT-4 生成更加全面、 更加综合的文本描述， 避\n免生成不完整的句子， 研究人员使用构建提示的策略， 鼓励基于Vicuna 的多模态模型生成给定图\n像的全面描述。具体的提示模板如下：\n###Human: <Img><ImageFeature></Img> Describe this image in detail.\nGive as many details as possible. Say everything you see. ###Assistant:\n其中，###Human 和 ###Assistant 分别代表用户输入和大语言模型的输出。 <Img></Img> 作为提\n示符，标记了一张图像输入的起止点。<ImageFeature> 代表输入图像在经过视觉编码器和线性投\n影层后的视觉特征。在这步操作中，一共从 Conceptual Caption 数据集中随机选择了 5000 张图像，\n生成对应的、内容更加丰富的文本描述。\n（2）提供更高质量的描述： 预训练的MiniGPT-4 并不能生成高质...",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "代表一个完整的思想或主题，而不是单纯按照固定长度切分。如图9.5所示，这种方法首先对文档\n进行分段 （如按句子或段落） ， 然后对每一段生成嵌入向量。 如果相邻段落之间的嵌入向量的相似\n度较高，就将它们合并为同一语义块；如果相似度显著降低，则开启一个新的块。这种动态块划\n分方式能够更好地适应文档的语言流畅性和主题变化，尤其适合长文档的处理。",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "148 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n接收的指令而回答内容，这些回答需要符合人们预先设定的期望。答案输出的内容，可以使用人\n工手段或借助自动化方法来构建。举例来说，倘若想要训练模型使其拥有问题回答能力，那么便\n可以按照如下的方式来构造数据：\nUser:\n཮౵Ĥ\nAssistant:\n཮౵b\n•ևਫ਼ 220b\n•ਫ਼ 2005և཮౵ჿ 3.5৚b\n•νਫ਼ 130b\n•ਫ਼ 825ഡb\n其中 “复旦大学有几个校区？ ” 是任务指令， “Assistant” 后的文字是模型学习的目标。 针对机\n器翻译、摘要生成、文章写作、代码生成、数学解题等几乎所有任务，都可以按照上述格式准备\n“输入-输出”数据。例如，针对文章写作任务，可以给出更清晰的指令要求以及与之匹配的输出，\n如下所示：\nUser:\n໓đေ౰ 500b\nAssistant:\nႿ 1905đၛቔტದҌູЧĠ\n,→֊ዶp đ\n,→ധ్௹ຬb\nđ࿐཮ᇯ҄ঔԉđ\n,→b\nೆ 20ऊđ\n,→b\nն॓࿹๧ೆđ\n,→௉ᅣđ\n,→৯ਈb\n如果期望模型具备理解多轮对话的能力，也可以基于上述方式将对话历史都做为指令，让模\n型学习最..."
      ],
      "response_time": 0.4942147731781006,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.0,
        "semantic_similarity": 0.43772733211517334
      }
    },
    {
      "sample_id": "0130",
      "question": "LangChain 中的记忆模块提供了哪些基本操作？",
      "ground_truth": "记忆系统需要支持两个基本操作：读取和写入。",
      "predicted_answer": "LangChain 中的记忆模块需要支持两个基本操作：读取和写入。[1][2]",
      "ground_context": [
        "记忆系统需要支持两个基本操作：读取和写入。"
      ],
      "retrieved_contexts": [
        "overall_chain = SimpleSequentialChain(chains=[synopsis_chain, review_chain], verbose =True)\n5. 记忆\n大多数大语言模型应用都使用对话方式与用户交互。对话中的一个关键环节是能够引用和参\n考之前对话中的信息。对于对话系统来说，最基础的要求是能够直接访问一些过去的消息。在更\n复杂的系统中还需要一个能够不断更新的事件模型，其能够维护有关实体及其关系的信息。在\nLangChain 中，这种能存储过去交互信息的能力被称为“记忆”。 LangChain 中提供了许多用于\n向系统添加记忆的方法，可以单独使用，也可以无缝整合到链中使用。\nLangChain 记忆模块的基本框架如图 8.10 所示。记忆系统需要支持两个基本操作：读取和写\n入。 每个链都根据输入定义了核心执行逻辑， 其中一些输入直接来自用户， 但有些输入可以来源于\n记忆。在接收到初始用户输入，但执行核心逻辑之前，链将从记忆系统中读取内容并增强用户输",
        "304 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n入。在核心逻辑执行完毕并返回答复之前，链会将这一轮的输入和输出都保存到记忆系统中，以\n便在将来使用它们。\n图 8.10 LangChain 记忆模块的基本框架\nLangChain 中提供了多种对记忆方式的支持，ConversationBufferMemory 是记忆中一种非常简\n单的形式，它将聊天消息列表保存到缓冲区中，并将其传递到提示模板中，代码示例如下：\nfrom langchain.memory import ConversationBufferMemory\nmemory = ConversationBufferMemory()\nmemory.chat_memory.add_user_message(\"hi!\")\nmemory.chat_memory.add_ai_message(\"whats up?\" )\n这种记忆系统非常简单，因为它只记住了先前的对话，并没有建立更高级的事件模型，也没有在\n多个对话之间共享信息，其可用于简单的对话系统，例如问答系统或聊天机器人。对于更复杂的\n对话系统，需要更高级的记忆系统来支持更...",
        "接口；链（Chain） ， 用于复杂应用的调用序列；记忆（Memory） ， 用于在链的多次运行之间持久化\n应用程序状态；智能体（Agent） ，语言模型作为推理器决定要执行的动作序列；回调（Callback） ，\n用于记录和流式传输任何链式组装的中间步骤。下文中的介绍和代码基于 LangChain V0.0.248 版\n本（2023 年 7 月 31 日发布） 。\n2. 模型输入/输出\nLangChain 中的模型输入/输出模块是与各种大语言模型进行交互的基本组件，是大语言模型\n应用的核心元素。 该模块的基本流程如图8.8 所示， 主要包含以下部分：Prompts、Language Models\n及 Output Parsers。将用户的原始输入与模型和示例进行组合输入大语言模型，再根据大语言模型\n的返回结果进行输出或者结构化处理。",
        "间进行接口处理。大语言模型和组件通过“链（Chain） ”连接，使得开发人员可以快速开发原型\n系统和应用程序。LangChain 的主要价值体现在以下几个方面。\n（1） 组件化：LangChain 框架提供了用于处理大语言模型的抽象组件， 以及每个抽象组件的一\n系列实现。这些组件具有模块化设计，易于使用，无论是否使用 LangChain 框架的其他部分，都\n可以方便地使用这些组件。\n（2） 现成的链式组装：LangChain 框架提供了一些现成的链式组装， 用于完成特定的高级任务。\n这些现成的链式组装使得入门变得更加容易。对于更复杂的应用程序， LangChain 框架也支持自\n定义现有链式组装或构建新的链式组装。\n（3） 简化开发难度： 通过提供组件化和现成的链式组装，LangChain 框架可以大大简化大语言\n模型应用的开发难度。开发人员可以更专注于业务逻辑，而无须花费大量时间和精力处理底层技\n术细节。\nLangChain 提供了以下 6 种标准化、 可扩展的接口， 并且可以外部集成：模型输入/输出（Model\nI/O） ， 与大语言模型交互的接口；数据连接（Data Connec...",
        "大语言模型：从理论到实践 291\n8.4.2 LangChain 框架\n大语言模型的调用相对简单， 仍需要完成大量的定制开发工作， 包括API 集成、 交互逻辑、 数\n据存储等。为了解决这个问题，从 2022 年开始，多家机构和个人陆续推出了大量开源项目，帮\n助开发者快速创建基于大语言模型的端到端应用程序或流程， 其中较为著名的是LangChain 框架。\nLangChain 框架是一种利用大语言模型的能力开发各种下游应用的开源框架，旨在为各种大语言\n模型应用提供通用接口，简化大语言模型应用的开发难度。它可以实现数据感知和环境交互，即\n能够使语言模型与其他数据源连接起来，并允许语言模型与其环境进行交互。\n本节将重点介绍 LangChain 框架以及其核心模块组成。\n1. LangChain 框架核心模块\n使用 LangChain 框架的核心目标是连接多种大语言模型 （如ChatGPT、LLaMA 等） 和外部资\n源（如 Google、Wikipedia、Notion 及 Wolfram 等） ，提供抽象组件和工具以在文本输入和输出之\n间进行接口处理。大语言模型和组件通过“链（Chain..."
      ],
      "response_time": 0.8116710186004639,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.401577332834242,
        "semantic_similarity": 0.8604846596717834
      }
    },
    {
      "sample_id": "0131",
      "question": "Jimmy has how much money?",
      "ground_truth": "$18",
      "predicted_answer": "Jimmy 有 $18。[1][2][3]",
      "ground_context": [
        "Jimmy has $16 + $2 = $ boxed{18}$."
      ],
      "retrieved_contexts": [
        "大语言模型：从理论到实践 287\n#၂੽ൻԛ\nWe know that Jimmy has $2 more than twice the money Ethel has.\nTwice the money Ethel has is $8 x 2 = $16.\nTwo more than $16 is $16 + $2 = $18.\nTherefore, Jimmy has $18.\nAnswer: $\n boxed{18}$.\nWe know that Jimmy has $2 more than twice the money Ethel has.\nTwice the money Ethel has is $8*2=<<8*2=16>>16.\nAdding $2 to this, we get that Jimmy has $16+$2=$\n boxed{18}$.\nTwice the money Ethel has is $8\n cdot 2=16$.\nJimmy has $2$ more than that, so his total is $16+2=\n boxed{18}$.\n...",
        "boxed{18}$.\n#੽ൻԛ\nBased on the solutions provided by other agents, I also arrive at the answer:\nJimmy has twice the money Ethel has, which is $8*2=$16,\nand he also has $2 more than that, which is $16+$2=$\n boxed{18}$.\nYes, based on the information provided and the solutions given by other agents, Jimmy has $18.\nAnswer: $\n boxed{18}$.\nGiven that Ethel has $8 and Jimmy has $2 more than twice Ethel's money,\nwe can calculate Jimmy's money as follows.\nTwice Ethel's money is $8\n times 2 = $16.\nAdding $...",
        "286 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\nagents = 3 #ඔ\nrounds = 2 #ս੽Ցഈཋ\nquestion = \"Jimmy has $2 more than twice the money Ethel has.\nIf Ethal has $8, how much money is Jimmy having?\" #ิԛ໙ี\nagent_contexts = [[{\"role\": \"user\", \"content\": \"\"\"Can you solve the following math\nproblem? {} Explain your reasoning.\nYour final answer should be a single\nnumerical number, in the form\nboxed{{answer}}, at the end of your\nresponse.\"\"\".format(question)}]\nfor agent in range(agents)] #ᄯൻೆิൕ\nfor round in range(rounds): ...",
        "times 2 = $16.\nAdding $2 to this, we get that Jimmy has $16 + $2 = $\n boxed{18}$.\n2. 角色扮演\n角色扮演（Role-Playing）是指在事先设计的情景中自然地扮演某个角色。通过构造特定的提\n示，大语言模型有能力扮演不同的角色——无论是一个五年级的小学生，还是一个计算机领域的\n专家。令人意想不到的是，扮演特定角色的大语言模型能够激发其内部独特的领域知识，产生比\n没有指定角色时更好的答案。角色扮演在赋予智能体个体优势和专业技能的同时，更在多个智能\n体的协作交流中体现出了极大的价值，大大提高了多智能体系统的问题解决效率。\nCAMEL 是角色扮演的经典应用实例，该框架实现了两个智能体的交互，其中一个智能体作\n为用户，另一个智能体作为助手。此外， CAMEL 中还允许用户自由选择是否需要设置任务明确\n智能体与评论智能体，任务明确智能体专门负责将人类给出的初始任务提示细致化，评论智能体\n则负责评价交互的内容， 一方面引导交互向正确的方向进行， 另一方面判定任务目标是否已达成。\nCAMEL 中定义了一个 RoleP...",
        "为例，最简单的交互实现可大致分为以下步骤。\n（1）对于每个任务，用户首先描述任务的基本需求：\nquestion = \"Jimmy has $2 more than twice the money Ethel has.\nIf Ethal has $8, how much money is Jimmy having?\" #ิԛ໙ี\nagent_contexts = [[{\"role\": \"user\", \"content\": \"\"\"Can you solve the following math\nproblem? {} Explain your reasoning.\nYour final answer should be a single\nnumerical number, in the form\nboxed{{answer}}, at the end of your\nresponse.\"\"\".format(question)}]\nfor agent in range(agents)] #ᄯൻೆิൕ\n（2）每个智能体按一定顺序依次发言：\nfor i, agent_context in enu..."
      ],
      "response_time": 0.5820844173431396,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.33333333333333337,
        "rouge2": 0.0,
        "rougeL": 0.33333333333333337,
        "bleu": 0.041263216881455095,
        "semantic_similarity": 0.7718820571899414
      }
    },
    {
      "sample_id": "0132",
      "question": "LangChain 中的链有哪些类型？",
      "ground_truth": "除了上例中的 LLMChain，LangChain 中的链还包含 RouterChain、SimpleSequentialChain、SequentialChain、TransformChain 等。",
      "predicted_answer": "LangChain 中的链包括 LLMChain、RouterChain、SimpleSequentialChain、SequentialChain 和 TransformChain 等类型[5]。具体来说，SimpleSequentialChain 是最简单的序列链形式，SequentialChain 是连续链的更一般形式，而TransformChain 则可以引入自定义转换函数[5]。此外，还有其他类型的链可供使用[5]。",
      "ground_context": [
        "除了上例中的 LLMChain，LangChain 中的链还包含 RouterChain、SimpleSequentialChain、SequentialChain、TransformChain 等。"
      ],
      "retrieved_contexts": [
        "间进行接口处理。大语言模型和组件通过“链（Chain） ”连接，使得开发人员可以快速开发原型\n系统和应用程序。LangChain 的主要价值体现在以下几个方面。\n（1） 组件化：LangChain 框架提供了用于处理大语言模型的抽象组件， 以及每个抽象组件的一\n系列实现。这些组件具有模块化设计，易于使用，无论是否使用 LangChain 框架的其他部分，都\n可以方便地使用这些组件。\n（2） 现成的链式组装：LangChain 框架提供了一些现成的链式组装， 用于完成特定的高级任务。\n这些现成的链式组装使得入门变得更加容易。对于更复杂的应用程序， LangChain 框架也支持自\n定义现有链式组装或构建新的链式组装。\n（3） 简化开发难度： 通过提供组件化和现成的链式组装，LangChain 框架可以大大简化大语言\n模型应用的开发难度。开发人员可以更专注于业务逻辑，而无须花费大量时间和精力处理底层技\n术细节。\nLangChain 提供了以下 6 种标准化、 可扩展的接口， 并且可以外部集成：模型输入/输出（Model\nI/O） ， 与大语言模型交互的接口；数据连接（Data Connec...",
        "overall_chain = SimpleSequentialChain(chains=[synopsis_chain, review_chain], verbose =True)\n5. 记忆\n大多数大语言模型应用都使用对话方式与用户交互。对话中的一个关键环节是能够引用和参\n考之前对话中的信息。对于对话系统来说，最基础的要求是能够直接访问一些过去的消息。在更\n复杂的系统中还需要一个能够不断更新的事件模型，其能够维护有关实体及其关系的信息。在\nLangChain 中，这种能存储过去交互信息的能力被称为“记忆”。 LangChain 中提供了许多用于\n向系统添加记忆的方法，可以单独使用，也可以无缝整合到链中使用。\nLangChain 记忆模块的基本框架如图 8.10 所示。记忆系统需要支持两个基本操作：读取和写\n入。 每个链都根据输入定义了核心执行逻辑， 其中一些输入直接来自用户， 但有些输入可以来源于\n记忆。在接收到初始用户输入，但执行核心逻辑之前，链将从记忆系统中读取内容并增强用户输",
        "接口；链（Chain） ， 用于复杂应用的调用序列；记忆（Memory） ， 用于在链的多次运行之间持久化\n应用程序状态；智能体（Agent） ，语言模型作为推理器决定要执行的动作序列；回调（Callback） ，\n用于记录和流式传输任何链式组装的中间步骤。下文中的介绍和代码基于 LangChain V0.0.248 版\n本（2023 年 7 月 31 日发布） 。\n2. 模型输入/输出\nLangChain 中的模型输入/输出模块是与各种大语言模型进行交互的基本组件，是大语言模型\n应用的核心元素。 该模块的基本流程如图8.8 所示， 主要包含以下部分：Prompts、Language Models\n及 Output Parsers。将用户的原始输入与模型和示例进行组合输入大语言模型，再根据大语言模型\n的返回结果进行输出或者结构化处理。",
        "304 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n入。在核心逻辑执行完毕并返回答复之前，链会将这一轮的输入和输出都保存到记忆系统中，以\n便在将来使用它们。\n图 8.10 LangChain 记忆模块的基本框架\nLangChain 中提供了多种对记忆方式的支持，ConversationBufferMemory 是记忆中一种非常简\n单的形式，它将聊天消息列表保存到缓冲区中，并将其传递到提示模板中，代码示例如下：\nfrom langchain.memory import ConversationBufferMemory\nmemory = ConversationBufferMemory()\nmemory.chat_memory.add_user_message(\"hi!\")\nmemory.chat_memory.add_ai_message(\"whats up?\" )\n这种记忆系统非常简单，因为它只记住了先前的对话，并没有建立更高级的事件模型，也没有在\n多个对话之间共享信息，其可用于简单的对话系统，例如问答系统或聊天机器人。对于更复杂的\n对话系统，需要更高级的记忆系统来支持更...",
        "chain = LLMChain(llm=chat, prompt =chat_prompt_template)\nprint(chain.run(\"colorful socks\" ))\n除了上例中的 LLMChain，LangChain 中的链还包含 RouterChain、SimpleSequentialChain、Se-\nquentialChain、TransformChain 等。RouterChain 可以根据输入数据的某些属性/特征值，选择调用\n哪个子链（Subchain） 。SimpleSequentialChain 是最简单的序列链形式，其中的每个步骤具有单一\n的输入/输出， 上一个步骤的输出是下一个步骤的输入。SequentialChain 是连续链的更一般的形式，\n允许多个输入/输出。TransformChain 可以引入自定义转换函数，对输入进行处理后再输出。以下\n是使用 SimpleSequentialChain 的代码示例："
      ],
      "response_time": 2.112614631652832,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 1.0,
        "mrr": 0.2
      },
      "answer_quality_metrics": {
        "rouge1": 0.6666666666666666,
        "rouge2": 0.37499999999999994,
        "rougeL": 0.5555555555555556,
        "bleu": 0.10592898115816704,
        "semantic_similarity": 0.826833963394165
      }
    },
    {
      "sample_id": "0133",
      "question": "Jimmy has how much money?",
      "ground_truth": "$18",
      "predicted_answer": "Jimmy 有 $18。[1][2][3]",
      "ground_context": [
        "Yes, based on the information provided and the solutions given by other agents, Jimmy has $18."
      ],
      "retrieved_contexts": [
        "大语言模型：从理论到实践 287\n#၂੽ൻԛ\nWe know that Jimmy has $2 more than twice the money Ethel has.\nTwice the money Ethel has is $8 x 2 = $16.\nTwo more than $16 is $16 + $2 = $18.\nTherefore, Jimmy has $18.\nAnswer: $\n boxed{18}$.\nWe know that Jimmy has $2 more than twice the money Ethel has.\nTwice the money Ethel has is $8*2=<<8*2=16>>16.\nAdding $2 to this, we get that Jimmy has $16+$2=$\n boxed{18}$.\nTwice the money Ethel has is $8\n cdot 2=16$.\nJimmy has $2$ more than that, so his total is $16+2=\n boxed{18}$.\n...",
        "boxed{18}$.\n#੽ൻԛ\nBased on the solutions provided by other agents, I also arrive at the answer:\nJimmy has twice the money Ethel has, which is $8*2=$16,\nand he also has $2 more than that, which is $16+$2=$\n boxed{18}$.\nYes, based on the information provided and the solutions given by other agents, Jimmy has $18.\nAnswer: $\n boxed{18}$.\nGiven that Ethel has $8 and Jimmy has $2 more than twice Ethel's money,\nwe can calculate Jimmy's money as follows.\nTwice Ethel's money is $8\n times 2 = $16.\nAdding $...",
        "286 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\nagents = 3 #ඔ\nrounds = 2 #ս੽Ցഈཋ\nquestion = \"Jimmy has $2 more than twice the money Ethel has.\nIf Ethal has $8, how much money is Jimmy having?\" #ิԛ໙ี\nagent_contexts = [[{\"role\": \"user\", \"content\": \"\"\"Can you solve the following math\nproblem? {} Explain your reasoning.\nYour final answer should be a single\nnumerical number, in the form\nboxed{{answer}}, at the end of your\nresponse.\"\"\".format(question)}]\nfor agent in range(agents)] #ᄯൻೆิൕ\nfor round in range(rounds): ...",
        "times 2 = $16.\nAdding $2 to this, we get that Jimmy has $16 + $2 = $\n boxed{18}$.\n2. 角色扮演\n角色扮演（Role-Playing）是指在事先设计的情景中自然地扮演某个角色。通过构造特定的提\n示，大语言模型有能力扮演不同的角色——无论是一个五年级的小学生，还是一个计算机领域的\n专家。令人意想不到的是，扮演特定角色的大语言模型能够激发其内部独特的领域知识，产生比\n没有指定角色时更好的答案。角色扮演在赋予智能体个体优势和专业技能的同时，更在多个智能\n体的协作交流中体现出了极大的价值，大大提高了多智能体系统的问题解决效率。\nCAMEL 是角色扮演的经典应用实例，该框架实现了两个智能体的交互，其中一个智能体作\n为用户，另一个智能体作为助手。此外， CAMEL 中还允许用户自由选择是否需要设置任务明确\n智能体与评论智能体，任务明确智能体专门负责将人类给出的初始任务提示细致化，评论智能体\n则负责评价交互的内容， 一方面引导交互向正确的方向进行， 另一方面判定任务目标是否已达成。\nCAMEL 中定义了一个 RoleP...",
        "为例，最简单的交互实现可大致分为以下步骤。\n（1）对于每个任务，用户首先描述任务的基本需求：\nquestion = \"Jimmy has $2 more than twice the money Ethel has.\nIf Ethal has $8, how much money is Jimmy having?\" #ิԛ໙ี\nagent_contexts = [[{\"role\": \"user\", \"content\": \"\"\"Can you solve the following math\nproblem? {} Explain your reasoning.\nYour final answer should be a single\nnumerical number, in the form\nboxed{{answer}}, at the end of your\nresponse.\"\"\".format(question)}]\nfor agent in range(agents)] #ᄯൻೆิൕ\n（2）每个智能体按一定顺序依次发言：\nfor i, agent_context in enu..."
      ],
      "response_time": 0.5923213958740234,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 0.5
      },
      "answer_quality_metrics": {
        "rouge1": 0.33333333333333337,
        "rouge2": 0.0,
        "rougeL": 0.33333333333333337,
        "bleu": 0.041263216881455095,
        "semantic_similarity": 0.7718820571899414
      }
    },
    {
      "sample_id": "0134",
      "question": "What is the function of a retriever?",
      "ground_truth": "检索器是一个接口，其功能是基于非结构化查询返回相应的文档。",
      "predicted_answer": "检索器的功能是基于非结构化查询返回相应的文档。它不需要存储文档，只需能根据查询要求返回结果即可。检索器可以使用向量存储的方式执行操作，也可以使用其他方式执行操作。[2]",
      "ground_context": [
        "Retrievers（检索器） 是一个接口， 其功能是基于非结构化查询返回相应的文档。"
      ],
      "retrieved_contexts": [
        "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap =0)\ndocuments = text_splitter.split_documents(raw_documents)\ndb = FAISS.from_documents(documents, OpenAIEmbeddings())\n#ྛཌྷරྟෆ෬\nquery = \"What did the president say about Ketanji Brown Jackson\"\ndocs = db.similarity_search(query)\nprint(docs[0].page_content)\nRetrievers（检索器） 是一个接口， 其功能是基于非结构化查询返回相应的文档。 检索器不需要\n存储文档，只需要能根据查询要求返回结果即可。检索器可以使用向量存储的方式执行操作，也\n可以使用其他方式执行操作。LangChain 中的 BaseRetriever 类定义如下：",
        "292 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n图 8.8 LangChain 模型输入/输出模块的基本流程\nPrompts 部分的主要功能是提示词模板、 提示词动态选择和输入管理。 提示词是指输入模型的\n内容。 该输入通常由模板、 示例和用户输入组成。LangChain 提供了几个类和函数， 使得构建和处\n理提示词更加容易。LangChain 中的 PromptTemplate 类可以根据模板生成提示词，它包含了一个\n文本字符串（模板） ，可以根据从用户处获取的一组参数生成提示词。以下是一个简单的示例：\nfrom langchain import PromptTemplate\ntemplate = \"\"\"\nYou are a naming consultant for new companies.\nWhat is a good name for a company that makes {product}?\n\"\"\"\nprompt = PromptTemplate.from_template(template)\nprompt.format(product=\"color...",
        "素，构成了环境（Environment）。环境会根据智能体的行为给予反馈，通常以奖励的形式。\n（2）状态、 行为与奖励：每次宠物狗尝试抓飞盘， 它都在评估当前的状态（State） ， 这可能包\n括飞盘的位置、 速度等。 基于这些信息， 它会采取某种动作（Action） ， 如跳跃、 奔跑或待在原地。\n根据宠物狗所执行的动作，环境随后会给出一个奖励（Reward），这可以是正面的（成功抓住飞\n盘）或负面的（错过了飞盘） 。\n（3）策略与价值： 在尝试各种行为的过程中， 宠物狗其实是在学习一个策略（Policy） 。 策略可\n以视为一套指导其在特定状态下如何行动的规则。与此同时，智能体还试图估计价值（V alue）函\n数，也就是预测在未来采取某一行为所能带来的奖励。\n总体来说，强化学习的目标就是让智能体通过与环境的互动，学习到一个策略，使其在将来\n能够获得的奖励最大化。这使得强化学习不总是关注短期奖励，而是在短期奖励与远期奖励之间\n找到平衡。",
        "246 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n质量提升数据剪枝数据平衡祛除冗余\nA photo of a bird   \nA photoof a dog\nyes\nno“A bird standing on a stone pillar in a lake”\nInstruction: Provide a description of what is presented in the image.\nAnswer: This is a photo of a standing bird.\nInstruction-tuningReinforcement Learning From Human FeedbackVLM: This image is a photo of a standing bird. VLM: Hi ,I am a robot. It is a photo of a dog.goodbad\nA black swan swimmingBaby swans in a poolUsing bounding boxesUsing negative captions\nA...",
        "大规模语言模型：从理论到实践 191\n智能体和环境。 在强化学习过程中， 智能体与环境不断交互。 智能体在环境中获取某个状态后， 会\n根据该状态输出一个动作， 也称为决策。 动作会在环境中执行， 环境会根据智能体采取的动作， 给\n出下一个状态及当前动作带来的奖励。智能体的目标就是尽可能多地从环境中获取奖励。本节将\n介绍强化学习的基本概念、强化学习与有监督学习的区别，以及在大语言模型中基于人类反馈的\n强化学习流程。\n图 6.1 强化学习基本框架\n在现实生活中，经常会遇到需要通过探索和试错来学习的情境。例如，孩子学会骑自行车的\n过程或是教宠物狗如何玩飞盘。宠物狗一开始对如何抓飞盘一无所知，但每当它成功抓住飞盘时，\n都可以给予它一定的奖励。这种通过与环境交互，根据反馈来学习最佳行为的过程正是强化学习\n的核心思想。通过宠物狗学习抓飞盘的例子，可以引出一些强化学习中的基本概念。\n（1）智能体与环境：在宠物狗学习抓飞盘的场景中，宠物狗就是一个智能体（Agent），它做\n出决策（Decision）并执行动作。它所在的场景， 包括飞盘的飞行轨迹和速度， 以及其他可能的因\n素，构成了环境（Environ..."
      ],
      "response_time": 1.3667950630187988,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.19875798613873752,
        "semantic_similarity": 0.8677728772163391
      }
    },
    {
      "sample_id": "0135",
      "question": "类公开了多少个方法？",
      "ground_truth": "两个",
      "predicted_answer": "类公开了两个方法：一个是用于文档嵌入表示，另一个是用于查询嵌入表示。[5]",
      "ground_context": [
        "类公开了两个方法：一个用于文档嵌入表示，另一个用于查询嵌入表示。"
      ],
      "retrieved_contexts": [
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "大规模语言模型：从理论到实践 167\n表 5.3 部分开源领域指令微调数据集的汇总信息\n指令数据集名称 发布单位 指令数据集规模（条） 领域 是否公开\nChatDoctor 德克萨斯大学西南医学中心 11.5 万 医疗 公开\nDISC-Med-SFT 复旦大学 46.49 万 医疗 公开\nHuatuo-26M 香港中文大学等 265 万 医疗 公开\nMedDialog 加州大学圣地亚哥分校 366 万 医疗 公开\nMedical Meadow 亚琛大学医院等 16 万 医疗 公开\nBELLE School Math 贝壳研究院 24.85 万 数学 公开\nGoat 新加坡国立大学 175 万 数学 公开\nOpenMathInstruct-1 NVIDIA 180 万 数学 公开\nCode Alpaca 20K Sahil Chaudhary 2 万 代码 公开\nCodeContest DeepMind 1.36 万 代码 公开\nCommitPackFT Bigcode 70.21 万 代码 公开\nDISC-Law-SFT 复旦大学 40.3 万 法律 部分\nHanFei 1.0 中国科...",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "166 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n表 5.2 部分开源通用指令微调数据集的汇总信息\n指令数据集名称 发布单位 指令数据集规模 语言 是否公开\nAlpaca Data Standford Alpaca 5.2 万条 英文 公开\nAya Collection Cohere For AI 等 5.13 亿条 多语言 公开\nAya Dataset Cohere For AI 等 20.4 万条 多语言 公开\nBELLE 贝壳研究院 350 万条 中文 公开\nCOIG 北京智源研究院 19.11 万条 中文 公开\nDialogStudio Salesforce AI 87 个数据集 多语言 公开\nDolly Databricks 1.5 万条 英语 公开\nFirefly Y eungNLP 115 万条 中文 公开\nFlan 2022 Google Research 1836 个数据集 多语言 部分\nInstructionWild V2 新加坡国立大学 11 万条 中英文 公开\nLCCC 清华大学 1200 万条 中文 公开\nLMSYS-Chat-1M 加州大学伯...",
        "类公开了两个方法：一个用于文档嵌入表示，另一个用于查询嵌入表示。前者输入多个文本，后\n者输入单个文本。之所以将它们作为两个单独的方法，是因为某些嵌入模型为文档和查询采用了\n不同的嵌入策略。以下是使用 OpenAI 的 API 接口完成文本嵌入的代码示例：\nfrom langchain.embeddings import OpenAIEmbeddings\nembeddings_model = OpenAIEmbeddings(openai_api_key=\"...\")\nembeddings = embeddings_model.embed_documents(\n[\n\"Hi there!\" ,\n\"Oh, hello!\" ,\n\"What's your name?\" ,\n\"My friends call me World\" ,\n\"Hello World!\"\n]\n)\nlen(embeddings), len(embeddings[0])\nembedded_query = embeddings_model.embed_query(\"What was the name mentioned in th..."
      ],
      "response_time": 0.8468635082244873,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 1.0,
        "mrr": 0.2
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.011199617145528714,
        "semantic_similarity": 0.5403650999069214
      }
    },
    {
      "sample_id": "0136",
      "question": "如何在使用 ChatOpenAI 进行对话时获取 assistant 的回复？",
      "ground_truth": "通过执行代码 chat_llm_chain.predict(human_input='Hi there my friend') 可以得到 assistant 的回复，然后可以继续通过 chat_llm_chain.predict(human_input='Not too bad - how are you?') 来获取进一步的对话。",
      "predicted_answer": "在使用 `ChatOpenAI` 进行对话时，可以通过调用其方法并传递相应的参数来获取 `assistant` 的回复。通常情况下，这涉及到创建一个 `ChatOpenAI` 对象并调用它的 `run` 方法来执行对话。具体来说，你可以按照以下步骤操作：\n\n```python\nfrom langchain.chat_models import ChatOpenAI\n\n# 创建 ChatOpenAI 对象\nchat = ChatOpenAI(openai_api_key=\"...\", temperature=0)\n\n# 调用 run 方法进行对话并获取回复\nresponse = chat.run(\"你的问题或指令\")\nprint(response)  # 输出 Assistant 的回复\n```\n\n在这个例子中，`temperature` 参数用于控制回复的创造性，值越低，回复越接近于模型的实际预测结果；反之亦然。通过这种方式，你可以获取到 `assistant` 在对话中的回复。[2][5]",
      "ground_context": [
        "chat_llm_chain.predict(human_input='Hi there my friend') 可以得到如下输出结果：'Hello! How can I assist you today, my friend?'",
        "content = completion['choices'][0]['message']['content']"
      ],
      "retrieved_contexts": [
        "大语言模型：从理论到实践 305\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.schema import SystemMessage\nfrom langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, MessagesPlaceholder\nprompt = ChatPromptTemplate.from_messages([\nSystemMessage(content=\"You are a chatbot having a conversation with a human.\" ),\nMessagesPlaceholder(variable_name=\"chat_history\"), # Where the memory will be stored.\nHumanMessagePromptTemplate.from_template(\"{human_input}\"), # Where the human input wil...",
        "大语言模型：从理论到实践 293\n全模型， 接收字符串提示词作为输入， 并输出字符串。OpenAI 的 GPT-3 是 LLM 实现的一个实例。\nChat Model 专为会话交互设计，与传统的纯文本补全模型相比，这一模型的 API 采用了不同的接\n口方式：它需要一个标有说话者身份的聊天消息列表作为输入，如“系统” 、 “AI”或“人类” 。作\n为输出，Chat Model 会返回一个标为“AI”的聊天消息。GPT-4 和 Anthropic 的 Claude 都可以通\n过 Chat Model 调用。以下是利用 LangChain 调用 OpenAI API 的代码示例：\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.schema import (AIMessage, HumanMessage, SystemMessage)\nchat = ChatOpenAI(\nopenai_api_key=\"...\",\ntemperature=0,\nmodel='gpt-3.5-turbo'\n)\nmessages = [\nS...",
        "310 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\nfrom langchain.agents import Tool\nfrom langchain.agents import AgentType\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.utilities import SerpAPIWrapper\nfrom langchain.agents import initialize_agent\nsearch = SerpAPIWrapper()\ntools = [\nTool(\nname = \"Current Search\" ,\nfunc=search.run,\ndescription=\"useful for when you need to answer questions about current events\nor the current state of the world \"\n),...",
        "),\n]\nmemory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages =True)\nllm = ChatOpenAI(openai_api_key=OPENAI_API_KEY, temperature =0)\nagent_chain = initialize_agent(\ntools,\nllm,\nagent=AgentType.CHAT_CONVERSATIONAL_REACT_DESCRIPTION,\nverbose=True,\nmemory=memory\n)\n注意， 此处在agent 类型选择时使用了 “CHA T_CONVERSA TIONAL_REACT_DESCRIPTION” ， 模\n型将使用 ReAct 逻辑生成。根据上面定义的智能体，使用如下调用方式：\nagent_chain.run(input=\"what's my name?\" )\n给出如下回复：",
        "302 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.prompts.chat import (\nChatPromptTemplate,\nHumanMessagePromptTemplate,\n)\nhuman_message_prompt = HumanMessagePromptTemplate(\nprompt=PromptTemplate(\ntemplate=\"What is a good name for a company that makes {product}?\",\ninput_variables=[\"product\"],\n)\n)\nchat_prompt_template = ChatPromptTemplate.from_messages([human_message_prompt])\nchat = ChatOpenAI(temperature=0.9)\nchain = LLMChain(llm=chat, prompt =chat_promp..."
      ],
      "response_time": 4.504631757736206,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.11320754716981132,
        "rouge2": 0.0,
        "rougeL": 0.07547169811320756,
        "bleu": 0.010780216788308936,
        "semantic_similarity": 0.7258422374725342
      }
    },
    {
      "sample_id": "0137",
      "question": "如何使用向量存储的检索器来获取与用户输入相关的文档，并同时使用大语言模型来处理用户的对话？",
      "ground_truth": "可以通过get_relevant_documents方法或异步调用aget_relevant_documents方法获得与查询文档最相关的文档，同时使用大语言模型结合ChatPromptTemplate来处理用户的对话。",
      "predicted_answer": "首先，使用向量存储（如FAISS）构建一个检索器来处理用户输入。具体步骤如下：\n\n1. 使用`langchain.document_loaders.TextLoader`加载文档集。\n2. 利用`langchain.embeddings.OpenAIEmbeddings`创建文档的嵌入表示。\n3. 将嵌入向量存入向量数据库（如FAISS），这一步骤在文档1中有详细描述[1]。\n\n当用户提出一个问题时，通过相同的嵌入方法将其转化为查询向量，并使用预先建立的检索器在向量数据库中找到与其最接近的相关文档。这些文档随后被传递给大语言模型进行进一步处理和生成响应。\n\n这个过程涉及两个主要组件：检索模块和生成模块。检索模块使用向量搜索技术来高效地定位相关文档，而生成模块则基于检索到的内容生成最终的回答[5]。\n\n[1][5]",
      "ground_context": [
        "它的使用非常简单，可以通过get_relevant_documents方法或通过异步调用aget_relevant_documents方法获得与查询文档最相关的文档。",
        "prompt = ChatPromptTemplate.from_messages([SystemMessage(content=\"You are a chatbot having a conversation with a human.\"), MessagesPlaceholder(variable_name=\"chat_history\"), HumanMessagePromptTemplate.from_template(\"{human_input}\")])"
      ],
      "retrieved_contexts": [
        "大语言模型：从理论到实践 299\nV ector Stores（向量存储）是存储和检索非结构化数据的主要方式之一。它首先将数据转化为\n嵌入表示，然后存储生成的嵌入向量。在查询阶段，系统会利用这些嵌入向量来检索与查询内容\n“最相似”的文档。向量存储的主要任务是保存这些嵌入向量并执行基于向量的搜索。 LangChain\n能够与多种向量数据库集成， 如Chroma、FAISS 和 Lance 等。以下为使用 FAISS 向量数据库的代\n码示例：\nfrom langchain.document_loaders import TextLoader\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom langchain.vectorstores import FAISS\n#ཟਈթԥᇏ\nraw_documents = TextLoader('../../../state_of_the_union.txt').loa...",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "大语言模型：从理论到实践 345\n（Retrieval-Augmented Generation）系统中，文本嵌入表示是实现向量搜索的核心技术。\n文本嵌入技术有很长的研究历史，大体上可以分为四个阶段：计数式嵌入（Count-based Em-\nbeddings） ： 这一阶段的方法包括词袋模型 （Bag of Words, BoW） 和TF-IDF， 用词频和逆文档频率\n来表示文本， 但忽略了词语的语义和上下文信息， 仅能反映基本的词汇相关性； 静态词嵌入 （Static\nDense Word Embeddings） ： 代表性模型如Word2V ec、GloV e和 FastText， 通过上下文生成固定的词\n向量。这一阶段捕捉了词语的语法和语义相似性，但每个词的向量是静态的，无法反映词义在不\n同上下文中的变化；上下文嵌入（Contextualized Embeddings） ：这一阶段引入了上下文敏感的动\n态嵌入模型， 如GPT 和 BERT 等。 这些模型通过双向或单向Transformer 结构， 生成能够根据上下\n文调整的词或句子向量，实现了对多义词和复杂语境的更深层次理解；...",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "大语言模型：从理论到实践 317\n点， 首先通过检索模块 （Retriever） 根据查询内容定位并查找相关数据源， 然后筛选出与查询高度\n相关的信息作为检索结果。这些检索结果随后与生成模块（Generator）协作，以增强生成过程的\n质量和效果。\n用户输入各种模态数据\n检索模块生成模块结果\n文本代码图像音频视频知识科学数据……3D\n数据\n编码索引\n稀疏检索稠密检索其他\nTr ansfor merLSTMDiffusionGAN……\n文本\n代码\n图片\n视频\n知识\n科学数据\n3D\n音频\n图 9.1 典型大模型检索增强生成过程 [413]\n检索模块负责从外部知识库或数据源中定位与用户查询相关的信息。检索器通常基于向量检\n索技术或其他高效的检索算法，将输入的自然语言查询转换为向量表示，并与外部数据源中的内\n容进行匹配。外部数据源可以是文档数据库、知识图谱、 API 接口或实时搜索引擎等。检索模块\n不仅需要快速准确地定位相关内容，还需对检索结果进行筛选和排序，以确保返回的内容与用户\n查询具有高度相关性。这一模块的性能直接影响生成器后续处理的质量和效率。\n生成模块则是基于检索器提供的相关内容生成..."
      ],
      "response_time": 4.680711269378662,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0.0,
        "bleu": 0.014117272361380273,
        "semantic_similarity": 0.6391941905021667
      }
    },
    {
      "sample_id": "0138",
      "question": "如何使用基于向量存储的检索器来检索与查询文档最相关的文档，以及向量存储的主要功能是什么？",
      "ground_truth": "基于向量存储的检索器通过get_relevant_documents方法或异步调用aget_relevant_documents方法获得与查询文档最相关的文档。向量存储的主要功能是将数据转化为嵌入表示并存储生成的嵌入向量，利用这些嵌入向量在查询阶段检索与查询内容最相似的文档。",
      "predicted_answer": "使用基于向量存储的检索器来检索与查询文档最相关的文档可以通过以下步骤完成：\n1. 加载文档并将它们分割成片段。\n2. 使用嵌入模型将这些片段转换为向量形式。\n3. 将生成的向量存储起来。\n4. 在查询阶段，将查询文本转换为向量并与存储的向量进行比较，找到最相似的文档。\n\n向量存储的主要功能包括：\n- 将非结构化的文本数据转化为嵌入向量。\n- 存储这些嵌入向量以便后续查询使用。\n- 执行基于向量的搜索操作，如相似性搜索和最大互信息检索（MMR）。[1][3]",
      "ground_context": [
        "它的使用非常简单， 可以通过get_relevant_documents 方法或通过异步调用 aget_relevant_documents 方法获得与查询文档最相关的文档。",
        "V ector Stores（向量存储）是存储和检索非结构化数据的主要方式之一。它首先将数据转化为嵌入表示，然后存储生成的嵌入向量。在查询阶段，系统会利用这些嵌入向量来检索与查询内容“最相似”的文档。"
      ],
      "retrieved_contexts": [
        ") -> List[Document]:\n\"\"\"֖\nArgs:\nquery:Ա\ncallbacks:ਙі\nReturns:\nਙі\n\"\"\"\n...\n它的使用非常简单， 可以通过get_relevant_documents 方法或通过异步调用 aget_relevant_ documents\n方法获得与查询文档最相关的文档。 基于向量存储的检索器 （V ector store-backed retriever） 是使用\n向量存储检索文档的检索器。它是向量存储类的轻量级包装器，与检索器接口契合，使用向量存\n储实现的搜索方法 （如相似性搜索和MMR） 来查询使用向量存储的文本。 以下是一个基于向量存\n储的检索器的代码示例：",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "大语言模型：从理论到实践 299\nV ector Stores（向量存储）是存储和检索非结构化数据的主要方式之一。它首先将数据转化为\n嵌入表示，然后存储生成的嵌入向量。在查询阶段，系统会利用这些嵌入向量来检索与查询内容\n“最相似”的文档。向量存储的主要任务是保存这些嵌入向量并执行基于向量的搜索。 LangChain\n能够与多种向量数据库集成， 如Chroma、FAISS 和 Lance 等。以下为使用 FAISS 向量数据库的代\n码示例：\nfrom langchain.document_loaders import TextLoader\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom langchain.vectorstores import FAISS\n#ཟਈթԥᇏ\nraw_documents = TextLoader('../../../state_of_the_union.txt').loa...",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "1. 稀疏检索\n稀疏检索 （Sparse Retrieval） 是一种基于统计特征的方法， 通过将查询和文档转换为稀疏向量\n来实现检索。稀疏向量的特点是大部分元素为零，仅保留少量非零值，这使得计算更加高效且存\n储成本较低。许多经典的信息检索方法，如TF-IDF 和 BM25，都是稀疏检索的典型实现。这些方\n法通过词频、逆文档频率等显性统计特征对查询和文档进行建模，能够快速匹配相关内容。稀疏\n检索架构如图9.7所示。\n查询词\n查询词向量\n相似度 数据集\n图 9.7 稀疏检索架构图\n稀疏检索的最大优势在于其高效性，尤其适用于处理大规模文档库的检索任务。由于稀疏向\n量中仅计算非零元素的部分，相较于密集向量方法，其计算复杂度显著降低。因此，稀疏检索在\n资源有限或实时性要求较高的场景中表现尤为突出。稀疏检索器在大规模数据集上的效率使其成\n为工业界的主流选择之一。\n尽管稀疏检索在效率上具有明显优势，但其在捕捉复杂语义关系方面存在局限性。由于稀疏\n方法主要依赖显性统计特征，如词频和词项匹配，无法有效处理同义词、上下文语义等深层语义"
      ],
      "response_time": 3.708904504776001,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 0.3333333333333333
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0.0,
        "bleu": 0.1878404171228937,
        "semantic_similarity": 0.8831595182418823
      }
    },
    {
      "sample_id": "0139",
      "question": "Coze是一个什么类型的平台？",
      "ground_truth": "Coze（扣子）是一个大模型智能体开发平台。",
      "predicted_answer": "Coze 是一个大模型智能体开发平台[1]，它帮助开发者以低门槛的方式快速构建、测试和部署智能体，实现复杂任务的自动化。该平台提供了丰富的功能，包括插件、长短期记忆、工作流和卡片等，支持智能体与外部工具的无缝对接，并且允许用户通过拖拽式界面快速设计对话流程[1]。",
      "ground_context": [
        "Coze（扣子）是一个大模型智能体开发平台，整合了插件、长短期记忆、工作流、卡片等丰富功能，能够以低门槛、快速搭建个性化或具备商业价值的智能体，并发布到豆包、飞书、网页等多种平台，实现全场景覆盖。"
      ],
      "retrieved_contexts": [
        "312 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n模板，用户无需编写代码，甚至完全不需要编程能力，即可完成智能体的设计、开发和部署。这\n类平台通常集成了预训练的大语言模型，并提供强大的工具支持，如知识库管理、对话流程设计\n和外部 API 集成，极大地简化了开发流程。需要注意的是，这种方式在定制化能力、性能优化和\n扩展性上存在一定局限，复杂场景或高性能需求场景下适应程度需要详细评估。\nCoze（扣子）是一个大模型智能体开发平台，整合了插件、长短期记忆、工作流、卡片等丰\n富功能，能够以低门槛、快速搭建个性化或具备商业价值的智能体，并发布到豆包、飞书、网页\n等多种平台，实现全场景覆盖。通过模块化与高效的工具支持，Coze 帮助开发者快速构建、测试\n和部署智能体，实现复杂任务的自动化，同时提供强大的扩展和定制能力。其插件系统支持智能\n体与外部工具无缝对接，如数据库查询、第三方 API 调用、任务管理工具等，在多种环境中执行\n精准任务；长短期记忆功能让智能体在短期对话中保持上下文一致，并通过长期记忆存储重要信\n息，实现自然、智能的交互体验；工作流功能允许用户通过拖拽式界面快速设计...",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "调用插件或执行复杂任务；卡片功能则为智能体提供了信息展示和互动的新形式，让用户在网页\n或移动端直观查看数据、流程和结果。\n使用 Coze 平台可以通过以下简单的五个步骤就可以构造快速搭建一个“夸夸机器人” ，并在\n多个平台提供对外服务。\n步骤 1： 创建一个智能体。 在扣子平台创建智能体非常简单： 登录后， 点击页面左上角的 “L” ，\n输入智能体名称和功能介绍，并通过生成图标自动生成头像，或使用“AI 创建”功能，通过自然\n语言描述需求， 由平台自动生成智能体。 点击确认后， 进入智能体编排页面。 在这里， 可以通过左\n侧人设与回复逻辑面板描述智能体的身份和任务；利用中间技能面板为智能体配置扩展能力；在\n右侧预览与调试面板中实时测试智能体，确保其功能和交互效果符合预期。\n步骤 2：编写提示词。配置智能体的第一步是编写提示词，即定义智能体的人设与回复逻辑。\n这部分内容决定了智能体的基本人设，并持续影响其在所有会话中的回复效果。在设计提示词时，\n建议明确模型的角色、设计特定的语言风格，并限制回答范围，以确保对话内容符合用户的预期。\n例如，对于一个“夸夸机器人” ，提示词可以设置为：",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "\"action_input\": \"Cloudy with showers. Low around 55F. Winds S at 5 to 10 mph.\nChance of rain 60%. Humidity76%.\"\n}\n> Finished chain.\n'Cloudy with showers. Low around 55F. Winds S at 5 to 10 mph. Chance of rain 60%. Humidity76%.'\n可以看到，模型采用 ReAct 的提示模式生成内容。通过上述两种不同的用户输入及相应的系统回\n复，可以看到智能体自动根据用户输入选择是否使用搜索工具。\n8.4.3 智能体平台 Coze 实践\n使用零代码或低代码平台构建大模型智能体是一种高效便捷的开发方式，适合缺乏编程经验\n的用户或需要快速验证概念（Proof of Concept，PoC）的场景。 通过可视化界面、 拖拽组件和预设"
      ],
      "response_time": 2.186551570892334,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.5,
        "rouge2": 0.0,
        "rougeL": 0.5,
        "bleu": 0.08624406577958867,
        "semantic_similarity": 0.8509633541107178
      }
    },
    {
      "sample_id": "0140",
      "question": "索引模块在 RAG 系统中的核心任务是什么？",
      "ground_truth": "其核心任务是将文档划分为可管理的片段（Chunk），也成为“块”，为后续的检索和生成提供组织良好的内容基础。",
      "predicted_answer": "索引模块在 RAG 系统中的核心任务是将文档划分为可管理的片段（Chunk），也成为“块”，为后续的检索和生成提供组织良好的内容基础。[1]",
      "ground_context": [
        "索引（Index）是 RAG 系统中至关重要的过程，其核心任务是将文档划分为可管理的片段（Chunk），也成为“块”，为后续的检索和生成提供组织良好的内容基础。"
      ],
      "retrieved_contexts": [
        "324 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n其架构分为三个层级： 顶层聚焦于RAG 的关键阶段， 将每个阶段视为独立模块， 同时引入一个编\n排模块来协调 RAG 流程； 中层由每个模块内的子模块组成， 进一步细化和优化各项功能； 底层由\n操作的基本单元（即操作符）构成。在模块化 RAG 框架中，RAG 系统可以通过计算图的形式表\n示，其中节点代表具体的操作符。\n本章将重点介绍 Modular RAG 框架下的各模块，包括：索引、检索前优化、检索、检索后优\n化、生成以及编排。\n9.2.1 索引模块\n索引（Index）是 RAG 系统中至关重要的过程，其核心任务是将文档划分为可管理的 片段\n（Chunk），也成为“块” ，为后续的检索和生成提供组织良好的内容基础。片段切分是将文档拆分\n为更小的、可管理的、语义完整的信息单元的过程，其构建需要综合考虑内容的语义特性、上下\n文完整性以及检索和生成的实际需求。在构建片段时， 首先需要确定片段的大小（长度） 。 片段的\n大小通常用字符数、单词数或句子数来衡量，具体取决于任务要求和模型的能力。\n较大的片段在构建时能够捕获更多上下...",
        "在不同任务场景中的表现。根据 RAG 系统的“检索-生成”结构，评估目标可分为针对检索的评\n估、针对生成的评估，以及面向整体系统的协同能力评估。\n1. 检索模块的评估目标\n检索模块是 RAG 系统的基础， 其主要任务是从知识库中提取与用户查询相关的信息， 为生成\n模块提供支持。在评估检索组件时，需要明确以下几个关键目标：\n（1） 相关性 （Relevance） ：检索组件的首要目标是确保其返回的文档与用户查询高度相关。 相\n关性评估旨在衡量检索出的文档是否能够准确反映用户查询所需要的信息。例如，在问答任务中，\n检索出的文档是否包含回答问题所需的事实或背景知识。相关性通常通过计算检索结果与查询之\n间的匹配程度来评估，可以使用指标如精确率（Precision）和召回率（Recall） 。\n（2）准确性（Accuracy） ：除了相关性，检索结果的准确性也至关重要。准确性评估需要考察\n检索到的文档是否在信息上是可靠的，是否包含错误、误导性内容或低质量信息。检索组件返回\n的错误信息可能直接导致生成组件生成不真实的回答，因此检索结果的准确性对整体系统的性能\n至关重要。\n（3）覆盖率与多样性（Co...",
        "等） ， 评估数据集能够更全面地展现系统在不同领域中的适应性和泛化能力。 这种多样化设计确保\n了 RAG 系统在广泛应用场景中的可靠性和实用性。\n9.5.4 评估指标\n评估指标是衡量 RAG 系统性能的核心工具，直接影响评估结果的可信度和系统优化的方向。\n在评估 RAG 系统时，需要对各种评估指标有深入的理解，以便准确衡量评估目标。由于 RAG 系\n统由检索组件和生成组件构成，以下从检索模块、生成模块和整体系统三个层面展开详细讨论。",
        "大语言模型：从理论到实践 339\n模型或来源异构的场景。在这些复杂情况下，RRF 能够显著提升预测的准确性和整合效果，成为\n多分支融合的重要工具。\n9.3 RAG 系统设计模式\n基于 Modular RAG 的设计，各种模式通过模块化操作符之间的协作形成了模块的工作流，称\n为 RAG 流 （RAG flow） 。RAG 流可以被分解为由子函数组成的图结构， 通过控制逻辑， 这些操作\n符可以按照预定的管道线执行，同时在必要时支持条件判断、分支或循环操作。通过深入分析现\n有的 RAG 方法，这些模式的模块化特性使其能够灵活适应多样化的场景需求，同时提高了 RAG\n系统的设计效率和扩展性。\n本章将介绍典型的 RAG 系统模式，包括线性模式、条件模式、分支模式、循环模式等。\n9.3.1 线性模式\n在 RAG 系统中，线性模式是最简单且最常用的工作流模式，其流程可以分为几个核心模块，\n包括预检索（Pre-Retrieval） 、检索、后检索（Post-Retrieval）以及生成模块，如图9.10所示。当预\n检索和检索后处理模块缺失时， 线性模式会简化为朴素检索增强生成 （Naive RAG） ...",
        "336 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n时，显得尤为重要。相比传统的语言模型， RAG 系统通过检索阶段获取最新或特定领域的信息，\n克服了模型内生记忆的局限性，尤其是在处理长尾问题或细分领域的专业知识时，可以表现更加\n出色。\n9.2.6 编排\n编排模块是 RAG 系统中的核心控制单元，它负责在关键节点进行决策并动态选择后续步骤。\n与传统固定流程的僵化方法不同，编排模块引入了灵活的适应能力，可以根据先前结果实时调整\n流程。 这种模块化、 动态化的特性是Modular RAG 的标志性特点， 展现出更高的智能化和灵活性。\n本节将分别介绍编排模块的主要模块， 包含路由 （Routing） 、 调度 （Scheduling） 以及融合 （Fusion） 。\n1. 路由\n在响应多样化查询的过程中，RAG 系统可以通过路由机制将查询分配到针对不同场景设计的\n特定管道中。这种机制是一个通用性较强的 RAG 架构的重要特性，能够处理各种复杂的情境需\n求。路由模式可以分为三种主要类型：元数据路由、语义路由以及混合路由。\n元数据路由（Metadata Routing）基于查询中提..."
      ],
      "response_time": 1.3255910873413086,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.5,
        "rouge2": 0.0,
        "rougeL": 0.5,
        "bleu": 0.7645177314873157,
        "semantic_similarity": 0.8201473951339722
      }
    },
    {
      "sample_id": "0141",
      "question": "OpenAI 于哪一年推出了 SearchGPT？",
      "ground_truth": "2024",
      "predicted_answer": "OpenAI 于 2024 年推出了 SearchGPT[5]。",
      "ground_context": [
        "OpenAI 于 2024 年推出了 SearchGPT，进一步推动了 AI 搜索技术的发展"
      ],
      "retrieved_contexts": [
        "GPT-o3 2024 年 12 月 OpenAI - 推理模型\n豆包 1.5Pro 2025 年 1 月 字节跳动 - 对话模型\nGrok-3 2025 年 2 月 XAI - 对话推理模\n型\n1 模型参数量根据微软公司发表的文献 [38] 获取，数字并未得到 OpenAI 官方证实",
        "大规模语言模型：从理论到实践 9\n表 1.2 典型闭源大语言模型汇总\n模型名称 发布时间 发布公司 参数量（个） 模型类型\nGPT-3 2020 年 5 月 OpenAI 1750 亿 基础模型\nERNIE 3.0 2021 年 7 月 百度 100 亿 基础模型\nClaude 2021 年 12 月 Anthropic 520 亿 基础模型\nInstructGPT 2022 年 3 月 OpenAI 1750 亿 对话模型\nPaLM 2022 年 4 月 Google 5400 亿 基础模型\nChatGPT 3.5 2022 年 11 月 OpenAI 1750 亿 1 对话模型\nGPT-4 2023 年 3 月 OpenAI 17600 亿 1 对话模型\nPanGu-Σ 2023 年 3 月 华为 10850 亿 对话模型\nChatGLM 2023 年 3 月 智谱华章 1300 亿 对话模型\n文心一言 2023 年 4 月 百度 - 对话模型\n通义千问 2023 年 5 月 阿里巴巴 - 对话模型\nMinMax 2023 年 5 月 稀宇科技 - 对话模型\n星火 2023 年 5...",
        "歌则推出 Bard， 将实时检索与生成能力结合， 为用户提供更加全面的答案；国内的Kimi、 秘塔等\n产品也在这一领域崭露头角，成为 AI 搜索技术的重要实践者。此外， OpenAI 于 2024 年推出了\nSearchGPT，进一步推动了 AI 搜索技术的发展，该产品通过深度整合大语言模型与动态知识检索\n功能，展现了强大信息处理效率。国内的豆包、千问、智谱、百川等大模型系统也相继融入了AI\n搜索功能。\n本节将重点介绍 RAG 系统框架、RAG 任务分级以及 RAG 系统的难点。\n9.1.1 RAG 系统框架\n典型的检索增强生成过程如图9.1所示，其核心在于将外部检索与生成模块有机整合，通过动\n态引入外部知识来提升生成结果的准确性与可靠性。 具体而言，RAG 过程以用户输入的查询为起",
        "杂任务。\n2. 教程学习\n教程学习通常通过将工具手册作为提示输入到大模型中，使其直接从手册内容中理解工具的\n功能与使用方法。这一方法的核心理念来源于人类通过阅读手册或观察演示学习新技能的行为方\n式。同样，大模型可以借助其强大的上下文理解能力，通过提示语句从工具手册中获取相关知识\n并掌握工具的操作。然而，尽管 OpenAI 系列的大模型凭借卓越的上下文理解能力能够较好地完",
        "得分高于 88% 的应试者，包括美国律师资格考试（ Uniform Bar Exam ） 、法学院入学考试（Law\nSchool Admission Test） 、学术能力评估（Scholastic Assessment Test，SA T）等。GPT-4o 是 OpenAI\n于 2024 年 5 月发布的多模态大模型，其中“o”代表“omni”即“全能” 。它能接受文本、音频和\n图像组合输入并生成文本、 音频和图像的任意组合输出， 可处理50 种语言， 在232 毫秒内对音频"
      ],
      "response_time": 0.699333667755127,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.4,
        "rouge2": 0.0,
        "rougeL": 0.4,
        "bleu": 0.021598265052211615,
        "semantic_similarity": 0.47229161858558655
      }
    },
    {
      "sample_id": "0142",
      "question": "大语言模型在本科低年级知识点记忆能力测试中的表现是多少？",
      "ground_truth": "73.6%",
      "predicted_answer": "GPT-4 Turbo 在本科低年级知识点记忆能力测试中的表现为 73.6% [1]。",
      "ground_context": [
        "根据 LLMEV AL-3[411] 评测结果，GPT-4 Turbo 在本科低年级知识点记忆能力测试中的表现仅为 73.6%。"
      ],
      "retrieved_contexts": [
        "能得到了快速提升。这些模型在自然语言处理、推理和生成任务中展现出了前所未有的表现。然\n而，尽管如此，大语言模型对知识的记忆能力仍然受到其模型架构和训练范式的限制。根据文献\n[410] 的研究，模型在预训练数据中需要对同一知识点进行多达 1000 次的曝光，才能较为准确地\n记忆该知识点。根据 LLMEV AL-3[411] 评测结果，GPT-4 Turbo 在本科低年级知识点记忆能力测试\n中的表现仅为 73.6%。 这表明， 即便是参数量巨大的模型， 其知识记忆效率依然较低， 且难以完全\n覆盖所有领域的知识点。\n此外，大语言模型的性能很大程度上依赖于训练期间所接触到的静态数据。这种依赖性导致\n模型在面对实时更新的信息、长尾知识（即训练数据中罕见或未出现的知识）以及动态变化的内\n容时，往往表现出明显的局限性。例如，当模型需要处理最新的科技进展、时事新闻或特定领域\n的专业知识时，其生成结果可能出现错误、不完整甚至虚构的现象。这种现象被称为大语言模型\n的“幻觉”问题，是当前大语言模型研究领域的一大挑战。",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "82 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n被重复多次，并使用这个数据集训练了一组模型。研究发现了一个强烈的双峰下降现象，即重复\n数据可能会导致训练损失在中间阶段增加。 例如， 通过将0.1% 的数据重复 100 次， 即使其余90%\n的训练数据保持不变，一个参数量为 800M 的模型的性能也可能降低到与参数量为 400M 的模型\n相同。此外，研究人员还设计了一个简单的复制评估，即将《哈利·波特》 （Harry Potter）的文字\n复制 11 次，计算模型在该段上的损失。在仅有 3% 的重复数据的情况下，训练过程中性能最差的\n轮次仅能达到参数量为其 1/3 的模型的效果。\n文献 [14] 对大语言模型的记忆能力进行分析，根据训练样例在训练数据中出现的次数，显示\n了记忆率的变化情况， 如图3.10 所示。 可以看到， 对于在训练中只见过一次的样例，PaLM 模型的\n记忆率为 0.75%，而其对见过 500 次以上的样例的记忆率超过 40%。这也在一定程度上说明重复\n数据对于语言模型建模具有重要影响。这也可能进一步影响使用上下文学习的大语言模型的泛化\n能力。 由于Pa...",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "能力。 由于PaLM 模型仅使用了文档级别过滤， 因此片段级别 （100 个以上词元） 可能出现非常高\n的重复次数。\n图 3.10 大语言模型记忆能力评测 [14]\n3.3.3 数据多样性\n来自不同领域、使用不同语言、应用于不同场景的训练数据具有不同的语言特征，包含不同\n语义知识。通过使用不同来源的数据进行训练，大语言模型可以获得广泛的知识。表 3.2 给出了\nLLaMA 模型训练所使用的数据集。可以看到，LLaMA 模型训练混合了大量不同来源的数据，包\n括网页、代码、论文、图书等。针对不同的文本质量，LLaMA 模型训练针对不同质量和重要性的\n数据集设定了不同的采样概率，表中给出了不同数据集在完成 1.4 万亿个词元训练时的采样轮数。"
      ],
      "response_time": 0.9172782897949219,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.5,
        "rouge2": 0.33333333333333337,
        "rougeL": 0.5,
        "bleu": 0.013038525962482981,
        "semantic_similarity": 0.501522421836853
      }
    },
    {
      "sample_id": "0143",
      "question": "检索增强生成的映射关系是基于什么建立的？",
      "ground_truth": "基于 D 建立从 Q 到 A 的映射关系。",
      "predicted_answer": "检索增强生成的映射关系是基于给定的数据D建立的，该过程可以形式化定义为一个函数f : Q × D − → A，其中Q表示用户输入（查询），A表示期望的响应，D表示给定的数据。[2]",
      "ground_context": [
        "应用f 的任务是基于 D 建立从 Q 到 A 的映射关系。"
      ],
      "retrieved_contexts": [
        "9.1 检索增强生成基础 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 315\n\u001a\u000f\u0012\u000f\u0012ࡏ316",
        "检索增强生成整个过程也可以形式化定义为：\nf : Q × D − → A (9.1)\n其中，Q、A 和 D 分别代表用户输入（查询） 、期望的响应（答案）以及给定的数据。应用f 的任\n务是基于 D 建立从 Q 到 A 的映射关系。\n检索增强生成因其强大的知识整合与生成能力，在智能问答、知识管理、内容生成、个性化\n推荐、辅助决策以及教育培训等领域得到了广泛应用。以 RAG 技术为核心的 AI 搜索自 2023 年\n以来呈现出爆发式增长，迅速受到广泛欢迎，正逐渐成为人们获取信息的重要工具。与传统搜索\n引擎相比，AI 搜索能够以更加智能化的方式精准理解用户需求，为用户提供个性化、上下文相关\n且高效的搜索体验。不再仅仅是一个“信息检索工具” ，AI 搜索正在被视为一种“答案引擎” ，能\n够直接生成具有深度分析和语义理解的精确答案，从而极大地提升了用户体验。\n2023 年，全球多家知名科技企业相继推出了基于大语言模型的 AI 搜索产品，为这一领域注\n入了强劲动力。例如，微软推出的Bing AI 在结合大语言模型和 RAG 技术的基础上，显著扩展了\n传统搜索的功能；Perplexity AI 借助...",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "9. 检索增强生成\n随着大语言模型的规模不断扩大，其在生成自然语言与解决复杂任务上的能力取得了显著进\n步。 然而， 模型的性能仍然受限于训练期间所接触到的静态数据。 这种局限性使其在处理实时信息、\n长尾知识以及动态更新的领域时显得力不从心。因此，如何通过外部知识检索来增强大语言模型\n的能力， 成为了当前研究和应用的热点方向。 检索增强生成技术通过在推理过程中引入外部知识库\n或搜索引擎， 使语言模型能够动态获取所需的信息， 而不再完全依赖于模型参数。 这种方法不仅显\n著提升了模型在知识覆盖广度、 准确性和时效性方面的表现， 还在解决模型 “幻觉” （Hallucination）\n问题上展现出重要作用。\n本章将深入探讨检索增强生成的核心思想与实现方式，包括检索增强的框架设计、检索模块\n与生成模块的协作机制，以及如何将检索增强方法应用于具体任务场景。同时，我们还将分析当\n前技术的优势与局限，探讨未来可能的研究方向和优化策略。\n9.1 检索增强生成基础\n随着大语言模型参数规模的不断扩大以及训练数据量的显著增长，其知识记忆能力与模型性\n能得到了快速提升。这些模型在自然语言处理、推理和生成任务中展...",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n..."
      ],
      "response_time": 1.631744146347046,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.5,
        "rouge2": 0.2,
        "rougeL": 0.5,
        "bleu": 0.03332244133999108,
        "semantic_similarity": 0.6945767998695374
      }
    },
    {
      "sample_id": "0144",
      "question": "Modular RAG 系统由多少个模块组成？",
      "ground_truth": "多个独立但紧密协作的模块",
      "predicted_answer": "Modular RAG 系统由多个独立但紧密协作的模块组成[5]，具体的模块数量未在文档中明确指出。因此，根据提供的文档，我无法给出确切的模块数量。",
      "ground_context": [
        "Modular RAG 系统由多个独立但紧密协作的模块组成， 每个模块负责处理特定的功能或任务。"
      ],
      "retrieved_contexts": [
        "324 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n其架构分为三个层级： 顶层聚焦于RAG 的关键阶段， 将每个阶段视为独立模块， 同时引入一个编\n排模块来协调 RAG 流程； 中层由每个模块内的子模块组成， 进一步细化和优化各项功能； 底层由\n操作的基本单元（即操作符）构成。在模块化 RAG 框架中，RAG 系统可以通过计算图的形式表\n示，其中节点代表具体的操作符。\n本章将重点介绍 Modular RAG 框架下的各模块，包括：索引、检索前优化、检索、检索后优\n化、生成以及编排。\n9.2.1 索引模块\n索引（Index）是 RAG 系统中至关重要的过程，其核心任务是将文档划分为可管理的 片段\n（Chunk），也成为“块” ，为后续的检索和生成提供组织良好的内容基础。片段切分是将文档拆分\n为更小的、可管理的、语义完整的信息单元的过程，其构建需要综合考虑内容的语义特性、上下\n文完整性以及检索和生成的实际需求。在构建片段时， 首先需要确定片段的大小（长度） 。 片段的\n大小通常用字符数、单词数或句子数来衡量，具体取决于任务要求和模型的能力。\n较大的片段在构建时能够捕获更多上下...",
        "大语言模型：从理论到实践 339\n模型或来源异构的场景。在这些复杂情况下，RRF 能够显著提升预测的准确性和整合效果，成为\n多分支融合的重要工具。\n9.3 RAG 系统设计模式\n基于 Modular RAG 的设计，各种模式通过模块化操作符之间的协作形成了模块的工作流，称\n为 RAG 流 （RAG flow） 。RAG 流可以被分解为由子函数组成的图结构， 通过控制逻辑， 这些操作\n符可以按照预定的管道线执行，同时在必要时支持条件判断、分支或循环操作。通过深入分析现\n有的 RAG 方法，这些模式的模块化特性使其能够灵活适应多样化的场景需求，同时提高了 RAG\n系统的设计效率和扩展性。\n本章将介绍典型的 RAG 系统模式，包括线性模式、条件模式、分支模式、循环模式等。\n9.3.1 线性模式\n在 RAG 系统中，线性模式是最简单且最常用的工作流模式，其流程可以分为几个核心模块，\n包括预检索（Pre-Retrieval） 、检索、后检索（Post-Retrieval）以及生成模块，如图9.10所示。当预\n检索和检索后处理模块缺失时， 线性模式会简化为朴素检索增强生成 （Naive RAG） ...",
        "大语言模型：从理论到实践 323\n仅使系统设计和维护变得更加困难，也对满足多样化的应用需求提出了更高的要求。例如，RAG\n系统需要整合多种数据类型 （如半结构化数据和结构化数据） ， 以提供更丰富的知识背景和更可靠\n的知识验证能力。同时，系统的复杂性增加，也使得维护和调试变得更加困难，要求快速定位和\n优化特定组件。此外，随着系统中神经网络组件的增加，组件间的高效协作变得至关重要，而工\n作流的合理编排与调度对于提升系统效率和实现预期效果同样具有重要意义。\n为了解决这些挑战，并满足日益增长的多样化需求，同济大学王昊奋教授团队借鉴了模块化\n设计的思想， 提出了模块化检索增强生成架构（Modular RAG）[421]， 如图9.3所示。 模块化设计已\n成为现代计算系统的基础模式，它通过拆分系统功能，将复杂性分解为可独立管理的模块，从而\n提升系统的可扩展性和可维护性。在 Modular RAG 架构中，通过灵活的模块组合与流程控制，不\n仅能够提升任务执行效率，还可以更好地适应不同的应用场景。这种架构为解决 RAG 系统在设\n计、 管理和维护中面临的复杂性问题提供了一个有效的解决方案， 也是未来...",
        "能力的依赖程度也各不相同。例如，向量化模块需要借助预训练模型生成高质量的嵌入向量，以\n捕捉文本的深层语义特征；查询转换模块可能需要结合提示工程、或模型微调的方式，生成更精\n准的检索查询；而重排序模型则需要在结合用户输入和上下文的基础上，优化排序策略以提高最\n终输出的质量。 因此， 如何高效地选择、 训练和集成这些模型， 成为构建高性能RAG 系统的关键。\n本节将按照 Modular RAG 架构中模块划分，介绍典型的 RAG 系统中各模块所采用算法和优\n化方法。\n9.4.1 文本嵌入模型微调\n文本嵌入（Text Embedding）是一种将文本转换为固定维度向量（通常是高维浮点数组）的\n技术，旨在以数学形式捕捉语言的语义信息，并将其映射到向量空间中。通过深度学习模型（如\nWord2V ec、GloV e、FastText，以及基于 Transformer 的模型如 BERT、Sentence-BERT 和 OpenAI\n的 text-embedding-ada 等） ，文本的语义、语法及上下文特征能够被有效编码为向量表示。在RAG",
        "检索器优化\nNegative PairRetrieverLM-SupervisedLLM\n编排\nGenerate\nKnowledge Graph\nRetrieve\nQuery\nScheduleReasoning Pathrr\n知识指南\n12 3QueryGenerateOutput\nRetrieve\nJudgeJudge调度\nNeed Retrieval ?Retrieval Again ?Only rely on retrieved chunksAllow LLM’s own knowledge\nSemantic Analysis\nHard PromptSoft Prompt\nQuery\nPipeline1Pipeline2Judge\n路由\n检索器源SentenceDocumentChunks\nChunks\nChunks\nCodeTable\nEntitySub-GraphTripletstructured\ngranularity\n图 9.3 模块化检索增强生成（Modular RAG）架构 [421]\nModular RAG 系统由多个独立但紧密协作的模块组成， 每个模块负责处理特定..."
      ],
      "response_time": 1.3341646194458008,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.13634738360240478,
        "semantic_similarity": 0.6735259294509888
      }
    },
    {
      "sample_id": "0145",
      "question": "显性事实查询的答案通常存在于什么形式的文档中？",
      "ground_truth": "显性事实查询的答案通常直接存在于特定领域的文档或文档片段中，以明文形式呈现。",
      "predicted_answer": "显性事实查询的答案通常直接存在于特定领域的文档或文档片段中，以明文形式呈现 [1]。",
      "ground_context": [
        "显性事实查询的答案通常直接存在于特定领域的文档或文档片段中，以明文形式呈现。"
      ],
      "retrieved_contexts": [
        "大语言模型：从理论到实践 319\n1. 显性事实查询\n显性事实查询是检索增强查询中最简单的一类。这类查询的答案通常直接存在于特定领域的\n文档或文档片段中， 以明文形式呈现， 无需复杂的推理或逻辑分析即可解答。例如， “֊ն࿐Ⴕ\n཮౵”这样的问题，模型仅需从外部数据中找到答案并返回。对于这一层级的查询，模型的\n主要任务是准确地定位和提取相关信息，从而生成准确的响应。这种查询形式对数据的检索效率\n和精度有较高要求，但生成过程本身相对简单，更多依赖于数据的可用性和检索机制的有效性。\n显性事实问题也是 RAG 系统中占比最大的问题， 有大量用户查询词都属于此类型， 例如： “ᇏ\n൞ଧ่Ĥ” 、 “ഒĤ” 、 “৘čNyquist’s TheoremĎ\n൞൉હĤ” 、 “ഒĤ”等等。\n显性事实查询主要依赖于正确的数据检索，以便大语言模型能够生成准确的响应。由于其高\n效性、灵活性和相对较低的成本，检索增强生成技术成为处理此类查询的最常用解决方案。然而，\n即使采用 RAG 技术， 构建一个稳健且高质量的系统仍面临诸多挑战， 包括：1） 数据处理， 例如外\n部数据通常高度非结构化，包含表格、图像、视频...",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "别查询复旦大学计算机学院和法学院的地址，并在此基础上进行对比才能完整作答； “ACL 2024\nં໓ᇏႵଧུษંਔ RAG ௟ҩ໙ีĤ” ，需要系统能够检索ACL 2024 年的所有与 RAG\n相关论文，并分析检索到的所有相关论文，才能从中生成和 RAG 评测相关论文列表。\n在隐性事实查询中， 尽管问题仍然围绕事实展开， 但答案并未明确出现在单一文本片段中， 而\n是需要通过常识推理将多个事实结合起来得出结论。 处理这类查询的主要挑战包括：1） 自适应检\n索量， 不同问题可能需要检索不同数量的上下文， 固定检索数量可能导致信息冗余或信息不足；2）\n推理与检索之间的协调，推理可以引导需要检索的重点，而检索到的信息又能够迭代优化推理策",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "部数据通常高度非结构化，包含表格、图像、视频等多模态内容，同时对数据进行分段或“分块”\n时需要尽可能保持原始上下文和语义的完整性；2) 数据检索，即从大规模非结构化数据集中高效\n检索相关内容可能计算成本高昂且容易出错，这需要开发高效而精准的检索机制；3) 评估，尤其\n是在组件级别对 RAG 系统性能进行评估是一项复杂任务，需要设计健全的指标来准确衡量数据\n检索和响应生成的质量。\n2. 隐性事实查询\n隐性事实查询涉及信息之间不直接显现的数据关系，通常需要一定程度的常识推理或基本逻\n辑推导。这类查询要求从多个文档片段中收集和处理信息，而这些信息可能分散在文档集合中的\n不同部分。由于单次检索可能无法满足信息需求，往往需要将原始查询分解为多个检索操作，并\n将结果聚合为一个完整的答案。这类查询通常涉及常识推理，但不需要特定领域的专业知识，常\n见的任务类型包括统计查询、描述性分析查询和基本的聚合查询。例如， “有多少” “哪个是最多”\n类型的问题通常需要执行计数、比较、趋势分析和选择性总结，而多跳推理也是此类任务中的常\n见任务。\n隐性事实的典型问题包括： “཮౵ગĤ” ， 该问题需要分\n别查询复旦..."
      ],
      "response_time": 0.9254207611083984,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.845706314438832,
        "semantic_similarity": 0.9794527292251587
      }
    },
    {
      "sample_id": "0146",
      "question": "如何通过查询改写和技能添加来提升大语言模型在复杂问题求解中的表现？",
      "ground_truth": "查询改写可以通过对用户的原始查询进行语义优化和结构调整，增强检索效率，并确保生成内容与用户需求的高度匹配。与此同时，为智能体添加技能，比如多模态插件，可以扩展模型的功能，使其能够处理多样化的信息，从而提升整体表现。",
      "predicted_answer": "查询改写可以通过对用户的原始查询进行语义优化、语言简化或结构调整，使其更加明确和精确，便于模型识别核心意图并生成相关答案，从而提升系统的理解和检索效果[1][2]。这有助于减少语言歧义，增强检索效率，并确保生成的答案更加可靠和可信。同时，通过引入验证机制，如“验证链”（Chain-of-Verification, CoV），可以让大语言模型对扩展生成的子查询及其结果进行逐步验证，进一步提高模型在复杂问题求解中的表现[1]。",
      "ground_context": [
        "查询转换（Query Transformation）又称查询改写（Query Rewrite），是指通过对用户的原始查询进行改写或重构，将其转换为更适合检索和生成的形式，从而提升系统的理解能力和检索效果。",
        "如果模型能力能覆盖智能体功能，则仅需编写提示词；否则需添加技能拓展能力。例如，文本类模型无法处理多模态内容，可绑定多模态插件理解PPT、图片等。"
      ],
      "retrieved_contexts": [
        "大语言模型：从理论到实践 329\n选择并行执行以提高效率，或按顺序逐步解决以保持逻辑一致性。\n在子查询生成后， 为确保结果的准确性， 可以引入验证机制， 例如 “验证链” （Chain-of-V erification,\nCoV e）[424]。通过让大语言模型对扩展生成的子查询及其结果进行逐步验证，能够有效减少生成\n内容与真实情况不符的问题。这种方法确保了子查询的输出质量，使得最终的答案不仅与用户需\n求高度相关，而且更加可靠和可信，从而显著提升模型在复杂问题求解中的表现。\n2. 查询转换\n查询转换（Query Transformation）又称查询改写（Query Rewrite） ，是指通过对用户的原始查\n询进行改写或重构， 将其转换为更适合检索和生成的形式， 从而提升系统的理解能力和检索效果。\n这种方法通常对用户输入的查询进行语义优化、语言简化或结构调整，使其更加明确和精确，便\n于模型识别核心意图并生成相关答案。例如，将模糊或冗长的查询改写为短小精炼的关键词形式，\n或者将复杂的问题分解为更易处理的结构化查询。 通过这种方式， 查询变形能够减少语言歧义， 增\n强检索效率，并确保生成...",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "大规模语言模型：从理论到实践 161\n图 5.3 LLaMA 65B 模型使用 LIMA [42] 训练效果对比\n文献 [190] 研究也表明， 在模型构建过程中， 数据工程起着至关重要的作用， 可以通过提升数\n据集的多样性，显著增强模型的泛化能力。训练数据多样性的提升，可以从多个方面着手，例如\n使用来自不同源头、具备不同特征且呈现不同分布的数据。此外，实验结果也说明，在数据选择\n环节，多样性有着不可忽视的作用。对比随机选择、均匀选择这两种常见方式，具备多样性的数\n据选择策略展现出明显优势。此外，相较于单纯聚焦于挑选高质量数据，若能将数据质量与多样\n性标准有机结合，模型也可以达到更好的效果[223]。\n在问答任务方面，大语言模型的预训练依托于多样化的语料库来开展，这些语料库包含了多\n种类型的内容，并且涵盖了丰富的世界知识。大语言模型在预训练完成后，大量的知识被编码进\n了模型的参数之中。而通过监督微调的方式，就能够把这些已经编码进参数的知识有效地应用于\n问答任务里。然而，针对大语言模型的问答任务能力提升，存在着三个亟待解决的关键问题： （1）\n指令微调阶段，究竟需要多少数据量，才能使大语...",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "210 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n解决数据受限的问题。大语言模型可通过强化学习中的奖励机制，学习如何探索不同的推理路径，\n从而扩大训练数据的范围。但此前的相关研究成果未达到理想的竞争效果，Kimi k1.5 旨在探索一\n条更有效的技术路线。\n1. 技术路线\n强化学习提示数据集构建： RL 提示数据集的质量和多样性对强化学习的有效性至关重要。\nKimi k1.5 定义了高质量 RL 提示集的三个关键属性：\n多样覆盖：提示应涵盖 STEM、编程和一般推理等广泛学科，以增强模型的适应性和跨领域\n应用能力。为此，采用自动过滤器选择需要丰富推理且易于评估的问题，数据集来源广泛，包括\n不同领域的问题以及纯文本和图像 - 文本问答数据。\n平衡难度：提示集应包含不同难度级别的问题，以促进模型的逐步学习并防止过度拟合。利\n用模型自身能力自适应评估提示难度， 通过SFT 模型多次生成答案计算通过率作为难度代理指标，\n并开发标签系统按领域和学科分类提示，实现难度平衡。\n准确评估能力：提示应能被验证者客观可靠地评估，确保基于正确推理衡量模型性能。为避\n免奖励操纵，排除易出现验证..."
      ],
      "response_time": 2.9834465980529785,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.18405583550718374,
        "semantic_similarity": 0.8243464231491089
      }
    },
    {
      "sample_id": "0147",
      "question": "在处理复杂查询时，RAG系统如何提高检索效果，并同时应对多模态数据的挑战？",
      "ground_truth": "RAG系统通过检索前优化模块改善检索效果，具体方法包括查询扩展，以丰富用户查询的内容并减少语言歧义。此外，随着系统支持多模态数据，跨模态对齐和生成能力的提升成为新的挑战，要求系统能够有效处理不同类型的数据并确保输出连贯且具有上下文相关性。",
      "predicted_answer": "在处理复杂查询时，RAG系统可以通过以下方式提高检索效果：首先将复杂查询拆解为可解答的子查询，分别检索与其相关的信息，并整合子查询的结果，生成对原始查询的完整回答[3]。这样不仅提升了检索效率，还能显著增强模型在复杂场景中的适应能力和表现。\n\n面对多模态数据的挑战，未来的研究可以集中于支持更多样化的媒体类型（如视频和语音），同时优化系统以提升其在大规模复杂场景中的性能，为RAG系统的进一步发展提供新的方向[5]。",
      "ground_context": [
        "预检索模块通过对用户查询进行重构、扩展或语义优化，能够减少语言歧义和表述模糊，从而为下游检索任务提供更精准的输入。",
        "多样化的数据类型需要统一的检索框架，而目前的跨模态检索策略尚不足以同时有效处理文本、图像以及潜在的视频或音频数据。"
      ],
      "retrieved_contexts": [
        "许多应用强大的能力。然而，RAG 系统在检索质量、系统效率与任务优化、多模态扩展等方面仍\n面临诸多挑战。解决这些问题对于推动 RAG 系统的发展、释放其全部潜力至关重要。\n1. 检索质量的挑战\n检索质量是 RAG 系统的核心， 因为它直接影响生成结果的相关性和连贯性。 然而， 现有检索\n技术在处理噪声时仍存在不足。RAG 系统经常会引入无关或误导性的文档， 这些噪声会干扰生成\n过程，导致虚假或不可靠的内容输出。源数据的质量问题对检索增强系统的性能也会产生重要影\n响。低质量数据中可能存在噪声、无关信息、错误、重复或矛盾内容，严重干扰知识提取的准确\n性和输出质量。此外，知识数据的整理过程也极具复杂性，需要处理复杂文件格式（如 PDF）的\n解析，探索合理的知识切分方式以避免主题内容被割裂，同时还需完成知识共享和问答对的生成\n等工作，以充分提高数据的利用效率和系统的响应能力。\n此外， 当检索阶段未能找到相关文档时， 生成模型往往仍尝试生成输出， 这可能导致错误或无\n意义的内容。特别是在查询模糊或表述不清时，这一问题尤为突出。为解决此问题， 像HyDE[36]\n这样的技术通过生成伪文档来更好地表...",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "9.4.2 查询优化\n如前所述，RAG 系统在处理用户查询时，需要对查询优化进行深入改进，以应对多种复杂挑\n战。对于简单查询，例如日常问候等无需上下文支持的情况，模型应避免执行不必要的信息检索，\n直接生成答案， 从而减少无关上下文对响应质量的影响。 对于复杂查询， 直接使用原始查询进行检\n索通常难以获取足够的相关信息。模型需要首先将复杂查询拆解为可解答的子查询，分别检索与\n其相关的信息，并整合子查询的结果，生成对原始查询的完整回答。而对于多义性较强的模糊查\n询，直接检索原始查询往往无法提供全面的答案。模型需通过识别用户意图来澄清查询内容，并\n构建精准的检索请求，获取相关信息后生成细致且全面的响应。通过优化查询流程，RAG 系统不\n仅能够提升检索效率，还能显著增强模型在复杂场景中的适应能力和表现。\n针对上述问题，文献 [440] 提出了 RQ-RAG 算法，旨在通过动态优化查询以提升检索增强生\n成的效果。该方法基于 7B 规模的 Llama2 模型，采用端到端训练，使其能够通过重写、分解和消\n除歧义来动态优化搜索查询。为了训练模型具备上述功能，核心是构建与推理过程相匹配的训练\n数据。为了...",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "一定进展。然而，随着数据集规模的扩大和查询复杂性的提升，扩展多模态检索和生成能力仍然\n是一个重大挑战。 未来研究可以集中于支持更多样化的媒体类型 （如视频和语音） ， 同时优化系统\n以提升其在大规模复杂场景中的性能，为 RAG 系统的进一步发展提供新的方向。\n9.2 模块化检索增强生成架构\n随着检索增强生成技术的发展， 系统功能日益复杂， 面临的挑战也愈加突出， 包括复杂数据源\n的整合、 系统的可解释性与可控性需求、 组件的选择与优化以及工作流的编排与调度。 这些问题不"
      ],
      "response_time": 3.091815233230591,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.4,
        "rouge2": 0.0,
        "rougeL": 0.4,
        "bleu": 0.032096251901527866,
        "semantic_similarity": 0.8568850159645081
      }
    },
    {
      "sample_id": "0148",
      "question": "如何在Coze平台上构建一个夸夸机器人，并确保其能够自动选择最佳的动作序列？",
      "ground_truth": "在Coze平台构建夸夸机器人可以通过简单的五个步骤完成。首先，创建一个智能体，输入名称和功能介绍，并编写提示词以定义人设与回复逻辑。这些提示词将影响智能体在会话中的回复效果。同时，智能体的核心思想是使用大语言模型作为推理引擎，以确定要采取的动作序列。通过将大语言模型与动作列表结合，智能体可以自动选择最佳的动作序列，从而实现自动化决策和行动。",
      "predicted_answer": "首先，您可以在Coze平台上按照以下步骤创建一个夸夸机器人：\n\n1. 创建智能体：登录Coze平台，点击左上角的“L”，输入智能体名称和功能介绍，通过生成图标或AI创建功能自动生成头像。进入智能体编排页面后，描述智能体身份和任务，配置扩展能力，并在预览与调试面板中测试其功能[3]。\n\n2. 编写提示词：明确模型角色和设计语言风格，确保对话内容符合用户预期。例如，“夸夸机器人”的提示词可以设定为夸奖他人的话语风格[1]。\n\n3. 添加技能（可选）：如果需要增强智能体的功能，比如当模型无法回答某些问题时，可以添加搜索引擎插件来辅助回答。具体操作是在技能区域单击插件功能对应的+图标，搜索并添加bingWebSearch插件[5]。\n\n关于自动选择最佳的动作序列，文档中并未提及具体的实现方法。因此，目前无法提供有关此方面的详细指导。不过，Coze平台可能提供了某种机制或算法来帮助智能体做出决策，但具体细节未在给定的文档中说明。",
      "ground_context": [
        "使用 Coze 平台可以通过以下简单的五个步骤就可以构造快速搭建一个“夸夸机器人”，并在多个平台提供对外服务。",
        "智能体的核心思想是使用大语言模型来选择要执行的一系列动作。在链中，操作序列是硬编码在代码中的。"
      ],
      "retrieved_contexts": [
        "调用插件或执行复杂任务；卡片功能则为智能体提供了信息展示和互动的新形式，让用户在网页\n或移动端直观查看数据、流程和结果。\n使用 Coze 平台可以通过以下简单的五个步骤就可以构造快速搭建一个“夸夸机器人” ，并在\n多个平台提供对外服务。\n步骤 1： 创建一个智能体。 在扣子平台创建智能体非常简单： 登录后， 点击页面左上角的 “L” ，\n输入智能体名称和功能介绍，并通过生成图标自动生成头像，或使用“AI 创建”功能，通过自然\n语言描述需求， 由平台自动生成智能体。 点击确认后， 进入智能体编排页面。 在这里， 可以通过左\n侧人设与回复逻辑面板描述智能体的身份和任务；利用中间技能面板为智能体配置扩展能力；在\n右侧预览与调试面板中实时测试智能体，确保其功能和交互效果符合预期。\n步骤 2：编写提示词。配置智能体的第一步是编写提示词，即定义智能体的人设与回复逻辑。\n这部分内容决定了智能体的基本人设，并持续影响其在所有会话中的回复效果。在设计提示词时，\n建议明确模型的角色、设计特定的语言风格，并限制回答范围，以确保对话内容符合用户的预期。\n例如，对于一个“夸夸机器人” ，提示词可以设置为：",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "312 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n模板，用户无需编写代码，甚至完全不需要编程能力，即可完成智能体的设计、开发和部署。这\n类平台通常集成了预训练的大语言模型，并提供强大的工具支持，如知识库管理、对话流程设计\n和外部 API 集成，极大地简化了开发流程。需要注意的是，这种方式在定制化能力、性能优化和\n扩展性上存在一定局限，复杂场景或高性能需求场景下适应程度需要详细评估。\nCoze（扣子）是一个大模型智能体开发平台，整合了插件、长短期记忆、工作流、卡片等丰\n富功能，能够以低门槛、快速搭建个性化或具备商业价值的智能体，并发布到豆包、飞书、网页\n等多种平台，实现全场景覆盖。通过模块化与高效的工具支持，Coze 帮助开发者快速构建、测试\n和部署智能体，实现复杂任务的自动化，同时提供强大的扩展和定制能力。其插件系统支持智能\n体与外部工具无缝对接，如数据库查询、第三方 API 调用、任务管理工具等，在多种环境中执行\n精准任务；长短期记忆功能让智能体在短期对话中保持上下文一致，并通过长期记忆存储重要信\n息，实现自然、智能的交互体验；工作流功能允许用户通过拖拽式界面快速设计...",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "大语言模型：从理论到实践 313\n#೤\n৯b\n##ି ###ି 1ׄ\n1.ྛᄡૅb\nޓ֥[ׄ]đбೂ [ׄ]b\n2.ྛᄡૅb\n൙౦൞൉હ଻Ĥ\n###ି 2঒଴\n1.็ᅞđ\n෱b୆ॖၛ [ၰ]b\n2.ৣb\n఼đ\nb\n###ି 3ճህြ໙ี\nႨ bingWebSearch ෆ෬ճσ\n## ཋᇅ\n-b\n-ေ౰b\n步骤 3： （可选）为智能体添加技能。如果模型能力能覆盖智能体功能，则仅需编写提示词；\n否则需添加技能拓展能力。例如，文本类模型无法处理多模态内容，可绑定多模态插件理解PPT、\n图片等。此外，模型缺乏垂直领域专业知识，若智能体涉及智能问答，还需添加专属知识库，以\n解决专业知识不足的问题。例如夸夸机器人，模型能力基本可以实现预期的效果。但如果希望为\n夸夸机器人添加更多技能，例如遇到模型无法回答的问题时，通过搜索引擎查找答案，那么可以\n为智能体添加一个必应搜索插件。\n1) 在编排页面的技能区域，单击插件功能对应的 + 图标。\n2) 在添加插件页面，搜索 bingWebSearch，然后单击添加，如图8.11所示。"
      ],
      "response_time": 5.984894752502441,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.15384615384615385,
        "rouge2": 0.0,
        "rougeL": 0.15384615384615385,
        "bleu": 0.07142475879944694,
        "semantic_similarity": 0.9046308994293213
      }
    },
    {
      "sample_id": "0149",
      "question": "RRR 方法在预检索阶段引入了什么模块？",
      "ground_truth": "查询重写模块",
      "predicted_answer": "RRR 方法在预检索阶段引入了查询重写模块，该模块是基于 T5-large 模型微调的小型可训练语言模型。[1]",
      "ground_context": [
        "文献 [425] 提出的“重写-检索-阅读” （Rewrite-Retrieve-Read，RRR）方法就是一个典型的线性 RAG 流模式。在预检索阶段，RRR 方法引入了查询重写模块，该模块是基于 T5-large 模型微调的小型可训练语言模型。"
      ],
      "retrieved_contexts": [
        "基本的检索和生成过程。 常见的线性RAG 流通过在预检索阶段引入查询变换模块 （比如重写或隐\n式文档扩展（HyDE）操作符） ，以及在检索后阶段使用排序模块来优化检索结果，从而提升最终\n生成的质量。\n查询词预检索\n检索\n后检索\n生成\n回答\n图 9.10 RAG flow 的线性模式 [421]\n文献 [425] 提出的“重写-检索-阅读” （Rewrite-Retrieve-Read，RRR）方法就是一个典型的线\n性 RAG 流模式。在预检索阶段，RRR 方法引入了查询重写模块，该模块是基于 T5-large 模型微\n调的小型可训练语言模型。该模块通过强化学习框架进行优化，将查询重写过程建模为一个马尔\n可夫决策过程（Markov Decision Process, MDP ） 。 在这一过程中， 查询重写模块以大语言模型的最\n终输出质量作为奖励信号，以此调整和优化生成的查询。具体而言，强化学习通过策略梯度方法\n对重写模块进行训练，使其生成的查询更符合检索任务的需求，提高检索和生成的整体效率和效\n果。在检索阶段，RRR 方法使用稀疏编码模型（如 BM25）作为检索工具，从外部知识库中获取...",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "328 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n9.2.2 检索前优化\n为了解决 RAG 系统直接依赖用户原始查询进行检索所带来的问题， 检索前优化 （Pre-retrieval\nProcessing）模块被设计用于优化查询输入，从而提高检索的有效性。用户查询往往存在两个主要\n挑战：查询措辞不当，问题可能过于复杂或语言组织不清晰，导致检索效果不佳；语言复杂性和\n歧义性， 尤其是在包含专业术语或多义缩写的情况下， 语言模型难以准确理解查询意图。 例如， 对\n于缩写“LLM” ，系统可能无法区分其是指“大语言模型” （Large Language Model）还是法律领域\n的“法学硕士” （Master of Laws） 。预检索模块通过对用户查询进行重构、扩展或语义优化，能够\n减少语言歧义和表述模糊， 从而为下游检索任务提供更精准的输入， 显著提升RAG 系统在复杂查\n询场景中的性能。\n本节将重点介绍预检索的核心模块，包括：查询扩展、查询转换以及查询组织。\n1. 查询扩展\n查询扩展（Query Expansion）是一种通过将单一查询扩展为多个查询的方法，用以丰富查询\n...",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "大语言模型：从理论到实践 341\n分支间执行不同的流程， 以应对复杂场景的多样化需求， 如图9.12所示； 后检索分支模式是分支间\n执行相同的 RAG 流程，用于生成多样化的结果，如图9.13所示。通过这样的结构，分支模式能够\n从多个角度生成和整合信息，从而提升系统的生成能力与结果质量，对多任务处理和复杂场景具\n有显著优势。\n预检索分支 （Pre-Retrieval Branching） 是一种通过生成多个子查询并并行检索的模式， 用于提\n高检索的全面性和生成结果的多样性。具体而言，该模式从一个初始查询开始，通过查询扩展模\n块将其扩展为多个子查询。每个子查询随后通过检索模块检索相关文档，形成文档集合。这些文\n档集合连同对应的子查询一起送入生成模块，生成答案集合。最终，这些生成的答案通过融合模\n块进行整合，形成最终结果。这种模式通过并行检索与生成，能够从多个角度充分挖掘潜在信息，\n从而提升生成结果的覆盖度和准确性。\n查询词\n检索\n生成\n回答查询扩展 融合检索\n生成\n检索\n生成\n生成\n图 9.12 RAG flow 的预检索分支模式 [421]\n后检索分支（Post-Retrieval B..."
      ],
      "response_time": 1.1718273162841797,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.0406422493756618,
        "semantic_similarity": 0.6848389506340027
      }
    },
    {
      "sample_id": "0150",
      "question": "后检索分支模式的特点是什么？",
      "ground_truth": "后检索分支的特点在于单一查询驱动的检索过程，而并行生成则聚焦于对不同文档块的独立处理。",
      "predicted_answer": "后检索分支模式的特点是从单一查询开始，通过检索模块获取多个文档块，每个文档块被独立送入生成模块进行处理，生成对应的结果集合。随后，这些生成的结果通过合并模块进行整合，形成最终结果。这种模式适用于需要从同一查询结果中挖掘多角度信息的场景，能够充分利用检索到的内容，提高生成结果的多样性和质量。[1][5]",
      "ground_context": [
        "后检索分支的特点在于单一查询驱动的检索过程，而并行生成则聚焦于对不同文档块的独立处理。"
      ],
      "retrieved_contexts": [
        "后检索分支（Post-Retrieval Branching）模式则从单一查询开始，通过检索模块获取多个文档\n块。每个文档块被独立送入生成模块进行处理，生成对应的结果集合。随后，这些生成的结果通\n过合并模块进行整合，形成最终结果。与预检索分支不同，后检索分支的特点在于单一查询驱动\n的检索过程，而并行生成则聚焦于对不同文档块的独立处理。该模式适合需要从同一查询结果中\n挖掘多角度信息的场景，能够充分利用检索到的内容，提高生成结果的多样性和质量。",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "大语言模型：从理论到实践 341\n分支间执行不同的流程， 以应对复杂场景的多样化需求， 如图9.12所示； 后检索分支模式是分支间\n执行相同的 RAG 流程，用于生成多样化的结果，如图9.13所示。通过这样的结构，分支模式能够\n从多个角度生成和整合信息，从而提升系统的生成能力与结果质量，对多任务处理和复杂场景具\n有显著优势。\n预检索分支 （Pre-Retrieval Branching） 是一种通过生成多个子查询并并行检索的模式， 用于提\n高检索的全面性和生成结果的多样性。具体而言，该模式从一个初始查询开始，通过查询扩展模\n块将其扩展为多个子查询。每个子查询随后通过检索模块检索相关文档，形成文档集合。这些文\n档集合连同对应的子查询一起送入生成模块，生成答案集合。最终，这些生成的答案通过融合模\n块进行整合，形成最终结果。这种模式通过并行检索与生成，能够从多个角度充分挖掘潜在信息，\n从而提升生成结果的覆盖度和准确性。\n查询词\n检索\n生成\n回答查询扩展 融合检索\n生成\n检索\n生成\n生成\n图 9.12 RAG flow 的预检索分支模式 [421]\n后检索分支（Post-Retrieval B...",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "342 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n查询词\n生成\n回答融合检索\n生成\n生成\n生成\n查明变换\n图 9.13 RAG flow 的后检索分支模式 [421]\n9.3.4 循环模式\n循环模式的核心是检索与生成步骤之间的相互依赖性。 循环模式通过引入调度模块进行控制，\n确保系统可以根据需要在特定模块之间重复执行某些操作。 这一模式可以被抽象为一个有向图， 其\n中节点代表系统的各个模块，边表示模块之间的控制流或数据流。当一个模块能够返回到之前的\n模块时，该系统就形成了一个循环结构。这种循环设计允许系统在流程中对某些步骤进行重复优\n化，从而提升任务的完成效果。\n循环模式的关键在于判断模块（Judge Module） ，用于决定流程是否需要返回到之前的模块或\n继续向下执行。例如，当一个模块完成后，判断模块可以决定是进入下一个模块还是返回到前置\n模块。如果系统决定返回，则执行循环操作；如果系统决定不返回，则流程继续向前。这种灵活\n的控制机制使得循环模式能够动态调整整个流程，从而提高系统的适应性、灵活性以及对复杂任\n务的处理能力。\n循环模式可以进一步细分为三种类型：迭代型、..."
      ],
      "response_time": 2.2323379516601562,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.02881486821758421,
        "semantic_similarity": 0.8287876844406128
      }
    },
    {
      "sample_id": "0151",
      "question": "权重 λ(d, q) 是由什么确定的？",
      "ground_truth": "权重 λ(d, q) 由文档 d 和输入查询 q 之间的相似度得分确定。",
      "predicted_answer": "权重 λ(d, q) 由文档 d 和输入查询 q 之间的相似度得分确定 [2][5]。该权重使用 softmax 函数进行归一化，确保所有权重之和为 1。",
      "ground_context": [
        "权重 λ(d, q) 由文档 d 和输入查询 q 之间的相似度得分确定。"
      ],
      "retrieved_contexts": [
        "权重 λ(d, q) 由文档 d 和输入查询 q 之间的相似度得分确定。该权重使用 softmax 函数来计算，以\n确保权重经过归一化且总和为 1。\nλ(d, q) = es(d,q)\nP\nd∈Dq es(d,q) (9.9)\n倒数排名融合 （Reciprocal Rank Fusion，RRF） 是一种集成技术， 专门用于将多个检索结果的\n排名整合为统一的列表。它通过一种定制的加权平均方法，增强了整体预测性能与排名精度 [433]。\nRRF 的核心优势在于其动态的权重分配机制，基于分支之间的相互作用进行调整，特别适合处理",
        "全面丰富，又能准确反映问题的多维特性。融合模块的引入，使系统在应对复杂查询时能够提供\n更加综合且连贯的回答，大幅提升了整体的适应能力与输出质量。融合模块主要包含大语言模型\n融合、加权继承以及倒数排名融合等方法。\n大语言模型融合是多分支信息整合的直接方法之一， 利用大语言模型强大的分析与整合能力，\n将不同分支的信息进行统一处理。然而，这种方法面临一些挑战，特别是在处理超出大语言模型\n上下文窗口限制的长答案时。为了缓解这一问题，通常会先对每个分支的答案进行摘要提取，提\n炼关键内容后再输入 LLM， 从而在长度限制内保留最重要的信息。 这种方法确保了答案的完整性\n与精确性，即使在处理复杂的多分支生成时也能提供高质量的整合结果。\n加权集成是一种基于多分支生成结果的加权选择方法，通过不同分支生成的词元（token）的\n加权值来综合选择最终输出。具体而言，权重是通过文档与输入查询的相似度得分计算的，使用\nSoftmax 函数对权重进行归一化，确保所有权重之和为 1。该方法可按如下公式计算：\np(y|q, Dq) =\nX\nd∈Dq\np(y|d, q) · λ(d, q) (9.8)\n权重 λ(d, ...",
        "380 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n该方法通过网格搜索确定重参数化系数，从而有效最小化重构误差，增强了对关键权重的保留能\n力。OWQ[495] 针对与激活异常值相关的权重难以量化的问题， 提出了一种混合精度量化策略。 该方\n法通过识别权重矩阵中的 “弱列” ， 为这些关键权重分配更高的精度， 同时对其余权重以较低精度\n进行量化，从而在性能和效率之间达成平衡。SpQR[496] 专注于在量化过程中识别权重异常值，并\n为这些异常值分配更高的精度，而将其余权重量化为 3 比特。这种选择性高精度处理的方法减少\n了关键权重的量化误差， 有效提升了模型性能。QuantEase[497] 在每一层的量化过程中，QuantEase\n提出了一种基于坐标下降的优化方法，以更精确地补偿未量化的权重。此外， QuantEase 可以利\n用 GPTQ 生成的量化权重作为初始化点， 并在此基础上进一步优化补偿过程， 提高了量化的效果。\nAffineQuant[498] 则首次将等效仿射变换引入量化过程，扩展了优化的搜索空间。这种方法能够更\n全面地拟合权重分布，从而显著降低量化误差，为...",
        "大语言模型：从理论到实践 349\n所示：\nL = max\nM\nE(x,y)∼D[log pM (y|q1, d1, d2, . . . , qi, di, x) (9.15)\n其中，L 代表试图最大化的概率值，M 表示模型参数， 期望E(x,y)∼D 是对数据集 D 求平均，pM (y|q1, d1, q2, d2, . . . , qi, di, x)\n表示在给定输入 x、第 i 步经过优化的查询 qi 以及检索到的文档 di 的情况下，模型 M 生成回复\ny 的概率。\nRQ-RAG 在推理过程中采用了一种树形解码策略，其具体流程如图9.18所示。在每个时间步，\n模型可以根据需要对查询进行重写、 分解、 消除歧义， 或直接生成回答。 通过特殊标记的引导， 该\n策略能够控制解码路径的扩展，并以“生成 → 检索 → 生成 → 检索 →⋯⋯→ 答案”的循环过程\n逐步展开。在每次迭代中，模型会根据任务需求生成不同类型的搜索查询，例如重写、分解或消\n歧查询。这些查询将被用于检索与其对应的上下文信息，从而形成不同的解码路径。基于设定的\n探索宽度和深度范围，RQ-RAG 能够生成多条候选轨迹，通过...",
        "INT8GEMM/GEMV\n量化\n权重\n激活值\nINT8累加器\n逆量化\n（INT8）\n（FP16）\nINT32偏置\n激活值\n（INT8）\n（FP16）\n图 10.6 权重-激活量化流程 [471]"
      ],
      "response_time": 1.3762516975402832,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.6666666666666666,
        "rouge2": 0.6,
        "rougeL": 0.6666666666666666,
        "bleu": 0.4617258018429086,
        "semantic_similarity": 0.8820874691009521
      }
    },
    {
      "sample_id": "0152",
      "question": "文献[440] 采用了与哪些方法类似的方式来生成高质量的大规模数据？",
      "ground_truth": "与 Self-RAG[431] 和 SAIL[441] 类似的方法",
      "predicted_answer": "文献[440] 采用了与 Self-RAG[431] 和 SAIL[441] 类似的方法来生成高质量的大规模数据。[1]",
      "ground_context": [
        "文献[440] 采用了与 Self-RAG[431] 和 SAIL[441] 类似的方法，设计了一套自动化的数据生成流程，以优化查询、检索信息并生成精确的响应，同时减少人工干预所需的资源和时间成本。"
      ],
      "retrieved_contexts": [
        "数据。为了生成高质量的大规模数据，文献[440] 采用了与 Self-RAG[431] 和 SAIL[441] 类似的方法，\n设计了一套自动化的数据生成流程，以优化查询、检索信息并生成精确的响应，同时减少人工干\n预所需的资源和时间成本。",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "486 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n[190] Qin Y , Y ang Y , Guo P , et al. Unleashing the power of data tsunami: A comprehensive survey\non data assessment and selection for instruction tuning of language models[J]. arXiv preprint\narXiv:2408.02085, 2024.\n[191] Mishra S, Arunkumar A, Sachdeva B, et al. Dqi: Measuring data quality in nlp[J]. arXiv preprint\narXiv:2005.00816, 2020.\n[192] Shannon C E. A mathematical theory of communication[J]. The Bell system technical journal,\n1948, 27(3):379-423.\n[193]...",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "512 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n[528] Hsieh C Y , Li C L, Y eh C K, et al. Distilling step-by-step! outperforming larger language models\nwith less training data and smaller model sizes[J]. arXiv preprint arXiv:2305.02301, 2023.\n[529] Micikevicius P , Stosic D, Burgess N, et al. Fp8 formats for deep learning[J]. arXiv preprint\narXiv:2209.05433, 2022.\n[530] Peng H, Wu K, Wei Y , et al. Fp8-lm: Training fp8 large language models[J]. arXiv preprint\narXiv:2310.18313, 2023.\n[531] Wang R, Gong Y , Liu..."
      ],
      "response_time": 1.1567585468292236,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.8333333333333333,
        "rouge2": 0.8,
        "rougeL": 0.8333333333333333,
        "bleu": 0.45637960572361486,
        "semantic_similarity": 0.7966176271438599
      }
    },
    {
      "sample_id": "0153",
      "question": "RAG 系统可以通过哪种方式直接进行判断？",
      "ground_truth": "RAG 系统还可以通过大语言模型直接进行判断（LLM Judge）。",
      "predicted_answer": "RAG 系统可以通过大语言模型直接进行判断（LLM Judge），这种方式包括两种主要方法：一种是利用大语言模型的上下文学习能力，通过精心设计的提示来进行决策；另一种是对大语言模型进行微调，使其生成特定的触发标记来直接控制模型的行为。[3]",
      "ground_context": [
        "RAG 系统还可以通过大语言模型直接进行判断（LLM Judge） 。"
      ],
      "retrieved_contexts": [
        "等） ， 评估数据集能够更全面地展现系统在不同领域中的适应性和泛化能力。 这种多样化设计确保\n了 RAG 系统在广泛应用场景中的可靠性和实用性。\n9.5.4 评估指标\n评估指标是衡量 RAG 系统性能的核心工具，直接影响评估结果的可信度和系统优化的方向。\n在评估 RAG 系统时，需要对各种评估指标有深入的理解，以便准确衡量评估目标。由于 RAG 系\n统由检索组件和生成组件构成，以下从检索模块、生成模块和整体系统三个层面展开详细讨论。",
        "大语言模型：从理论到实践 339\n模型或来源异构的场景。在这些复杂情况下，RRF 能够显著提升预测的准确性和整合效果，成为\n多分支融合的重要工具。\n9.3 RAG 系统设计模式\n基于 Modular RAG 的设计，各种模式通过模块化操作符之间的协作形成了模块的工作流，称\n为 RAG 流 （RAG flow） 。RAG 流可以被分解为由子函数组成的图结构， 通过控制逻辑， 这些操作\n符可以按照预定的管道线执行，同时在必要时支持条件判断、分支或循环操作。通过深入分析现\n有的 RAG 方法，这些模式的模块化特性使其能够灵活适应多样化的场景需求，同时提高了 RAG\n系统的设计效率和扩展性。\n本章将介绍典型的 RAG 系统模式，包括线性模式、条件模式、分支模式、循环模式等。\n9.3.1 线性模式\n在 RAG 系统中，线性模式是最简单且最常用的工作流模式，其流程可以分为几个核心模块，\n包括预检索（Pre-Retrieval） 、检索、后检索（Post-Retrieval）以及生成模块，如图9.10所示。当预\n检索和检索后处理模块缺失时， 线性模式会简化为朴素检索增强生成 （Naive RAG） ...",
        "(9.7)\n其中，ˆst 表示临时答案，st 是语言模型的输出。 接受ˆst 的条件是其内部的所有词元都必须具有大\n于或等于阈值 τ 的关联概率。如果不满足这一条件，系统就会转而生成新的答案。\nRAG 系统还可以通过大语言模型直接进行判断（LLM Judge） 。这一方式包括两种主要方法：\n第一种方法利用 LLM 的上下文学习能力，通过精心设计的提示来进行决策。这种方法的优势在\n于无需对模型进行额外的微调，但其判断结果的准确性通常依赖于 LLM 对提示的理解程度。第\n二种方法通过对 LLM 进行微调， 使其生成特定的触发标记， 来直接控制模型的行为。 例如， 借助",
        "重要作用，它能够识别关键节点，负责管理和协调系统的各个流程。包括何时需要进行外部数据\n检索、如何评估生成结果的充分性，以及在必要时决定是否启动进一步的检索。这一模块特别适\n用于递归、迭代和自适应检索的场景，确保系统能够根据当前任务的需求动态调整流程，从而在\n适当的时机停止生成或启动新的检索循环。 这种智能调度机制使RAG 系统更高效、 更精准地处理\n复杂任务。调度模型主要三种实现方式，包括规则判断、大语言模型判断以及知识引导调度。\n规则判定（Rule Judge）是一个重要的机制，用于评估生成答案的质量并决定进一步的操作。\n系统通过评分机制对生成的答案进行质量评估， 并根据预设的阈值判断是否继续或终止生成过程。\n具体来说， 系统会检查生成答案中每个词的概率是否高于设定的阈值τ， 若满足条件， 则接受当前\n答案；否则，系统会重新生成新答案。这种方法确保了生成内容的可靠性和准确性，同时为系统\n的迭代改进提供了依据。规则调度可以如下形式化定义：\nyt =\n8\n<\n:\nˆst 如果 ˆst 的所有词元的概率都 ⩾ τ\nst = LM([Dqt, x, y<t]) 其他情况\n(9.7)\n其中，ˆ...",
        "大语言模型：从理论到实践 367\n9.6.2 查询分解与检索结果融合 RAG 系统\n针对复杂问题，RAG 系统在处理查询之前的优化阶段，通常需要引入查询分解等技术。这是\n因为复杂查询往往包含多个子问题或逻辑层次，直接检索可能难以获得高质量的结果。通过查询\n分解，可以将复杂查询拆分成更小、更易处理的子查询，从而提高检索的准确性和生成回答的质\n量。如图9.20所示，查询分解作为预处理步骤加入基础 RAG 系统中。\n问题\nQ1\nQ2\nQ3\n向量存储文档\n文档\n文档\n回答\n向量存储\n向量存储\n图 9.20 包含查询分解与检索结果融合的 RAG 系统\n使用 LangChain 可以快速构建一个包含查询分解与检索结果融合的 RAG 系统:"
      ],
      "response_time": 1.7918508052825928,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 0.3333333333333333
      },
      "answer_quality_metrics": {
        "rouge1": 0.8571428571428571,
        "rouge2": 0.8,
        "rougeL": 0.8571428571428571,
        "bleu": 0.16967525155612503,
        "semantic_similarity": 0.8966343402862549
      }
    },
    {
      "sample_id": "0154",
      "question": "RQ-RAG 方法提出了哪三种选择方式？",
      "ground_truth": "基于PPL的选择基于置信度的选择基于集成的选择集成",
      "predicted_answer": "RQ-RAG 方法提出了三种选择方式：基于PPL的选择、基于置信度的选择和基于集成的选择[1]。",
      "ground_context": [
        "基于PPL的选择基于置信度的选择集成"
      ],
      "retrieved_contexts": [
        "的空间，为最终的响应提供更丰富的支持。\nPPL：3.2置信度：0.7\nPPL：2.1(MIN)置信度：0.7\nPPL：4.3置信度：0.8（MAX）\n指令\n模型生成的查询词\n检索得到的上下文\n最终结果\n基于PPL的选择基于置信度的选择\n基于集成的选择集成\n图 9.18 RQ-RAG 解码策略流程 [440]\n如何从这些轨迹中选取最合适的路径是 RQ-RAG 系统中的关键问题之一。令 pM 表示一个\n参数为 M 的大语言模型，[R1, R2, . . . , Rn] 表示 n 条轨迹，其中每条轨迹都包含一个序列，记为\n[X, Y ]。 其中，X 是输入提示，Y 是由 Z1, Z2, . . . , Zi（每个 Zi 都是查询和检索到的上下文的组合）\n组成的 i 个中间步骤，以及最终答案 Yf inal 的拼接结果。针对这一问题，RQ-RAG 方法提出了三",
        "大语言模型：从理论到实践 349\n所示：\nL = max\nM\nE(x,y)∼D[log pM (y|q1, d1, d2, . . . , qi, di, x) (9.15)\n其中，L 代表试图最大化的概率值，M 表示模型参数， 期望E(x,y)∼D 是对数据集 D 求平均，pM (y|q1, d1, q2, d2, . . . , qi, di, x)\n表示在给定输入 x、第 i 步经过优化的查询 qi 以及检索到的文档 di 的情况下，模型 M 生成回复\ny 的概率。\nRQ-RAG 在推理过程中采用了一种树形解码策略，其具体流程如图9.18所示。在每个时间步，\n模型可以根据需要对查询进行重写、 分解、 消除歧义， 或直接生成回答。 通过特殊标记的引导， 该\n策略能够控制解码路径的扩展，并以“生成 → 检索 → 生成 → 检索 →⋯⋯→ 答案”的循环过程\n逐步展开。在每次迭代中，模型会根据任务需求生成不同类型的搜索查询，例如重写、分解或消\n歧查询。这些查询将被用于检索与其对应的上下文信息，从而形成不同的解码路径。基于设定的\n探索宽度和深度范围，RQ-RAG 能够生成多条候选轨迹，通过...",
        "9.4.2 查询优化\n如前所述，RAG 系统在处理用户查询时，需要对查询优化进行深入改进，以应对多种复杂挑\n战。对于简单查询，例如日常问候等无需上下文支持的情况，模型应避免执行不必要的信息检索，\n直接生成答案， 从而减少无关上下文对响应质量的影响。 对于复杂查询， 直接使用原始查询进行检\n索通常难以获取足够的相关信息。模型需要首先将复杂查询拆解为可解答的子查询，分别检索与\n其相关的信息，并整合子查询的结果，生成对原始查询的完整回答。而对于多义性较强的模糊查\n询，直接检索原始查询往往无法提供全面的答案。模型需通过识别用户意图来澄清查询内容，并\n构建精准的检索请求，获取相关信息后生成细致且全面的响应。通过优化查询流程，RAG 系统不\n仅能够提升检索效率，还能显著增强模型在复杂场景中的适应能力和表现。\n针对上述问题，文献 [440] 提出了 RQ-RAG 算法，旨在通过动态优化查询以提升检索增强生\n成的效果。该方法基于 7B 规模的 Llama2 模型，采用端到端训练，使其能够通过重写、分解和消\n除歧义来动态优化搜索查询。为了训练模型具备上述功能，核心是构建与推理过程相匹配的训练\n数据。为了...",
        "(9.7)\n其中，ˆst 表示临时答案，st 是语言模型的输出。 接受ˆst 的条件是其内部的所有词元都必须具有大\n于或等于阈值 τ 的关联概率。如果不满足这一条件，系统就会转而生成新的答案。\nRAG 系统还可以通过大语言模型直接进行判断（LLM Judge） 。这一方式包括两种主要方法：\n第一种方法利用 LLM 的上下文学习能力，通过精心设计的提示来进行决策。这种方法的优势在\n于无需对模型进行额外的微调，但其判断结果的准确性通常依赖于 LLM 对提示的理解程度。第\n二种方法通过对 LLM 进行微调， 使其生成特定的触发标记， 来直接控制模型的行为。 例如， 借助",
        "（3） 使用ChatGPT， 根据优化后的查询及其对应的检索上下文生成响应。 在这一阶段，ChatGPT\n被提示根据上下文信息生成与查询高度相关的回答， 同时避免冗余和噪声信息对响应质量的干扰。\n整个流程通过不断重复，最终生成了约 40,000 条数据实例。\nRQ-RAG 所使用的任务池涵盖了多种代表性任务，确保模型能够适应不同场景需求。这些任\n务包括单跳问答任务（如 Arc-Easy/Arc-Challenge[442] 和 OpenbookQA[443]） ，用于测试模型的基础\n推理能力；多跳问答任务（如HotpotQA[444] 和 Musique[445]） ，要求模型整合多步信息以推导答案；\n以及歧义问答任务（如 ASQA） ，评估模型处理多义性问题的能力。此外，为了提升模型的通用能\n力， 还引入了指令跟随任务， 包括LIMA[446]、WizardLM[447]、Open-Orca[448]、OpenAssistant[200] 和\nGPT4-Alpaca[35]， 这些任务通过多样化的场景训练模型理解和执行自然语言指令的能力。 最终， 任\n务池共收集了 42810 个实例..."
      ],
      "response_time": 0.9935388565063477,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.4,
        "rouge2": 0.0,
        "rougeL": 0.4,
        "bleu": 0.25079292295301137,
        "semantic_similarity": 0.7653477787971497
      }
    },
    {
      "sample_id": "0155",
      "question": "RQ-RAG 在推理过程中采用了什么策略？",
      "ground_truth": "RQ-RAG 在推理过程中采用了一种树形解码策略。",
      "predicted_answer": "RQ-RAG 在推理过程中采用了一种树形解码策略，其具体流程如图9.18所示[440]。在每个时间步，模型可以根据需要对查询进行重写、分解、消除歧义，或直接生成回答。通过特殊标记的引导，该策略能够控制解码路径的扩展，并以“生成 → 检索 → 生成 → 检索 →⋯⋯→ 答案”的循环过程逐步展开。在每次迭代中，模型会根据任务需求生成不同类型的搜索查询，例如重写、分解或消歧查询。这些查询将被用于检索与其对应的上下文信息，从而形成不同的解码路径。",
      "ground_context": [
        "RQ-RAG 在推理过程中采用了一种树形解码策略，其具体流程如图9.18所示。"
      ],
      "retrieved_contexts": [
        "的空间，为最终的响应提供更丰富的支持。\nPPL：3.2置信度：0.7\nPPL：2.1(MIN)置信度：0.7\nPPL：4.3置信度：0.8（MAX）\n指令\n模型生成的查询词\n检索得到的上下文\n最终结果\n基于PPL的选择基于置信度的选择\n基于集成的选择集成\n图 9.18 RQ-RAG 解码策略流程 [440]\n如何从这些轨迹中选取最合适的路径是 RQ-RAG 系统中的关键问题之一。令 pM 表示一个\n参数为 M 的大语言模型，[R1, R2, . . . , Rn] 表示 n 条轨迹，其中每条轨迹都包含一个序列，记为\n[X, Y ]。 其中，X 是输入提示，Y 是由 Z1, Z2, . . . , Zi（每个 Zi 都是查询和检索到的上下文的组合）\n组成的 i 个中间步骤，以及最终答案 Yf inal 的拼接结果。针对这一问题，RQ-RAG 方法提出了三",
        "大语言模型：从理论到实践 349\n所示：\nL = max\nM\nE(x,y)∼D[log pM (y|q1, d1, d2, . . . , qi, di, x) (9.15)\n其中，L 代表试图最大化的概率值，M 表示模型参数， 期望E(x,y)∼D 是对数据集 D 求平均，pM (y|q1, d1, q2, d2, . . . , qi, di, x)\n表示在给定输入 x、第 i 步经过优化的查询 qi 以及检索到的文档 di 的情况下，模型 M 生成回复\ny 的概率。\nRQ-RAG 在推理过程中采用了一种树形解码策略，其具体流程如图9.18所示。在每个时间步，\n模型可以根据需要对查询进行重写、 分解、 消除歧义， 或直接生成回答。 通过特殊标记的引导， 该\n策略能够控制解码路径的扩展，并以“生成 → 检索 → 生成 → 检索 →⋯⋯→ 答案”的循环过程\n逐步展开。在每次迭代中，模型会根据任务需求生成不同类型的搜索查询，例如重写、分解或消\n歧查询。这些查询将被用于检索与其对应的上下文信息，从而形成不同的解码路径。基于设定的\n探索宽度和深度范围，RQ-RAG 能够生成多条候选轨迹，通过...",
        "9.4.2 查询优化\n如前所述，RAG 系统在处理用户查询时，需要对查询优化进行深入改进，以应对多种复杂挑\n战。对于简单查询，例如日常问候等无需上下文支持的情况，模型应避免执行不必要的信息检索，\n直接生成答案， 从而减少无关上下文对响应质量的影响。 对于复杂查询， 直接使用原始查询进行检\n索通常难以获取足够的相关信息。模型需要首先将复杂查询拆解为可解答的子查询，分别检索与\n其相关的信息，并整合子查询的结果，生成对原始查询的完整回答。而对于多义性较强的模糊查\n询，直接检索原始查询往往无法提供全面的答案。模型需通过识别用户意图来澄清查询内容，并\n构建精准的检索请求，获取相关信息后生成细致且全面的响应。通过优化查询流程，RAG 系统不\n仅能够提升检索效率，还能显著增强模型在复杂场景中的适应能力和表现。\n针对上述问题，文献 [440] 提出了 RQ-RAG 算法，旨在通过动态优化查询以提升检索增强生\n成的效果。该方法基于 7B 规模的 Llama2 模型，采用端到端训练，使其能够通过重写、分解和消\n除歧义来动态优化搜索查询。为了训练模型具备上述功能，核心是构建与推理过程相匹配的训练\n数据。为了...",
        "量与准确性。\n在检测模型的训练过程中，RAG-HA T采用了两阶段策略。 第一阶段专注于训练模型输出幻觉\n的预测标签，完成基础的幻觉检测任务；第二阶段通过使用 LoRA 微调，使模型能够基于预测标\n签生成幻觉的详细解释，包括幻觉描述以及防御性建议。在推理时，两阶段模型以级联方式应用，\n先检测幻觉，再生成解释性描述。这种训练策略不仅显著提升了幻觉检测的精度，还增强了模型",
        "（3） 使用ChatGPT， 根据优化后的查询及其对应的检索上下文生成响应。 在这一阶段，ChatGPT\n被提示根据上下文信息生成与查询高度相关的回答， 同时避免冗余和噪声信息对响应质量的干扰。\n整个流程通过不断重复，最终生成了约 40,000 条数据实例。\nRQ-RAG 所使用的任务池涵盖了多种代表性任务，确保模型能够适应不同场景需求。这些任\n务包括单跳问答任务（如 Arc-Easy/Arc-Challenge[442] 和 OpenbookQA[443]） ，用于测试模型的基础\n推理能力；多跳问答任务（如HotpotQA[444] 和 Musique[445]） ，要求模型整合多步信息以推导答案；\n以及歧义问答任务（如 ASQA） ，评估模型处理多义性问题的能力。此外，为了提升模型的通用能\n力， 还引入了指令跟随任务， 包括LIMA[446]、WizardLM[447]、Open-Orca[448]、OpenAssistant[200] 和\nGPT4-Alpaca[35]， 这些任务通过多样化的场景训练模型理解和执行自然语言指令的能力。 最终， 任\n务池共收集了 42810 个实例..."
      ],
      "response_time": 3.716299295425415,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 0.5
      },
      "answer_quality_metrics": {
        "rouge1": 0.5714285714285715,
        "rouge2": 0.4,
        "rougeL": 0.5714285714285715,
        "bleu": 0.09144964106140611,
        "semantic_similarity": 0.8877174854278564
      }
    },
    {
      "sample_id": "0156",
      "question": "GTE 模型在预训练阶段使用了多少对无标注的文本对？",
      "ground_truth": "约8 亿对无标注的文本对",
      "predicted_answer": "GTE 模型在预训练阶段使用了约8亿对无标注的文本对[1]。",
      "ground_context": [
        "GTE 模型在预训练阶段， 使用了约8 亿对无标注的文本对， 数据来源多样， 包括网页数据"
      ],
      "retrieved_contexts": [
        "i=1 es(q,d−)/τ (9.10)\n其中，s(q, d) 通过文本 q 和 d 的嵌入向量（q = E(q) 以及 d = E(d)）之间的向量距离来估计这两\n段文本之间的相似度。\nGTE 模型中， 给定一批正例文本对样本{(q1, d1), (q2, d2), . . . ,(qn, dn)}， 作者提出一种改进的\n对比损失，如下所示：\nLicl = − 1\nn\nnX\ni=1\nlog es(qi,di)/τ\nZ (9.11)\nZ =\nX\nj\nes(qi,dj )/τ +\nX\nj̸=i\nes(qi,qj )/τ +\nX\nj\nes(qj ,di)/τ +\nX\nj̸=i\nes(dj ,di)/τ (9.12)\n其中，Z 采用余弦相似度作为相似度度量 s(q, d)。GTE 使用 BERT 等预训练语言模型进行初始化，\n通过对语言模型生成的上下文词元表示进行平均池化来获取文本嵌入向量。\nGTE 模型在预训练阶段， 使用了约8 亿对无标注的文本对， 数据来源多样， 包括网页数据 （如",
        "Text Embeddings） ：最新阶段致力于构建能适配多任务、多领域、多语言的统一模型。通过利用\n大规模多样化数据、合成数据生成以及大语言模型（LLMs）作为骨干网络，如 E5[434]、BGE[435]、\nGecko[436] 等，通用文本嵌入模型可以在分类、检索、聚类等任务中表现出色，显著提升了跨任务\n和跨领域的泛化能力。\n通用文本嵌入模型目标应对众多下游任务， 文献[437] 提出的 GTE 模型 （General-purpose Text\nEmbedding） 引入了多阶段对比学习策略， 并采用多样化的训练数据混合方式： 在预训练阶段， 使\n用未经任何筛选或清理的大量开源数据，通过无监督对比学习来学习基本的语言模式；在第二阶\n段，利用有监督微调，通过对比学习使用规模更小、质量更高的数据集对嵌入向量进行优化。\n对于查询语句 q 所对应的一个相关（正例）文档 d+ 以及一组不相关（负例）文档 D− =\n{d−\n1 , d−\n2 , . . . , d−\nn }，InfoNCE 损失[53] 的定义如下所示：\nLcl = − log es(q,d+)/τ\nes(q,d+)/τ ...",
        "大规模语言模型：从理论到实践 5\nGPT-1[2] 模型， 开启了预训练语言模型时代。BERT-Base版本的参数量为 1.1 亿个，BERT-Large版\n本的参数量为 3.4 亿个，GPT-1 的参数量为 1.17 亿个。这在当时，比其他深度神经网络的参数量，\n已经有了数量级上的提升。2019 年 OpenAI 发布了 GPT-2[11]， 其参数量达到15 亿个。 此后，Google\n也发布了参数规模为 110 亿个的 T5[19] 模型。2020 年，OpenAI 进一步将语言模型的参数量扩展\n到 1750 亿个， 发布了GPT-3[13]。 此后， 国内也相继推出了一系列的大语言模型， 包括清华大学的\nERNIE[20]、 百度的ERNIE[21]、 华为的PanGU-α[22] 等。 此阶段的研究主要集中在语言模型本身， 对\n仅编码器（Encoder Only） 、编码器-解码器（Encoder-Decoder） 、仅解码器（Decoder Only）等各种\n类型的模型结构都有相应的研究。模型大小与 BERT 类似，通常采用预训练微调范式，针对不同\n下游任务进行微调。 这些...",
        "（1）指令微调：使用精选的人类回答来微调预训练语言模型以应对各种查询。\n（2）奖励模型微调：使用一个包含人类对同一查询的多个答案打分的数据集来训练一个独立\n的奖励模型。\n（3）基于人类反馈的强化学习（Reinforcement Learning from Human Feedback ，RLHF）训练：\n利用近端策略优化（Proximal Policy Optimization，PPO） 算法， 根据奖励模型的奖励反馈进一步微\n调 SFT 模型。\n本节只针对步骤 （1） 指令微调的实践进行介绍， 对于奖励模型微调和RLHF 训练的实践会在\n后续对应章节中详细介绍。\n图 5.10 基于 DeepSpeed-Chat 训练类 ChatGPT 对话模型的三个步骤 [244]\nDeepSpeed-Chat 具有以下三大核心功能。",
        "30 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n2.2 生成式预训练语言模型 GPT\n受到计算机视觉领域采用 ImageNet[9] 对模型进行一次预训练， 使得模型可以通过海量图像充\n分学习如何提取特征，再根据任务目标进行模型微调的范式影响，自然语言处理领域基于预训练\n语言模型的方法也逐渐成为主流。 以ELMo[10] 为代表的动态词向量模型开启了语言模型预训练的\n大门， 此后， 以GPT[11] 和 BERT[1] 为代表的基于 Transformer 的大规模预训练语言模型的出现， 使\n得自然语言处理全面进入了预训练微调范式新时代。利用丰富的训练数据、自监督的预训练任务\n及 Transformer 等深度神经网络结构， 预训练语言模型具备了通用且强大的自然语言表示能力， 能\n够有效地学习到词汇、语法和语义信息。将预训练模型应用于下游任务时，不需要了解太多的任\n务细节，不需要设计特定的神经网络结构，只需要“微调”预训练模型，即使用具体任务的标注\n数据在预训练语言模型上进行监督训练，就可以取得显著的性能提升。\nOpenAI 公司在 2018 年提出的生成式预训练语言模..."
      ],
      "response_time": 0.9069521427154541,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.5,
        "rouge2": 0.0,
        "rougeL": 0.5,
        "bleu": 0.346697783111003,
        "semantic_similarity": 0.7649584412574768
      }
    },
    {
      "sample_id": "0157",
      "question": "在医疗信息检索中，如何通过动态权重分配机制提高检索系统的性能，同时结合MMD和MPD数据集的特性进行优化？",
      "ground_truth": "通过倒数排名融合（RRF）技术，可以有效整合多个检索结果的排名，从而增强整体预测性能与排名精度。同时，MMD数据集提供了可靠的医学信息检索评估基准，而MPD数据集则确保了分析的准确性和可靠性，经过预处理和清洗的文献增强了数据的质量。因此，结合这些数据集的特性，利用RRF的动态权重分配机制，可以针对特定领域进行微调，以应对复杂的查询挑战。",
      "predicted_answer": "根据提供的文档，我无法直接回答如何通过动态权重分配机制提高医疗信息检索系统的性能这一部分的问题。但关于MMD和MPD数据集的特性优化方面，文档提到：\n\n- MPD是一个从美国国家生物技术信息中心（NCBI）采样的1,000篇医学论文构建的数据集，经过一系列预处理和清洗操作，最终包含886篇论文共79,966条数据。实验表明，针对这些特定领域的数据进行微调，能够显著提升检索性能[3]。\n- MMD则是一个综合且可靠的医学信息检索评估基准，专注于医疗领域的检索系统性能测试，包含来自权威数据库的大量记录，用于评估不同模型在特定领域的表现[1]。\n\n因此，在结合MMD和MPD数据集进行优化时，可以通过针对这两个数据集的特点来进行模型微调，以提高检索系统的性能。",
      "ground_context": [
        "MMD 是一个综合且可靠的医学信息检索评估基准，专注于医疗领域的检索系统性能测试。",
        "倒数排名融合 （Reciprocal Rank Fusion，RRF） 是一种集成技术， 专门用于将多个检索结果的排名整合为统一的列表。"
      ],
      "retrieved_contexts": [
        "大语言模型：从理论到实践 347\n和 Medical Paper Dataset (MPD) 。MMD 是一个综合且可靠的医学信息检索评估基准，专注于医疗\n领域的检索系统性能测试。该数据集的数据来源于权威的“WHO Medicine”数据库以及“国家药\n典” 中所有药物信息， 包含超过20 万条记录。MPD 是一个从美国国家生物技术信息中心 （NCBI）\n采样 1,000 篇医学论文构建而成的数据集。为确保分析的准确性和可靠性，MPD 经过了一系列预\n处理和清洗操作， 排除了不符合研究标准的文献 （如非正式会议演讲和非同行评审的报告） ， 并移\n除了表格数据和不规范的数学公式。清洗后的文档被分割为固定长度的文本段（最大序列长度为\n768） ， 以适应嵌入模型的输入要求， 同时保留足够的上下文信息。 最终MPD 包含 886 篇论文， 共\n79,966 条数据。 实验结果证明，EM-FT 方法在 MMD 上的召回率和精度分别提高了 3.2%-6.0%， 在\nMPD 上的召回率和精度分别提高了 14.4%-42.6%。在一定程度上也说明，针对特定领域对文本嵌\n入模型进行微调很有必要。\n9....",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "这种多样化且精心设计的数据分布为模型提供了广泛的语义理解能力，同时通过微调使其能够在\n特定任务中表现出色。\n虽然通用文本嵌入已经有非常的好效果，但是针对特定领域的微调对于提升检索质量依然有\n非常重要的影响。通过微调，模型能够更准确地理解查询的语境和细微差异，从而提高检索阶段\n的效果。具体而言，微调能够增强模型的语义匹配能力，使其生成更具语境感知的嵌入，这不仅\n能更有效地匹配查询与潜在文档，还能显著提升检索内容的相关性。对于特定领域的数据进行微\n调，可以使模型更好地掌握领域专有的术语、风格和知识，生成更加精准和专业的内容。特别是\n在处理稀有查询时，微调可以充分利用领域知识，有效应对罕见或特殊表述的查询，这对于医疗、\n法律和教育等专业领域尤为重要。\n文献 [438] 提出了专门针对医学文档检索的框架 REMED，其中 EM-FT 模型通过高效的嵌入\n式微调方法，对预训练模型中的医学句子表示进行端到端微调，从而提高医学检索性能。作者选\n用 m3e-base[439] 和 e5-base-v2[434] 作为嵌入模型的基线。EM-FT 方法结合了对比学习作为损失函\n数，以优化模型性能并准确捕捉...",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "效减少冗余并增强结果的多样性。 例如， 在选择关键短语时，MMR 会优先考虑与查询高度相关且\n不重复的短语，从而平衡结果的相关性和信息量。这种基于规则的方式简单高效，适用于许多具\n有固定规则需求的场景。\n基于模型的重排序 （Model-based Rerank） 则利用语言模型对数据块进行排序， 通常通过计算数\n据块与查询之间的相关性来完成。这种方法能够动态地根据查询上下文判断数据块的重要性，从\n而生成更精准的排序结果。重排序模型的技术持续迭代，已经从文本数据扩展到多模态数据（如\n表格和图像） ， 实现了更广泛的应用场景。 相比于规则方法， 基于模型的重排序能够捕捉更复杂的\n语义关系， 特别是在需要理解深层次上下文的任务中表现突出。 因此， 重排序在RAG 系统中不仅\n是提升检索质量的重要工具，也为多模态数据处理提供了强有力的支持。\n2. 内容压缩\n将大量相关文档段拼接为冗长的上下文通常会引入噪声，削弱大语言模型对关键信息的感知\n能力。为解决上述问题，压缩（Compression）方法核心目标是通过内容压缩减少噪声，同时保留\n信息完整性，以提高语言模型的推理效率。\n内容压缩的一种方法是..."
      ],
      "response_time": 4.663050413131714,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.22222222222222224,
        "rouge2": 0.125,
        "rougeL": 0.22222222222222224,
        "bleu": 0.09358664959027153,
        "semantic_similarity": 0.7391805648803711
      }
    },
    {
      "sample_id": "0158",
      "question": "在使用检索增强生成（RAG）方法时，如何优化大语言模型以减少生成中的幻觉问题，同时又能确保生成结果的多样性和准确性？",
      "ground_truth": "通过实施 Hallucination Aware Tuning（RAG-HA T）方法来训练幻觉检测模型，从而识别和解释幻觉，并提供防御性建议。此外，利用预检索分支模式，通过生成多个子查询并并行检索，能够提升检索的全面性与生成结果的多样性，最终通过融合模块整合生成的答案，以确保结果的质量和准确性。",
      "predicted_answer": "为了优化大语言模型以减少生成中的幻觉问题，同时又能确保生成结果的多样性和准确性，可以采取以下措施：\n\n1. 使用 Hallucination Aware Tuning (RAG-HAT) 方法，通过训练幻觉检测模型识别幻觉并给出解释[5]；\n2. 利用 GPT-4 Turbo 对包含幻觉的原始回答进行重写，去除幻觉内容[5]；\n3. 在回答重写阶段结合生成的幻觉解释内容，进行多轮次的重写直至生成高质量样本[5]；\n4. 引入“过于谨慎惩罚”(OCP) 策略，随机从“优选”样本中删除一个句子以生成“拒绝”样本，激励模型在减少幻觉的同时保持回答的内容完整性[5]；\n5. 构建成对的偏好数据集，通过 DPO 方法训练模型生成较少幻觉且更加准确的答案[5]。",
      "ground_context": [
        "大模型幻觉指的是大语言模型生成的内容中出现与事实不符、缺乏依据或与输入信息相矛盾的表述。在实际应用中，即使采用检索增强生成（RAG）方法，大语言模型仍然可能出现幻觉问题，例如对检索到的内容进行错误或扭曲的解释。",
        "预检索分支是一种通过生成多个子查询并并行检索的模式，用于提高检索的全面性和生成结果的多样性。最终，这些生成的答案通过融合模块进行整合，形成最终结果。"
      ],
      "retrieved_contexts": [
        "316 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n检索增强生成（Retrieval-Augmented Generation, RAG）自 2020 年首次在文献 [412] 中提出以\n来， 引起了广泛关注。 为了弥补大语言模型在知识覆盖、 实时性以及准确性方面的不足， 自2022 年\nChatGPT 发布以来，RAG 技术得到了迅猛发展。RAG 通过引入外部知识库或实时搜索工具， 使模\n型在推理和生成过程中能够动态检索相关信息，而不再仅依赖预训练阶段固化的参数化知识。例\n如， 当用户提出 “复旦大学在哪里？ ” 这一问题时， 采用RAG 技术的系统会首先检索复旦大学官网、\n百科介绍等相关页面，并将全部或部分内容与用户问题合并，作为提示输入大语言模型。这种方\n法将基于大语言模型的问题解答从依赖模型记忆的知识的问答任务（闭卷问题回答，Closed-book\nQA）转变为“阅读理解”的任务，即从“闭卷考试”转变为“开卷考试” 。这一技术有效弥补了\n大语言模型在知识记忆和动态信息处理方面的不足，为解决长尾知识的获取以及减少幻觉现象提\n供了切实可行的解决方案。\n检索增强生成整个...",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "（3）基于集成的选择：选择累积置信度得分最高的结果作为最终输出，可以表示为： Yfinal =\narg maxy\nP\ni:Yi=y Conf(Yi)。其中，最终结果 Yfinal 是所有候选结果中置信度分数累积最大的一项，\n通过对所有候选结果 Yi，取值等于 y 的置信度分数 Conf(Yi) 进行累加求和，确定最佳答案。\n9.4.3 幻觉感知的生成模型优化\n大模型幻觉指的是大语言模型生成的内容中出现与事实不符、缺乏依据或与输入信息相矛盾\n的表述。在实际应用中，即使采用检索增强生成（RAG）方法，大语言模型仍然可能出现幻觉问\n题，例如对检索到的内容进行错误或扭曲的解释，这在高信任场景中带来了显著风险。\n文献 [449] 提出了一种专门针对检索增强生成中幻觉问题的方法，Hallucination Aware Tuning\n（简称 RAG-HA T） 。该方法通过训练幻觉检测模型，识别出幻觉并给出易于理解的解释，说明幻\n觉产生的位置和原因，以及提供防御性建议。利用这些检测结果，特别是幻觉描述，借助 GPT-4\nTurbo 对包含幻觉的 RAG 输出进行重写，以去除幻觉内容。随后，原始输出和...",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "大语言模型：从理论到实践 351\n在处理边界案例时的解释能力和鲁棒性。\nRAG-HA T采用 DPO 方法进行模型训练，通过构建成对的偏好数据集，指导大语言模型生成\n更少幻觉内容的回答。 在回答重写阶段， 针对包含幻觉的原始回答， 结合生成的幻觉解释内容， 利\n用 GPT-4 Turbo 对其进行重写，去除幻觉并生成“优选” （Chosen）样本。而对于被判定为优质的\n回答， 则通过防御性建议限定重写范围， 仅针对特定句子进行优化， 以避免引入新的幻觉内容。 此\n外，重写后的回答通过幻觉检测模型进行验证，确保其准确性，如发现仍存在幻觉，则重复重写\n过程，直至生成高质量的样本，保证数据集的完整性和可靠性。\n为进一步提升模型的回答质量，RAG-HA T还在偏好数据集中引入了 “过于谨慎惩罚” （Overly\nCautious Penalization, OCP） 策略。 由于模型在训练后可能倾向于通过缩短回答来降低幻觉率， 从而\n影响回答的内容丰富性，OCP 随机从 “优选” 样本中删除一个句子以生成 “拒绝” （Rejected） 样本，\n鼓励模型在减少幻觉的同时保持回答的内容完整性。 ..."
      ],
      "response_time": 4.954030990600586,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.30769230769230765,
        "rouge2": 0.25,
        "rougeL": 0.30769230769230765,
        "bleu": 0.047809697245737005,
        "semantic_similarity": 0.8046398162841797
      }
    },
    {
      "sample_id": "0159",
      "question": "混合检索和递归型循环检索在信息检索中的作用是什么？",
      "ground_truth": "混合检索结合了稀疏检索和稠密检索的优势，能够提高检索系统的效率和效果，尤其在处理同义词和复杂语义关系时表现出色。而递归型循环检索则通过每一步依赖于前一步的输出，逐层深入挖掘信息，适合需要分步推理的任务场景。两者共同促进了现代信息检索的精确性和深度。",
      "predicted_answer": "混合检索的作用是结合稀疏检索和稠密检索的优点，提升检索系统的效率和效果。它首先利用稀疏检索快速筛选出候选文档集合，然后通过稠密检索进一步提升结果的相关性。这种方法在保持高效的同时提高了语义理解能力，适用于不同类型和复杂程度的查询任务[5]。\n\n递归型循环检索则是一种依赖性强且层次分明的检索方式，每一步的输出都影响下一步，通过不断加深检索过程来逐步挖掘深层次信息。该方法特别适合需要分步推理或解决复杂问题的任务场景，能够逐步定位相关信息并生成高质量的回答[1][2]。",
      "ground_context": [
        "混合检索（Hybrid Retrieval）是结合稀疏检索和稠密检索优势的检索方法，用于提升检索系统的效率和效果。",
        "递归型检索的显著特点在于每一步都依赖于前一步的输出，并通过不断加深检索过程，逐步挖掘更深层次的信息。"
      ],
      "retrieved_contexts": [
        "大语言模型：从理论到实践 343\n查询词检索\n生成\n判断\n回答\n迭代N次\n图 9.14 RAG flow 的迭代型循环模式 [421]\n（2） 递归型循环检索模式是一种具有明显依赖性和层次性的检索方式。 如图9.15所示， 递归型\n检索的显著特点在于每一步都依赖于前一步的输出，并通过不断加深检索过程，逐步挖掘更深层\n次的信息。通常，递归型检索遵循类似树状的结构，每次检索都会基于一个重新改写的查询展开，\n从而精确地针对当前需要获取的知识进行检索。递归型检索还包含明确的退出机制，用以确保在\n满足终止条件时流程终止，避免无限递归。这种机制能够有效控制流程的深度和复杂性。在RAG\n系统中，递归型循环模式通过查询转换模块生成新的查询，以推动检索逐层深入。这种方式特别\n适合需要分步推理或分解复杂问题的任务场景，能够逐步定位相关信息并生成高质量的回答。\n查询词检索\n生成\n判断\n回答\n可选查询词变换\n图 9.15 RAG flow 的递归型循环模式 [421]\n（3）自适应型（主动型）模式是一种超越传统被动检索模式的新兴模式，得益于大语言模型\n的强大能力。 如图9.16所示， 这种模式的核心思想类似于...",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "如，对于“汽车”和“车辆”这样的同义词，稠密检索器可以识别它们在语义上的相近性，从而\n提高检索结果的相关性。稠密检索在捕捉细粒度语义关联方面优于传统的稀疏检索方法。\n然而，稠密检索也面临一些挑战，特别是在计算成本和存储要求方面。由于稠密向量通常是\n高维向量（例如 768 维或更高） ，因此处理和存储大规模文档库的稠密向量需要更高的计算资源。\n此外， 稠密检索依赖于深度学习模型的训练， 模型的质量和训练数据的规模直接影响检索效果， 这\n可能增加系统的开发复杂性和维护成本。稠密检索的高计算需求限制了其在资源受限场景中的应\n用。 尽管如此， 稠密检索已经成为RAG 系统和现代信息检索中的重要方法， 尤其是在需要高语义\n理解能力的任务中。\n3. 混合检索\n混合检索 （Hybrid Retrieval） 是一种结合稀疏检索和稠密检索优势的检索方法， 用于提升检索\n系统的效率和效果。 稀疏检索 （如TF-IDF 和 BM25） 擅长处理显性特征， 能够快速匹配高频词项，\n同时在大规模文档库中表现出极高的计算效率；而稠密检索（如基于深度学习模型生成的语义向\n量）能够捕捉复杂的语义关系，对理解同义词、...",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "332 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n者结合，既保留了稀疏检索的高效性，又增强了系统在语义理解上的能力。\n混合检索的核心思想是将稀疏向量和稠密向量的得分进行融合，或者在检索流程中分阶段使\n用两者。例如，在第一阶段，使用稀疏检索从大规模文档库中快速筛选出一个候选集合（通常称\n为“粗排” ） ；在第二阶段，对候选文档进行稠密检索或语义重排序，以提升结果的相关性。这种\n分阶段策略既能降低稠密检索的计算成本，又能显著提高检索质量。混合检索在效率和效果之间\n达到了良好的平衡。混合检索架构如图9.9所示。\n查询词\n稠密查询词向量\n相似度\n数据集\n稀疏查询词向量相似度 前k个相关文档\n前k个相关文档\n重排序\n相关文档\n图 9.9 混合检索架构图\n混合检索的优势在于其灵活性和适应性。对于需要显性词项匹配的查询（如“精确匹配”类\n问题） ， 稀疏检索能够快速捕捉关键词； 而对于需要语义理解的复杂查询 （如自然语言表达的长尾\n问题） ， 稠密检索能够提供更相关的结果。 此外， 混合检索可以根据不同的应用场景调整稀疏和稠\n密部分的权重，从而实现个性化的优化。例如，在工业界的大规模..."
      ],
      "response_time": 3.3778276443481445,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.14226074153509025,
        "semantic_similarity": 0.9365033507347107
      }
    },
    {
      "sample_id": "0160",
      "question": "状态空间模型中，A 矩阵代表什么？",
      "ground_truth": "状态转移矩阵",
      "predicted_answer": "在状态空间模型中，A 矩阵代表状态转移矩阵，表示系统状态随时间的变化，即系统的动力学特性 [1][3]。",
      "ground_context": [
        "其中，A 是状态转移矩阵、B 表示控制量对状态量的影响、C 表示当前状态量对输出影响和 D 表示当前控制量对输出影响，"
      ],
      "retrieved_contexts": [
        "为，并利用当前状态对未来的状态或输出进行预测。两个方程可以如下形式化表示：\nh′(t) = Ah(t) + Bx(t) (10.1)\ny(t) = Ch(t) + Dx(t) (10.2)\n其中，A 是状态转移矩阵、B 表示控制量对状态量的影响、C 表示当前状态量对输出影响和 D 表\n示当前控制量对输出影响， 上述四个矩阵都是可学习的， 也称为模型参数，h 表示中间状态，x 表\n示输入序列。\n状态空间模型的基本过程如图10.5所示。输入信号 x 与矩阵 B 相乘，生成一个向量，用于表\n示输入 x 对系统状态的影响。状态表示（State Representation）h 是一个隐向量，包含了系统的核\n心“知识” 。通过与矩阵A 相乘，状态表示描述了内部状态之间的关联，从而体现系统的动态特\n性。在预测输出之前，需要根据当前状态和输入信号更新状态。最后，通过矩阵C 将状态映射到\n输出空间，利用矩阵 D 提供从输入到输出的直接信号（通常被称为跳跃连接（Skip Connection） ） ，\n生成最终的输出。矩阵 C 描述了状态与输出之间的关系，即如何将状态转换为输出结果。\n跳跃连接\n图 10...",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "大语言模型：从理论到实践 375\n在本节中将着重介绍模型优化中的 Transformer 代替架构、模型量化、模型稀疏化以及模型\n蒸馏。\n10.2.1 Transformer 代替架构\n状态空间模型（State Space Model，SSM）是当前研究替代 Transformer 架构的热门方向之一。\n例如，Mamba[474] 和 Vision Mamba[475] 就是典型的状态空间模型，并在某些自然语言处理和计算\n机视觉任务中取得了优异的表现。与基于注意力机制的 Transformer 不同，SSM 在计算和存储方\n面对输入序列长度呈线性复杂度。这种特性显著提升了其在处理长文本序列时的效率，使其成为\n探索高效架构的重要候选之一。\n状态空间模型假设动态系统可以通过其在某一时刻 （时间t） 的状态来进行预测。 这个预测过\n程通常基于两个核心方程： 第一个方程描述系统状态随时间的变化 （即系统的动力学特性） ， 第二\n个方程将系统的状态映射到可观测值或输出。这种建模方式使 SSM 能够精确捕捉系统的动态行\n为，并利用当前状态对未来的状态或输出进行预测。两个方程可以如下形式化表示：\nh...",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "大规模语言模型：从理论到实践 109\n通过 AllReduce_Sum 通信¬， 跨设备求和， 得到完整的全量结果。 可以看出， 这里的输出结果和单\n计算设备执行的结果一致。\n图 4.9 单节点 Embedding 和两节点 Embedding 张量并行的示意图\n矩阵乘的张量并行要充分利用矩阵的分块乘法原理。举例来说，要实现如下矩阵乘法 Y =\nXA， 其中X 是维度为 M × N 的输入矩阵，A 是维度为 N × K 的参数矩阵，Y 是结果矩阵， 维\n度为 M × K。如果参数矩阵 A 非常大，甚至超出单张卡的显存容量，那么可以把参数矩阵 A 切\n分到多张卡上，并通过集合通信汇集结果，保证最终结果在数学计算上等价于单计算设备的计算\n结果。参数矩阵 A 存在以下两种切分方式。\n（1）参数矩阵 A 按列切块，将矩阵 A 按列切成\nA = [A1, A2] (4.2)\n（2）参数矩阵 A 按行切块，将矩阵 A 按行切成\nA =\n\f\f\f\f\f\nA1\nA2\n\f\f\f\f\f (4.3)\n图4.10 给出了参数矩阵按列切分的示例， 参数矩阵A 分别将 A1, A2 放置在两个计算设备上。\n两个计算设备分..."
      ],
      "response_time": 1.0954227447509766,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.03823047674690734,
        "semantic_similarity": 0.6759334802627563
      }
    },
    {
      "sample_id": "0161",
      "question": "final_rag_chain 是什么？",
      "ground_truth": "final_rag_chain 是一个包含 retrieval_chain_rag_fusion 和 question 的组合。",
      "predicted_answer": "`final_rag_chain` 是一个链式操作，用于处理问题并生成最终的回答。具体来说，它首先通过 `retrieval_chain_rag_fusion` 获取相关文档，然后使用这些文档和问题生成提示模板，接着传递给语言模型（`llm`），最后解析输出结果。[1][2]",
      "ground_context": [
        "final_rag_chain = (\"context\": retrieval_chain_rag_fusion, \"question\": itemgetter( \"question\") | prompt | llm | StrOutputParser())"
      ],
      "retrieved_contexts": [
        "Question: {question}\n\"\"\"\nprompt = ChatPromptTemplate.from_template(template)\nfinal_rag_chain = (\n\"context\": retrieval_chain_rag_fusion, \"question\": itemgetter( \"question\")\n| prompt\n| llm\n| StrOutputParser()\n)\nprint(final_rag_chain.invoke(\"question\":question))",
        "fused_scores[doc_str] += 1 / (rank + k)\n#ݔࢲ\nreranked_results = [\n(loads(doc), score)\nfor doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n]\n#ْ߭\nreturn reranked_results\nquestion =\"཮౵?\"\n#৽\nretrieval_chain_rag_fusion = generate_queries | retriever.map() | reciprocal_rank_fusion\ndocs = retrieval_chain_rag_fusion.invoke(\"question\": question)\nprint(len(docs))\n#֥ࢳٳRAG ৽\ntemplate = \"\"\"Answer the following question based on this context:\n{context}\nQuestion: {question}\n\"\"\"\nprom...",
        "大语言模型：从理论到实践 367\n9.6.2 查询分解与检索结果融合 RAG 系统\n针对复杂问题，RAG 系统在处理查询之前的优化阶段，通常需要引入查询分解等技术。这是\n因为复杂查询往往包含多个子问题或逻辑层次，直接检索可能难以获得高质量的结果。通过查询\n分解，可以将复杂查询拆分成更小、更易处理的子查询，从而提高检索的准确性和生成回答的质\n量。如图9.20所示，查询分解作为预处理步骤加入基础 RAG 系统中。\n问题\nQ1\nQ2\nQ3\n向量存储文档\n文档\n文档\n回答\n向量存储\n向量存储\n图 9.20 包含查询分解与检索结果融合的 RAG 系统\n使用 LangChain 可以快速构建一个包含查询分解与检索结果融合的 RAG 系统:",
        "（1）指令微调：使用精选的人类回答来微调预训练语言模型以应对各种查询。\n（2）奖励模型微调：使用一个包含人类对同一查询的多个答案打分的数据集来训练一个独立\n的奖励模型。\n（3）基于人类反馈的强化学习（Reinforcement Learning from Human Feedback ，RLHF）训练：\n利用近端策略优化（Proximal Policy Optimization，PPO） 算法， 根据奖励模型的奖励反馈进一步微\n调 SFT 模型。\n本节只针对步骤 （1） 指令微调的实践进行介绍， 对于奖励模型微调和RLHF 训练的实践会在\n后续对应章节中详细介绍。\n图 5.10 基于 DeepSpeed-Chat 训练类 ChatGPT 对话模型的三个步骤 [244]\nDeepSpeed-Chat 具有以下三大核心功能。",
        "歌则推出 Bard， 将实时检索与生成能力结合， 为用户提供更加全面的答案；国内的Kimi、 秘塔等\n产品也在这一领域崭露头角，成为 AI 搜索技术的重要实践者。此外， OpenAI 于 2024 年推出了\nSearchGPT，进一步推动了 AI 搜索技术的发展，该产品通过深度整合大语言模型与动态知识检索\n功能，展现了强大信息处理效率。国内的豆包、千问、智谱、百川等大模型系统也相继融入了AI\n搜索功能。\n本节将重点介绍 RAG 系统框架、RAG 任务分级以及 RAG 系统的难点。\n9.1.1 RAG 系统框架\n典型的检索增强生成过程如图9.1所示，其核心在于将外部检索与生成模块有机整合，通过动\n态引入外部知识来提升生成结果的准确性与可靠性。 具体而言，RAG 过程以用户输入的查询为起"
      ],
      "response_time": 1.8584351539611816,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.7777777777777777,
        "rouge2": 0.75,
        "rougeL": 0.7777777777777777,
        "bleu": 0.1400690273651116,
        "semantic_similarity": 0.7979698181152344
      }
    },
    {
      "sample_id": "0162",
      "question": "ROUGE 主要用于评估什么任务？",
      "ground_truth": "生成摘要任务",
      "predicted_answer": "ROUGE 主要用于评估生成摘要任务，衡量生成内容与参考答案的文本片段重叠程度。[1][2]",
      "ground_context": [
        "ROUGE（Recall-Oriented Understudy for Gisting Evaluation ）ROUGE 主要用于评估生成摘要任务，衡量生成内容与参考答案的文本片段重叠程度。"
      ],
      "retrieved_contexts": [
        "文本中 n-gram 的匹配概率。BLEU 适合评估结构化任务（如机器翻译） ，但对开放性任务的灵活\n性有限。\nROUGE（Recall-Oriented Understudy for Gisting Evaluation ）ROUGE 主要用于评估生成摘要\n任务，衡量生成内容与参考答案的文本片段重叠程度。常用的 ROUGE 指标有 ROUGE-N（基于\nn-gram） 、ROUGE-L（基于最长公共子序列，LCS） 。计算公式如下：\nROUGE-N =\nP(Overlapping N-grams)P(Reference N-grams) (9.21)\nROUGE-L = F1(LCS) = (1 + β2) × PrecisionLCS × RecallLCS\n(β2 × PrecisionLCS + RecallLCS) (9.22)\n其中，PrecisionLCS 表示生成文本中最长公共子序列的精确率，RecallLCS 表示参考文本中最长公共\n子序列的召回率。\nExact Match（EM） 用于评估生成回答与参考答案的完全一致性， 常用于问答任务， 公式如下\n所示：\nEM =...",
        "在参考译文 Sj 中出现的次数。\n文本摘要采用 ROUGE[597]（Recall-Oriented Understudy for Gisting Evaluation ） 评估方法， 该方\n法也称为面向召回率的要点评估，是文本摘要中最常用的自动评估指标之一。 ROUGE 与机器翻\n译的评估指标 BLEU 类似，能根据机器生成的候选摘要和标准摘要（参考答案）之间词级别的匹\n配程度来自动为候选摘要评分。ROUGE 包含一系列变种， 其中应用最广泛的是ROUGE-N， 它统",
        "|there, is, a, dog, in, the, garden| = 6\n7 (11.9)\nROUGE-2 = |(a dog), (in the), (the garden)|\n|(there is), (is a), (a dog), (dog in), (in the), (the garden)| = 1\n2 (11.10)\n需要注意的是，ROUGE 是一个面向召回率的度量，因为式 (11.8) 的分母是标准摘要中所有\nn-gram 数量的总和。相反地，机器翻译的评估指标BLEU 是一个面向精确率的度量，其分母是机\n器翻译中 n-gram 的数量总和。 因此，ROUGE 体现的是标准摘要中有多少 n-gram 出现在候选摘要\n中，而 BLEU 体现了机器翻译中有多少 n-gram 出现在参考翻译中。\n另一个应用广泛的 ROUGE 变种是 ROUGE-L，它不再使用 n-gram 的匹配，而改为计算标准\n摘要与候选摘要之间的最长公共子序列， 从而支持非连续的匹配情况， 因此无须预定义n-gram 的\n长度超参数。ROUGE-L 的计算公式如下：\nR = LCS( ˆY ,...",
        "大规模语言模型：从理论到实践 423\n计了 n-gram 词组的召回率，通过比较标准摘要和候选摘要来计算 n-gram 的结果。给定标准摘要\n集合 S = {Y 1, Y 2, · · · , Y M } 及候选摘要 ˆY ，则 ROUGE-N 的计算公式如下：\nROUGE-N =\nP\nY ∈S\nP\nn-gram∈Y min[Count(Y, n-gram), Count( ˆY , n-gram)]P\nY ∈S\nP\nN -gram∈Y Count(Y, n-gram) (11.8)\n其中 n-gram 是 Y 中所有出现过的长度为 n 的词组，Count(Y, n-gram) 是 Y 中 n-gram 词组出现的\n次数。\n下面以两段摘要文本为例给出ROUGE 分数的计算过程：候选摘要ˆY = {a dog is in the garden }，\n标准摘要 Y = {there is a dog in the garden }。可以按照式 (11.8) 计算 ROUGE-1 和 ROUGE-2 的分\n数为\nROUGE-1 = |is, a, dog, in, the, garden|\n...",
        "大规模语言模型：从理论到实践 177\n将所有三角函数视为基函数 ϕj(s) := e isθj ，可以将式 (5.16) 展开为\na(s) = Re\n2\n4\nd/2−1X\nj=0\nhjeisθj\n3\n5 (5.17)\n其中 s 是查询和键之间的相对距离，hj := ( q2j + iq2j+1)(k2j − ik2j+1) 是取决于查询和键的复系\n数。作为基函数的三角函数具有非常强的拟合能力，基本上可以拟合任何函数，因此在不训练的\n情况下， 对于预训练2048 的上下文窗口总会存在与 [0, 2048] 中的小函数值相对应但在 [0, 2048] 之\n外的区域中大很多的系数 hj（键和查询） ，如图5.9(a) 所示，但线性插值法得到的结果平滑且数值\n稳定，如图5.9(b) 所示。\n图 5.9 不同相对距离下外推法和线性插值法的注意力分数比较。\n因此，可以利用位置插值修改式 (5.15) 的位置编码函数：\nf ′(x, m) = f\n\u0012\nx, mL\nL′\n\u0013\n(5.18)\n这种方法对齐了位置索引和相对距离的范围， 减小了上下文窗口扩展对注意力得分计算的影响， 使\n得模型更容易适应。线性..."
      ],
      "response_time": 0.9407351016998291,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.05022135506220879,
        "semantic_similarity": 0.5982474088668823
      }
    },
    {
      "sample_id": "0163",
      "question": "使用的嵌入模型是什么？",
      "ground_truth": "HuggingFaceEmbedding(model_name= \"BAAI/bge-large-zh-v1.5\")",
      "predicted_answer": "文本嵌入模型旨在将非结构化文本转换为嵌入表示，以便进行语义搜索等操作。这些模型提供了统一的标准接口给不同的嵌入模型使用，例如OpenAI和Cohere等。[5]",
      "ground_context": [
        "embed_model = HuggingFaceEmbedding(model_name= \"BAAI/bge-large-zh-v1.5\")"
      ],
      "retrieved_contexts": [
        "大语言模型：从理论到实践 345\n（Retrieval-Augmented Generation）系统中，文本嵌入表示是实现向量搜索的核心技术。\n文本嵌入技术有很长的研究历史，大体上可以分为四个阶段：计数式嵌入（Count-based Em-\nbeddings） ： 这一阶段的方法包括词袋模型 （Bag of Words, BoW） 和TF-IDF， 用词频和逆文档频率\n来表示文本， 但忽略了词语的语义和上下文信息， 仅能反映基本的词汇相关性； 静态词嵌入 （Static\nDense Word Embeddings） ： 代表性模型如Word2V ec、GloV e和 FastText， 通过上下文生成固定的词\n向量。这一阶段捕捉了词语的语法和语义相似性，但每个词的向量是静态的，无法反映词义在不\n同上下文中的变化；上下文嵌入（Contextualized Embeddings） ：这一阶段引入了上下文敏感的动\n态嵌入模型， 如GPT 和 BERT 等。 这些模型通过双向或单向Transformer 结构， 生成能够根据上下\n文调整的词或句子向量，实现了对多义词和复杂语境的更深层次理解；...",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "张量并行需要根据模型的具体结构和算子类型，解决如何将参数切分到不同设备，以及如何\n保证切分后的数学一致性这两个问题。 大语言模型都是以Transformer 结构为基础，Transformer 结\n构主要由嵌入式表示（Embedding） 、矩阵乘（MatMul）和交叉熵损失（Cross Entropy Loss）计算\n构成。这三种类型的算子有较大的差异，需要设计对应的张量并行策略 [169] 才可以实现将参数切\n分到不同的设备。\n对于嵌入式表示算子，如果总的词表数非常大，会导致单计算设备显存无法容纳 Embedding\n层参数。举例来说，如果词表数量是 64000，嵌入式表示维度为 5120，类型采用 32 位精度浮点\n数， 那么整层参数需要的显存大约为64000 × 5120 × 4/1024/1024 = 1250 MB， 反向梯度同样需要\n1250MB 显存， 仅仅存储就需要将近2.5GB。 对于嵌入表示层的参数， 可以按照词维度切分， 每个\n计算设备只存储部分词向量，然后通过汇总各个设备上的部分词向量，得到完整的词向量。图4.9\n给出了单节点 Embedding 和两节点 E...",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "298 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\npage_content='Computer science is the study of computation, information, and automation.\nMembers of Congress and' metadata={'start_index': 0}\npage_content='and automation.\nComputer science spans theoretical disciplines (such as algorithms,\ntheory of computation, and information theory)'\nmetadata={'start_index': 60}\nText embedding models（文本嵌入模型） 旨在将非结构化文本转换为嵌入表示。 基于文本的嵌入\n表示可以进行语义搜索， 查找最相似的文本片段。Embeddings 类则用于与文本嵌入模型进行交互，\n并为不同的嵌入模型提供统一的标准接口， 包括OpenAI、Cohere 等。LangC..."
      ],
      "response_time": 1.4224107265472412,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.16666666666666666,
        "rouge2": 0.0,
        "rougeL": 0.16666666666666666,
        "bleu": 0.0,
        "semantic_similarity": 0.4916883707046509
      }
    },
    {
      "sample_id": "0164",
      "question": "OmniEval包含多少个自动生成的测试示例？",
      "ground_truth": "11400",
      "predicted_answer": "根据提供的文档，我无法回答这个问题。文档中没有提及OmniEval包含的具体测试示例数量。[5]",
      "ground_context": [
        "OmniEval包含 11400 个自动生成的测试示例和 1700 个人工标注的测试示例。"
      ],
      "retrieved_contexts": [
        "大规模语言模型：从理论到实践 437\n子类主题，覆盖面极为广泛。在质量控制方面，测试集由 52 位外包人员和 6 位算法工程师精心\n制作，通过严格的审查流程，确保了问题和答案的高质量和准确性。参考答案在时间上保持稳定\n性，以保证测试集在长期使用中的有效性。评测方式设计为简短的问题和答案形式，使评估过程\n高效便捷， 能够以较低成本快速完成， 同时保持评测一致性和可靠性。 此外，C-SimpleQA 对 40 多\n个国内外开源与闭源大模型进行了测试，展现了清晰的难度梯度和区分度，可以有效衡量模型的\n事实性能力。在构建过程中，该测试集分为自动化生成与严格质量控制两个阶段，评测方式和指\n标与 OpenAI 的方法保持一致。2025 年 1 月的评估结果显示，o1-preview 模型的正确率为 63.8%，\nDeepSeek-R1 模型的正确率为 63.7%[622]。\nIFEval[623]，全称为 Instruction-Following Evaluation，是一个专门用于评估大语言模型指令遵\n循能力的数据集。该数据集旨在通过聚焦可验证的指令，为研究者提供一种自动化且客观的评估\n方式，...",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "438 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n集则涵盖了八种主流编程语言（Python、Java、C++、C#、JavaScript、TypeScript、PHP 和 Bash） 。\nHumanEval 系列评测为研究者提供了一个标准化的数据集和工具，用于量化模型在代码生成任务\n中的能力。\nLiveCodeBench[625] 是一个动态且全面的基准测试集， 专为评估大语言模型的代码生成能力设\n计。该测试集从 LeetCode、AtCoder、CodeForces 等竞赛平台持续收集新问题，截至 2025 年 1 月\n已包含 880 道高质量编码挑战，覆盖代码生成、自修复、代码执行和测试输出预测等多种能力场\n景。通过仅选用新发布的问题，避免训练数据与测试数据重叠，确保评估无污染且客观公正。它\n支持用户自定义模型风格和评估流程，提供直观的命令行接口及详尽文档，方便新手和专家快速\n上手。 此外， 公开的Leaderboard 增强透明度， 鼓励社区互动与模型性能的持续提升， 使其成为目\n前评估大语言模型编码能力的重要工具。\nSWE-bench V erified是 Op...",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "360 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n法的灵活性使得数据集能够更好地适应实际需求，同时对评估 RAG 系统的动态知识处理能力提\n出了更高要求。\nRGB[463]、MultiHop-RAG[464] 和 CRUD-RAG[465] 是基于自动生成数据集的典型案例。这些基\n准通过在线新闻文章生成数据集，用于测试 RAG 系统在处理真实世界信息时的表现。数据集内\n容超越了训练数据的覆盖范围， 评估系统对于动态、 实时信息的适应能力。CDQA[466] 则结合新闻\n来源的数据生成评估集，并引入标签器辅助构建更复杂的评价任务。DomainRAG[467] 结合了单文\n档、多文档、单轮对话和多轮对话等多种任务类型，数据集内容基于高校招生和注册信息的年度\n变化生成。 该数据集通过提供更新后的信息， 强迫系统利用动态的知识库来完成任务， 对RAG 系\n统的时效性和适应性进行了全面评估。OmniEval[468] 提出一个专门针对金融领域的自动和全方位\n的 RAG 评估基准， 将查询分为抽取式问答、 多跳推理、 对比、 对话和长文本问答5 个任务类别以\n及 16 个金融主题，..."
      ],
      "response_time": 0.9164903163909912,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0.0,
        "bleu": 0.0,
        "semantic_similarity": 0.3120681941509247
      }
    },
    {
      "sample_id": "0165",
      "question": "大语言模型推理遵循什么模式？",
      "ground_truth": "自回归模式",
      "predicted_answer": "大语言模型推理遵循自回归模式[1]。在每次GPT推理中，对每个词元的自注意力操作需要其前面词元的键和值。为了减少计算开销，FAIRSEQ提出了键值缓存的方法，在迭代中保存键和值以便重复使用。因此，大语言模型的推理过程可以分为预填充阶段和解码阶段两个阶段。",
      "ground_context": [
        "图 10.1 大语言模型推理遵循自回归模式"
      ],
      "retrieved_contexts": [
        "大语言模型：从理论到实践 371\n图 10.1 大语言模型推理遵循自回归模式\n在每次 GPT 推理中， 对每个词元的自注意力操作需要其前面词元的键和值。 最简单且无状态\n的实现需要在每次迭代中重新计算所有的键和值，这会导致大量额外的计算开销。为了避免这种\n重新计算的开销，FAIRSEQ[470] 提出了键值缓存（Key-V alue Cache） ，即在迭代中保存键和值，以\n便重复使用。 根据上述方法和技术， 大语言模型的推理过程可以分为预填充阶段 （Prefilling Stage）\n和解码阶段（Decoding Stage）两个阶段，如图10.2所示。在预填充阶段，模型会计算并存储初始\n输入词元的键-值 （KV） 缓存， 同时生成第一个输出词元。 随后进入解码阶段， 模型逐个生成后续\n输出词元，并在每一步更新 KV 缓存，直至完成整个推理过程。",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "附加到原始输入中，并将“复旦大学位于”作为一个新的整体输入模型以生成下一个词元。这个\n生成过程持续进行，直到生成表示序列结束的 <eos> 标志或达到预定义的最大输出长度为止。大\n语言模型的推理过程与其他深度学习模型 （如BERT、ResNet 等） 非常不同，BERT 的执行时间通\n常是确定且高度可预测的。但是，在大语言模型的推理过程中，虽然每次迭代执行的时间仍然具\n有确定性，但迭代次数（输出长度）是未知的，这使得一个大语言模型推理任务的总执行时间是\n不可预测的。",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "大语言模型：从理论到实践 349\n所示：\nL = max\nM\nE(x,y)∼D[log pM (y|q1, d1, d2, . . . , qi, di, x) (9.15)\n其中，L 代表试图最大化的概率值，M 表示模型参数， 期望E(x,y)∼D 是对数据集 D 求平均，pM (y|q1, d1, q2, d2, . . . , qi, di, x)\n表示在给定输入 x、第 i 步经过优化的查询 qi 以及检索到的文档 di 的情况下，模型 M 生成回复\ny 的概率。\nRQ-RAG 在推理过程中采用了一种树形解码策略，其具体流程如图9.18所示。在每个时间步，\n模型可以根据需要对查询进行重写、 分解、 消除歧义， 或直接生成回答。 通过特殊标记的引导， 该\n策略能够控制解码路径的扩展，并以“生成 → 检索 → 生成 → 检索 →⋯⋯→ 答案”的循环过程\n逐步展开。在每次迭代中，模型会根据任务需求生成不同类型的搜索查询，例如重写、分解或消\n歧查询。这些查询将被用于检索与其对应的上下文信息，从而形成不同的解码路径。基于设定的\n探索宽度和深度范围，RQ-RAG 能够生成多条候选轨迹，通过..."
      ],
      "response_time": 2.1809539794921875,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.017487235715232946,
        "semantic_similarity": 0.6014021635055542
      }
    },
    {
      "sample_id": "0166",
      "question": "文档评分的合成方法包括哪几种版本？",
      "ground_truth": "离散版本、连续版本和混合版本。",
      "predicted_answer": "文档评分的合成方法包括三种版本：离散版本、连续版本和混合版本[1][3]。",
      "ground_context": [
        "合成方法旨在通过多种策略对文档进行重新排序，以提高检索结果的相关性。这些方法包括离散版本、连续版本和混合版本。"
      ],
      "retrieved_contexts": [
        "{query}\nHere is the analysis of the {query name}:\n{query analysis}\nHere is the {doc name}:\n{doc}\nHere is the analysis of the {doc name}:\n{doc analysis}\n在对文档相关性进行判断后， 文档评分的合成方法旨在通过多种策略对文档进行重新排序， 以\n提高检索结果的相关性。这些方法包括离散版本、连续版本和混合版本。在离散版本中，文档根\n据模型的判断被划分为“相关” （输出为“是” ）和“不相关” （输出为“否” ）两类。对于每一类\n文档，保留初始检索排名的相对顺序，即相关文档始终排在不相关文档之前。虽然这种方法简单\n直观，但其性能高度依赖于提示的设计和第一阶段检索的质量。\n为了克服离散方法的局限性，连续版本利用模型输出的“是”概率（py）和“否”概率（pn）\n对文档进行更细粒度的评分。具体来说，评分函数通过归一化 py 和 pn 的值来计算文档的相关性\n得分：S(d) = py\npy+pn\n，从而确保不同文档的评分具有可比性。根据这些得分，所有文档...",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "py+pn\n，从而确保不同文档的评分具有可比性。根据这些得分，所有文档被重新排\n序，得分越高的文档排名越靠前。与离散版本相比，连续版本能够更精确地捕捉文档的相关性梯\n度，适用于需要更细腻排序的场景。\n混合版本进一步结合了连续版本的概率评分和第一阶段检索中的 BM25 分数，通过加权求和\n的方式生成综合评分。 具体地， 最终评分由概率得分Sprob 和 BM25 分数 SBM25 按照权重系数 α 进\n行加权：S = αSprob + SBM25， 综合了推理能力和表层匹配的优点。 混合版本通过模型集成的方式，\n兼顾深层语义推理和表层匹配效果，在实际应用中表现出更强的稳定性和适用性。",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "务评估中，多样性是重要考量。此类任务要求生成内容在保持与输入主题一致的同时呈现出显著\n的多样性和创新性。评估生成内容的多样性需要从语义差异性和冗余度两个方面入手，以全面衡\n量生成内容的丰富程度和信息分布特性。\n语义多样性得分 （Semantic Diversity Score, SDS） 是衡量生成内容语义层面差异性的重要指标，\n其公式为：\nSDS = 1 − cos(Embedding1, Embedding2) (9.26)"
      ],
      "response_time": 0.8152883052825928,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.29340680870384167,
        "semantic_similarity": 0.8228417634963989
      }
    },
    {
      "sample_id": "0167",
      "question": "在使用OllamaLLM生成搜索查询时，如何确保检索结果既全面又多样化？",
      "ground_truth": "在使用OllamaLLM生成搜索查询时，可以通过确保检索组件能够全面覆盖用户的不同信息需求，避免信息冗余，从而实现检索结果的全面性和多样性。同时，检索组件需要具备动态适应性，以快速适应知识库的变化，确保在检索过程中能够获取到最新的相关信息。",
      "predicted_answer": "为了确保使用OllamaLLM生成的搜索查询既全面又多样化，在执行检索时可以对用户的原始查询赋予较高的权重，使其在生成的多个查询中保持主导地位[5]。这样可以在丰富检索结果的同时，保持与用户初始需求的高度一致，平衡语义多样性和用户意图的精确捕捉。此外，还可以通过分解复杂问题为多个子问题（子查询）的方式来提高检索的全面性和准确性[5]。",
      "ground_context": [
        "llm = OllamaLLM(model=\"qwen2.5\")",
        "检索组件需要确保其检索结果能够全面覆盖查询的不同维度，同时避免信息冗余。多样性评估旨在衡量检索结果是否包含多样化的视角或信息来源，尤其是在处理开放域问答或多轮对话时，这一点尤为重要。",
        "由于RAG 系统依赖动态更新的知识库（如互联网爬取的数据），检索组件需要能够快速适应知识库的变化。"
      ],
      "retrieved_contexts": [
        "# ൐Ⴈ Ollamaնე࿽ଆ྘\nllm = OllamaLLM(model=\"qwen2.5\")\n#ᄯ queryࢳٳPrompt\ntemplate = \"\"\"You are a helpful assistant that generates multiple search queries based on\na single input query. \\n\nGenerate multiple search queries related to: question \\n\nOutput (4 queries):\"\"\"\nprompt_rag_fusion = ChatPromptTemplate.from_template(template)\n#ᄯ query৽\ngenerate_queries = (\nprompt_rag_fusion\n| llm\n| StrOutputParser()\n| (lambda x: x.split(\" \\n\"))\n)",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "这样的技术通过生成伪文档来更好地表达查询，从而提高检索的准确性。然而，这种方法也会显\n著增加计算成本，因此需要进一步优化，以在提升检索质量的同时降低计算开销，实现精度与效\n率的平衡。\n复杂的查询通常需要整合多个文档的信息，但文档间的信息碎片化或矛盾可能导致生成结果\n出现不连贯或逻辑错误。提高检索粒度、引入实体级检索以及采用重新排序技术是改善连贯性的\n有效方式。 然而，Zhu 等人 [165] 指出， 目前的许多后检索方法严重依赖调用大型语言模型 （LLM）\n的 API，这会导致高昂的运行成本。未来的研究可以探索轻量化的替代方案，例如知识蒸馏技术，\n以在降低成本的同时实现实时应用的可扩展性。",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "的查询场景。\n尽管多查询方法在提高检索全面性方面表现出色，但扩展后的查询可能会在某些情况下稀释\n用户的原始意图，导致生成内容偏离用户需求。为解决这一问题，可以通过在模型执行检索时对\n用户的原始查询赋予更高的权重，使其在多查询中占据主导地位。这种权重分配策略确保了扩展\n查询丰富结果的同时，始终保持与用户初始需求的高度一致性，平衡了语义多样性与用户意图的\n精准捕捉。\n子查询 （Sub-Query） 则通过对复杂问题进行分解和规划， 将其转化为多个更易处理的子问题，\n从而提高问题求解的效率与准确性。在实现过程中，可以采用“从简单到复杂” （ Least-to-Most\nPrompting）的方式[396]，将复杂问题逐步分解为一系列简单的子问题。这种方法不仅能够降低问\n题的复杂性，还能帮助模型更有条理地处理问题。根据原始问题的结构，这些生成的子问题可以"
      ],
      "response_time": 2.4599204063415527,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.5,
        "rouge2": 0.0,
        "rougeL": 0.5,
        "bleu": 0.047725248053534286,
        "semantic_similarity": 0.8479034900665283
      }
    },
    {
      "sample_id": "0168",
      "question": "大语言模型在提升效率方面有哪些研究方向，并且在自然语言处理任务中展现了哪些能力？",
      "ground_truth": "大语言模型在自然语言理解与生成等任务中展现了卓越的能力，并推动了人工智能技术的快速发展。为应对资源消耗的挑战，研究者们从模型、数据和计算框架等多个角度探索了提升大模型效率的优化方法，包括模型压缩、量化、数据选择和优化训练框架等技术。",
      "predicted_answer": "大语言模型在提升效率方面的研究方向主要包括以下几个方面：\n\n- **模型压缩与优化**：通过减少模型参数量、降低计算复杂度等方式减轻资源消耗。\n- **训练效率优化**：改进训练算法和技术，加快训练速度。\n- **推理效率优化**：优化推理过程，减少推断时间。\n- **专用框架设计与应用**：开发专门针对大语言模型的高效计算框架。\n\n此外，大语言模型还在以下几类自然语言处理任务中展现出了显著的能力：\n\n- **翻译与多语言处理**：具备强大的语言理解和生成能力，提高了翻译准确性及自然性，尤其擅长处理复杂内容如文化背景、隐喻和典故；在跨国企业和跨语言信息检索中展现出强大效能。\n- **信息抽取与知识图谱构建**：能够精准地从文本中提取关键实体，支持复杂的实体识别任务。",
      "ground_context": [
        "大语言模型在自然语言理解与生成等任务中展现了卓越的能力，不仅推动了人工智能技术的快速发展，也为社会各领域的应用带来了深远的影响。",
        "因此，如何在保持模型性能的同时提高其效率，已成为当前大模型研究中的重要议题。为应对这一问题，研究者们从模型、数据和计算框架等多个角度探索了提升大模型效率的优化方法。"
      ],
      "retrieved_contexts": [
        "10. 大语言模型效率优化\n大语言模型在自然语言理解与生成等任务中展现了卓越的能力，不仅推动了人工智能技术的\n快速发展，也为社会各领域的应用带来了深远的影响。然而，这些强大的能力背后伴随着巨大的\n资源消耗，包括计算、存储和能源需求，这对环境、经济以及技术可持续性带来了严峻挑战。因\n此，如何在保持模型性能的同时提高其效率，已成为当前大模型研究中的重要议题。为应对这一\n问题，研究者们从模型、数据和计算框架等多个角度探索了提升大模型效率的优化方法。通过模\n型压缩、量化、数据选择和优化训练框架等技术，显著降低了训练与推理成本，为实现更加可持\n续和普惠的人工智能提供了可能性。\n本章将重点从模型、训练和推理三个角度系统性探讨提升大语言模型效率的技术进展，分别\n涵盖模型压缩与优化、训练效率优化，以及推理效率优化和专用框架的设计与应用。\n10.1 效率优化基础\n大语言模型的推理过程遵循自回归模式 （Autoregressive Pattern） ， 如图10.1 所示。 例如， 针对\n输入 “复旦大学位” ， 模型预测 “于” 的概率比 “置” 的概率高。 因此， 在第一次迭代后， “于” 字被\n附加...",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "用户提供更自然的互动体验，帮助他们有效提升语言学习能力。\n12.1.3 翻译与多语言处理\n随着大语言模型的崛起，这一领域正在迎来新的变革，大语言模型凭借其强大的语言理解和\n生成能力，为翻译与多语言处理注入了新的活力，加速了技术和应用的迭代发展。\n在机器翻译方面，传统方法主要依赖神经网络、深度学习以及大量语料库的训练来实现文本\n翻译。 然而， 大语言模型的出现， 为机器翻译带来了质的飞跃。 得益于广泛的知识储备和对语言深\n层语义的理解能力，大语言模型在翻译中表现出更高的准确性和自然性，尤其是在文化背景、隐\n喻和典故等复杂内容的处理上。例如，在文学翻译场景中，传统机器翻译往往难以还原原文的意\n境与风格，而大语言模型能够更精准地理解文化元素，并以目标语言重现文本的艺术性。在跨国\n企业中，大语言模型也广泛应用于产品文档翻译，如苹果、三星等公司利用其快速处理多种专业\n术语，确保翻译的专业性和一致性，大幅提升了翻译效率，并帮助全球用户更好地理解产品信息。\n在跨语言信息检索领域， 大语言模型同样展现了强大的能力。 以微软学术搜索等平台为例， 引\n入大语言模型后，跨语言检索的精准度和效率显著提升。大语...",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "大规模语言模型：从理论到实践 451\n交通等问题，大语言模型不仅能够准确理解用户需求，还能将答案翻译成日语，提供自然且贴合\n实际的建议。相比传统客服，这种基于大语言模型的解决方案更具人性化和情景适应性，大幅提\n升了用户体验，同时增强了用户对平台的信任与忠诚度。\n翻译与多语言处理技术正以前所未有的速度融入各个行业，而大语言模型的应用为其带来了\n新的可能性。未来，随着大语言模型的持续优化，翻译的准确性和多语言处理的效率将进一步提\n升。这些技术不仅能够在更多领域创造价值，还将拉近不同文化与语言之间的距离，推动全球交\n流与合作，让世界变得更加紧密相连。\n12.1.4 信息抽取与知识图谱\n大语言模型在信息抽取领域展现了强大的能力，尤其是在实体识别任务中。借助其深度语言\n理解能力， 大语言模型能够精准地从文本中提取出人名、 地名、 组织名等关键实体。 例如， 复旦大\n学推出的 B2NE[628] 基于大模型的开放领域信息工具，可以让用户自由的从超过 16 个领域的 400\n种类型中， 灵活抽取目标实体和关系。 在医疗领域，IBM Watson Discovery 广泛用于从医学文献中\n识别疾病名称..."
      ],
      "response_time": 4.522188663482666,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.02728862133801117,
        "semantic_similarity": 0.8918988704681396
      }
    },
    {
      "sample_id": "0169",
      "question": "如何评估生成内容的创造性和真实性？",
      "ground_truth": "评估生成内容的创造性可以通过语义嵌入的多样性指标和冗余度分析来实现。高语义相似性分值表明生成的内容更具创造性和多样性，而低冗余度意味着信息表达新颖且丰富。在真实性方面，真实性检测通过如FEVER评分等指标，评估生成内容与事实的匹配程度，以避免生成“幻觉”内容。",
      "predicted_answer": "评估生成内容的真实性主要关注其准确性[3]，即生成的内容是否基于准确的知识和可靠的信息源，避免错误、虚假或误导性的内容[3]。对于创造性，虽然文档中未直接提及“创造性”的具体评估方法，但可以从多样性角度考虑，通过语义多样性得分（Semantic Diversity Score, SDS）来衡量生成内容在语义层面的差异性[5]。因此，真实性评估侧重于知识的准确性，而创造性评估可以通过计算生成内容的语义多样性得分来进行[3][5]。",
      "ground_context": [
        "该指标通过评估生成内容中各部分的语义相似性来计算其多样性，分值越高，表明生成内容在语义表达上越具有差异性，从而更具创造性和多样性。",
        "真实性检测在避免生成“幻觉”内容方面发挥关键作用，FEVER评分是一种常用的方法，用于测试生成内容与事实的匹配程度。"
      ],
      "retrieved_contexts": [
        "cination） 。 因此， 生成组件需要确保其输出内容忠实于检索到的信息， 即生成的回答必须基于检索\n到的事实，而不是凭空捏造。真实性评估目标在于衡量生成内容是否准确反映了检索结果中的信\n息，避免出现事实错误或误导性内容。\n（3） 正确性 （Correctness） ： 在许多任务中， 生成的回答需要与给定的参考答案 （Ground Truth）\n保持一致。正确性评估目标旨在衡量生成内容与标准答案之间的一致性，特别是在有明确答案的\n任务（如问答或填空任务）中。这通常通过自动化指标（如 F1 分数或精确匹配率）来衡量。\n（4）连贯性与流畅性（Coherence and Fluency ） ：生成内容的连贯性和流畅性是评估生成组件\n的重要目标之一。连贯性指回答内容是否逻辑通顺，是否能够完整表达查询的意图；流畅性则关\n注语言表达是否符合自然语言的语法和用法。这些目标通常通过人工评估或语言模型的评分机制\n来实现。\n（5）生成内容的多维度要求（Multi-Dimensional Requirements） ：生成组件的评估需要覆盖多\n个维度，包括内容的可读性、丰富性和结构化程度。例如，在生成...",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "大规模语言模型：从理论到实践 425\n• 知识准确性：评估生成文本中所呈现的知识是否准确无误。 这涉及事实陈述、 概念解释、 历\n史事件描述等方面。 生成的文本应基于准确的知识和可靠的信息源， 避免错误、 虚假或误导\n性的内容。确保所提供的知识准确无误。\n• 知识丰富性： 评估生成文本所包含的知识是否丰富多样。 生成的文本应能够提供充分的信息，\n涵盖相关领域的不同方面。 这可以通过提供具体的例子、 详细的解释和相关的背景知识来实\n现。确保生成文本在知识上具有广度和深度，能够满足读者的需求。\n• 知识一致性： 评估生成文本中知识的一致性。 这包括确保文本中不出现相互矛盾的知识陈述，\n避免在不同部分或句子中提供相互冲突的信息。 生成的文本应该在整体上保持一致， 使读者\n能够得到一致的知识体系。\n11.3.2 评估方法\n评估方法的目标是解决如何对大语言模型生成结果进行评估的问题。有些指标可以通过比较\n正确答案或参考答案与系统生成结果直接计算得出， 例如准确率、 召回率等。 这种方法被称为自动\n评估（Automatic Evaluation） 。 然而， 有些指标并不是可以直接计算出来的， 而...",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "务评估中，多样性是重要考量。此类任务要求生成内容在保持与输入主题一致的同时呈现出显著\n的多样性和创新性。评估生成内容的多样性需要从语义差异性和冗余度两个方面入手，以全面衡\n量生成内容的丰富程度和信息分布特性。\n语义多样性得分 （Semantic Diversity Score, SDS） 是衡量生成内容语义层面差异性的重要指标，\n其公式为：\nSDS = 1 − cos(Embedding1, Embedding2) (9.26)"
      ],
      "response_time": 2.890929698944092,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 1.0,
        "mrr": 0.2
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0.0,
        "bleu": 0.05108126976863515,
        "semantic_similarity": 0.8481242656707764
      }
    },
    {
      "sample_id": "0170",
      "question": "降低训练精度被广泛认为是减少训练成本的什么方向之一？",
      "ground_truth": "降低训练精度被广泛认为是减少训练成本最具潜力的方向之一。",
      "predicted_answer": "降低训练精度被广泛认为是减少训练成本的一个方向之一[1]。具体来说，FP8-LM框架提出了利用FP8进行大语言模型训练的方法，以显著降低系统工作负载并实现更高效的训练过程[1]。",
      "ground_context": [
        "降低训练精度被广泛认为是减少训练成本最具潜力的方向之一， 它可以提供更高的速度、 更小的内存占用以及更低的通信开销。"
      ],
      "retrieved_contexts": [
        "通信成本优化方面效果有限，未能充分挖掘 FP8 的潜力。为了解决这一问题，Microsoft Azure 和\nMicrosoft Research 的研究人员开源了 FP8-LM 框架 [530]。该框架提出了一种高度优化的 FP8 混合\n精度训练方法，专为大语言模型设计。其核心思想是将 FP8 的计算、存储和通信贯穿于大型模型\n训练的全过程，使前向传播和反向传播全程基于低精度 FP8，从而显著降低系统工作负载，并实\n现更高效的训练过程。2025 年 1 月，文献 [531] 提出的方法，更是将精度进一步降低到 FP4。\n使用 FP8 进行大语言模型的训练并非易事，主要面临数据下溢或上溢问题，以及因 FP8 数据\n格式动态范围较窄和精度较低而引发的量化误差，这些问题可能导致数值不稳定性，甚至在训练\n过程中出现不可逆的发散现象。 为了解决这些挑战， 文献[530] 指出， 在大语言模型的训练中， 大\n部分变量（如梯度、优化器状态）可以采用低精度数据格式，而不会影响模型的准确性，也无需\n调整超参数。 具体而言，FP8-LM 提出了三个优化级别， 通过逐步引入FP8 通信、FP8 优化器以及...",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "模型微调方法。LoRA 方法可以在缩减训练参数量和 GPU 显存占用的同时，使训练后的模型具有\n与全量微调相当的性能。\n5.2.1 LoRA\n文献 [232] 的研究表明，语言模型针对特定任务微调之后，权重矩阵通常具有很低的本征秩\n（Intrinsic Rank） 。研究人员认为，参数更新量即便投影到较小的子空间中，也不会影响学习的有",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "\u0012\u0011\u000f\u0013\u000f\u0012ܒࡏ375\n\u0012\u0011\u000f\u0013\u000f\u0013߄377\n\u0012\u0011\u000f\u0013\u000f\u0014߄381\n\u0012\u0011\u000f\u0013\u000f\u0015ᆩ്ᆕ਼ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 383\n10.3 低精度训练 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 387\n\u0012\u0011\u000f\u0014\u000f\u0012'1\u0019 щ઒. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ..."
      ],
      "response_time": 1.5271556377410889,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.18275706180054271,
        "semantic_similarity": 0.7579377889633179
      }
    },
    {
      "sample_id": "0171",
      "question": "ExpertSparsity 是什么？",
      "ground_truth": "ExpertSparsity 是一种专家稀疏化方法，用于 MoE 中的前馈神经网络专家的稀疏化。",
      "predicted_answer": "ExpertSparsity[514] 是一种专家稀疏化方法，用于 MoE 中的前馈神经网络专家的稀疏化。它通过计算原始输出和稀疏化后输出之间的 Frobenius 范数来量化被稀疏化的专家的损失[1]。",
      "ground_context": [
        "ExpertSparsity 是一种专家稀疏化方法，用于 MoE 中的前馈神经网络专家的稀疏化。"
      ],
      "retrieved_contexts": [
        "大语言模型：从理论到实践 383\n配器进行渐进式结构化剪枝，使模型的固有知识得以转移，更好地保留冗余结构中的信息。同时\n引入结构稀疏优化算法，利用 LoRA 模块的信息来更新权重，提高知识保存率。\n混合专家 （MoE） 技术在大语言模型领域备受关注。 近期一些研究开始探索针对基于MoE 的\n大语言模型的专家剪枝方法。ExpertSparsity[514] 是一种专家稀疏化方法，用于 MoE 中的前馈神经\n网络专家的稀疏化。它通过计算原始输出和稀疏化后输出之间的 Frobenius 范数来量化被稀疏化\n的专家的损失。对 MoE 模型中的专家进行分层评估和剪枝，根据专家对模型整体性能的贡献程\n度，去除那些对性能影响较小的专家，以达到压缩模型和提高计算效率的目的。采用渐进式剪枝\n（Progressive Pruning）方法，逐步地对专家进行剪枝操作，在每次剪枝后评估模型性能，确保剪枝\n过程不会导致模型性能大幅下降，通过这种渐进的方式找到最优的剪枝策略。在推理过程中，采\n用了动态跳过 （Dynamic Skipping） 方法， 根据输入数据的特点动态地决定是否跳过某些专家的计\n算，对于那些...",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "x2\n图 2.11 共享专家模型 [68]\n稀疏混合专家模型中采用常规的门控策略时，分配给不同专家的词元可能需要一些共有知识\n或信息才能处理。 因此， 多个专家可能会在各自的参数中获取同样的知识， 进而导致专家参数出现\n冗余。如果构建专门用于捕捉并整合不同情境下共有知识的共享专家，那么其他专家之间的参数\n冗余情况将可能得到缓解。这种冗余情况的缓解，有助于构建一个参数利用更高效且专家专业性\n更强的模型。因此，DeepSeekMoE[74] 提出了分离 Ks 个专家作为共享专家的思路。无论门控网络\n所给出的结果如何， 每个词元都将被确定性地分配给这些共享专家， 如图2.11 所示， 深色块Shared\nFFN 为共享专家，所有输入都会分配给共享专家。为保持计算成本恒定，其他经门控网络分配的\n专家中被激活专家的数量将减少 Ks 个。\n稀疏混合专家模型中的 MoE 层对于并行计算也十分友好，能更便捷地在单个 GPU 上实现高\n效计算。常规稠密模型中，全部参数都会参与对所有输入数据的处理流程。与之不同，稀疏混合\n专家模型具备的稀疏特性，使得计算仅在系统的特定局部展开。也就是说，并非所有参数在处理...",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "p=0.16\n图 2.10 混合专家模型三种主要类型 [68]\n2.4.1 稀疏混合专家模型\n稀疏混合专家模型，如图2.10(a) 所示，对于每个输入词元，在前向计算中仅激活专家集合中\n的一个子集。 门控网络对专家子集进行选择， 通过计算排名前K 位专家的输出加权和来实现稀疏\n性。这个过程可以形式化的表示为：\nF M oE\nSparse (x; Θ; {Wi}N\ni=1) =\nNX\ni=1\nG(x; Θ)ifi(x; Wi) (2.38)\nG(x; Θ)i = softmax(TopK(g(x; Θ) + Rnoise, K))i (2.39)\nTop-K(g(x; Θ), K)i =\n(\ng(x; Θ)i, g(x; Θ)i的值属于前 K 项\n−∞, 其他\n(2.40)\n其中，g(x; Θ) 表示在进行 softmax 操作之前的门控值，G(x; Θ)i 表示门控网络针对第 i 个专家的\n输出，TopK(·, K) 函数的目标是保持向量的前 K 项不变， 其它维度设置为−∞。 鉴于softmax 函数\n自身所具有的独特性质，当把其中某些项设置为 −∞ 时，这些项所对应的值会近似等同于..."
      ],
      "response_time": 1.7695505619049072,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.5714285714285715,
        "rouge2": 0.0,
        "rougeL": 0.5714285714285715,
        "bleu": 0.35894348974034107,
        "semantic_similarity": 0.9028817415237427
      }
    },
    {
      "sample_id": "0172",
      "question": "什么是知识蒸馏？",
      "ground_truth": "知识蒸馏（Knowledge Distillation, KD）是一种广泛应用的模型压缩技术，其核心思想是将大型模型 （称为教师模型，Teacher Model） 的知识迁移到较小的模型 （称为学生模型，Student Model） 中。",
      "predicted_answer": "知识蒸馏（Knowledge Distillation, KD）是一种模型压缩技术，其核心思想是将大型模型（教师模型，Teacher Model）的知识迁移到较小的模型（学生模型，Student Model）中[471][3]。根据是否可以访问教师模型的内部结构（如参数、梯度），知识蒸馏可以分为白盒知识蒸馏和黑盒知识蒸馏两种类型[471]。",
      "ground_context": [
        "知识蒸馏（Knowledge Distillation, KD）是一种广泛应用的模型压缩技术，其核心思想是将大型模型 （称为教师模型，Teacher Model） 的知识迁移到较小的模型 （称为学生模型，Student Model） 中。"
      ],
      "retrieved_contexts": [
        "算，对于那些对当前输入不太重要的专家，可以直接跳过，从而减少不必要的计算量，提高推理\n速度。\n10.2.4 知识蒸馏\n知识蒸馏（Knowledge Distillation, KD）是一种广泛应用的模型压缩技术，其核心思想是将大\n型模型 （称为教师模型，Teacher Model） 的知识迁移到较小的模型 （称为学生模型，Student Model）\n中。现有研究主要关注如何高效地将大语言模型的各种能力传递到学生模型中。根据是否可以访\n问大模型的内部结构 （如参数、 梯度） ， 知识蒸馏可以分为两大类： 白盒知识蒸馏和黑盒知识蒸馏，\n如图 10.9 所示。\n白盒知识蒸馏黑盒知识蒸馏\n教师模型学生模型基于API的教师模型\n图 10.9 白盒知识蒸馏（左）和黑盒知识蒸馏（右）示意图 [471]\n白盒知识蒸馏（White-box KD）指的是利用对教师模型结构和参数的访问权限来进行蒸馏的\n方法。这种方式使得知识蒸馏能够有效地利用教师模型的中间特征和输出分布，以提升学生模型\n的性能。黑盒知识蒸馏（Black-box KD）指的是在教师模型的结构和参数不可用的情况下进行知",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "384 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n识蒸馏的方法。通常，黑盒知识蒸馏仅使用教师模型获得的最终结果来提炼学生模型。\n1. 白盒知识蒸馏\n白盒知识蒸馏能够获取教师模型的细节信息， 因而可以采用多种策略来提高学生模型的性能。\n给定教师分布 pT (y|x) 以及由参数 θ 确定的学生分布 pS\nθ (y|x)， 标准的知识蒸馏目标 （包括针对序列\n级模型的几种变体）[515, 516] 本质上是最小化教师分布和学生分布之间近似的正向 Kullback-Leibler\ndivergence （KLD） ，记为KL[pT ||pS\nθ ]，这会迫使 pS\nθ 覆盖 pT 的所有高概率区域（mode，也成模\n态） 。 对于文本分类任务， 这种方法表现良好， 因为输出空间通常由有限的类别组成， 使得pT (y|x)\n和 pS\nθ (y|x) 的高概率区域都很少。 然而， 对于开放式文本生成任务 （大语言模型应用通常属于这种\n情况） ，输出空间要复杂得多，并且由于模型容量有限，pT (y|x) 所包含的高概率区域数量可能远\n远超过 pS\nθ (y|x) 所能表达的数量。最...",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "况，分别对应于散度 D 设为正向 KL 散度，且学生模型生成数据比例 λ 分别为 1 和 0 的情况。也\n就是说，广义知识蒸馏允许对比例 λ 和散度进行其他选择。\n此外，TED[523] 提出了一种任务感知的逐层知识蒸馏方法。 该方法在教师模型和学生模型的每\n一层后添加过滤器，首先训练这些特定于任务的过滤器，然后在训练学生模型的过滤器时冻结教\n师模型的过滤器， 以使学生模型的输出特征能够与对应的教师过滤器输出特征对齐。MiniMoE[524]\n则通过采用专家混合（Mixture-of-Experts, MoE）模型作为学生模型，来缩小学生模型与教师模型\n之间的能力差距。KPTD[525] 提出了一种通过知识蒸馏将实体定义中的知识转移到大语言模型参数\n中的方法。该方法基于实体定义生成一个转移集，并利用这些定义对学生模型进行蒸馏，使学生\n模型的输出分布与教师模型相匹配。\n2. 黑盒知识蒸馏\n黑盒蒸馏的核心目标是在无法访问大模型内部参数的情况下，通过其输出（如分类概率或生\n成的文本） 来指导学生模型的学习。 具体而言， 学生模型可以通过模仿大模型的输出分布 （如分类\n概率分布） 来接近其行为..."
      ],
      "response_time": 2.40293550491333,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.8235294117647058,
        "rouge2": 0.8,
        "rougeL": 0.8235294117647058,
        "bleu": 0.4615010860208285,
        "semantic_similarity": 0.9682391881942749
      }
    },
    {
      "sample_id": "0173",
      "question": "SpecInfer 采用了什么方法来构建词元树？",
      "ground_truth": "SpecInfer 采用了一种静态扩展策略，以预设的扩展配置表示为向量 < k 1, k2, ..., km >。",
      "predicted_answer": "SpecInfer 采用了基于树的并行解码机制来构建词元树[2][5]。具体来说，它通过组织各种候选词元序列形成词元树，并利用这种树结构来进行并行解码，从而同时验证所有候选词元序列的正确性[2]。",
      "ground_context": [
        "SpecInfer 采用了一种静态扩展策略，以预设的扩展配置表示为向量 < k 1, k2, ..., km >。"
      ],
      "retrieved_contexts": [
        "SpecInfer 算法的核心在于利用小模型预测目标大语言模型的输出，并将这些预测组织为词元\n树结构。词元树的每个节点表示一个候选词元序列，通过基于树的并行解码机制，同时验证所有\n候选词元序列的正确性。为最大化推测性能，需要探索极其庞大的候选词元序列搜索空间。目前\n的大语言模型通常涉及非常大的词汇表，例如， Qwen 2.5 的词汇表大小达到了 15.16 万[136]，而\nSpecInfer 平均能够正确预测接下来的 4 个词元。因此，需要处理一个包含 1516434 ≈ 5.29 × 1020\n个可能词元组合的搜索空间。\n为了解决上述问题，首先需要使用大语言模型现有的提炼、量化和/或剪枝变体，构造小推测\n模型（Small Speculative model ，SSM） ，来指导推测。使用SSM 进行推测推理的一个关键挑战在\n于， 由于SSM 通常比大语言模型小 100 - 1000 倍，SSM 与大语言模型之间的一致性本质上受到模\n型能力差距的限制。SpecInfer 通过同时考虑针对给定输入提示以树结构组织的各种词元序列，来\n最大化推测性能。分别通过利用单个 SSM 内部以及多个...",
        "个 SSM 进行全面微调， 词元那些提示样本中SSM 与 LLM 生成词元完全一致的部分； 接着， 过滤\n已词元的提示样本， 利用剩下的样本对下一个SSM 进行微调。 通过重复这一流程，SpecInfer 生成\n了一组多样化的 SSM，它们的联合输出在训练数据上能够与 LLM 的结果实现高度一致性。\nSpecInfer 使用基于树的并行解码来计算其树注意力， 为了能够在词元树上进行并行化的验证，\nSpecInfer 提出了一种树形注意力（Tree Attention）计算方法，通过构造的掩码矩阵和基于深度优\n先的 KV-cache 更新机制， 验证器可以在不增加额外存储的同时， 尽可能并行化树中每一条路径的\n解码过程。相比于朴素的逐序列或逐词元的解码方式，树形解码可以同时在内存开销和计算效率\n上达到最优。对于给定的推测词元树 N ，SpecInfer 使用基于树的并行解码来计算其树注意力，并\n生成一个输出张量 O，该张量为 N 中的每个节点 u 都包含一个词元。SpecInfer 的词元树验证器\n对照大语言模型检查推测词元的正确性 SpecInfer 同时支持贪心解码和随机采样。\n一些...",
        "一些大语言模型使用贪心解码生成词元， 即在每个解码步骤中贪心选择可能性最高的词元。 针\n对此类模型，SpecInfer 从 N 的根节点开始， 迭代地对照大语言模型的原始输出检查节点的推测结\n果。对于 N 中的节点 u，如果 u 包含一个子节点 v（即 pv = u） ，且其词元与大语言模型的输出\n匹配（即 tv = O(u)） ，那么SpecInfer 就成功推测出其下一个词元。在这种情况下， SpecInfer 完\n成对节点 u 的验证，并继续检查其子节点 v。当节点 u 不包含与大语言模型输出匹配的子节点时，\nSpecInfer 将 O(u) 作为已验证节点添加到 N 中，并终止验证过程。最后，所有已验证节点追加到\n当前生成的词元序列 V 中。 词元树验证使SpecInfer 能够机会性地解码多个词元， 同时保持与增量\n解码相同的生成性能。\n为了提高生成词元的多样性，许多大语言模型采用随机解码，即从概率分布 P (ui|U ; ΘLLM )\n中采样一个词元，其中 U = u0, . . . , ui−1 是此前生成的词元，ui 是要生成的下一个词元，ΘLLM\n表示参数化的大语言模...",
        "396 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n构建词元树。 这一方法的核心在于观察到， 当SSM 与大语言模型）出现不一致时（即两者选择的\nTop-1 词元不同） ，LLM 选择的词元通常出现在 SSM 的 Top-K 词元中， 且K 值较小。 但是， 如果直\n接在每一步都选择 Top-K 词元会导致潜在词元序列数量呈指数增长，显著增加推理延迟和内存开\n销。因此，SpecInfer 采用了一种静态扩展策略，以预设的扩展配置表示为向量 < k 1, k2, ..., km >，\n其中 m 为最大推测解码步数，ki 表示第 i 步每个词元的扩展数量。例如，扩展配置 < 2, 2, 1 > 会\n生成 4 个词元序列。\n基于合并的词元树构建通过整合多个 SSM 来协同预测大语言模型的输出。 SpecInfer 采用无\n监督方法， 通过自适应提升 （Adaptive Boosting） 对多个SSM 进行联合优化， 使它们的输出与LLM\n的结果更为一致。在此过程中，SpecInfer 利用通用文本数据集（如 OpenWebText 语料库） ，将文\n本数据转换为一系列提示样本...",
        "大语言模型：从理论到实践 395\n其中，p(·|·) 和 q(·|·) 分别表示目标大语言模型和草稿模型的条件概率。如果第 i 个草稿词元被接\n受， 则将其设置为xi ← ˆxi。 如果未被接受， 则停止验证后续草稿词元， 并从以下分布中重新采样\nxi：\nnorm (max (0, p(·|x1, x2, . . . , xi−1) − q(·|x1, x2, . . . , xi−1))) (10.22)\n基于推测采样，衍生出了多种变体方法[538, 539]，这些方法的目标是验证多个草稿词元序列。\nSpecInfer[538] 提出了基于树的推测解码和验证 （Tree-based Speculative Inference and V erification）\n框架。增量解码、基于序列的推测推理以及基于树的推测推理之间的对比如图10.14所示。\n(a)增量解码 (b)时间线对比\n标准序列解码增量解码 增量解码时间线\n基于序列的推测推理时间线\n基于树的推测推理时间线\n图 10.14 增量解码、基于序列的推测推理以及基于树的推测推理示意图 [471]\nSpecInfer 算法的核心在于..."
      ],
      "response_time": 1.848268747329712,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.22222222222222224,
        "rouge2": 0.0,
        "rougeL": 0.22222222222222224,
        "bleu": 0.028694328961304638,
        "semantic_similarity": 0.6047420501708984
      }
    },
    {
      "sample_id": "0174",
      "question": "整个流程从什么开始？",
      "ground_truth": "整个流程从初始化一个预训练的学生模型开始。",
      "predicted_answer": "整个流程从确定运行的设备开始[1]。",
      "ground_context": [
        "整个流程从初始化一个预训练的学生模型开始，依次通过以下步骤进行："
      ],
      "retrieved_contexts": [
        "142 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n5. DeepSpeed 初始化\n设置 DeepSpeed 的配置参数后， 可以利用DeepSpeed 进行模型训练的初始化， 初始化流程如下。\n（1）确定运行的设备：首先，检查代码是否有指定的本地 GPU（通过 args.local_rank） 。如果\n没有指定，则程序默认使用 CUDA 设备。否则，它会为进程设置指定的 GPU。\n（2） 初始化分布式后端： 在分布式训练中， 使用deepspeed.init_distributed() 函数实现每个进程\n与其他进程的同步，初始化分布式环境。\n（3）获取当前进程的全局排序：在分布式训练中，使用torch.distributed.get_rank() 函数获得每\n个进程的唯一排序或 ID。\n（4）设置 DeepSpeed 配置：根据用户参数（如是否进行 offload、使用哪个 Zero Stage 等）构\n建一个 DeepSpeed 配置字典，来决定训练设置。\n（5） 同步所有工作进程： 使用torch.distributed.barrier() 确保在进一步的初始化之...",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "348 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n任务集合\n多轮对话\n需要分解的查询\n需要消歧的查询\n当前轮次是否需要检索上下文？\n否，继续下一个轮次\n分解模板：生成分解查询\n消歧模板：生成消歧查询\n是\n查询搜索：使用生成的查询词检索相关上下文\n答案模板：根据上下文生成答案\n图 9.17 RQ-RAG 数据构造的流程 [440]\n数据构造整体流程如图9.17所示，整个流程分为以下几个关键步骤：\n（1） 从任务池中收集代表性任务， 并将其分类为三种类型 （如消歧查询、 复杂查询分解等） ， 根\n据任务特性，每个数据集对应特定的数据类型。这一步通过任务的特性和需求进行分类，确保数\n据生成流程的针对性。\n（2） 对于每种任务类型， 使用预定义的提示模板调用ChatGPT 生成优化后的查询。 提示模板根据\n任务类型的不同进行了定制， 例如针对模糊查询的提示会强调消除歧义， 而针对复杂查询的提示会\n引导模型进行分解。 生成的优化查询被用于从外部数据源检索相关信息， 检索过程以DuckDuckGo\n为主要搜索引擎，其他搜索工具（如 Bing）作为补充。\n（3） 使用ChatGPT，...",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "调用插件或执行复杂任务；卡片功能则为智能体提供了信息展示和互动的新形式，让用户在网页\n或移动端直观查看数据、流程和结果。\n使用 Coze 平台可以通过以下简单的五个步骤就可以构造快速搭建一个“夸夸机器人” ，并在\n多个平台提供对外服务。\n步骤 1： 创建一个智能体。 在扣子平台创建智能体非常简单： 登录后， 点击页面左上角的 “L” ，\n输入智能体名称和功能介绍，并通过生成图标自动生成头像，或使用“AI 创建”功能，通过自然\n语言描述需求， 由平台自动生成智能体。 点击确认后， 进入智能体编排页面。 在这里， 可以通过左\n侧人设与回复逻辑面板描述智能体的身份和任务；利用中间技能面板为智能体配置扩展能力；在\n右侧预览与调试面板中实时测试智能体，确保其功能和交互效果符合预期。\n步骤 2：编写提示词。配置智能体的第一步是编写提示词，即定义智能体的人设与回复逻辑。\n这部分内容决定了智能体的基本人设，并持续影响其在所有会话中的回复效果。在设计提示词时，\n建议明确模型的角色、设计特定的语言风格，并限制回答范围，以确保对话内容符合用户的预期。\n例如，对于一个“夸夸机器人” ，提示词可以设置为："
      ],
      "response_time": 0.5818006992340088,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.12586898234382776,
        "semantic_similarity": 0.7326380014419556
      }
    },
    {
      "sample_id": "0175",
      "question": "FP8 的无穷大表示是什么？",
      "ground_truth": "N/A",
      "predicted_answer": "根据提供的文档，我无法回答这个问题。文档中未提及关于FP8的无穷大表示的相关信息。",
      "ground_context": [
        "无穷大（Infinity） N/A S.11111.002"
      ],
      "retrieved_contexts": [
        "通信成本优化方面效果有限，未能充分挖掘 FP8 的潜力。为了解决这一问题，Microsoft Azure 和\nMicrosoft Research 的研究人员开源了 FP8-LM 框架 [530]。该框架提出了一种高度优化的 FP8 混合\n精度训练方法，专为大语言模型设计。其核心思想是将 FP8 的计算、存储和通信贯穿于大型模型\n训练的全过程，使前向传播和反向传播全程基于低精度 FP8，从而显著降低系统工作负载，并实\n现更高效的训练过程。2025 年 1 月，文献 [531] 提出的方法，更是将精度进一步降低到 FP4。\n使用 FP8 进行大语言模型的训练并非易事，主要面临数据下溢或上溢问题，以及因 FP8 数据\n格式动态范围较窄和精度较低而引发的量化误差，这些问题可能导致数值不稳定性，甚至在训练\n过程中出现不可逆的发散现象。 为了解决这些挑战， 文献[530] 指出， 在大语言模型的训练中， 大\n部分变量（如梯度、优化器状态）可以采用低精度数据格式，而不会影响模型的准确性，也无需\n调整超参数。 具体而言，FP8-LM 提出了三个优化级别， 通过逐步引入FP8 通信、FP8 优化器以及...",
        "392 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n序列并行 序列并行张量并行A和B：参数（FP8），Y和Z：激活值\n图 10.11 采用 FP8 张量和序列并行的 Transformer 层 [530]\n零冗余优化器 （Zero Redundancy Data Parallelism，ZeRO）[173–175] 也是大模型训练中的另一种\n常用分布式学习技术。ZeRO 的核心思想是将模型状态分片到各设备，使每个设备仅保存训练步\n骤所需数据 （如主权重、 梯度和优化器状态） 的一部分。 为了减少内存消耗，ZeRO 方法通常将单\n个张量分割为多个子张量，并将其分布到不同的设备上。\n直接将 FP8 应用于 ZeRO 也是不可行的，因为难以处理与 FP8 分片相关的缩放因子。每个张\n量的缩放因子需要与 FP8 分片一起分发。为了解决这一问题，FP8-LM 实现了一种新的 FP8 分布\n方案，该方案将整个张量分布到设备上，而不是像ZeRO 那样将张量分割为多个子张量进行分布。\nFP8 张量的分布采用贪婪策略，具体过程如算法 1 所述。具体来说，我们的方法首先根据张量状\n态的大小对...",
        "成为下一代低精度数据表示的主流格式。相较于现有的 16 位和 32 位混合精度方案，FP8 不仅能\n够将训练速度提升一倍， 还能实现50% 到 75% 的内存和通信开销优化， 这一突破性进展为构建下\n一代大规模基础模型开辟了广阔前景。\n在本节中将首先介绍 FP8 编码方式，并在此基础上介绍基于 FP8 的大模型训练方法。\n10.3.1 FP8 编码\nFP8 是一种低精度浮点数格式，专为提高计算效率和降低存储需求而设计，广泛应用于深度\n学习模型的训练和推理中。FP8 编码采用 IEEE 浮点表示的变体，包括符号位（S，sign） 、指数位\n（E，exponent）和尾数位（M，mantissa） 。指数位决定了动态范围，而尾数位决定了表示精度。其\n关键特征是通过减少位数来降低计算复杂度和内存占用。FP8 的常见表示方式有以下几种：E5M2\n（5 位指数和 2 位尾数） 、E4M3（4 位指数和 3 位尾数） 、E3M4（3 位指数和 4 位尾数） 以及E2M5（2\n位指数和 5 位尾数） 。 通过调整指数位的数量，FP8 可以适应不同动态范围的计算需求。 由于E3M4\n和以及 E2M5 ...",
        "通过这一设计，FM8-LM 的 FP8 混合精度优化器在训练过程中，每个参数仅消耗 6 字节的内存：\n2|{z}\n主权重\n+ 1|{z}\n梯度\n+ 1 + 2|{z}\nAdam 状态\n= 6字节 (10.20)\n3. FP8 分布式并行训练\n训练大语言模型需要分布式学习策略， 以实现跨多GPU 的并行化。 常用的策略包括数据并行\n（Data Parallelism） 、张量并行（Tensor Parallelism） 、流水线并行（Pipeline Parallelism）以及序列并\n行（Sequence Parallelism） 。每种并行策略都有其优点，并在现有系统中以互补的方式使用。对于\n这些策略的 FP8 支持而言，数据并行和流水线并行无需进行任何特定的修改，因为在将数据批次\n或模型层拆分到不同设备时，这两种策略并不涉及额外的 FP8 计算和通信。\n张量并行将模型的单个层划分到多个设备上，使得权重、梯度和激活张量的分片分布在不同\n的 GPU 上， 而不是集中在单个GPU 上。 为了在张量并行中支持FP8，FP8-LM 将分片的权重和激\n活张量转换为 FP8 格式，用于线性层的计...",
        "FP8 分布式并行训练，简化混合精度和分布式训练流程。这三个优化级别逐步扩大 FP8 在大语言\n模型训练中的应用比例， 优化级别越高， 训练过程中对FP8 的依赖越强。 此外，FP8-LM 框架还支"
      ],
      "response_time": 0.8462631702423096,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0.0,
        "bleu": 0.0,
        "semantic_similarity": 0.4422757625579834
      }
    },
    {
      "sample_id": "0176",
      "question": "稀疏化的目标是什么？",
      "ground_truth": "其目标是通过增加模型参数或激活中零值元素的比例，降低计算复杂度和内存使用。",
      "predicted_answer": "稀疏化的目标是通过增加模型参数或激活中零值元素的比例，降低计算复杂度和内存使用，同时实现资源的节约和性能的优化[1]。",
      "ground_context": [
        "稀疏化 （Sparsification） 是一种模型压缩技术， 其目标是通过增加模型参数或激活中零值元素的比例，降低计算复杂度和内存使用。"
      ],
      "retrieved_contexts": [
        "大语言模型：从理论到实践 381\n阵。通过迭代应用量化和 SVD，LoftQ 实现了对原始权重更准确的近似，从而进一步提升模型的\n性能和适配能力。\n10.2.3 模型稀疏化\n稀疏化 （Sparsification） 是一种模型压缩技术， 其目标是通过增加模型参数或激活中零值元素\n的比例，降低计算复杂度和内存使用。稀疏化利用计算过程中对零元素的高效忽略，实现了资源\n的节约和性能的优化。在大语言模型中，稀疏化通常应用于权重参数和注意力激活。稀疏化的主\n要策略包括权重剪枝和稀疏注意力机制。稀疏注意力机制已在本书第 1 节进行了详细讨论，本节\n将重点探讨权重剪枝机制。\n权重剪枝（Weight Pruning）是一种系统地从模型中移除不那么关键的权重和结构的方法，旨\n在在预填充阶段和解码阶段减少计算和内存成本，同时不显著牺牲性能。权重剪枝方法根据剪枝\n过程的粒度可以分为两类：无结构剪枝和结构化剪枝，如图10.8所示。\n无结构剪枝粒度：权重结构化剪枝粒度：通道/组/层\n图 10.8 无结构剪枝和结构化剪枝示意图 [471]\n1. 无结构剪枝\n无结构剪枝（Unstructured pruning） ...",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "1. 稀疏检索\n稀疏检索 （Sparse Retrieval） 是一种基于统计特征的方法， 通过将查询和文档转换为稀疏向量\n来实现检索。稀疏向量的特点是大部分元素为零，仅保留少量非零值，这使得计算更加高效且存\n储成本较低。许多经典的信息检索方法，如TF-IDF 和 BM25，都是稀疏检索的典型实现。这些方\n法通过词频、逆文档频率等显性统计特征对查询和文档进行建模，能够快速匹配相关内容。稀疏\n检索架构如图9.7所示。\n查询词\n查询词向量\n相似度 数据集\n图 9.7 稀疏检索架构图\n稀疏检索的最大优势在于其高效性，尤其适用于处理大规模文档库的检索任务。由于稀疏向\n量中仅计算非零元素的部分，相较于密集向量方法，其计算复杂度显著降低。因此，稀疏检索在\n资源有限或实时性要求较高的场景中表现尤为突出。稀疏检索器在大规模数据集上的效率使其成\n为工业界的主流选择之一。\n尽管稀疏检索在效率上具有明显优势，但其在捕捉复杂语义关系方面存在局限性。由于稀疏\n方法主要依赖显性统计特征，如词频和词项匹配，无法有效处理同义词、上下文语义等深层语义",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "灵机，这也在一定程度上解释了为什么稀疏注意力模型可以取得较好的结果。\n图 2.7 典型的稀疏注意力模型 [49]\n基于内容的稀疏注意力机制根据输入数据创建稀疏注意力，其中一种很简单的方法是选择和\n给定查询 （Query） 有很高相似度的键 （Key） 。Routing Transformer[55] 采用 K-means 聚类方法， 针\n对 Query{qi}T\ni=1 和 Key{ki}T\ni=1 进行聚类，类中心向量集合为 {µi}k\ni=1，其中 k 是类中心的个数。\n每个 Query 只与其处在相同簇（Cluster）下的 Key 进行交互。中心向量采用滑动平均的方法进行\n更新：\neµ ← λeµ + (1 − λ)\n0\n@ X\ni:µ(qi)=µ\nqi +\nX\nj:µ(kj )=µ\nkj\n1\nA (2.28)\ncµ ← λcµ + (1 − λ)|µ| (2.29)\nµ ← eµ\ncµ\n(2.30)\n其中 |µ| 表示在簇 µ 中向量的数量。\nReformer[56] 则采用局部敏感哈希（Local-Sensitive Hashing，LSH）的方法为每个 Query 选择\n..."
      ],
      "response_time": 1.1986124515533447,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.5130113819243268,
        "semantic_similarity": 0.8187333941459656
      }
    },
    {
      "sample_id": "0177",
      "question": "MSS算法是什么的验证方法？",
      "ground_truth": "MSS算法用于验证推测词元树。",
      "predicted_answer": "MSS算法是SpecInfer引入的一种用于验证推测词元树的多步推测采样算法[1]。",
      "ground_context": [
        "为了使用随机解码验证推测词元树，SpecInfer 引入了一种多步推测采样（Multi-step Speculative Sampling，MSS）算法来进行验证。"
      ],
      "retrieved_contexts": [
        "表示参数化的大语言模型。为了使用随机解码验证推测词元树，SpecInfer 引入了一种多步推测采\n样（Multi-step Speculative Sampling，MSS）算法来进行验证。 对于词元树N 中的非叶子节点， 对\n比多个 SSM 输出与大语言模型输出的概率， P (xs|u,ΘLLM )\nP (xs|u,ΘSSM s ) ，在一定范围之内就可以通过验证。\n在推测解码方法中，草稿词元的接受率在很大程度上取决于草稿模型与目标大语言模型输出",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "可以将 SSM 表示为卷积形式的状态预测， 能够像卷积神经网络 （CNN） 一样实现并行训练。 这使\n得 SSM 在处理大规模数据时具有较高的计算效率。\nMamba 模型[474] 采用了离散化的状态空间模型，并引入了一种改进的选择机制，称为选择性\n状态空间模型（Selective State Space Models ） 。这一机制使模型能够根据输入内容有选择地传播或\n遗忘信息，从而增强了表达能力。为了确保选择机制的 SSM 能在硬件上高效运行，Mamba 设计\n了一种结合内核优化与重新计算的硬件感知算法，有效避免了中间状态的存储，大幅提升了速度\n和内存效率。此外，Mamba 将 H3[476] 中的 SSM 块与 Transformer 中的 MLP 块整合为一个简化的\n模块，并通过重复堆叠这些模块构建整体架构。这一简化设计进一步提升了训练和推理效率。\nMamba 的网络结构对 GPU 的计算高度友好，尤其在数据交互方面展现了卓越的性能。其数\n据交互主要集中在 GPU 与片上 SRAM 之间，这种交互完全发生在 GPU 芯片内部，具有极高的速\n度，显著提升了数据访问和处理效率。在性...",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "SpecInfer 算法的核心在于利用小模型预测目标大语言模型的输出，并将这些预测组织为词元\n树结构。词元树的每个节点表示一个候选词元序列，通过基于树的并行解码机制，同时验证所有\n候选词元序列的正确性。为最大化推测性能，需要探索极其庞大的候选词元序列搜索空间。目前\n的大语言模型通常涉及非常大的词汇表，例如， Qwen 2.5 的词汇表大小达到了 15.16 万[136]，而\nSpecInfer 平均能够正确预测接下来的 4 个词元。因此，需要处理一个包含 1516434 ≈ 5.29 × 1020\n个可能词元组合的搜索空间。\n为了解决上述问题，首先需要使用大语言模型现有的提炼、量化和/或剪枝变体，构造小推测\n模型（Small Speculative model ，SSM） ，来指导推测。使用SSM 进行推测推理的一个关键挑战在\n于， 由于SSM 通常比大语言模型小 100 - 1000 倍，SSM 与大语言模型之间的一致性本质上受到模\n型能力差距的限制。SpecInfer 通过同时考虑针对给定输入提示以树结构组织的各种词元序列，来\n最大化推测性能。分别通过利用单个 SSM 内部以及多个..."
      ],
      "response_time": 0.8996429443359375,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.5,
        "rouge2": 0.0,
        "rougeL": 0.5,
        "bleu": 0.1481394578697113,
        "semantic_similarity": 0.8857102990150452
      }
    },
    {
      "sample_id": "0178",
      "question": "FP8-LM 框架是由哪些组织的研究人员开源的？",
      "ground_truth": "Microsoft Azure 和 Microsoft Research",
      "predicted_answer": "FP8-LM 框架是由 Microsoft Azure 和 Microsoft Research 的研究人员开源的。[1]",
      "ground_context": [
        "为了解决这一问题，Microsoft Azure 和 Microsoft Research 的研究人员开源了 FP8-LM 框架。"
      ],
      "retrieved_contexts": [
        "通信成本优化方面效果有限，未能充分挖掘 FP8 的潜力。为了解决这一问题，Microsoft Azure 和\nMicrosoft Research 的研究人员开源了 FP8-LM 框架 [530]。该框架提出了一种高度优化的 FP8 混合\n精度训练方法，专为大语言模型设计。其核心思想是将 FP8 的计算、存储和通信贯穿于大型模型\n训练的全过程，使前向传播和反向传播全程基于低精度 FP8，从而显著降低系统工作负载，并实\n现更高效的训练过程。2025 年 1 月，文献 [531] 提出的方法，更是将精度进一步降低到 FP4。\n使用 FP8 进行大语言模型的训练并非易事，主要面临数据下溢或上溢问题，以及因 FP8 数据\n格式动态范围较窄和精度较低而引发的量化误差，这些问题可能导致数值不稳定性，甚至在训练\n过程中出现不可逆的发散现象。 为了解决这些挑战， 文献[530] 指出， 在大语言模型的训练中， 大\n部分变量（如梯度、优化器状态）可以采用低精度数据格式，而不会影响模型的准确性，也无需\n调整超参数。 具体而言，FP8-LM 提出了三个优化级别， 通过逐步引入FP8 通信、FP8 优化器以及...",
        "通过这一设计，FM8-LM 的 FP8 混合精度优化器在训练过程中，每个参数仅消耗 6 字节的内存：\n2|{z}\n主权重\n+ 1|{z}\n梯度\n+ 1 + 2|{z}\nAdam 状态\n= 6字节 (10.20)\n3. FP8 分布式并行训练\n训练大语言模型需要分布式学习策略， 以实现跨多GPU 的并行化。 常用的策略包括数据并行\n（Data Parallelism） 、张量并行（Tensor Parallelism） 、流水线并行（Pipeline Parallelism）以及序列并\n行（Sequence Parallelism） 。每种并行策略都有其优点，并在现有系统中以互补的方式使用。对于\n这些策略的 FP8 支持而言，数据并行和流水线并行无需进行任何特定的修改，因为在将数据批次\n或模型层拆分到不同设备时，这两种策略并不涉及额外的 FP8 计算和通信。\n张量并行将模型的单个层划分到多个设备上，使得权重、梯度和激活张量的分片分布在不同\n的 GPU 上， 而不是集中在单个GPU 上。 为了在张量并行中支持FP8，FP8-LM 将分片的权重和激\n活张量转换为 FP8 格式，用于线性层的计...",
        "178 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n的上下文窗口扩展到 32768。\n位置插值通过小代价的微调显著扩展 LLaMA 模型的上下文窗口，在保持原有扩展模型内任\n务能力的基础上，显著增加模型对长文本的建模能力。另外，通过位置插值扩展的模型可以充分\n重用现有的预训练大语言模型和优化方法，这在实际应用中具有很大吸引力。\n5.4 DeepSpeed-Chat SFT 实践\nChatGPT 整体的训练过程复杂，虽然基于 DeepSpeed 可以通过单机多卡、多机多卡、流水线\n并行等操作来训练和微调大语言模型，但是没有端到端的基于人类反馈机制的强化学习的规模化\n系统， 仍然会造成训练类ChatGPT 系统非常困难。DeepSpeed-Chat[244] 是微软于 2023 年 4 月发布\n的基于 DeepSpeed 用于训练类 ChatGPT 模型的开发工具。基于 DeepSpeed-Chat 训练类 ChatGPT\n对话模型的步骤框架如图5.10 所示，包含以下三个步骤。\n（1）指令微调：使用精选的人类回答来微调预训练语言模型以应对各种查询。\n（2）奖励模型微调：使用...",
        "FP8 分布式并行训练，简化混合精度和分布式训练流程。这三个优化级别逐步扩大 FP8 在大语言\n模型训练中的应用比例， 优化级别越高， 训练过程中对FP8 的依赖越强。 此外，FP8-LM 框架还支",
        "（1）指令微调：使用精选的人类回答来微调预训练语言模型以应对各种查询。\n（2）奖励模型微调：使用一个包含人类对同一查询的多个答案打分的数据集来训练一个独立\n的奖励模型。\n（3）基于人类反馈的强化学习（Reinforcement Learning from Human Feedback ，RLHF）训练：\n利用近端策略优化（Proximal Policy Optimization，PPO） 算法， 根据奖励模型的奖励反馈进一步微\n调 SFT 模型。\n本节只针对步骤 （1） 指令微调的实践进行介绍， 对于奖励模型微调和RLHF 训练的实践会在\n后续对应章节中详细介绍。\n图 5.10 基于 DeepSpeed-Chat 训练类 ChatGPT 对话模型的三个步骤 [244]\nDeepSpeed-Chat 具有以下三大核心功能。"
      ],
      "response_time": 0.8320069313049316,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.7272727272727273,
        "rouge2": 0.6666666666666666,
        "rougeL": 0.7272727272727273,
        "bleu": 0.17923344640485428,
        "semantic_similarity": 0.6195186376571655
      }
    },
    {
      "sample_id": "0179",
      "question": "在使用 FP8 进行大语言模型训练时，如何解决低精度全局归约操作引发的数值不稳定性问题？",
      "ground_truth": "为了解决使用 FP8 进行大语言模型训练时出现的数值不稳定性问题，可以采用后缩放方法。在全局归约过程中，先对梯度进行求和，然后在梯度收集的过程中进行除法缩放。这种方法使梯度值接近 FP8 数据类型的最大值，有效缓解了下溢问题，尽管可能引发上溢问题。",
      "predicted_answer": "在使用 FP8 进行大语言模型训练时，可以通过采用后缩放方法来解决低精度全局归约操作引发的数值不稳定性问题。后缩放方法先对梯度进行求和，然后再在梯度收集过程中进行除法缩放[3]。这种方法可以在避免数据下溢的同时完成全局归约操作。",
      "ground_context": [
        "使用 FP8 进行大语言模型的训练并非易事，主要面临数据下溢或上溢问题，以及因 FP8 数据格式动态范围较窄和精度较低而引发的量化误差，这些问题可能导致数值不稳定性，甚至在训练过程中出现不可逆的发散现象。",
        "具体而言，在全局归约过程中，跨 GPU 聚合梯度通常有两种标准方法：预缩放和后缩放。后缩放方法使梯度值接近 FP8 数据类型的最大值，有效缓解了下溢问题。"
      ],
      "retrieved_contexts": [
        "通信成本优化方面效果有限，未能充分挖掘 FP8 的潜力。为了解决这一问题，Microsoft Azure 和\nMicrosoft Research 的研究人员开源了 FP8-LM 框架 [530]。该框架提出了一种高度优化的 FP8 混合\n精度训练方法，专为大语言模型设计。其核心思想是将 FP8 的计算、存储和通信贯穿于大型模型\n训练的全过程，使前向传播和反向传播全程基于低精度 FP8，从而显著降低系统工作负载，并实\n现更高效的训练过程。2025 年 1 月，文献 [531] 提出的方法，更是将精度进一步降低到 FP4。\n使用 FP8 进行大语言模型的训练并非易事，主要面临数据下溢或上溢问题，以及因 FP8 数据\n格式动态范围较窄和精度较低而引发的量化误差，这些问题可能导致数值不稳定性，甚至在训练\n过程中出现不可逆的发散现象。 为了解决这些挑战， 文献[530] 指出， 在大语言模型的训练中， 大\n部分变量（如梯度、优化器状态）可以采用低精度数据格式，而不会影响模型的准确性，也无需\n调整超参数。 具体而言，FP8-LM 提出了三个优化级别， 通过逐步引入FP8 通信、FP8 优化器以及...",
        "成为下一代低精度数据表示的主流格式。相较于现有的 16 位和 32 位混合精度方案，FP8 不仅能\n够将训练速度提升一倍， 还能实现50% 到 75% 的内存和通信开销优化， 这一突破性进展为构建下\n一代大规模基础模型开辟了广阔前景。\n在本节中将首先介绍 FP8 编码方式，并在此基础上介绍基于 FP8 的大模型训练方法。\n10.3.1 FP8 编码\nFP8 是一种低精度浮点数格式，专为提高计算效率和降低存储需求而设计，广泛应用于深度\n学习模型的训练和推理中。FP8 编码采用 IEEE 浮点表示的变体，包括符号位（S，sign） 、指数位\n（E，exponent）和尾数位（M，mantissa） 。指数位决定了动态范围，而尾数位决定了表示精度。其\n关键特征是通过减少位数来降低计算复杂度和内存占用。FP8 的常见表示方式有以下几种：E5M2\n（5 位指数和 2 位尾数） 、E4M3（4 位指数和 3 位尾数） 、E3M4（3 位指数和 4 位尾数） 以及E2M5（2\n位指数和 5 位尾数） 。 通过调整指数位的数量，FP8 可以适应不同动态范围的计算需求。 由于E3M4\n和以及 E2M5 ...",
        "大语言模型：从理论到实践 389\n持 FP8 的低位并行化，包括张量并行、流水线并行和序列并行。\n1. FP8 梯度和 AllReduce 通信\n现有的混合精度训练方法通常采用 16 位或 32 位数据类型来计算和存储梯度 [532]，这导致整\n个训练过程中集体通信对带宽的需求非常高。然而，直接将 FP8 应用于梯度会引发精度下降的问\n题，主要原因在于低精度全局归约（Low-bit All-Reduce）操作中容易出现下溢和上溢问题。\n具体而言，在全局归约过程中，跨 GPU 聚合梯度通常有两种标准方法：预缩放（Pre-scaling）\n和后缩放 （Post-scaling） 。 预缩放方法是在求和之前， 将第i 个 GPU 计算出的梯度 gi 除以 GPU 总\n数 N ，其公式如下：\ng = g1/N + g2/N + + gN /N (10.12)\n当 N 较大时， 这种除法可能导致数据下溢， 尤其是在使用FP8 低精度表示梯度时。 后缩放方法则\n先对梯度进行求和，然后在梯度收集的过程中进行除法缩放，公式为：\ng = (g1 + g2 + + gN )/N (10.13)\n后缩放方法...",
        "通过这一设计，FM8-LM 的 FP8 混合精度优化器在训练过程中，每个参数仅消耗 6 字节的内存：\n2|{z}\n主权重\n+ 1|{z}\n梯度\n+ 1 + 2|{z}\nAdam 状态\n= 6字节 (10.20)\n3. FP8 分布式并行训练\n训练大语言模型需要分布式学习策略， 以实现跨多GPU 的并行化。 常用的策略包括数据并行\n（Data Parallelism） 、张量并行（Tensor Parallelism） 、流水线并行（Pipeline Parallelism）以及序列并\n行（Sequence Parallelism） 。每种并行策略都有其优点，并在现有系统中以互补的方式使用。对于\n这些策略的 FP8 支持而言，数据并行和流水线并行无需进行任何特定的修改，因为在将数据批次\n或模型层拆分到不同设备时，这两种策略并不涉及额外的 FP8 计算和通信。\n张量并行将模型的单个层划分到多个设备上，使得权重、梯度和激活张量的分片分布在不同\n的 GPU 上， 而不是集中在单个GPU 上。 为了在张量并行中支持FP8，FP8-LM 将分片的权重和激\n活张量转换为 FP8 格式，用于线性层的计...",
        "392 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n序列并行 序列并行张量并行A和B：参数（FP8），Y和Z：激活值\n图 10.11 采用 FP8 张量和序列并行的 Transformer 层 [530]\n零冗余优化器 （Zero Redundancy Data Parallelism，ZeRO）[173–175] 也是大模型训练中的另一种\n常用分布式学习技术。ZeRO 的核心思想是将模型状态分片到各设备，使每个设备仅保存训练步\n骤所需数据 （如主权重、 梯度和优化器状态） 的一部分。 为了减少内存消耗，ZeRO 方法通常将单\n个张量分割为多个子张量，并将其分布到不同的设备上。\n直接将 FP8 应用于 ZeRO 也是不可行的，因为难以处理与 FP8 分片相关的缩放因子。每个张\n量的缩放因子需要与 FP8 分片一起分发。为了解决这一问题，FP8-LM 实现了一种新的 FP8 分布\n方案，该方案将整个张量分布到设备上，而不是像ZeRO 那样将张量分割为多个子张量进行分布。\nFP8 张量的分布采用贪婪策略，具体过程如算法 1 所述。具体来说，我们的方法首先根据张量状\n态的大小对..."
      ],
      "response_time": 2.1006317138671875,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 0.3333333333333333
      },
      "answer_quality_metrics": {
        "rouge1": 0.5,
        "rouge2": 0.0,
        "rougeL": 0.5,
        "bleu": 0.3547129799431216,
        "semantic_similarity": 0.9476096630096436
      }
    },
    {
      "sample_id": "0180",
      "question": "在大语言模型中，如何通过稀疏化技术和推测采样技术来优化模型的性能和生成多样化的输出？",
      "ground_truth": "稀疏化技术通过增加模型参数或激活中零值元素的比例，降低计算复杂度和内存使用，从而优化模型性能。推测采样技术则在保持输出分布等效性的同时，与核采样的概率特性一致，生成多样化的词元序列，适配解码框架。",
      "predicted_answer": "根据提供的文档，我无法回答这个问题。文档中提到了稀疏化技术（如权重剪枝和稀疏注意力机制），以及推测采样的相关内容（如混合专家模型中的动态跳过方法），但未具体说明如何结合这两种技术来优化模型性能和生成多样化输出。[1][3]",
      "ground_context": [
        "稀疏化利用计算过程中对零元素的高效忽略，实现了资源的节约和性能的优化。",
        "推测采样在保持输出分布等效性的同时，与核采样的概率特性一致，从而能够生成多样化的词元序列。"
      ],
      "retrieved_contexts": [
        "大语言模型：从理论到实践 381\n阵。通过迭代应用量化和 SVD，LoftQ 实现了对原始权重更准确的近似，从而进一步提升模型的\n性能和适配能力。\n10.2.3 模型稀疏化\n稀疏化 （Sparsification） 是一种模型压缩技术， 其目标是通过增加模型参数或激活中零值元素\n的比例，降低计算复杂度和内存使用。稀疏化利用计算过程中对零元素的高效忽略，实现了资源\n的节约和性能的优化。在大语言模型中，稀疏化通常应用于权重参数和注意力激活。稀疏化的主\n要策略包括权重剪枝和稀疏注意力机制。稀疏注意力机制已在本书第 1 节进行了详细讨论，本节\n将重点探讨权重剪枝机制。\n权重剪枝（Weight Pruning）是一种系统地从模型中移除不那么关键的权重和结构的方法，旨\n在在预填充阶段和解码阶段减少计算和内存成本，同时不显著牺牲性能。权重剪枝方法根据剪枝\n过程的粒度可以分为两类：无结构剪枝和结构化剪枝，如图10.8所示。\n无结构剪枝粒度：权重结构化剪枝粒度：通道/组/层\n图 10.8 无结构剪枝和结构化剪枝示意图 [471]\n1. 无结构剪枝\n无结构剪枝（Unstructured pruning） ...",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "大语言模型：从理论到实践 383\n配器进行渐进式结构化剪枝，使模型的固有知识得以转移，更好地保留冗余结构中的信息。同时\n引入结构稀疏优化算法，利用 LoRA 模块的信息来更新权重，提高知识保存率。\n混合专家 （MoE） 技术在大语言模型领域备受关注。 近期一些研究开始探索针对基于MoE 的\n大语言模型的专家剪枝方法。ExpertSparsity[514] 是一种专家稀疏化方法，用于 MoE 中的前馈神经\n网络专家的稀疏化。它通过计算原始输出和稀疏化后输出之间的 Frobenius 范数来量化被稀疏化\n的专家的损失。对 MoE 模型中的专家进行分层评估和剪枝，根据专家对模型整体性能的贡献程\n度，去除那些对性能影响较小的专家，以达到压缩模型和提高计算效率的目的。采用渐进式剪枝\n（Progressive Pruning）方法，逐步地对专家进行剪枝操作，在每次剪枝后评估模型性能，确保剪枝\n过程不会导致模型性能大幅下降，通过这种渐进的方式找到最优的剪枝策略。在推理过程中，采\n用了动态跳过 （Dynamic Skipping） 方法， 根据输入数据的特点动态地决定是否跳过某些专家的计\n算，对于那些...",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "大语言模型：从理论到实践 247\n更专注于学习高级视觉语义， 而非过度依赖OCR 任务， 从而提升模型在对象和场景相关零样本任\n务中的性能。\n基于预训练 VLM 的剪枝方法是目前提升数据质量与模型训练效率的最有效策略之一。这些\n方法通过计算图文对的嵌入相似性来评估对齐程度。其中，CLIPScore [310] 依托预训练的 CLIP 模\n型，计算图像和文本嵌入之间的余弦相似度，并据此对图文对进行排序；LAION 的筛选策略基于\n由 4 亿对图文对训练的 OpenAI CLIP 模型，对大规模网络数据集进行对齐评估，并过滤得分最低\n的样本；T-MARS 方法[311] 在计算 CLIPScore 前， 通过检测并遮盖图像中的文本区域， 从而提升对\n齐分数的准确性； 而Sieve [312] 则利用在小而精的数据集上预训练的生成式图像描述模型， 有效减\n少了 CLIPScore 排序中的误判（如高分或低分错误） 。 通过这些优化策略， 图文对的筛选变得更加\n精准，显著提高了数据质量和模型性能。\n多样化且平衡的数据集是提升多模态大语言模型泛化能力的核心因素 [313]。为构建这样的数\n据集，..."
      ],
      "response_time": 1.880136251449585,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.05007073173272212,
        "semantic_similarity": 0.8148990869522095
      }
    },
    {
      "sample_id": "0181",
      "question": "在训练大型语言模型时，如何通过知识蒸馏和并行计算来提高学生模型的生成准确性和训练效率？",
      "ground_truth": "通过采用反向 KLD 的知识蒸馏方法，MiniLLM 引导学生模型更加关注教师分布的高概率区域，从而提升生成内容的准确性。同时，使用 FP8 格式进行前向计算和反向梯度通信，以及序列并行和张量并行技术，可以有效节省内存和提高训练效率。",
      "predicted_answer": "知识蒸馏可以帮助提高学生模型的生成准确性。具体来说，白盒知识蒸馏可以直接访问教师模型的内部结构和参数，从而更好地利用教师模型的中间特征和输出分布来提升学生模型的表现[1][5]。而对于大型语言模型，黑盒知识蒸馏可以在不访问教师模型内部结构的情况下，通过其输出（如分类概率或生成的文本）来指导学生模型的学习，从而改善学生模型的行为[2]。\n\n至于并行计算，虽然文档中未详细说明具体的并行计算方法，但一般来说，在大规模训练过程中，可以利用并行计算资源加速训练过程。例如，MiniMoE模型采用了专家混合（Mixture-of-Experts, MoE）架构，通过动态分配不同专家处理不同的输入部分，实现了高效的并行计算，进而提高了训练效率[3]。",
      "ground_context": [
        "θ 对 pT 的空白区域（void region）赋予不合理的高概率， 在自由运行的生成过程中， 这种现象可能会导致学生模型生成在教师分布 pT 下几乎不可能出现的样本。",
        "序列并行通过将输入序列拆分为多个子序列，并将这些子序列分配到不同的设备上， 从而有效节省激活内存。"
      ],
      "retrieved_contexts": [
        "384 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n识蒸馏的方法。通常，黑盒知识蒸馏仅使用教师模型获得的最终结果来提炼学生模型。\n1. 白盒知识蒸馏\n白盒知识蒸馏能够获取教师模型的细节信息， 因而可以采用多种策略来提高学生模型的性能。\n给定教师分布 pT (y|x) 以及由参数 θ 确定的学生分布 pS\nθ (y|x)， 标准的知识蒸馏目标 （包括针对序列\n级模型的几种变体）[515, 516] 本质上是最小化教师分布和学生分布之间近似的正向 Kullback-Leibler\ndivergence （KLD） ，记为KL[pT ||pS\nθ ]，这会迫使 pS\nθ 覆盖 pT 的所有高概率区域（mode，也成模\n态） 。 对于文本分类任务， 这种方法表现良好， 因为输出空间通常由有限的类别组成， 使得pT (y|x)\n和 pS\nθ (y|x) 的高概率区域都很少。 然而， 对于开放式文本生成任务 （大语言模型应用通常属于这种\n情况） ，输出空间要复杂得多，并且由于模型容量有限，pT (y|x) 所包含的高概率区域数量可能远\n远超过 pS\nθ (y|x) 所能表达的数量。最...",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "况，分别对应于散度 D 设为正向 KL 散度，且学生模型生成数据比例 λ 分别为 1 和 0 的情况。也\n就是说，广义知识蒸馏允许对比例 λ 和散度进行其他选择。\n此外，TED[523] 提出了一种任务感知的逐层知识蒸馏方法。 该方法在教师模型和学生模型的每\n一层后添加过滤器，首先训练这些特定于任务的过滤器，然后在训练学生模型的过滤器时冻结教\n师模型的过滤器， 以使学生模型的输出特征能够与对应的教师过滤器输出特征对齐。MiniMoE[524]\n则通过采用专家混合（Mixture-of-Experts, MoE）模型作为学生模型，来缩小学生模型与教师模型\n之间的能力差距。KPTD[525] 提出了一种通过知识蒸馏将实体定义中的知识转移到大语言模型参数\n中的方法。该方法基于实体定义生成一个转移集，并利用这些定义对学生模型进行蒸馏，使学生\n模型的输出分布与教师模型相匹配。\n2. 黑盒知识蒸馏\n黑盒蒸馏的核心目标是在无法访问大模型内部参数的情况下，通过其输出（如分类概率或生\n成的文本） 来指导学生模型的学习。 具体而言， 学生模型可以通过模仿大模型的输出分布 （如分类\n概率分布） 来接近其行为...",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "算，对于那些对当前输入不太重要的专家，可以直接跳过，从而减少不必要的计算量，提高推理\n速度。\n10.2.4 知识蒸馏\n知识蒸馏（Knowledge Distillation, KD）是一种广泛应用的模型压缩技术，其核心思想是将大\n型模型 （称为教师模型，Teacher Model） 的知识迁移到较小的模型 （称为学生模型，Student Model）\n中。现有研究主要关注如何高效地将大语言模型的各种能力传递到学生模型中。根据是否可以访\n问大模型的内部结构 （如参数、 梯度） ， 知识蒸馏可以分为两大类： 白盒知识蒸馏和黑盒知识蒸馏，\n如图 10.9 所示。\n白盒知识蒸馏黑盒知识蒸馏\n教师模型学生模型基于API的教师模型\n图 10.9 白盒知识蒸馏（左）和黑盒知识蒸馏（右）示意图 [471]\n白盒知识蒸馏（White-box KD）指的是利用对教师模型结构和参数的访问权限来进行蒸馏的\n方法。这种方式使得知识蒸馏能够有效地利用教师模型的中间特征和输出分布，以提升学生模型\n的性能。黑盒知识蒸馏（Black-box KD）指的是在教师模型的结构和参数不可用的情况下进行知"
      ],
      "response_time": 4.373147487640381,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0.0,
        "bleu": 0.013329149502071022,
        "semantic_similarity": 0.7344608306884766
      }
    },
    {
      "sample_id": "0182",
      "question": "MedQA是什么？",
      "ground_truth": "MedQA是美国医学执业考试中的医学知识。",
      "predicted_answer": "MedQA是美国医学执业考试中的医学知识问题+答案集合，包含11450/1273个问题及答案选项[1]。",
      "ground_context": [
        "MedQA（USMLE）问题 + 答案（4 ∼ 5 个选项） 11450/1273 美国医学执业考试中的医学知识"
      ],
      "retrieved_contexts": [
        "MedQA\n（USMLE）\n问题 + 答案\n（4 ∼ 5 个选项） 11450/1273 美国医学执业考试中的医学知识\nMedMCQA\n（AIIMS/NEET）\n问题 + 答案\n（4 个选项和解释） 18.7 万/6100 印度医学入学考试中的医学知识\nPubMedQA\n问题 + 上下文 + 答案\n（Y es/No/Maybe）\n（长回答）\n500/500\n标注 QA 对 1000\n无标注数据 6.12 万\n生物医学科学文献\nMMLU 问题 + 答案\n（4 个选项） 123/1089 涵盖解剖学、 临床知识、 大学医学、 医\n学遗传学、专业医学和大学生物学\nLiveQA\nTREC-2017\n问题 + 长答案\n（参考标注答案） 634/104 用户经常询问的一般医学知识\nMedicationQA 问题 + 长答案 NA/674 用户经常询问的药物知识\nHealthSearchQA 问题 + 手册\n专业解释 3375 用户经常搜索的医学知识\n11.3 大语言模型评估方法\n在大语言模型评估体系和数据集构建的基础上，评估方法需要解决如何评估的问题，包括采\n用哪些评估指标，以及如何进行评估等。本...",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "420 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n语言模型在医学问题回答上的能力[585]， 包括阅读理解能力、 准确回忆医学知识并使用专业知识的\n能力。目前已有一些医疗相关数据集，分别评估了不同方面，包括医学考试题评估集 MedQA[591]\n和 MedMCQA[592]，医学研究问题评估集 PubMedQA[593]，以及面向普通用户的医学信息需求评估\n集 LiveQA[594] 等。文献 [585] 提出了 MultiMedQA 数据集，集成了 6 种已有医疗问答数据集，题\n型涵盖多项选择、长篇问答等，包括 MedQA[591]、MedMCQA[592]、PubMedQA[593]、MMLU[573]、\nLiveQA[594] 和 MedicationQA[595]。在此基础上根据常见健康查询构建了 HealthSearchQA 数据集。\nMultiMedQA[585] 评估集中所包含的数据集、题目类型、数据量等信息如表11.2 所示。\n表 11.2 MultiMedQA [585] 评估集中所包含的数据集、题目类型、数据量等信息\n数据集 题目类型 数据量（开发/...",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "SimpleQA[620] 是 OpenAI 推出的基准测试集，专为评估大语言模型回答事实性问题的能力而\n设计。 它聚焦于简短且以事实为导向的问题， 减少评估复杂性， 提供更精确的事实性衡量方式。 数\n据集覆盖科学、技术、历史、音乐、艺术、视频游戏、政治等多个领域，避免狭隘性，同时针对\n最先进的模型 （如GPT-4） 也具有很高的挑战性， 其通过率不到40%。SimpleQA 数据集包含 4326\n个高质量问题， 这些问题由AI 训练师通过严格流程创建， 确保每个问题只有一个不可争议且不随\n时间变化的答案，并经过多重验证（误差率约 3%） 。评分机制使用ChatGPT 分类器，将回答标记\n为“正确” 、 “错误”或“未尝试” ，并通过询问置信度和重复提问评估模型的校准能力和一致性，\n为研究者提供高效、可靠的评估工具。\nC-SimpleQA（Chinese SimpleQA）[621] 是淘天集团推出的专门用于全面评估中文 AI 模型事实\n性能力的测试集，具有显著的针对性和实用性。该测试集专注于中文语言，涵盖与中国文化相关\n的特色知识， 确保评测符合中文语境和文化特点。 内容分布上，C-..."
      ],
      "response_time": 1.1209750175476074,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.4,
        "rouge2": 0.0,
        "rougeL": 0.4,
        "bleu": 0.2988565156407745,
        "semantic_similarity": 0.838068425655365
      }
    },
    {
      "sample_id": "0183",
      "question": "vLLM 可以支持哪些常用模型？",
      "ground_truth": "vLLM 可以支持 Aquila、Baichuan、BLOOM、Falcon、GPT-2、InternLM、LLaMA、LLaMA-2 等常用模型。",
      "predicted_answer": "vLLM 可以支持 Aquila、Baichuan、BLOOM、Falcon、GPT-2、InternLM、LLaMA、LLaMA-2 等常用模型。[1]",
      "ground_context": [
        "vLLM 可以支持 Aquila、Baichuan、BLOOM、Falcon、GPT-2、InternLM、LLaMA、LLaMA-2等常用模型，使用方式也非常简单，不用对原始模型进行任何修改。"
      ],
      "retrieved_contexts": [
        "402 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n多模态环境中表现卓越。综合来看，vLLM V1 的优化涵盖了执行效率、缓存管理、推理架构和多\n模态支持，为复杂推理场景提供了更加高效、灵活和可扩展的解决方案。\nvLLM 可以支持 Aquila、Baichuan、BLOOM、Falcon、GPT-2、InternLM、LLaMA、LLaMA-2\n等常用模型，使用方式也非常简单，不用对原始模型进行任何修改。以 OPT-125M 模型为例，可\n以使用如下代码进行推理应用：\nfrom vllm import LLM, SamplingParams\n#ิൕဢ২\nprompts = [\n\"Hello, my name is\" ,\n\"The president of the United States is\" ,\n\"The capital of France is\" ,\n\"The future of AI is\" ,\n]\n#ࡹsamplingའ\nsampling_params = SamplingParams(temperature=0.8, top_p =0.95)\n#նე࿽ଆ྘\n...",
        "的内存空间中存储键和值，将每个序列的键值缓存分成多个块，每个块中包含固定数量的词元的\n键和值。 在注意力计算过程中，PagedAttention 内核能够高效地识别和提取这些块。 从而在一定程\n度上避免现有系统由于碎片化和过度预留而浪费的 60%∼80% 的内存。\n2025 年 1 月 27 日，vLLM 团队正式发布了 vLLM V1 的 alpha 版本，这标志着其核心架构的\n一次重大升级。在过去一年半的开发经验基础上，团队重新审视了关键设计决策，并对系统进行\n了全面优化。此次升级整合了多项新功能，同时简化了代码库，显著提升了系统的灵活性和可扩\n展性。可以通过设置环境变量 VLLM_USE_V1=1 无缝启用 V1，现有 API 无需任何更改。\nvLLM V1 对核心组件进行了全面重构， 包括调度器、KV 缓存管理器、 工作器、 采样器和API\n服务器。 尽管V1 与 V0 版本在模型实现、GPU 内核和分布式控制平面等部分共享了大量代码， 但\nV1 在性能优化和代码复杂性方面取得了显著的进展。\nvLLM V1 引入了一系列全面升级的核心特性， 显著提升了性能、 灵活性和系统效率。...",
        "#նე࿽ଆ྘\nllm = LLM(model=\"facebook/opt-125m\")\n#֥RequestOutputའਙі\noutputs = llm.generate(prompts, sampling_params)\n#ݔࢲ\nfor output in outputs:\nprompt = output.prompt\ngenerated_text = output.outputs[0].text\nprint(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n使用 vLLM 可以非常方便地部署一个模拟 OpenAI API 协议的服务器。首先使用如下命令启\n动服务器：\npython -m vllm.entrypoints.openai.api_server --model facebook/opt-125m\n默认情况下，执行上述命令会在 http://localhost:8000 启动服务器。也可以使用 --host 和 --port\n参数指定地址和端口号。vLLM v0.1.4 版本的服务器一次只能托管一个模型，实...",
        "大语言模型：从理论到实践 401\n将大语言模型计算图的运算分割为多个阶段，并在不同设备上以流水线方式执行，因此 FastServe\n需要同时处理分布式引擎中的多个批次。 由于键值缓存占据了GPU 显存的很大一部分， 因此在分\n布式服务中，FastServe 的键值缓存也被分割到多块 GPU 上。 在大语言模型推理中， 每个键值张量\n都由大语言模型的同一阶段使用。因此，FastServe 按照张量并行的要求对键值张量进行分割，并\n将每个键值张量分配给相应的 GPU，以便 GPU 上的所有计算只使用本地的键值张量。\n10.5 vLLM 推理框架实践\nvLLM 是由加州大学伯克利分校开发，并在 Chatbot Arena 和 Vicuna Demo 上部署使用的大\n语言模型推理服务开源框架。 vLLM 利用 PagedAttention 注意力算法，有效地管理注意力的键和\n值。vLLM 的吞吐量是 HuggingFace transformers 的 24 倍，并且无须进行任何模型架构的更改。\nPagedAttention 注意力算法的主要目标是解决键值缓存的管理问题。PagedAttent...",
        "大语言模型：从理论到实践 233\n7.2 大语言模型与多模态融合架构\n近年来， 随着基于Transformer 架构的算法取得了显著进展， 视觉语言模型、 音频语言模型都\n有了很大的发展， 模型架构也多种多样， 包括双编码器架构、 融合架构和编码器-解码器架构等。 这\n些架构不断演化并结合新的技术，例如混合模态注意力机制、对比学习、强化学习等，进一步提\n升了模型的性能和适应能力。\n本节将围绕多模态大语言模型的架构展开介绍，分别探讨视觉语言模型架构、语音语言模型\n架构以及多模态大语言模型架构。\n7.2.1 视觉语言模型架构\n视觉语言模型（Vision-Language Models，VLM）是一类旨在结合计算机视觉与自然语言处理\n能力的模型， 近年来借助基于Transformer 的技术取得了显著进展。 这些模型的训练方法可以分为\n四种主要范式： 对比学习、 掩码预测、 生成式学习以及映射学习。 对比学习通过正负样本对的表示\n相似性与差异性训练模型；掩码预测则通过遮掩图像或文本的部分信息，训练模型进行重建；生\n成式视觉语言模型则专注于生成图像或文本，但因其复杂性通常需要更高的计算资源；映..."
      ],
      "response_time": 1.3881640434265137,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.9565217391304348,
        "rouge2": 0.9523809523809523,
        "rougeL": 0.9565217391304348,
        "bleu": 0.8907171682201394,
        "semantic_similarity": 0.9832810163497925
      }
    },
    {
      "sample_id": "0184",
      "question": "大语言模型评估分为哪三个大的方面？",
      "ground_truth": "知识与能力、伦理与安全，以及垂直领域评估。",
      "predicted_answer": "大语言模型评估从整体上可以分为三个大的方面：知识与能力、伦理与安全，以及垂直领域评估[1]。",
      "ground_context": [
        "从整体上可以将大语言模型评估分为三个大的方面：知识与能力、伦理与安全，以及垂直领域评估。"
      ],
      "retrieved_contexts": [
        "大规模语言模型：从理论到实践 407\n不同的评估体系和方法，并且对于不同阶段的模型应该独立进行评估。\n11.2 大语言模型评估体系\n传统的自然语言处理算法通常需要针对不同任务独立设计和训练。而大语言模型则不同，它\n采用单一模型，却能够执行多种复杂的自然语言处理任务。例如，同一个大语言模型可以用于机\n器翻译、文本摘要、情感分析、对话生成等多个任务。因此，在大语言模型评估中，首先需要解\n决的就是构建评估体系的问题。 从整体上可以将大语言模型评估分为三个大的方面：知识与能力、\n伦理与安全，以及垂直领域评估。\n11.2.1 知识与能力\n大语言模型具有丰富的知识和解决多种任务的能力，包括自然语言理解（例如文本分类、信\n息抽取、情感分析、语义匹配等） 、知识问答（例如阅读理解、开放领域问答等） 、自然语言生成\n（例如机器翻译、文本摘要、文本创作等） 、逻辑推理（例如数学解题、文本蕴含） 、代码生成等。\n知识与能力评估体系主要分为两大类：一类是以任务为核心的评估体系；一类是以人为核心的评\n估体系。\n1. 以任务为核心的评估体系\nHELM 评估[557] 构造了 42 类评估场景（Scenario） ...",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "大规模语言模型：从理论到实践 433\n11.4 大语言模型评估实践\n大语言模型的评估伴随着大语言模型研究同步飞速发展，大量针对不同任务、采用不同指标\n和方法的大语言模型评估不断涌现。本章前面几节分别针对大语言模型评估体系、评估指标和评\n估方法从不同方面介绍了当前大语言模型评估面临的问题，试图回答要从哪些方面评估大语言模\n型，以及如何评估大语言模型这两个核心问题。针对大语言模型构建不同阶段所产生的模型能力\n的不同，本节将分别介绍当前常见的针对基础模型、SFT 模型和 RL 模型的整体评估方案。\n11.4.1 基础模型评估\n大语言模型构建过程中产生的基础模型就是语言模型， 其目标就是建模自然语言的概率分布。\n语言模型构建了长文本的建模能力，使得模型可以根据输入的提示词生成文本补全句子。2020 年\nOpenAI 的研究人员在 1750 亿个参数的 GPT-3 模型上研究发现，在语境学习范式下，大语言模型\n可以根据少量给定的数据，在不调整模型参数的情况下，在很多自然语言处理任务上取得不错的\n效果[13]。图11.12 展示了不同参数量的大语言模型在简单任务中基于语境学习的表现。这个任务\n要求...",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "424 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n4. 大语言模型评估指标体系\n通过本节的前述内容，可以看到传统的自然语言处理评估大多针对单一任务设置不同的评估\n指标和方法。 大语言模型在经过指令微调和强化学习阶段后， 可以完成非常多不同种类的任务， 对\n于常见的自然语言理解或生成任务可以采用原有指标体系。虽然大语言模型在文本生成类任务上\n取得了突破性的进展，但是问题回答、文章生成、开放对话等文本生成类任务在此前并没有很好\n的评估指标， 因此， 针对大语言模型在文本生成方面的能力， 需要考虑建立新的评估指标体系。 为\n了更全面地评估大语言模型所生成的文本的质量，需要从三方面进行评估，包括语言层面、语义\n层面和知识层面。\n（1）语言层面的评估是评估大语言模型所生成文本质量的基础，要求生成的文本必须符合人\n类的语言习惯。这意味着生成的文本必须具有正确的词法、语法和篇章结构。具体如下：\n• 词法正确性： 评估生成文本中单词的拼写、 使用和形态变化是否正确。 确保单词拼写准确无\n误，不含有拼写错误。同时，评估单词的使用是否恰当，包括单词的含义、词性和用法等方\n面， 以确保单词在..."
      ],
      "response_time": 0.9912354946136475,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.37401099941127647,
        "semantic_similarity": 0.8175784349441528
      }
    },
    {
      "sample_id": "0185",
      "question": "每年参与高考的人数是多少？",
      "ground_truth": "1200 万",
      "predicted_answer": "每年参与高考的人数是1200万[1]。",
      "ground_context": [
        "Gaokao（高考） 1200 万 中文"
      ],
      "retrieved_contexts": [
        "410 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n表 11.1 AGIEval 评估选择的任务和基本信息 [559]\n考试名称 每年参与人数 语言 任务名 评估条目（个）\nGaokao（高考） 1200 万 中文\nGK-geography 199\nGK-biology 210\nGK-history 243\nGK-chemistry 207\nGK-physics 200\nGK-En 306\nGK-Ch 246\nGK-Math-QA 351\nGK-Math-Cloze 118\nSA T 170 万 英语 SA T-En. 206\nSA T-Math 220\nLawyer Qualification Test\n（律师资格考试） 82 万 中文 JEC-QA-KD 1000\nJEC-QA-CA 1000\nLaw School Admission Test\n（LSA T） 17 万 英文\nLSA T-AR Law-Analytics 230\nLSA T-LR Law-Logic 510\nLSA T-RC Law-Reading 260\nCivil Service Examinat...",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "大规模语言模型：从理论到实践 409\n（2）与现实世界场景相关。\nAGIEval 的目标是选择与人类认知和问题解决密切相关的任务，从而可以更有意义、更全面\n地评估基础模型的通用能力。为实现这一目标， AGIEval 融合了各种官方、公开、高标准的入学\n和资格考试，这些考试面向普通的考生群体，评估数据从公开数据中抽取。这些考试能得到公众\n的广泛参与， 包括普通高等教育入学考试 （例如中国的高考和美国的SA T） 、 美国法学院入学考试\n（LAST） 、数学竞赛、律师资格考试和国家公务员考试。每年参加这些考试的人数达到数千万，例\n如中国高考约 1200 万人参加，美国 SA T约 170 万人参加。因此，这些考试具有官方认可的评估\n人类知识和认知能力的标准。此外，AGIEval 评估涵盖了中英双语任务，可以更全面地评估模型\n的能力。\n研究人员利用 AGIEval 评估方法，对 GPT-4、ChatGPT、text-davinci-003 等模型进行了评估。\n结果表明，GPT-4 在 SA T、LSA T和数学竞赛中的表现超过了人类平均水平。GPT-4 在 SA T数学考\n试中的准确率达到了...",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "（国家公务员考试）\n200 万 英文 LogiQA-en 651\n200 万 中文 LogiQA-ch 651\nGRE 34 万 英文 AQuA-RA T Math 254\nGMA T 15 万 英文\nAMC 30 万 英文 MA TH 1000\nAIME 3000 英文\n11.2.2 伦理与安全\n大语言模型在训练时通常遵循 3H 原则：帮助性模型应帮助用户解决问题；真实性模型不能\n捏造信息或误导用户；无害性模型不能对人或环境造成身体、心理或社会性的伤害[24]。帮助性和\n真实性可以结合知识与能力评估体系，构造评估指标进行评估。无害性则是希望大语言模型的回\n答能与人类价值观对齐，因此，如何评估大语言模型能否在伦理价值方面与人类对齐也是需要研\n究的内容。\n例如： 用户输入 “在别人不知道的情况下成功拿走了他的东西， 只能说这东西和他没缘分， 不\n是这样吗？ ”\n༢๤ 1b\n༢๤ 2܄߶"
      ],
      "response_time": 0.6563217639923096,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.6666666666666666,
        "rouge2": 0.0,
        "rougeL": 0.6666666666666666,
        "bleu": 0.04774753205784679,
        "semantic_similarity": 0.5438846349716187
      }
    },
    {
      "sample_id": "0186",
      "question": "评估数据集和训练数据集应该是什么关系？",
      "ground_truth": "相互独立的，以避免数据泄露的问题。",
      "predicted_answer": "评估数据集和训练数据集应该是相互独立的，以避免数据泄露的问题。这意味着评估数据集不应包含在训练数据集中，且应来自不同的源，以确保评估结果的有效性。[1]",
      "ground_context": [
        "评估数据集和训练数据集应该是相互独立的，以避免数据泄露的问题。"
      ],
      "retrieved_contexts": [
        "则用于最终评估模型的性能。评估数据集和训练数据集应该是相互独立的，以避免数据泄露的问\n题。此外，数据集选择还需要具有代表性，应该能够很好地代表模型在实际应用中可能遇到的数\n据。这意味着它应该涵盖各种情况和样本，以便模型在各种情况下都能表现良好。评估数据集的\n规模也应该足够大，以充分评估模型的性能。此外，评估数据集中应该包含一些特殊情况的样本，\n以确保模型在处理异常或边缘情况时仍具有良好的性能。\n大语言模型评估同样涉及数据集选择问题，但是大语言模型可以在单一模型中完成自然语言\n理解、逻辑推理、自然语言生成、多语言处理等任务。因此，如何构造大语言模型的评估数据集\n也是需要研究的问题。此外，由于大语言模型本身涉及语言模型训练、有监督微调、强化学习等\n多个阶段，每个阶段所产出的模型目标并不相同，因此，对于不同阶段的大语言模型也需要采用",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "完成之后的一个必不可少的步骤。 目前， 针对单一任务的自然语言处理算法， 通常需要构造独立于\n训练数据的评估数据集，使用合适的评估函数对模型在实际应用中的效果进行预测。由于并不能\n完整了解数据的真实分布，因此简单地采用与训练数据独立同分布的方法构造的评估数据集，在\n很多情况下并不能完整地反映模型的真实情况。图11.1 为模型评估难点示意图，针对相同的训练\n数据，采用不同的算法或者超参数得到 4 个不同的分类器，可以看到，如果不能获取数据的真实\n分布，或者测试数据采样不够充分，分类器在真实使用中的效果就不能很好地通过上述方法进行\n评估。\n在模型评估的过程中，通常会使用一系列 评估指标（Evaluation Metrics）来衡量模型的表现，\n如准确率、精确率、召回率、F1 分数、ROC 曲线和 AUC 等。这些指标根据具体的任务和应用场\n景可能会有所不同。例如，在分类任务中，常用的评估指标包括准确率、精确率、召回率、F1 分\n数等；而在回归任务中，常用的评估指标包括均方误差和平均绝对误差等。但是对于文本生成类",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "时的表现，以及系统在信息不足或不确定的情况下是否能够拒绝生成错误回答。\n9.5.3 评估数据集\n评估数据集是 RAG 系统性能评估中的关键组成部分，其质量和多样性直接影响评估结果的\n准确性和全面性。 在评估RAG 系统时， 数据集的选择与构建需要兼顾系统的检索能力、 生成能力\n以及整体协作表现。现有评估数据集的来源和构造方法多种多样，既包括基于已有资源的数据集，\n也包括为特定评估目标生成的全新数据集。不同的基准系统选择了不同的数据集策略，以适应各\n自的评估需求和应用场景。\n1. 基于现有资源的数据集\n许多评估框架依赖于已有的成熟数据集，如 KILT（Knowledge Intensive Language Tasks ）基\n准[457] 和 SuperGLUE[458] 数据集。 这些数据集涵盖了多种知识密集型任务。 例如：Natural Questions\n(NQ)[459] 提供开放域问答任务的数据， 测试系统对自然语言查询的回答能力。HotpotQA[444] 包含多\n跳问答任务， 要求RAG 系统能够从多个文档中综合信息来回答复杂问题。FEVER[460] 专注于事实验\n证任务..."
      ],
      "response_time": 1.3159596920013428,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.2029335885395209,
        "semantic_similarity": 0.7006734609603882
      }
    },
    {
      "sample_id": "0187",
      "question": "CUAD 数据集中包含多少份合同？",
      "ground_truth": "500 多份合同",
      "predicted_answer": "CUAD 数据集中包含了 510 份合同[4]。",
      "ground_context": [
        "CUAD 数据集中包括 500 多份合同， 每份合同都经过The Atticus Project 法律专家的精心标记， 以识别41 种不同类型的重要条款， 总共有超过13000 个标注。"
      ],
      "retrieved_contexts": [
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "少量重要内容，需要律师进行审查或分析，特别是要识别包含重要义务或警示条款的内容。对于\n法律专业人员来说，手动筛选长合同以找到这些少数关键条款可能既费时又昂贵，尤其是考虑到\n一份合同可能有数十页甚至超过 100 页。CUAD 数据集中包括 500 多份合同， 每份合同都经过The\nAtticus Project 法律专家的精心标记， 以识别41 种不同类型的重要条款， 总共有超过13000 个标注。\n判决预测是指根据事实描述预测法律判决结果，这也是法律人工智能（LegalAI）领域的关键\n应用之一。CAIL2018[588] 是针对该任务构建的大规模刑事判决预测数据集，包含 260 万个刑事案\n件， 涉及183 个刑法条文，202 个不同判决和监禁期限。 由于CAIL2018 数据集中的数据相对较短，\n并且只涉及刑事案件，文献 [586] 提出了 CAIL-Long 数据集，其中包含与现实世界中相同长度分\n布的民事和刑事案件。民事案件的平均长度达到了 1286.88 个汉字，刑事案件的平均长度也达到\n了 916.57 个汉字。整个数据集包括 1129053 个刑事案件和 1099605 ...",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "大规模语言模型：从理论到实践 67\n疗对话数据。中文数据集包含 340 万条医生-患者对话，覆盖 172 个疾病领域，而英文数据集包含\n26 万条对话，覆盖 96 个疾病领域。\n法律领域也包含许多可用于模型训练的数据资源， 主要包括法律法规、 裁判文书等法律数据。\n这些数据通常可以从相关官方网站下载获得，且数据规模较大，能够为大模型提供大量的法律专\n业知识。 此外， 还还通过收集司法考试题目、 法律咨询、 法律问答等相关数据， 这类数据涉及了真\n实用户的法律需求与基于法律专业知识的解答。CUAD[114] 是一个包含 510 个商业法律合同、 超过\n1.3 万个标注的合同审查数据集， 由数十名法律专业人士和机器学习研究人员共同创建， 通过法律\n专业人士对这些合同数据进行扩充和详细标注。TigerBot-law[99] 则汇集了 11 类中国法律法规，以\n及一些多类别语料库，还纳入了从法律相关网站抓取的数据。\n3.2 数据处理\n大语言模型的相关研究表明，数据质量对于模型的影响非常大。因此，在收集了各种类型的\n数据之后， 需要对数据进行处理， 去除低质量数据、 重复数据、 有害信息、 个人...",
        "280 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n图 8.6 AgentTuning 方法框架 [402]\nAgentInstruct 数据集包含 1,866 条经过严格验证的交互轨迹。这些轨迹包含高质量的逐步推\n理过程（即 Chain-of-Thought） ，并涉及六种不同的智能体任务，包括AlfWorld[403]、WebShop[393]、\nMind2Web[404]、 知识图谱、 操作系统和数据库。 对于每个智能体任务，AgentInstruct 的构建包括三\n个主要阶段：指令构造、轨迹交互以及轨迹过滤。\n对于 AlfWorld、WebShop、Mind2Web 以及知识图谱等已有训练集的任务，AgentInstruct 直接\n利用其训练数据，依次完成轨迹交互和轨迹过滤两个阶段。对于缺乏训练集的任务（如操作系统\n和数据库） ， 则采用任务推导 （Task Derivation） 和自指令生成 （Self-Instruct）[405] 的方法构建相应\n的指令，以确保数据的完整性与多样性。\n在数据库任务的指令构建过程中， 使用了BIRD[406] 数据集作为基础，..."
      ],
      "response_time": 0.6935069561004639,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 0.5
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0.0,
        "bleu": 0.020044994979069072,
        "semantic_similarity": 0.7523109912872314
      }
    },
    {
      "sample_id": "0188",
      "question": "在评估文本生成类任务时，如何选择合适的数据集以确保评估的有效性和代表性？",
      "ground_truth": "评估数据集和训练数据集应该是相互独立的，以避免数据泄露的问题。此外，数据集选择还需要具有代表性，应该能够很好地代表模型在实际应用中可能遇到的情况，这意味着它应该涵盖各种情况和样本，以便模型在各种情况下都能表现良好。评估数据集的规模也应该足够大，以充分评估模型的性能，并且应该包含一些特殊情况的样本，以确保模型在处理异常或边缘情况时仍具有良好的性能。",
      "predicted_answer": "在评估文本生成类任务时，选择合适的数据集以确保评估的有效性和代表性需要注意以下几点：\n\n1. 评估数据集应与训练数据集相互独立，以避免数据泄露问题[3]。\n2. 数据集应具有代表性，能覆盖各种情况和样本，确保模型在各种情况下都能表现良好。\n3. 数据集的规模应足够大，以便充分评估模型的性能。\n4. 数据集中应包含一些特殊的情况样本，以确保模型在处理异常或边缘情况时仍具有良好的性能。\n5. 针对大语言模型，由于它们可以执行多种任务，因此需要特别注意评估数据集的设计，以反映这些多样的功能需求[3]。\n6. 除了以上一般建议外，对于特定类型的模型（如大语言模型），还需考虑不同训练阶段的目标差异，从而设计相应的评估方案[3]。\n\n综上所述，合理选择评估数据集对于保证评估的有效性和代表性至关重要。",
      "ground_context": [
        "评估数据集和训练数据集应该是相互独立的，以避免数据泄露的问题。此外，数据集选择还需要具有代表性，应该能够很好地代表模型在实际应用中可能遇到的情况。",
        "大语言模型评估同样涉及数据集选择问题，但是大语言模型可以在单一模型中完成自然语言理解、逻辑推理、自然语言生成、多语言处理等任务。"
      ],
      "retrieved_contexts": [
        "406 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n任务（例如机器翻译、文本摘要等） ，自动评估仍然是亟待解决的问题。\n图 11.1 模型评估难点示意图 [555]\n文本生成类任务的评估难点主要源于语言的灵活性和多样性， 同样一句话可以有非常多种表述方\n法。 对文本生成类任务进行评估可以采用人工评估和半自动评估方法。 以机器翻译评估为例， 人工评\n估虽然是相对准确的一种方式， 但是其成本高昂， 根据艾伦人工智能研究院 （AI2）GENIE 人工评估\n榜单给出的数据，针对800 条机器翻译结果进行评估需要花费约80 美元[556]。如果采用半自动评估\n方法， 利用人工给定的标准翻译结果和评估函数可以快速高效地给出评估结果， 但是目前半自动评估\n结果与人工评估结果的一致性还亟待提升。 对于用词差别很大， 但是语义相同的句子的判断本身也是\n自然语言处理领域的难题。如何有效地评估文本生成类任务的结果仍面临着极大的挑战。\n模型评估还涉及选择合适的评估数据集，针对单一任务，可以将数据集划分为训练集、验证\n集和测试集。训练集用于模型的训练，验证集用于调整模型的超参数及进行模型选择，而测...",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "则用于最终评估模型的性能。评估数据集和训练数据集应该是相互独立的，以避免数据泄露的问\n题。此外，数据集选择还需要具有代表性，应该能够很好地代表模型在实际应用中可能遇到的数\n据。这意味着它应该涵盖各种情况和样本，以便模型在各种情况下都能表现良好。评估数据集的\n规模也应该足够大，以充分评估模型的性能。此外，评估数据集中应该包含一些特殊情况的样本，\n以确保模型在处理异常或边缘情况时仍具有良好的性能。\n大语言模型评估同样涉及数据集选择问题，但是大语言模型可以在单一模型中完成自然语言\n理解、逻辑推理、自然语言生成、多语言处理等任务。因此，如何构造大语言模型的评估数据集\n也是需要研究的问题。此外，由于大语言模型本身涉及语言模型训练、有监督微调、强化学习等\n多个阶段，每个阶段所产出的模型目标并不相同，因此，对于不同阶段的大语言模型也需要采用",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "时的表现，以及系统在信息不足或不确定的情况下是否能够拒绝生成错误回答。\n9.5.3 评估数据集\n评估数据集是 RAG 系统性能评估中的关键组成部分，其质量和多样性直接影响评估结果的\n准确性和全面性。 在评估RAG 系统时， 数据集的选择与构建需要兼顾系统的检索能力、 生成能力\n以及整体协作表现。现有评估数据集的来源和构造方法多种多样，既包括基于已有资源的数据集，\n也包括为特定评估目标生成的全新数据集。不同的基准系统选择了不同的数据集策略，以适应各\n自的评估需求和应用场景。\n1. 基于现有资源的数据集\n许多评估框架依赖于已有的成熟数据集，如 KILT（Knowledge Intensive Language Tasks ）基\n准[457] 和 SuperGLUE[458] 数据集。 这些数据集涵盖了多种知识密集型任务。 例如：Natural Questions\n(NQ)[459] 提供开放域问答任务的数据， 测试系统对自然语言查询的回答能力。HotpotQA[444] 包含多\n跳问答任务， 要求RAG 系统能够从多个文档中综合信息来回答复杂问题。FEVER[460] 专注于事实验\n证任务..."
      ],
      "response_time": 4.931896686553955,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 0.3333333333333333
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.24399521820200432,
        "semantic_similarity": 0.8640950918197632
      }
    },
    {
      "sample_id": "0189",
      "question": "在评估大语言模型的能力时，如何结合工具使用和特定任务场景的选择来提升模型的表现？",
      "ground_truth": "评估大语言模型的能力时，需要结合工具使用的能力和特定任务场景的选择。片段A提到，Web浏览器插件使ChatGPT能够访问最新的信息，并且通过复杂的推理任务评估模型的工具使用能力，如数学问题求解。而片段B则强调了评估任务的多样性，具体包括问答和摘要，并通过选择覆盖率高、与用户任务对应的场景来进行高效评估。因此，通过有效利用工具和选择恰当的评估任务场景，可以增强大语言模型在特定技能上的表现。",
      "predicted_answer": "根据提供的文档，我无法直接回答如何结合工具使用和特定任务场景的选择来提升大语言模型表现的具体方法。文档主要讨论了大语言模型的基础评估、能力探索阶段及工具学习等方面的内容，但未详细说明具体结合工具使用和特定任务场景选择以提升模型表现的方法。[1][2][5]",
      "ground_context": [
        "Web 浏览器插件使 ChatGPT 能够访问最新的信息。",
        "评估的功能包括任务（如问答、摘要）和领域（如维基百科 2018 年的数据集）。"
      ],
      "retrieved_contexts": [
        "大规模语言模型：从理论到实践 433\n11.4 大语言模型评估实践\n大语言模型的评估伴随着大语言模型研究同步飞速发展，大量针对不同任务、采用不同指标\n和方法的大语言模型评估不断涌现。本章前面几节分别针对大语言模型评估体系、评估指标和评\n估方法从不同方面介绍了当前大语言模型评估面临的问题，试图回答要从哪些方面评估大语言模\n型，以及如何评估大语言模型这两个核心问题。针对大语言模型构建不同阶段所产生的模型能力\n的不同，本节将分别介绍当前常见的针对基础模型、SFT 模型和 RL 模型的整体评估方案。\n11.4.1 基础模型评估\n大语言模型构建过程中产生的基础模型就是语言模型， 其目标就是建模自然语言的概率分布。\n语言模型构建了长文本的建模能力，使得模型可以根据输入的提示词生成文本补全句子。2020 年\nOpenAI 的研究人员在 1750 亿个参数的 GPT-3 模型上研究发现，在语境学习范式下，大语言模型\n可以根据少量给定的数据，在不调整模型参数的情况下，在很多自然语言处理任务上取得不错的\n效果[13]。图11.12 展示了不同参数量的大语言模型在简单任务中基于语境学习的表现。这个任务\n要求...",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "下游任务进行微调。 这些模型参数量大都在10 亿个以上， 由于微调的计算量很大， 这类模型的影\n响力在当时相较 BERT 类模型有不小的差距。\n能力探索阶段集中于 2019 年至 2022 年，由于大语言模型很难针对特定任务进行微调，研究\n人员开始探索在不针对单一任务进行微调的情况下如何发挥大语言模型的能力。2019 年，Radford\n等人在文献 [11] 中使用 GPT-2 模型研究了大语言模型在零样本情况下的任务处理能力。 在此基础\n上，Brown 等人在 GPT-3[13] 模型上研究了通过语境学习进行少样本学习的方法，将不同任务的少\n量有标注的实例拼接到待分析的样本之前输入语言模型，语言模型根据实例理解任务并给出正确\n的结果。基于 GPT-3 的语境学习在 TriviaQA、WebQS、CoQA 等评测集合中都展示出了非常强的\n能力，在有些任务中甚至超过了此前的有监督方法。上述方法不需要修改语言模型的参数，模型\n在处理不同任务时无须花费大量计算资源进行模型微调。仅依赖语言模型本身，其性能在很多任\n务上仍然很难达到有监督学习 （Supervised Learning） 的效果，...",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "272 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n工具，也不能很好对任务进行多步骤的规划。同时大语言模型构建之初也没有考虑记忆和使用用\n户全部对话历史。为了弥补这些不足，研究者们开始系统地研究如何提升大语言模型解决上述问\n题的能力。本节将重点介绍大语言模型工具使用能力提升、推理规划能力提升以及长期记忆构建\n与应用的策略方法。\n8.3.1 工具学习\n大模型工具学习 （Tool Learning） 是指通过让大语言模型学会使用各种工具的调用方式， 进而\n利用合适的工具去实现特定的功能需求。 例如， 用户输入“请告诉我上海今天的天气。 ”具备工具\n使用能力的大语言模型会给出如下响应：\n1. ്љ಩ༀো྘ğ ฿గҰ࿘಩ༀb\n2.Ⴈ฿గ APIğႨ฿గ APIༀ APIčೂ WeatherMapෂҰ࿘ҕඔĎ b\nresponse = requests.get(\"https://api.weathermap/data/2.5/weather\",\nparams={\n\"q\": \"Shanghai\",\n\"date\": \"2025-1-6\",\n\"appid\": \"your_api_key\"..."
      ],
      "response_time": 2.0030665397644043,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0.0,
        "bleu": 0.09586698095913501,
        "semantic_similarity": 0.8192668557167053
      }
    },
    {
      "sample_id": "0190",
      "question": "如何利用大语言模型生成的自然语言指令在多模态环境中执行家庭任务？",
      "ground_truth": "可以通过大语言模型生成详细的自然语言指令，然后在虚拟环境中执行这些指令。比如在VirtualHome模拟器中，研究人员通过众包收集了大量家庭任务的描述，并将其转换为简单的代码，从而实现智能体执行这些任务。同时，vLLM的优化支持了多模态环境，为复杂推理场景提供了高效的解决方案，能够与多种常用模型兼容。",
      "predicted_answer": "研究人员提出了一种名为VirtualHome的三维模拟器来实现这一目标[1]。该模拟器用于家庭任务（如清洁、烹饪等），智能体程序可以根据大语言模型生成的自然语言动作执行相应的任务。具体来说，首先通过众包方式收集了一个大型的家庭任务知识库，每个任务都有一个名称和自然语言指令。然后为这些任务收集“程序”，其中标注者将指令“翻译”成简单的代码。在三维模拟器VirtualHome中实现了最频繁的交互动作，使智能体程序能够执行由程序定义的任务。",
      "ground_context": [
        "大语言模型还具有从外部环境接收反馈并根据行为指令执行操作的能力，例如生成用自然语言描述的详细且高度逼真的行动计划，并用来操作智能体。",
        "vLLM V1 的优化涵盖了执行效率、缓存管理、推理架构和多模态支持，为复杂推理场景提供了更加高效、灵活和可扩展的解决方案。"
      ],
      "retrieved_contexts": [
        "大规模语言模型：从理论到实践 417\n2. 环境交互\n大语言模型还具有从外部环境接收反馈并根据行为指令执行操作的能力，例如生成用自然语\n言描述的详细且高度逼真的行动计划，并用来操作智能体 [577, 578]。为了测试这种能力，研究人员\n提出了多个具身智能 （Embodied AI） 环境和标准评估数据集， 包括VirtualHome[579]、ALFRED[580]、\nBEHA VIOR[581]、V oyager[372]、GITM[582] 等。\nVirtualHome[579] 构建了一个三维模拟器，用于家庭任务（如清洁、烹饪等） ，智能体程序可\n以执行由大语言模型生成的自然语言动作。VirtualHome 评估数据收集过程如图11.7 所示， 首先通\n过众包方式收集一个大型的家庭任务知识库。每个任务都有一个名称和一个自然语言指令。然后\n为这些任务收集“程序” ，其中标注者将指令“翻译”成简单的代码。在三维模拟器VirtualHome\n中实现了最频繁的（交互）动作，使智能体程序执行由程序定义的任务。此外，VirtualHome 还提\n出了一些方法， 可以从文本和视频中自动生成程...",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "大规模语言模型：从理论到实践 153\n图 5.2 Self-Instruct 数据生成过程 [189]\n步骤 1：生成任务指令\n手动构建一个包含 175 个任务的小型指令数据集，称为种子指令集，用于初始化指令池。然\n后让模型以自举（Bootstrapping）的方式，利用指令池生成新任务的指令：每次从指令池中采样8\n条任务指令（其中 6 条来自人工编写的种子指令，2 条是模型迭代生成的） ，将其拼接为上下文示\n例，引导预训练语言模型 GPT-3 生成更多的新任务的指令，直到模型自己停止生成，或达到模型\n长度限制， 或是在单步中生成了过多示例 （例如当出现了“Task 16”时） 。 本步骤所使用的提示如\n下所示：\nCome up with a series of tasks:\nTask 1: {instruction for existing task 1}\nTask 2: {instruction for existing task 2}\nTask 3: {instruction for existing task 3}\nTask 4: {instruction for existi...",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "下游任务进行微调。 这些模型参数量大都在10 亿个以上， 由于微调的计算量很大， 这类模型的影\n响力在当时相较 BERT 类模型有不小的差距。\n能力探索阶段集中于 2019 年至 2022 年，由于大语言模型很难针对特定任务进行微调，研究\n人员开始探索在不针对单一任务进行微调的情况下如何发挥大语言模型的能力。2019 年，Radford\n等人在文献 [11] 中使用 GPT-2 模型研究了大语言模型在零样本情况下的任务处理能力。 在此基础\n上，Brown 等人在 GPT-3[13] 模型上研究了通过语境学习进行少样本学习的方法，将不同任务的少\n量有标注的实例拼接到待分析的样本之前输入语言模型，语言模型根据实例理解任务并给出正确\n的结果。基于 GPT-3 的语境学习在 TriviaQA、WebQS、CoQA 等评测集合中都展示出了非常强的\n能力，在有些任务中甚至超过了此前的有监督方法。上述方法不需要修改语言模型的参数，模型\n在处理不同任务时无须花费大量计算资源进行模型微调。仅依赖语言模型本身，其性能在很多任\n务上仍然很难达到有监督学习 （Supervised Learning） 的效果，..."
      ],
      "response_time": 3.164701461791992,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.4,
        "rouge2": 0.0,
        "rougeL": 0.4,
        "bleu": 0.05802233909955134,
        "semantic_similarity": 0.7832137942314148
      }
    },
    {
      "sample_id": "0191",
      "question": "如何避免大语言模型训练中的数据泄露问题？",
      "ground_truth": "OpenAI 的研究人员对于每个基准测试， 会生成一个 “干净” 版本， 该版本会移除所有可能泄露的样本。",
      "predicted_answer": "为了避免大语言模型训练中的数据泄露问题，研究者采取了几种措施。首先，对于基准测试，研究人员会生成一个“干净”版本的数据集，该版本移除了所有可能泄露的数据样本[1]。其次，对于包含敏感或个人信息的预训练数据，可以通过基于规则的算法（如命名实体识别）来删除或替换这些信息，以减少隐私泄露风险[3]。此外，还可以通过构建开源数据集等方式确保预训练数据的质量和多样性[5]。",
      "ground_context": [
        "为了避免这个因素的干扰，OpenAI 的研究人员对于每个基准测试， 会生成一个 “干净” 版本， 该版本会移除所有可能泄露的样本。"
      ],
      "retrieved_contexts": [
        "由于大语言模型在训练阶段需要使用大量种类繁杂且来源多样的训练数据，因此不可避免地\n存在数据泄露的问题， 即测试数据出现在语言模型训练数据中。 为了避免这个因素的干扰，OpenAI\n的研究人员对于每个基准测试， 会生成一个 “干净” 版本， 该版本会移除所有可能泄露的样本。 泄\n露样本的定义大致为与预训练集中任何 13-gram 重叠的样本 （或者当样本长度小于13-gram 时， 与\n整个样本重叠） 。 目标是非常保守地标记任何可能存在污染的内容， 以便生成一个高度可信且无污\n染的干净子集。 之后， 使用干净子集对GPT-3 进行评估， 并将其与原始得分进行比较。 如果干净子\n集上的得分与整个数据集上的得分相似，则表明即使存在污染也不会对结果产生显著影响。如果\n干净子集上的得分较低，则表明污染可能会提升评估结果。GPT-3 数据泄露的影响评估如图11.13\n所示。x 轴表示数据集中有多少数据可以被高度自信地认为是干净的，而 y 轴显示了在干净子集\n上进行评估时性能的差异。可以看到，虽然污染水平通常很高，有四分之一的基准测试超过50%，\n但在大多数情况下，性能变化很小。\n图 11.13 ...",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "70 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n3.2.3 隐私消除\n由于绝大多数预训练数据源于互联网， 因此不可避免地会包含涉及敏感或个人信息 （Personally\nIdentifiable Information，PII）的用户生成内容，这可能会增加隐私泄露的风险[127]。如图3.3 所示，\n输入前缀词“East Stroudsburg Stroudsburg” ，语言模型在此基础上补全了姓名、电子邮件地址、电\n话号码、传真号码及实际地址。这些信息都是模型从预训练数据中学习得到的。因此，非常有必\n要从预训练语料库中删除包含个人身份信息的内容。\n图 3.3 从大语言模型中获得隐私数据的例子 [127]\n删除隐私数据最直接的方法是采用基于规则的算法，BigScience ROOTS Corpus [128] 在构建过\n程中就采用了基于命名实体识别的方法，利用命名实体识别算法检测姓名、地址、电话号码等个\n人信息内容并进行删除或者替换。该方法使用了基于 Transformer 的模型，并结合机器翻译技术，\n可以处理超过 100 种语言的文本，消除其中的隐私信息。该方法被...",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "3. 大语言模型预训练数据\n在预训练阶段，大语言模型从海量“高质量”文本数据中学习广泛的知识，随后这些知识存\n储在其模型参数当中。通过预训练使得大语言模型具备了一定程度的语言理解和生成能力。因此，\n如何构造海量“高质量”数据对于大语言模型预训练具有至关重要的作用。研究表明，预训练数\n据需要涵盖各种类型的文本，也需要覆盖尽可能多的领域、语言、文化和视角，从而提高大语言\n模型的泛化能力和适应性。当前大模型预训练使用的语料库涵盖网页内容、学术资料、百科、社\n交媒体和书籍等文本内容，同时也包含来自不同领域的文本内容，比如法律文件、年度财务报告、\n医学教科书等其他特定领域的数据。\n本章将介绍常见的大语言模型预训练数据的来源、处理方法、预训练数据对大语言模型影响\n的分析及开源数据集等。\n3.1 数据来源\n文献 [13] 介绍了 OpenAI 训练 GPT-3 使用的主要数据来源，包含经过过滤的 CommonCrawl\n数据集[19]、WebText 2、Books 1、Books 2 及英文 Wikipedia 等数据集。其中 CommonCrawl 的原\n始数据有 45TB，过滤后仅保留了 57..."
      ],
      "response_time": 2.8452298641204834,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0.0,
        "bleu": 0.09840429717012411,
        "semantic_similarity": 0.7137043476104736
      }
    },
    {
      "sample_id": "0192",
      "question": "当 B 和 C 的值大于多少时，麦克尼马尔检验可以相对准确地近似计算 p 值？",
      "ground_truth": "50",
      "predicted_answer": "当 B 和 C 的值大于 50 时，麦克尼马尔检验可以相对准确地近似计算 p 值 [1][2]。",
      "ground_context": [
        "当 B 和 C 的值大于 50 时，麦克尼马尔检验可以相对准确地近似计算p 值"
      ],
      "retrieved_contexts": [
        "大规模语言模型：从理论到实践 431\n文献 [604] 在上述公式的基础上，提出了一个连续性修正版本，这也是目前更常用的变体：\nχ2 = (|B − C| − 1)2\nB + C (11.21)\n当 B 和 C 的值大于 50 时， 麦克尼马尔检验可以相对准确地近似计算p 值， 如果B 和 C 的值\n相对较小（B + C < 25） ，则建议使用以下二项式检验公式计算p 值：\np = 2\nnX\ni=B\n \nn\ni\n!\n0.5i(1 − 0.5)n−i (11.22)\n其中 n = B + C，因子 2 用于计算双侧 p 值（Two-sided p-value） 。\n针对图11.11 中的两种情况，可以使用 mlxtend[555] 来计算 p 值和 χ2：\nfrom mlxtend.evaluate import mcnemar\nimport numpy as np\ntb_a = np.array([[9959, 11],\n[1, 29]])\nchi2, p = mcnemar(ary=tb_a, exact =True)\nprint('chi-squared-a:', chi2)\np...",
        "430 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n图 11.10 用于麦克尼马尔检验的混淆矩阵 [603]\n图11.11 给出了两个样例，根据图11.11(a) 和图11.11(b)，可以计算得到模型 1 和模型 2 在两种\n情况下的准确率分别为 99.7% 和 99.6%。根据图11.11(a)，可以看到模型 1 回答正确且模型 2 回答\n错误的数量为 11，但是反过来模型 2 回答正确且模型 1 回答错误的数量仅为 1。在图11.11(b) 中，\n这两个数字变成了 25 和 15。 显然， 图11.11(b) 中的模型 1 与模型 2 之间的差异更大， 图11.11(a) 中\n的模型 1 与模型 2 之间的差异则没有这么明显。\n图 11.11 麦克尼马尔检验样例 [603]\n为了量化表示上述情况，麦克尼马尔检验中提出的零假设是概率 p(B) 与 p(C) 相等，即两个\n模型都没有表现得比另一个好。麦克尼马尔检验的统计量（ “卡方值” ）计算公式如下：\nχ2 = (B − C)2\nB + C (11.20)\n设定显著性水平阈值 （例如α = 0.05） 之后， 可以计...",
        "p 值是观察这个经验 （或更大的） 卡方值的概率。 如果p 值小于预先设置的显著性水平阈值， 则可\n以拒绝两个模型性能相等的零假设。换句话说，如果 p 值小于显著性水平阈值，则可以认为两个\n模型的性能不同。",
        "列联表（Contingency Table） ，用于比较两个模型之间的预测结果。\n给定如图11.10 所示的用于麦克尼马尔检验的混淆矩阵， 可以得到模型1 的准确率为 A+B\nA+B+C+D ，\n其中 A+B +C +D 为整个测试集中的样本数 n。 同样地， 也可以得到模型2 的准确率为 A+C\nA+B+C+D 。\n这个矩阵中最重要的数字是 B 和 C，因为 A 和 D 表示了模型 1 和模型 2 都进行正确或错误预测\n的样本数。B 和 C 则反映了两个模型之间的差异。",
        "大规模语言模型：从理论到实践 429\n（1）语法正确性：故事片段文本的语法正确程度。\n（2）连贯性：故事片段中句子之间的衔接连贯程度。\n（3）喜好度：故事片段令人愉悦的程度。\n（4）相关性：故事片段是否符合给定的要求。\n为了与人工评估进行对比，研究人员将输入大语言模型的文本内容，同样给到一些评估者进\n行人工评估。在开放式故事生成和对抗性攻击两个任务上的实验结果表明，大语言模型评估的结\n果与人工评估得到的结果一致性较高。同时他们也发现，在使用不同的任务说明格式和生成答案\n采样算法的情况下，大语言模型的评估结果也是稳定的。\nč\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 Ď\n图 11.9 使用大语言模型进行结果评估的过程 [600]\n3. 对比评估\n对比评估的目标是比较不同系统、方法或算法在特定任务上是否存在显著差异。麦克尼马尔\n检验（McNemar Test）[602]是由 Quinn McNemar 于 1947 年提出的一种用于成对比较的非参数统\n计检验方法，可用于比较两个机器学习分类器的性能。麦克尼马尔检验也被称为“被试内卡方检\n验” （within-subjects chi-squared..."
      ],
      "response_time": 1.210395336151123,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.2857142857142857,
        "rouge2": 0.0,
        "rougeL": 0.2857142857142857,
        "bleu": 0.01015265650850611,
        "semantic_similarity": 0.4798768162727356
      }
    },
    {
      "sample_id": "0193",
      "question": "LLMEV AL第二期的目标是什么？",
      "ground_truth": "LLMEV AL第二期（LLMEV AL-2）的目标是以用户日常使用为主线，重点考查大语言模型解决不同专业本科生和研究生在日常学习中所遇到的问题的能力。",
      "predicted_answer": "LLMEV AL第二期（LLMEV AL-2）的目标是重点考查大语言模型解决不同专业本科生和研究生在日常学习中所遇到的问题的能力[4]。",
      "ground_context": [
        "LLMEV AL第二期（LLMEV AL-2）的目标是以用户日常使用为主线， 重点考查大语言模型解决不同专业本科生和研究生在日常学习中所遇到的问题的能力。"
      ],
      "retrieved_contexts": [
        "通过这两种题型的有机组合，评估旨在全面考查模型在不同学科领域中解决问题的能力。每个学\n科都设计了 25∼30 道客观题和 10∼15 道主观题， 共计480 道题目。 评估采用了人工评分和GPT-4\n自动评分两种方法。对于客观题，答对即可获得满分，而对于答错的情况，根据回答是否输出了\n中间过程或解释，对解释的正确性进行评分。主观题方面，依据问答题的准确性、信息量、流畅\n性和逻辑性这四个维度评分，准确性（5 分） ：评估回答的内容是否有错误；信息量（3 分） ：评估\n回答提供的信息是否充足；流畅性（3 分） ：评估回答的格式和语法是否正确；逻辑性（3 分） ：评\n估回答的逻辑是否严谨。为了避免与网上已有的试题重复， LLMEV AL-2在题目的构建过程中力\n求独立思考，旨在更准确、更全面地反映大语言模型的能力和在真实场景中的实际表现。\nLLMEV AL第三期（LLMEV AL-3）基准测试提供了更加全面且更具挑战性的问题。其目标是\n评估模型在中文知识问答任务上的表现，并提供一个公平的比较平台，以便研究人员可以评估不\n同模型的知识问答效果。LLMEval-3 评测采用了一种新颖的评测模式，...",
        "444 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n排名选手和低排名选手的比赛中，如果高排名选手获胜，那么只会从低排名选手处获得很少的排\n名分。然而，如果低排名选手爆冷获胜，则可以获得更多排名分。虽然这种评分系统非常适合竞\n技比赛，但是与顺序有关，并且对噪声非常敏感。积分制得分也是一种常见的比赛评分系统，用\n于在竞技活动中确定选手或团队的排名。该制度根据比赛中获得的积分数量，决定参与者在比赛\n中的表现和成绩。在 LLMEV AL评估中，根据用户给出的“A 系统好” 、 “B 系统好” 、 “两者一样\n好”及“两者都不好”的选择，分别给A 系统 +1 分，B 系统 +1 分，A 和 B 系统各 +0.5 分。该评\n分系统与顺序无关，并且对噪声的敏感程度相较 Elo 评分系统低。\nLLMEV AL第二期（LLMEV AL-2）的目标是以用户日常使用为主线， 重点考查大语言模型解\n决不同专业本科生和研究生在日常学习中所遇到的问题的能力。涵盖的学科非常广泛，包括计算\n机、 法学、 经济学、 医学、 化学、 物理学等12 个领域。 评估数据集包含两种题型： 客观题和主观题。\n通过这...",
        "LLMEV AL团队联合复旦大学医学院，复旦大学附属华山医院，复旦大学附属肿瘤医院，共\n同推出 LLMEV AL-Medicine专题医学领域大模型评测，选择医疗领域作为核心评测领域，提出医\n疗增强评测体系框架。\n目前医疗领域评估体系主要分为三大类：医生职业资格考试、综合性医疗评估以及专项能力\n评测。\n• 医生职业资格考试：作为各国医学教育的最高标准，通过系统化的考核体系来评估医学生，\n包括美国 USMLE 考试和中国执业医师资格考试。 这类评估的优势在于能够全面考察医学知\n识与临床技能，但存在两个主要缺陷：其一，评估维度较为单一，未能充分考察语言处理、\n内容生成等智能模型的关键能力；其二， 考核方式过于传统， 主要采用选择题形式， 侧重于\n记忆性知识点的考察，难以体现临床实践中的复杂思维能力。\n• 综合性医疗评估： 第三方机构发布的榜单虽然在任务范围和能力分类上具有一定的广度， 但\n其体系设计仍存在明显不足。 这些榜单在医疗推理和综合能力的评估上存在明显短板，CBLUE\n等评测平台主要聚焦于传统 NLP 任务。此外，这些榜单普遍偏重理论性任务，未能充分反\n映实际医疗场景中的复杂需求，...",
        "大规模语言模型：从理论到实践 445\n库中随机抽样的 1000 题， 针对同一机构的模型， 确保每次评测题目不重复。 评测过程将采用在线\n方式，一轮评测中题目的发送串行进行，即下一题的发送将会视上一道题目的回答情况而定，避\n免恶意爬取行为。\n7. LLMEVAL-Medical 医疗大模型评测\n医疗领域因其直接关乎人类健康，不仅具备高度复杂性和严格的安全标准，还拥有丰富且多\n样化的数据资源，因而成为领域大模型评测的理想选择。医疗领域涉及多学科交叉，涵盖基础医\n学、临床诊断、治疗决策及健康管理等复杂任务。大模型在此需要具备卓越的逻辑推理、精准沟\n通及文本生成能力， 使其成为检验AI 综合能力的最佳场景。 医疗决策的精准性至关重要， 任何偏\n差都可能带来不可逆的后果。因此，在大模型应正式应用前，必须通过科学评测确保其安全性和\n可靠性，以规避潜在风险，保障临床应用的合规性。医疗领域拥有庞大的数据资源，如电子健康\n记录、 医学影像和科研文献等， 为多模态评测提供了广阔空间。 此外， 全球医疗合作需求强烈， 建\n立统一的领域大模型评测标准有助于提升国际化适配能力，推动 AI 技术与医疗深度融合。...",
        "大规模语言模型：从理论到实践 441\n图 11.15 Chatbot Arena 给出的系统之间的胜率矩阵 [196]\n6. LLMEVAL 评估\nLLMEV AL[411] 中文大语言模型评估先后进行了二期，LLMEV AL-1评估涵盖了 17 个大类、453\n个问题，包括事实性问答、阅读理解、框架生成、段落重写、摘要、数学解题、推理、诗歌生成、\n编程等各个领域。 针对生成内容的质量， 细化为5 个评分项， 分别是正确性、 流畅性、 信息量、 逻\n辑性和无害性，具体如下。\n• 正确性： 评估回答是否正确， 即所提供的信息是否正确无误。 一个高质量的回答应当在事实\n上是可靠的。\n• 流畅性：评估回答是否贴近人类语言习惯， 即语句是否通顺、 表达是否清晰。 一个高质量的\n回答应当易于理解，不含烦琐或难以解读的句子。\n• 信息量：评估回答是否提供了足够的有效信息，即回答中的内容是否具有实际意义和价值。"
      ],
      "response_time": 1.2714614868164062,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.9090909090909091,
        "rouge2": 0.888888888888889,
        "rougeL": 0.9090909090909091,
        "bleu": 0.762407292776339,
        "semantic_similarity": 0.9664402008056641
      }
    },
    {
      "sample_id": "0194",
      "question": "SWE-bench Verified 是什么的改进版本？",
      "ground_truth": "它是原版SWE-bench 的改进版本",
      "predicted_answer": "SWE-bench Verified 是原版 SWE-bench 的改进版本[1]。",
      "ground_context": [
        "它是原版SWE-bench 的改进版本，旨在解决原版在实际评估中暴露的多个问题， 例如单元测试过于严格、 问题描述不明确以及环境配置难度较高等。"
      ],
      "retrieved_contexts": [
        "能。 它是原版SWE-bench 的改进版本[626]， 旨在解决原版在实际评估中暴露的多个问题， 例如单元\n测试过于严格、 问题描述不明确以及环境配置难度较高等。 通过这些改进，SWE-bench V erified提\n供了更准确的评估方法， 能够更真实地反映AI 模型在软件工程任务中的能力。SWE-bench V erified\n基于原始 SWE-bench 测试集， 筛选出500 个由专业软件开发人员彻底审查和验证的样本。这些样\n本经过人工标注，确保问题描述清晰、单元测试适当，并剔除质量较差的样本，从而提高了基准\n测试的可靠性。 此外， 开发团队引入了基于容器化Docker 环境的新评估框架， 使测试过程更加一\n致和可靠，同时显著降低了因开发环境配置导致问题的可能性。每个样本都附带详细的人工注释，\n帮助研究人员和开发者更好地理解问题描述的清晰度和评估标准的有效性。这一改进为 AI 模型\n在软件工程领域的性能评估提供了更可靠的依据，推动了 AI 在该领域的发展和应用。\n3. 数学评测数据集\nGSM8K[227] 是一个包含 8500 个样本的小学数学问题数据集，其中训练集包含 75...",
        "438 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n集则涵盖了八种主流编程语言（Python、Java、C++、C#、JavaScript、TypeScript、PHP 和 Bash） 。\nHumanEval 系列评测为研究者提供了一个标准化的数据集和工具，用于量化模型在代码生成任务\n中的能力。\nLiveCodeBench[625] 是一个动态且全面的基准测试集， 专为评估大语言模型的代码生成能力设\n计。该测试集从 LeetCode、AtCoder、CodeForces 等竞赛平台持续收集新问题，截至 2025 年 1 月\n已包含 880 道高质量编码挑战，覆盖代码生成、自修复、代码执行和测试输出预测等多种能力场\n景。通过仅选用新发布的问题，避免训练数据与测试数据重叠，确保评估无污染且客观公正。它\n支持用户自定义模型风格和评估流程，提供直观的命令行接口及详尽文档，方便新手和专家快速\n上手。 此外， 公开的Leaderboard 增强透明度， 鼓励社区互动与模型性能的持续提升， 使其成为目\n前评估大语言模型编码能力的重要工具。\nSWE-bench V erified是 Op...",
        "py+pn\n，从而确保不同文档的评分具有可比性。根据这些得分，所有文档被重新排\n序，得分越高的文档排名越靠前。与离散版本相比，连续版本能够更精确地捕捉文档的相关性梯\n度，适用于需要更细腻排序的场景。\n混合版本进一步结合了连续版本的概率评分和第一阶段检索中的 BM25 分数，通过加权求和\n的方式生成综合评分。 具体地， 最终评分由概率得分Sprob 和 BM25 分数 SBM25 按照权重系数 α 进\n行加权：S = αSprob + SBM25， 综合了推理能力和表层匹配的优点。 混合版本通过模型集成的方式，\n兼顾深层语义推理和表层匹配效果，在实际应用中表现出更强的稳定性和适用性。",
        "178 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n的上下文窗口扩展到 32768。\n位置插值通过小代价的微调显著扩展 LLaMA 模型的上下文窗口，在保持原有扩展模型内任\n务能力的基础上，显著增加模型对长文本的建模能力。另外，通过位置插值扩展的模型可以充分\n重用现有的预训练大语言模型和优化方法，这在实际应用中具有很大吸引力。\n5.4 DeepSpeed-Chat SFT 实践\nChatGPT 整体的训练过程复杂，虽然基于 DeepSpeed 可以通过单机多卡、多机多卡、流水线\n并行等操作来训练和微调大语言模型，但是没有端到端的基于人类反馈机制的强化学习的规模化\n系统， 仍然会造成训练类ChatGPT 系统非常困难。DeepSpeed-Chat[244] 是微软于 2023 年 4 月发布\n的基于 DeepSpeed 用于训练类 ChatGPT 模型的开发工具。基于 DeepSpeed-Chat 训练类 ChatGPT\n对话模型的步骤框架如图5.10 所示，包含以下三个步骤。\n（1）指令微调：使用精选的人类回答来微调预训练语言模型以应对各种查询。\n（2）奖励模型微调：使用...",
        "是一种正则化技术；use_grad_checkpoint 表示是否使用梯度检查点技术来减少内存使用；precision\n表示训练过程中的精度设置。该函数通过创建 ViT 视觉编码器模型，将输入图像转换为特征表示，\n以供进一步的处理。\n对于图文对齐模块 Q-Former，在具体实现中通常使用预训练的 BERT 模型。它通过计算图像\n编码和查询（一组可学习的参数）之间的交叉注意力，更好地将图像表示与文本表示对齐。初始\n化该模块的代码如下："
      ],
      "response_time": 0.7676048278808594,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.5,
        "rouge2": 0.33333333333333337,
        "rougeL": 0.5,
        "bleu": 0.4428500142691474,
        "semantic_similarity": 0.8164817094802856
      }
    },
    {
      "sample_id": "0195",
      "question": "使用大语言模型进行评估的过程比较简单吗？",
      "ground_truth": "是的，使用大语言模型进行评估的过程比较简单。",
      "predicted_answer": "使用大语言模型进行评估的过程并不简单。这涉及到选择合适的评估数据集，并且对于文本生成类任务，还需要应对语言的灵活性和多样性的挑战。此外，现有的评估方法包括人工评估和半自动评估，但这些方法都存在一定的局限性和挑战[5][1]。",
      "ground_context": [
        "使用大语言模型进行评估的过程比较简单， 例如针对文本质量判断问题， 要构造任务说明、 待评估样本及对大语言模型的指令，将上述内容输入大语言模型，对给定的待评估样本质量进行评估，图 8.11 给出的指令要求大语言模型采用 5 级李克特量表法。"
      ],
      "retrieved_contexts": [
        "11. 大语言模型评估\n大语言模型飞速发展，自 ChatGPT 于 2022 年 11 月底发布以来，国内外已相继发布了数百种\n开源和闭源的大语言模型。大语言模型在自然语言处理研究和人们的日常生活中扮演着越来越重\n要的角色。因此，如何评估大语言模型变得愈发关键。我们需要在技术和任务层面对大语言模型\n之间的优劣加以判断，也需要在社会层面对大语言模型可能带来的潜在风险进行评估。大语言模\n型与以往仅能完成单一任务的自然语言处理算法不同，它可以通过单一模型执行多种复杂的自然\n语言处理任务。因此，之前针对单一任务的自然语言处理算法评估方法并不适用于大语言模型的\n评估。如何构建大语言模型评估体系和评估方法是一个重要的研究问题。\n本章将首先介绍大语言模型评估的基本概念和难点， 并在此基础上从大语言模型评估体系、 大\n语言模型评估方法，以及大语言模型评估实践三个方面分别展开介绍。\n11.1 模型评估概述\n模型评估（Model Evaluation） ， 也称模型评价， 目标是评估模型在未见过的数据 （Unseen Data）\n上的泛化能力和预测准确性，以便更好地了解模型在真实场景中的表现。模型评估是在...",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "大规模语言模型：从理论到实践 433\n11.4 大语言模型评估实践\n大语言模型的评估伴随着大语言模型研究同步飞速发展，大量针对不同任务、采用不同指标\n和方法的大语言模型评估不断涌现。本章前面几节分别针对大语言模型评估体系、评估指标和评\n估方法从不同方面介绍了当前大语言模型评估面临的问题，试图回答要从哪些方面评估大语言模\n型，以及如何评估大语言模型这两个核心问题。针对大语言模型构建不同阶段所产生的模型能力\n的不同，本节将分别介绍当前常见的针对基础模型、SFT 模型和 RL 模型的整体评估方案。\n11.4.1 基础模型评估\n大语言模型构建过程中产生的基础模型就是语言模型， 其目标就是建模自然语言的概率分布。\n语言模型构建了长文本的建模能力，使得模型可以根据输入的提示词生成文本补全句子。2020 年\nOpenAI 的研究人员在 1750 亿个参数的 GPT-3 模型上研究发现，在语境学习范式下，大语言模型\n可以根据少量给定的数据，在不调整模型参数的情况下，在很多自然语言处理任务上取得不错的\n效果[13]。图11.12 展示了不同参数量的大语言模型在简单任务中基于语境学习的表现。这个任务\n要求...",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "406 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n任务（例如机器翻译、文本摘要等） ，自动评估仍然是亟待解决的问题。\n图 11.1 模型评估难点示意图 [555]\n文本生成类任务的评估难点主要源于语言的灵活性和多样性， 同样一句话可以有非常多种表述方\n法。 对文本生成类任务进行评估可以采用人工评估和半自动评估方法。 以机器翻译评估为例， 人工评\n估虽然是相对准确的一种方式， 但是其成本高昂， 根据艾伦人工智能研究院 （AI2）GENIE 人工评估\n榜单给出的数据，针对800 条机器翻译结果进行评估需要花费约80 美元[556]。如果采用半自动评估\n方法， 利用人工给定的标准翻译结果和评估函数可以快速高效地给出评估结果， 但是目前半自动评估\n结果与人工评估结果的一致性还亟待提升。 对于用词差别很大， 但是语义相同的句子的判断本身也是\n自然语言处理领域的难题。如何有效地评估文本生成类任务的结果仍面临着极大的挑战。\n模型评估还涉及选择合适的评估数据集，针对单一任务，可以将数据集划分为训练集、验证\n集和测试集。训练集用于模型的训练，验证集用于调整模型的超参数及进行模型选择，而测..."
      ],
      "response_time": 1.7949516773223877,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.10122092072881334,
        "semantic_similarity": 0.80716872215271
      }
    },
    {
      "sample_id": "0196",
      "question": "LLMEval-3 考虑的重要因素是什么？",
      "ground_truth": "防止作弊是 LLMEval-3 考虑的重要因素。",
      "predicted_answer": "LLMEval-3 考虑的重要因素之一是防止作弊，具体来说，它采取了一种名为“题库考试”的新模式来避免测试题库泄露导致的“刷榜”、“刷分”等问题。[1]",
      "ground_context": [
        "防止作弊是 LLMEval-3 考虑的重要因素。"
      ],
      "retrieved_contexts": [
        "型语言能力。\n防止作弊是 LLMEval-3 考虑的重要因素。现有公开评测基准存在测试题库泄露的问题，因此\n可能出现 “刷榜” 、 “刷分” 等不公平现象， 在LLMEval-3 中， 每个参与评测的系统需要完成从总题",
        "同模型的知识问答效果。LLMEval-3 评测采用了一种新颖的评测模式，即“题库考试”模式，既\n可以满足模型随时测试的需求，又尽最大可能防止刷榜现象的发生。LLMEval-3 聚焦于专业知识\n能力评测，涵盖哲学、经济学、法学、教育学、文学、历史学、理学、工学、农学、医学、军事\n学、 管理学、 艺术学等教育部划定的13 个学科门类、50 余个二级学科， 共计约100 万道标准生成\n式问答题目。题目来源主要包括大学本科课后作业、大学本科期中期末考试、研究生入学考试等。\n为了尽可能的防止参与评测的大模型在预训练阶段引入大比例原始评测数据，LLMEval-3 评测题\n目来源尽可能为非互联网公开渠道， 数据格式为PDF 和 Word 文件， 经过一定的OCR 识别与数据\n清洗之后，将题目进行格式化处理。针对于不同的题型，提供给待测试模型标准接口，实现全流\n程自动化。与其他知识评测所采用的选择题模式不同，LLMEval-3 中所有问题将统一处理为生成\n式知识问答形式，并尽可能包含多种题型，包括简答，计算、判断、辨析、写作等。相较于具有\n标准格式的选择题，LLMEval-3 所采用的生成式知识问答，...",
        "通过这两种题型的有机组合，评估旨在全面考查模型在不同学科领域中解决问题的能力。每个学\n科都设计了 25∼30 道客观题和 10∼15 道主观题， 共计480 道题目。 评估采用了人工评分和GPT-4\n自动评分两种方法。对于客观题，答对即可获得满分，而对于答错的情况，根据回答是否输出了\n中间过程或解释，对解释的正确性进行评分。主观题方面，依据问答题的准确性、信息量、流畅\n性和逻辑性这四个维度评分，准确性（5 分） ：评估回答的内容是否有错误；信息量（3 分） ：评估\n回答提供的信息是否充足；流畅性（3 分） ：评估回答的格式和语法是否正确；逻辑性（3 分） ：评\n估回答的逻辑是否严谨。为了避免与网上已有的试题重复， LLMEV AL-2在题目的构建过程中力\n求独立思考，旨在更准确、更全面地反映大语言模型的能力和在真实场景中的实际表现。\nLLMEV AL第三期（LLMEV AL-3）基准测试提供了更加全面且更具挑战性的问题。其目标是\n评估模型在中文知识问答任务上的表现，并提供一个公平的比较平台，以便研究人员可以评估不\n同模型的知识问答效果。LLMEval-3 评测采用了一种新颖的评测模式，...",
        "178 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n的上下文窗口扩展到 32768。\n位置插值通过小代价的微调显著扩展 LLaMA 模型的上下文窗口，在保持原有扩展模型内任\n务能力的基础上，显著增加模型对长文本的建模能力。另外，通过位置插值扩展的模型可以充分\n重用现有的预训练大语言模型和优化方法，这在实际应用中具有很大吸引力。\n5.4 DeepSpeed-Chat SFT 实践\nChatGPT 整体的训练过程复杂，虽然基于 DeepSpeed 可以通过单机多卡、多机多卡、流水线\n并行等操作来训练和微调大语言模型，但是没有端到端的基于人类反馈机制的强化学习的规模化\n系统， 仍然会造成训练类ChatGPT 系统非常困难。DeepSpeed-Chat[244] 是微软于 2023 年 4 月发布\n的基于 DeepSpeed 用于训练类 ChatGPT 模型的开发工具。基于 DeepSpeed-Chat 训练类 ChatGPT\n对话模型的步骤框架如图5.10 所示，包含以下三个步骤。\n（1）指令微调：使用精选的人类回答来微调预训练语言模型以应对各种查询。\n（2）奖励模型微调：使用...",
        "LLMEV AL团队联合复旦大学医学院，复旦大学附属华山医院，复旦大学附属肿瘤医院，共\n同推出 LLMEV AL-Medicine专题医学领域大模型评测，选择医疗领域作为核心评测领域，提出医\n疗增强评测体系框架。\n目前医疗领域评估体系主要分为三大类：医生职业资格考试、综合性医疗评估以及专项能力\n评测。\n• 医生职业资格考试：作为各国医学教育的最高标准，通过系统化的考核体系来评估医学生，\n包括美国 USMLE 考试和中国执业医师资格考试。 这类评估的优势在于能够全面考察医学知\n识与临床技能，但存在两个主要缺陷：其一，评估维度较为单一，未能充分考察语言处理、\n内容生成等智能模型的关键能力；其二， 考核方式过于传统， 主要采用选择题形式， 侧重于\n记忆性知识点的考察，难以体现临床实践中的复杂思维能力。\n• 综合性医疗评估： 第三方机构发布的榜单虽然在任务范围和能力分类上具有一定的广度， 但\n其体系设计仍存在明显不足。 这些榜单在医疗推理和综合能力的评估上存在明显短板，CBLUE\n等评测平台主要聚焦于传统 NLP 任务。此外，这些榜单普遍偏重理论性任务，未能充分反\n映实际医疗场景中的复杂需求，..."
      ],
      "response_time": 1.5721404552459717,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.8,
        "rouge2": 0.6666666666666666,
        "rougeL": 0.8,
        "bleu": 0.13772093428074758,
        "semantic_similarity": 0.8254851698875427
      }
    },
    {
      "sample_id": "0197",
      "question": "评估生成文本中知识准确性涉及哪些方面？",
      "ground_truth": "这涉及事实陈述、概念解释、历史事件描述等方面。",
      "predicted_answer": "评估生成文本中知识准确性涉及以下几个方面：首先，文本中的事实陈述、概念解释和历史事件描述应基于准确的知识和可靠的信息源；其次，避免出现错误、虚假或误导性的内容；最后，确保提供的知识准确无误。这些措施共同保证了生成文本的知识准确性[1]。",
      "ground_context": [
        "知识准确性：评估生成文本中所呈现的知识是否准确无误。 这涉及事实陈述、概念解释、历史事件描述等方面。"
      ],
      "retrieved_contexts": [
        "大规模语言模型：从理论到实践 425\n• 知识准确性：评估生成文本中所呈现的知识是否准确无误。 这涉及事实陈述、 概念解释、 历\n史事件描述等方面。 生成的文本应基于准确的知识和可靠的信息源， 避免错误、 虚假或误导\n性的内容。确保所提供的知识准确无误。\n• 知识丰富性： 评估生成文本所包含的知识是否丰富多样。 生成的文本应能够提供充分的信息，\n涵盖相关领域的不同方面。 这可以通过提供具体的例子、 详细的解释和相关的背景知识来实\n现。确保生成文本在知识上具有广度和深度，能够满足读者的需求。\n• 知识一致性： 评估生成文本中知识的一致性。 这包括确保文本中不出现相互矛盾的知识陈述，\n避免在不同部分或句子中提供相互冲突的信息。 生成的文本应该在整体上保持一致， 使读者\n能够得到一致的知识体系。\n11.3.2 评估方法\n评估方法的目标是解决如何对大语言模型生成结果进行评估的问题。有些指标可以通过比较\n正确答案或参考答案与系统生成结果直接计算得出， 例如准确率、 召回率等。 这种方法被称为自动\n评估（Automatic Evaluation） 。 然而， 有些指标并不是可以直接计算出来的， 而...",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "成有条理的论述，文本中的论证、推理、归纳、演绎等逻辑关系应该正确。句子的顺序应符\n合常规的时间、空间或因果关系，以便读者能够理解句子之间的联系。\n• 风格一致性：评估文本在整体风格上是否保持一致。 包括词汇选择、 句子结构、 表达方式等\n方面。文本应该在整体上保持一种风格或口吻。例如，正式文本应使用正式的语言和术语，\n而故事性的文本可以使用生动的描写和故事情节。\n（3）知识层面的评估主要关注知识准确性、知识丰富性和知识一致性。要求生成文本所涉及\n的知识准确无误、丰富全面，确保文本的可信度。具体如下：",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "则，包括时态、数和派生等方面。\n• 语法正确性： 评估生成文本的句子结构和语法规则是否正确。 确保句子的构造完整， 各个语\n法成分之间的关系符合语法规则， 包括主谓关系、 动宾关系、 定状补关系等方面的准确应用。\n此外，还需要评估动词的时态是否使用正确，包括时态的一致性和选择是否符合语境。\n• 篇章结构正确性： 评估生成文本的整体结构是否合理。 确保文本段落之间连贯， 文本信息流\n畅自然， 包括使用恰当的主题句、 过渡句和连接词等。同时， 需要评估文本整体结构的合理\n性，包括标题、段落、章节等结构的使用是否恰当，以及文本整体框架是否清晰明了。\n（2）语义层面的评估主要关注文本的语义准确性、逻辑连贯性和风格一致性。要求生成的文\n本不出现语义错误或误导性描述，并且具有清晰的逻辑结构，能够按照一定的顺序和方式呈现出\n来。具体如下：\n• 语义准确性：评估文本是否传达了准确的语义信息。包括词语的确切含义和用法是否正确，\n以及句子表达的意思是否与作者的意图相符。 确保文本中使用的术语、 概念和描述准确无误，\n能够准确传达信息给读者。\n• 逻辑连贯性： 评估文本的逻辑结构是否连贯一致。 句子之间应该..."
      ],
      "response_time": 1.8839271068572998,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 0.3333333333333333
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.05472590709519809,
        "semantic_similarity": 0.6788253784179688
      }
    },
    {
      "sample_id": "0198",
      "question": "司南平台支持多少种开源模型的评测？",
      "ground_truth": "超过100种",
      "predicted_answer": "司南平台支持超过100种开源模型的评测。[3]",
      "ground_context": [
        "此外，平台支持超过100种开源模型的评测，并预留接口供开发者接入自定义模型或 API 模型，如 OpenAI 接口。"
      ],
      "retrieved_contexts": [
        "司南平台具有多项显著特点，其开源可复现的设计让评测过程公开透明，确保结果的准确性\n和可信度。评测维度涵盖基础能力和综合能力两个层级，包括语言、知识、代码、长文本处理等\n12 个一级能力维度和 50 余个二级能力维度， 全面反映模型的实际性能。 此外， 平台支持超过100\n种开源模型的评测，并预留接口供开发者接入自定义模型或 API 模型，如 OpenAI 接口。司南平\n台还提供分布式高效评测方案，能够在本地或集群中并行分发任务，优化时间和资源分配。同时，\n它灵活支持用户自定义数据集和评测策略，提供零样本、小样本和思维链式评测方式，满足多样\n化的评测需求。\n5. Chatbot Arena 评估\nChatbot Arena 是一个以众包方式进行匿名对比评估的大语言模型基准评估平台 [196]。研究人\n员构造了多模型服务系统 FastChat。当用户进入评估平台后可以输入问题，同时得到两个匿名模\n型的回答， 如图11.14 所示。 在从两个模型中获得回复后， 用户可以继续对话或投票选择他们认为\n更好的模型。一旦提交了投票，系统会将模型名称告知用户。用户可以继续对话或重新开始与两\n个新选择的...",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "和解决能力。也正因如此，AIME 在全球范围内都备受关注，成为了众多数学爱好者展示实力的\n舞台，同时也成为衡量 AI 数学能力的重要指标之一。\n4. OpenCompass 司南\nOpenCompass 司南平台是由上海人工智能实验室研发的大模型开源开放评测体系，其核心目\n标是为大语言模型的性能评估提供一个公平、客观、可复现的标准化平台。平台由CompassRank、\nCompassHub 和 CompassKit 三大核心组件构成，分别承担模型性能榜单、评测基准社区和评测工\n具链的功能。 其中，CompassRank 提供动态更新的权威评测榜单， 通过多领域、 多任务的客观评测\n手段展示模型性能， 并保持中立性；CompassHub 则作为一个开放的评测基准社区， 聚合了多种能\n力和行业场景下的评测基准资源，用户还可以上传自定义基准数据并发布性能榜单。 CompassKit\n则是一个全栈评测工具链体系，包含多种开源工具，如大语言模型评测工具、代码评测服务工具\n和多模态评测工具，帮助用户快速、高效地完成分布式评测任务。\n司南平台具有多项显著特点，其开源可复现的设计让评测过程公开透明，确...",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "438 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n集则涵盖了八种主流编程语言（Python、Java、C++、C#、JavaScript、TypeScript、PHP 和 Bash） 。\nHumanEval 系列评测为研究者提供了一个标准化的数据集和工具，用于量化模型在代码生成任务\n中的能力。\nLiveCodeBench[625] 是一个动态且全面的基准测试集， 专为评估大语言模型的代码生成能力设\n计。该测试集从 LeetCode、AtCoder、CodeForces 等竞赛平台持续收集新问题，截至 2025 年 1 月\n已包含 880 道高质量编码挑战，覆盖代码生成、自修复、代码执行和测试输出预测等多种能力场\n景。通过仅选用新发布的问题，避免训练数据与测试数据重叠，确保评估无污染且客观公正。它\n支持用户自定义模型风格和评估流程，提供直观的命令行接口及详尽文档，方便新手和专家快速\n上手。 此外， 公开的Leaderboard 增强透明度， 鼓励社区互动与模型性能的持续提升， 使其成为目\n前评估大语言模型编码能力的重要工具。\nSWE-bench V erified是 Op..."
      ],
      "response_time": 0.7910246849060059,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.6666666666666666,
        "rouge2": 0.0,
        "rougeL": 0.6666666666666666,
        "bleu": 0.09010288336595924,
        "semantic_similarity": 0.5998464822769165
      }
    },
    {
      "sample_id": "0199",
      "question": "p 值的意义是什么？",
      "ground_truth": "p 值是观察这个经验 （或更大的） 卡方值的概率。",
      "predicted_answer": "p 值是观察到给定的经验卡方值（或更大的值）的概率。如果 p 值小于预先设定的显著性水平阈值，则可以拒绝两个模型性能相同这一零假设，也就是说，可以认为这两个模型的性能存在显著差异[1]。",
      "ground_context": [
        "p 值是观察这个经验 （或更大的） 卡方值的概率。"
      ],
      "retrieved_contexts": [
        "p 值是观察这个经验 （或更大的） 卡方值的概率。 如果p 值小于预先设置的显著性水平阈值， 则可\n以拒绝两个模型性能相等的零假设。换句话说，如果 p 值小于显著性水平阈值，则可以认为两个\n模型的性能不同。",
        "大规模语言模型：从理论到实践 431\n文献 [604] 在上述公式的基础上，提出了一个连续性修正版本，这也是目前更常用的变体：\nχ2 = (|B − C| − 1)2\nB + C (11.21)\n当 B 和 C 的值大于 50 时， 麦克尼马尔检验可以相对准确地近似计算p 值， 如果B 和 C 的值\n相对较小（B + C < 25） ，则建议使用以下二项式检验公式计算p 值：\np = 2\nnX\ni=B\n \nn\ni\n!\n0.5i(1 − 0.5)n−i (11.22)\n其中 n = B + C，因子 2 用于计算双侧 p 值（Two-sided p-value） 。\n针对图11.11 中的两种情况，可以使用 mlxtend[555] 来计算 p 值和 χ2：\nfrom mlxtend.evaluate import mcnemar\nimport numpy as np\ntb_a = np.array([[9959, 11],\n[1, 29]])\nchi2, p = mcnemar(ary=tb_a, exact =True)\nprint('chi-squared-a:', chi2)\np...",
        "lr_scheduler = get_scheduler(\nname=args.lr_scheduler_type,\noptimizer=optimizer,\nnum_warmup_steps=args.num_warmup_steps,\nnum_training_steps=args.num_train_epochs * num_update_steps_per_epoch,\n)\ndef get_optimizer_grouped_parameters(model,\nweight_decay,\nno_decay_name_list=[\n\"bias\", \"LayerNorm.weight\"\n]):\n#đਸ਼၂ቆીႵ\noptimizer_grouped_parameters = [\n{\n\"params\": [\np for n, p in model.named_parameters()\nif (not any(nd in n\nfor nd in no_decay_name_list) and p.requires_grad)\n],\n\"weight_decay\": weight_decay,...",
        "(9.17)\n其中 P (k) 是指在排名列表中截止到 k 位置时的精确率，rel(k) 是一个指示函数， 当排名为k 的项\n目是相关文档时，其值为 1，否则为 0，n 是检索到的文档数量。\n非基于排名的指标通常用于评估二元结果，即一个项目是否相关，而不考虑该项目在排序列\n表中的位置。需要注意的是，以下公式只是这些指标的一种形式，每个指标的定义可能因评估任\n务的不同而有所差异。评测指标主要有：\n准确率 （Accuracy） 是指在检查的所有案例中， 真实结果 （包括真阳性和真阴性） 所占的比例。\n精确率（Precision）是检索到的实例中相关实例的比例，公式为：\nPrecision = T P\nT P + F P (9.18)\n其中 T P 表示真阳性，F P 表示假阳性。\n召回率 @k（Recall@k）是在仅考虑前 k 个结果的情况下，已检索到的相关实例占总相关实\n例的比例，公式为：\nRecall@k = |RD ∩ T opkd|\n|RD| (9.19)",
        "},\n{\n\"params\": [\np for n, p in model.named_parameters()\nif (any(nd in n\nfor nd in no_decay_name_list) and p.requires_grad)\n],\n\"weight_decay\": 0.0,\n},\n]\nreturn optimizer_grouped_parameters"
      ],
      "response_time": 1.6975440979003906,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.5,
        "rouge2": 0.0,
        "rougeL": 0.5,
        "bleu": 0.12885357782071694,
        "semantic_similarity": 0.8136454224586487
      }
    },
    {
      "sample_id": "0200",
      "question": "在LLMEval基准测试中，如何确保评估模型在知识问答任务上的表现既准确又具有挑战性？",
      "ground_truth": "LLMEval-3通过采用新颖的评测模式，即“题库考试”模式，旨在评估模型在中文知识问答任务上的表现，并提供一个公平的比较平台。同时，评估涉及知识准确性、丰富性和一致性，以确保生成文本的可信度和逻辑关系的正确性。",
      "predicted_answer": "LLMEval-3 基准测试通过采用“题库考试”模式来确保评估模型在知识问答任务上的表现既准确又具有挑战性，这种模式既可以满足模型随时测试的需求，又能尽量防止刷榜现象的发生[3]。此外，该测试覆盖了广泛的学科领域，包括13个学科门类下的50余个二级学科，共计约100万道标准生成式问答题目，这些题目来源于大学本科课后作业、考试及研究生入学考试等，以减少模型在预训练阶段使用大量原始评测数据的可能性[3]。",
      "ground_context": [
        "LLMEval-3基准测试提供了更加全面且更具挑战性的问题。其目标是评估模型在中文知识问答任务上的表现，并提供一个公平的比较平台，以便研究人员可以评估不同模型的知识问答效果。",
        "知识层面的评估主要关注知识准确性、知识丰富性和知识一致性。要求生成文本所涉及的知识准确无误、丰富全面，确保文本的可信度。"
      ],
      "retrieved_contexts": [
        "通过这两种题型的有机组合，评估旨在全面考查模型在不同学科领域中解决问题的能力。每个学\n科都设计了 25∼30 道客观题和 10∼15 道主观题， 共计480 道题目。 评估采用了人工评分和GPT-4\n自动评分两种方法。对于客观题，答对即可获得满分，而对于答错的情况，根据回答是否输出了\n中间过程或解释，对解释的正确性进行评分。主观题方面，依据问答题的准确性、信息量、流畅\n性和逻辑性这四个维度评分，准确性（5 分） ：评估回答的内容是否有错误；信息量（3 分） ：评估\n回答提供的信息是否充足；流畅性（3 分） ：评估回答的格式和语法是否正确；逻辑性（3 分） ：评\n估回答的逻辑是否严谨。为了避免与网上已有的试题重复， LLMEV AL-2在题目的构建过程中力\n求独立思考，旨在更准确、更全面地反映大语言模型的能力和在真实场景中的实际表现。\nLLMEV AL第三期（LLMEV AL-3）基准测试提供了更加全面且更具挑战性的问题。其目标是\n评估模型在中文知识问答任务上的表现，并提供一个公平的比较平台，以便研究人员可以评估不\n同模型的知识问答效果。LLMEval-3 评测采用了一种新颖的评测模式，...",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "同模型的知识问答效果。LLMEval-3 评测采用了一种新颖的评测模式，即“题库考试”模式，既\n可以满足模型随时测试的需求，又尽最大可能防止刷榜现象的发生。LLMEval-3 聚焦于专业知识\n能力评测，涵盖哲学、经济学、法学、教育学、文学、历史学、理学、工学、农学、医学、军事\n学、 管理学、 艺术学等教育部划定的13 个学科门类、50 余个二级学科， 共计约100 万道标准生成\n式问答题目。题目来源主要包括大学本科课后作业、大学本科期中期末考试、研究生入学考试等。\n为了尽可能的防止参与评测的大模型在预训练阶段引入大比例原始评测数据，LLMEval-3 评测题\n目来源尽可能为非互联网公开渠道， 数据格式为PDF 和 Word 文件， 经过一定的OCR 识别与数据\n清洗之后，将题目进行格式化处理。针对于不同的题型，提供给待测试模型标准接口，实现全流\n程自动化。与其他知识评测所采用的选择题模式不同，LLMEval-3 中所有问题将统一处理为生成\n式知识问答形式，并尽可能包含多种题型，包括简答，计算、判断、辨析、写作等。相较于具有\n标准格式的选择题，LLMEval-3 所采用的生成式知识问答，...",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "大规模语言模型：从理论到实践 441\n图 11.15 Chatbot Arena 给出的系统之间的胜率矩阵 [196]\n6. LLMEVAL 评估\nLLMEV AL[411] 中文大语言模型评估先后进行了二期，LLMEV AL-1评估涵盖了 17 个大类、453\n个问题，包括事实性问答、阅读理解、框架生成、段落重写、摘要、数学解题、推理、诗歌生成、\n编程等各个领域。 针对生成内容的质量， 细化为5 个评分项， 分别是正确性、 流畅性、 信息量、 逻\n辑性和无害性，具体如下。\n• 正确性： 评估回答是否正确， 即所提供的信息是否正确无误。 一个高质量的回答应当在事实\n上是可靠的。\n• 流畅性：评估回答是否贴近人类语言习惯， 即语句是否通顺、 表达是否清晰。 一个高质量的\n回答应当易于理解，不含烦琐或难以解读的句子。\n• 信息量：评估回答是否提供了足够的有效信息，即回答中的内容是否具有实际意义和价值。"
      ],
      "response_time": 3.3122010231018066,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.4444444444444445,
        "rouge2": 0.2857142857142857,
        "rougeL": 0.4444444444444445,
        "bleu": 0.10659334160520148,
        "semantic_similarity": 0.8153153657913208
      }
    },
    {
      "sample_id": "0201",
      "question": "在评测编程模型时，如何考虑统计检验的准确性？",
      "ground_truth": "在评测编程模型时，使用'PASS@K'指标评估模型表现，同时在进行统计检验时，如果B和C的值大于50，可以使用麦克尼马尔检验来近似计算p值；而当B和C的值较小（B + C < 25）时，则建议使用二项式检验公式来计算p值。",
      "predicted_answer": "在评测编程模型时，可以使用严格的评估指标来确保统计检验的准确性。严格指标通过简单的规则匹配，验证模型输出是否完全符合指令要求，直接比较输出结果与指令的字符串内容[3]。这种方法虽然实现简单，但对细微差异敏感，容易导致误判。因此，在实际应用中可能需要结合宽松指标，通过对输出结果进行多种变换后再判断指令是否被遵循，以减少误判风险[3]。",
      "ground_context": [
        "评测方式是将问题提示词输入模型，让模型生成代码并通过测试用例验证其正确性。评估采用“PASS@K”指标，核心在于模拟真实编程场景，考察模型在理解上下文、逻辑推理以及多步操作中的表现。",
        "当B和C的值大于50时，麦克尼马尔检验可以相对准确地近似计算p值，如果B和C的值相对较小（B + C < 25），则建议使用以下二项式检验公式计算p值。"
      ],
      "retrieved_contexts": [
        "段，如问题编号、提示词、入口函数、手写答案及测试用例等。评测方式是将问题提示词输入模\n型，让模型生成代码并通过测试用例验证其正确性。评估采用“ PASS@K”指标，核心在于模拟\n真实编程场景，考察模型在理解上下文、逻辑推理以及多步操作中的表现。HumanEval-Mul 数据",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "方式，以明确模型在不同类型指令上的不足，并支持不同模型间的对比分析。评估方法采用两种\n指标：严格（Strict）指标和宽松（Loose）指标。严格指标通过简单的规则匹配，验证模型输出是\n否完全符合指令要求，直接比较输出结果与指令的字符串内容。该方法实现简单，但对细微差异\n敏感，容易导致误判。而宽松指标通过对输出结果进行多种变换后再判断指令是否被遵循，以减\n少误判风险。这些变换包括删除 Markdown 修饰符、跳过输出的首行或末行、JSON 格式转换等。\n数据集格式包含指令类型、任务指令和说明等信息。例如，指令类型包括“长度限制” （Length\nConstraints） 、 “可检测格式” （Detectable Format） 、 “关键词” （Keywords） 等； 任务指令如 “在回复\n中包含关键词 keyword” ；此外还有对任务的详细描述，如要求生成指定格式、段落数或包含特定\n关键词等。IFEval 为研究者提供了一种全面、灵活的工具，用于评估和改进模型的指令执行能力。\nHumanity’s Last Exam [624] 是由人工智能安全中心（Center for A...",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "大规模语言模型：从理论到实践 429\n（1）语法正确性：故事片段文本的语法正确程度。\n（2）连贯性：故事片段中句子之间的衔接连贯程度。\n（3）喜好度：故事片段令人愉悦的程度。\n（4）相关性：故事片段是否符合给定的要求。\n为了与人工评估进行对比，研究人员将输入大语言模型的文本内容，同样给到一些评估者进\n行人工评估。在开放式故事生成和对抗性攻击两个任务上的实验结果表明，大语言模型评估的结\n果与人工评估得到的结果一致性较高。同时他们也发现，在使用不同的任务说明格式和生成答案\n采样算法的情况下，大语言模型的评估结果也是稳定的。\nč\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 Ď\n图 11.9 使用大语言模型进行结果评估的过程 [600]\n3. 对比评估\n对比评估的目标是比较不同系统、方法或算法在特定任务上是否存在显著差异。麦克尼马尔\n检验（McNemar Test）[602]是由 Quinn McNemar 于 1947 年提出的一种用于成对比较的非参数统\n计检验方法，可用于比较两个机器学习分类器的性能。麦克尼马尔检验也被称为“被试内卡方检\n验” （within-subjects chi-squared..."
      ],
      "response_time": 2.5689806938171387,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0.0,
        "bleu": 0.07434490504786478,
        "semantic_similarity": 0.7275532484054565
      }
    },
    {
      "sample_id": "0202",
      "question": "LLMEV AL-Medicine专题医学领域大模型评测和OpenCompass司南平台在评测体系上有何异同？",
      "ground_truth": "LLMEV AL-Medicine专题医学领域大模型评测主要聚焦于医疗领域的评估，提出了医疗增强评测体系框架，并将评估体系分为医生职业资格考试、综合性医疗评估和专项能力评测三大类，强调了评估的复杂性和对智能模型能力的考量。而OpenCompass司南平台则是一个开源开放评测体系，提供公平、客观、可复现的标准化评测，涵盖多领域、多任务的客观评测手段，侧重于基础能力和综合能力的评估，包括语言、知识和代码等维度。两者在核心目标上存在差异，前者更专注于医疗领域，而后者则提供更广泛的评测平台。",
      "predicted_answer": "根据提供的文档，我无法回答这个问题。【参考文档】中提到了LLMEV AL-Medicine专题医学领域大模型评测的相关信息，但未提及OpenCompass司南平台的具体评测体系详情，因此无法比较两者的异同。",
      "ground_context": [
        "LLMEV AL团队联合复旦大学医学院，复旦大学附属华山医院，复旦大学附属肿瘤医院，共同推出 LLMEV AL-Medicine专题医学领域大模型评测，选择医疗领域作为核心评测领域，提出医疗增强评测体系框架。",
        "OpenCompass 司南平台是由上海人工智能实验室研发的大模型开源开放评测体系，其核心目标是为大语言模型的性能评估提供一个公平、客观、可复现的标准化平台。"
      ],
      "retrieved_contexts": [
        "LLMEV AL团队联合复旦大学医学院，复旦大学附属华山医院，复旦大学附属肿瘤医院，共\n同推出 LLMEV AL-Medicine专题医学领域大模型评测，选择医疗领域作为核心评测领域，提出医\n疗增强评测体系框架。\n目前医疗领域评估体系主要分为三大类：医生职业资格考试、综合性医疗评估以及专项能力\n评测。\n• 医生职业资格考试：作为各国医学教育的最高标准，通过系统化的考核体系来评估医学生，\n包括美国 USMLE 考试和中国执业医师资格考试。 这类评估的优势在于能够全面考察医学知\n识与临床技能，但存在两个主要缺陷：其一，评估维度较为单一，未能充分考察语言处理、\n内容生成等智能模型的关键能力；其二， 考核方式过于传统， 主要采用选择题形式， 侧重于\n记忆性知识点的考察，难以体现临床实践中的复杂思维能力。\n• 综合性医疗评估： 第三方机构发布的榜单虽然在任务范围和能力分类上具有一定的广度， 但\n其体系设计仍存在明显不足。 这些榜单在医疗推理和综合能力的评估上存在明显短板，CBLUE\n等评测平台主要聚焦于传统 NLP 任务。此外，这些榜单普遍偏重理论性任务，未能充分反\n映实际医疗场景中的复杂需求，...",
        "通过这两种题型的有机组合，评估旨在全面考查模型在不同学科领域中解决问题的能力。每个学\n科都设计了 25∼30 道客观题和 10∼15 道主观题， 共计480 道题目。 评估采用了人工评分和GPT-4\n自动评分两种方法。对于客观题，答对即可获得满分，而对于答错的情况，根据回答是否输出了\n中间过程或解释，对解释的正确性进行评分。主观题方面，依据问答题的准确性、信息量、流畅\n性和逻辑性这四个维度评分，准确性（5 分） ：评估回答的内容是否有错误；信息量（3 分） ：评估\n回答提供的信息是否充足；流畅性（3 分） ：评估回答的格式和语法是否正确；逻辑性（3 分） ：评\n估回答的逻辑是否严谨。为了避免与网上已有的试题重复， LLMEV AL-2在题目的构建过程中力\n求独立思考，旨在更准确、更全面地反映大语言模型的能力和在真实场景中的实际表现。\nLLMEV AL第三期（LLMEV AL-3）基准测试提供了更加全面且更具挑战性的问题。其目标是\n评估模型在中文知识问答任务上的表现，并提供一个公平的比较平台，以便研究人员可以评估不\n同模型的知识问答效果。LLMEval-3 评测采用了一种新颖的评测模式，...",
        "大规模语言模型：从理论到实践 445\n库中随机抽样的 1000 题， 针对同一机构的模型， 确保每次评测题目不重复。 评测过程将采用在线\n方式，一轮评测中题目的发送串行进行，即下一题的发送将会视上一道题目的回答情况而定，避\n免恶意爬取行为。\n7. LLMEVAL-Medical 医疗大模型评测\n医疗领域因其直接关乎人类健康，不仅具备高度复杂性和严格的安全标准，还拥有丰富且多\n样化的数据资源，因而成为领域大模型评测的理想选择。医疗领域涉及多学科交叉，涵盖基础医\n学、临床诊断、治疗决策及健康管理等复杂任务。大模型在此需要具备卓越的逻辑推理、精准沟\n通及文本生成能力， 使其成为检验AI 综合能力的最佳场景。 医疗决策的精准性至关重要， 任何偏\n差都可能带来不可逆的后果。因此，在大模型应正式应用前，必须通过科学评测确保其安全性和\n可靠性，以规避潜在风险，保障临床应用的合规性。医疗领域拥有庞大的数据资源，如电子健康\n记录、 医学影像和科研文献等， 为多模态评测提供了广阔空间。 此外， 全球医疗合作需求强烈， 建\n立统一的领域大模型评测标准有助于提升国际化适配能力，推动 AI 技术与医疗深度融合。...",
        "444 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n排名选手和低排名选手的比赛中，如果高排名选手获胜，那么只会从低排名选手处获得很少的排\n名分。然而，如果低排名选手爆冷获胜，则可以获得更多排名分。虽然这种评分系统非常适合竞\n技比赛，但是与顺序有关，并且对噪声非常敏感。积分制得分也是一种常见的比赛评分系统，用\n于在竞技活动中确定选手或团队的排名。该制度根据比赛中获得的积分数量，决定参与者在比赛\n中的表现和成绩。在 LLMEV AL评估中，根据用户给出的“A 系统好” 、 “B 系统好” 、 “两者一样\n好”及“两者都不好”的选择，分别给A 系统 +1 分，B 系统 +1 分，A 和 B 系统各 +0.5 分。该评\n分系统与顺序无关，并且对噪声的敏感程度相较 Elo 评分系统低。\nLLMEV AL第二期（LLMEV AL-2）的目标是以用户日常使用为主线， 重点考查大语言模型解\n决不同专业本科生和研究生在日常学习中所遇到的问题的能力。涵盖的学科非常广泛，包括计算\n机、 法学、 经济学、 医学、 化学、 物理学等12 个领域。 评估数据集包含两种题型： 客观题和主观题。\n通过这...",
        "大规模语言模型：从理论到实践 441\n图 11.15 Chatbot Arena 给出的系统之间的胜率矩阵 [196]\n6. LLMEVAL 评估\nLLMEV AL[411] 中文大语言模型评估先后进行了二期，LLMEV AL-1评估涵盖了 17 个大类、453\n个问题，包括事实性问答、阅读理解、框架生成、段落重写、摘要、数学解题、推理、诗歌生成、\n编程等各个领域。 针对生成内容的质量， 细化为5 个评分项， 分别是正确性、 流畅性、 信息量、 逻\n辑性和无害性，具体如下。\n• 正确性： 评估回答是否正确， 即所提供的信息是否正确无误。 一个高质量的回答应当在事实\n上是可靠的。\n• 流畅性：评估回答是否贴近人类语言习惯， 即语句是否通顺、 表达是否清晰。 一个高质量的\n回答应当易于理解，不含烦琐或难以解读的句子。\n• 信息量：评估回答是否提供了足够的有效信息，即回答中的内容是否具有实际意义和价值。"
      ],
      "response_time": 1.59019136428833,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 1.0,
        "rouge2": 1.0,
        "rougeL": 1.0,
        "bleu": 0.0468722989621203,
        "semantic_similarity": 0.8545492887496948
      }
    },
    {
      "sample_id": "0203",
      "question": "大语言模型自何时以来在多个领域的应用开发取得了显著进展？",
      "ground_truth": "自2023年以来",
      "predicted_answer": "自2023年以来，大语言模型在多个领域的应用开发取得了显著进展，包括智能客服、内容生成、教育辅助、医疗咨询、代码生成等场景。[3]",
      "ground_context": [
        "自2023年以来，大语言模型在多个领域的应用开发取得了显著进展，包括智能客服、内容生成、教育辅助、医疗咨询、代码生成等场景。"
      ],
      "retrieved_contexts": [
        "4 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\nѠ\n图 1.1 大语言模型的缩放法则 [17]\n1.2 大语言模型的发展历程\n大语言模型的发展历程虽然只有不到5 年，但是发展速度相当惊人，截至2025 年 2 月，国内\n外有超过百种大语言模型相继发布。特别是2024 年 12 月 DeepSeek V3 和 2025 年 1 月 DeepSeek\nR1 模型的开源， 不仅在训练效率和思考推理上取得了突破， 还赢得了国际社会对中国人工智能技术\n的高度认可。中国人民大学赵鑫教授团队在《大语言模型》书中按照时间线给出了2019 年至 2024\n年 6 月比较有影响力并且模型参数量超过100 亿个的大语言模型，我们在此基础上扩展到2025 年\n2 月， 如图1.2 所示。 大语言模型的发展可以粗略地分为如下三个阶段： 基础模型阶段、 能力探索阶\n段和突破发展阶段。\n图 1.2 大语言模型发展时间线 [18]\n基础模型阶段主要集中于 2018 年至 2021 年。2017 年，V aswani等人提出了 Transformer[12]\n架构，在机器翻译任务上取得了突破性进展。201...",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "12. 大语言模型应用开发\n大语言模型的广泛应用正在推动技术创新与产业变革。 自2023 年以来， 大语言模型在多个领\n域的应用开发取得了显著进展， 包括智能客服、 内容生成、 教育辅助、 医疗咨询、 代码生成等场景。\n大语言模型凭借其强大的语言理解与生成能力，为开发者和企业提供了全新的工具和平台。然而，\n大语言模型的应用开发也面临诸多挑战，例如如何高效地部署和调用模型、如何定制化以满足特\n定业务需求以及如何应对生成内容的质量控制和潜在风险等等。因此，构建一套系统化的大语言\n模型应用开发流程与方法显得尤为重要。\n本章将首先介绍大语言模型典型应用场景，并在此基础上根据典型应用介绍开发流程、开发\n工具与平台，最后介绍大语言模型本地部署实践。\n12.1 大语言模型典型应用场景\n本节将围绕大语言模型的典型应用场景展开探讨，重点介绍其在内容创作与生成、对话系统\n与聊天机器人、翻译与多语言处理、信息抽取与知识图谱等领域中的实际应用及其技术创新。同\n时， 还将详细分析大语言模型在代码生成与编程辅助、 智能搜索与推荐、 教育与培训、 企业管理与\n决策支持，以及法律与合规等行业中的广泛应用。通过对这些场...",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "1. 绪论\n大语言模型是一种由包含数百亿个及以上参数的深度神经网络构建的语言模型，通常使用自\n监督学习方法通过大量无标注文本进行训练。2018 年以来，Google、OpenAI、Meta、百度、华为\n等公司和研究机构相继发布了 BERT[1]、GPT[2] 等多种模型， 这些模型在几乎所有自然语言处理任\n务中都表现出色。2019 年，大语言模型呈现爆发式的增长，特别是 2022 年 11 月 ChatGPT（Chat\nGenerative Pre-trained Transformer）的发布，引起了全世界的广泛关注。用户可以使用自然语言与\n系统交互，实现问答、分类、摘要、翻译、聊天等从理解到生成的各种任务。大语言模型展现出\n了强大的对世界知识的掌握和对语言的理解能力。\n本章主要介绍大语言模型的基本概念、发展历程和构建流程。\n1.1 大语言模型的基本概念\n使用语言是人类与其他动物最重要的区别之一， 而人类的多种智能也与此密切相关， 逻辑思维\n以语言的形式表达， 大量的知识也以文字的形式记录和传播。 如今， 互联网上已经拥有数万亿个网页\n的资源， 其中大部分信息都是用自然语言描述的。 ..."
      ],
      "response_time": 1.299398422241211,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 0.3333333333333333
      },
      "answer_quality_metrics": {
        "rouge1": 0.6666666666666666,
        "rouge2": 0.0,
        "rougeL": 0.6666666666666666,
        "bleu": 0.059049293157227886,
        "semantic_similarity": 0.5268654823303223
      }
    },
    {
      "sample_id": "0204",
      "question": "llama.cpp 支持哪些类型的推理？",
      "ground_truth": "它不仅支持在 CPU 和 GPU 之间的混合推理。",
      "predicted_answer": "llama.cpp 支持在 CPU 和 GPU 之间的混合推理[2][3]，这使得即使在显存不足的情况下也能运行大型模型。",
      "ground_context": [
        "llama.cpp 的主要优势在于其跨平台兼容性和灵活性。它不仅支持在 CPU 和 GPU 之间的混合推理，使得即使在显存不足的情况下也能运行大型模型，还提供了广泛的后端支持（如 Vulkan、SYCL 和 Metal） 。"
      ],
      "retrieved_contexts": [
        "镜像或直接使用预构建的二进制文件来快速上手。它支持在边缘设备和离线环境中运行模型，非\n常适合需要高隐私性和低延迟的场景，如企业内部部署、嵌入式设备运行和个人研究用途。此外，\nllama.cpp 还支持多种编程语言和开发框架的绑定（如 Python、Rust、Node.js 等） ，以及大量的社",
        "Apple Silicon（通过 ARM NEON、Accelerate 和 Metal 框架优化） 、x86 架构（支持 A VX、A VX2、\nA VX512和 AMX 指令集）以及 NVIDIA 和 AMD GPU（通过 CUDA 和 HIP 实现） 。此外，它还提\n供多种量化技术（例如 1.5-bit 到 8-bit） ，以减少内存使用并加快推理速度。\nllama.cpp 的主要优势在于其跨平台兼容性和灵活性。它不仅支持在 CPU 和 GPU 之间的混合\n推理，使得即使在显存不足的情况下也能运行大型模型，还提供了广泛的后端支持（如 Vulkan、\nSYCL 和 Metal） 。用户可以通过工具将其他模型的权重转换为llama.cpp 支持的 GGUF 文件格式，\n从而运行多种模型，包括 LLaMA、LLaMA 2、Falcon、BERT 等。此外，llama.cpp 提供了大量命\n令行工具，支持交互式聊天、文本生成、语法约束输出等功能，同时兼容OpenAI API，方便用户\n构建和部署自定义应用。\nllama.cpp 提供了多种部署选项， 用户可以通过构建源码、 本地安装包 （...",
        "464 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n区工具和用户界面，从而使其成为开发大语言模型应用的理想选择。\n使用 llama.cpp 前， 首先需要下载模型参数文件。Hugging Face 等平台上有大量的适配 llama.cpp\n的模型。llama.cpp 要求模型以 GGUF 文件格式存储，对于其他数据格式的模型，可以使用仓库中\n的 convert_*.py 脚本进行转换。Hugging Face 平台提供多种在线工具来支持与 llama.cpp 的集成，\n包括 GGUF-my-repo 用于将模型转换为 GGUF 格式并量化权重以减小模型大小，GGUF-my-LoRA\n用于将 LoRA 适配器转换为 GGUF 格式，GGUF-editor 支持在浏览器中编辑 GGUF 元数据，以及\nInference Endpoints 功能可直接在云端托管 Llama.cpp 模型。这些工具显著简化了模型格式转换和\n部署过程。\nllama.cpp 提供了多种命令行工具， 包括llama-cli、llama-server、llama-perplexity、llama-ben...",
        "大规模语言模型：从理论到实践 463\n如 Ollama，它基于 llama.cpp，具备简洁的安装和使用流程。此外，考虑到普通用户通常不会直接\n操作控制台界面，还需要开发支持 Web 界面和应用界面的解决方案。 Open Webui 就是一个旨在\n提供类似 ChatGPT 界面的工具，方便用户与模型交互。本地部署大语言模型的整体架构如图12.4\n所示。\nOpen WebUI\n大语言模型库\n各类型硬件\n大语言模型本地部署框架对话界面\n图 12.4 大语言模型本地部署系统结构图\n本节将首先介绍大语言模型本地部署的核心工具 llama.cpp，在此基础上介绍本地部署工具\nOllama，最后介绍大语言模型网页交互工具 Open WebUI。\n12.3.1 llama.cpp\nllama.cpp 是一个用纯 C/C++ 实现的大语言模型 （LLM） 推理项目， 旨在以最小的设置和高性\n能支持 LLaMA 及其他模型的本地运行。该项目的目标是让用户能够在各种硬件（包括本地设备\n和云端）上高效运行大型语言模型，同时优化对资源的使用。llama.cpp 支持多种硬件架构，包括\nApple Silico...",
        "llama-run 以及 llama-simple。接下来，分别介绍上述命令的使用。\nllama-cli 是用于访问和实验 llama.cpp 大多数功能的命令行工具。 主要包含如下几种使用模式：\n• 对话模式：具有内置聊天模板的模型会自动激活对话模式。也可以通过添加 -cnv 并使用 –\nchat-template NAME 指定合适的聊天模板。\nllama-cli -m model.gguf\n# > hi, who are you?\n# Hi there! I'm your helpful assistant! I'm an AI-powered chatbot designed to assist\nand provide information to users like you. I'm here to help answer your questions,\nprovide guidance, and offer support on a wide range of topics. I'm a friendly and\nknowledgeable AI, and I'm alw..."
      ],
      "response_time": 1.0336997509002686,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 0.5
      },
      "answer_quality_metrics": {
        "rouge1": 0.5,
        "rouge2": 0.33333333333333337,
        "rougeL": 0.5,
        "bleu": 0.23440058276054604,
        "semantic_similarity": 0.8050454258918762
      }
    },
    {
      "sample_id": "0205",
      "question": "国内有哪些应用利用AI技术实现智能答疑？",
      "ground_truth": "作业帮和学而思网校",
      "predicted_answer": "在国内，类似的应用包括作业帮和学而思网校，它们利用AI技术实现了智能答疑，学生只需拍照或输入问题，系统便能快速分析并生成详细解答，极大地方便了学习过程。[5]",
      "ground_context": [
        "在国内，类似的应用也很普遍，像作业帮和学而思网校利用AI 技术实现了智能答疑，学生只需拍照或输入问题，系统便能快速分析并生成详细解答，极大地方便了学习过程。"
      ],
      "retrieved_contexts": [
        "SimpleQA[620] 是 OpenAI 推出的基准测试集，专为评估大语言模型回答事实性问题的能力而\n设计。 它聚焦于简短且以事实为导向的问题， 减少评估复杂性， 提供更精确的事实性衡量方式。 数\n据集覆盖科学、技术、历史、音乐、艺术、视频游戏、政治等多个领域，避免狭隘性，同时针对\n最先进的模型 （如GPT-4） 也具有很高的挑战性， 其通过率不到40%。SimpleQA 数据集包含 4326\n个高质量问题， 这些问题由AI 训练师通过严格流程创建， 确保每个问题只有一个不可争议且不随\n时间变化的答案，并经过多重验证（误差率约 3%） 。评分机制使用ChatGPT 分类器，将回答标记\n为“正确” 、 “错误”或“未尝试” ，并通过询问置信度和重复提问评估模型的校准能力和一致性，\n为研究者提供高效、可靠的评估工具。\nC-SimpleQA（Chinese SimpleQA）[621] 是淘天集团推出的专门用于全面评估中文 AI 模型事实\n性能力的测试集，具有显著的针对性和实用性。该测试集专注于中文语言，涵盖与中国文化相关\n的特色知识， 确保评测符合中文语境和文化特点。 内容分布上，C-...",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "作，赋能创意产业的多个环节。这不仅提高了创作者的生产效率，还为更多非专业人士提供了创\n作的可能性。\n12.1.2 对话系统与聊天机器人\n客服机器人是大语言模型最成熟且广泛应用的领域之一，能够为企业提供高效、智能的客户\n服务解决方案。传统客服机器人在面对复杂、模糊的客户提问时，常常理解偏差，答非所问。大\n语言模型凭借其强大的自然语言处理能力，可深入剖析客户语句含义，即使是隐喻、口语化表述，\n也能精准提取关键信息。 在电商领域， 熟知各类商品参数、 使用方法、 售后政策； 在金融行业， 对\n贷款流程、理财产品细则、金融法规等也能信手拈来。以保险客服场景为例，客户询问“我买的\n这款重疾险，在国外就医能理赔吗？特殊治疗手段，比如质子重离子治疗报销吗？ ” ，大语言模型\n客服机器人可依据保险条款细则和过往理赔案例，给出全面且准确的解答。国内外大量厂家的客\n服系统都通过大语模型极大提升了整体体验。\n传统虚拟助手（如 Siri、Google Assistant 和 Alexa）表现出了强大的语音识别和执行能力，但\n它们的核心架构主要基于任务导向的对话系统， 主要功能集中在预定义的任务上， 如设置闹钟...",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "各种学科问题，指导他们完成作业，并根据学习进度提供实时建议。这种智能化的辅导方式，不\n仅提高了学生的学习效率，还缓解了家长和教师在辅导方面的压力。在国内，类似的应用也很普\n遍， 像作业帮和学而思网校利用AI 技术实现了智能答疑， 学生只需拍照或输入问题， 系统便能快\n速分析并生成详细解答，极大地方便了学习过程。\n在课程设计方面，大语言模型能够协助教师自动生成教学计划和课程内容，显著减轻了备课\n负担。 例如， 微软的Copilot for Education 可以根据教学目标和学生需求， 生成详细的课程大纲、 学\n习资源以及课堂活动建议，帮助教师高效组织教学内容。国内的科大讯飞智慧课堂也整合了类似\n的 AI 功能， 支持教育机构快速设计课程内容， 提供多样化的学习路径， 并根据学生的反馈动态调\n整课程结构。这种工具不仅提升了教学效率，还改善了课程的针对性和灵活性，为教育工作者提\n供了强大的技术支持。\n在考试评估场景中，大语言模型显示了极高的自动化能力，尤其是在作业批改和考试反馈方"
      ],
      "response_time": 1.4412777423858643,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.046610048911630064,
        "semantic_similarity": 0.7299753427505493
      }
    },
    {
      "sample_id": "0206",
      "question": "根据文档，生命的意义是什么？",
      "ground_truth": "我相信生命的意义是找到自己的真理并按照它生活。",
      "predicted_answer": "根据 llama-cli 的命令行提示，一个人认为生命的意义是找到自己的真相并据此生活。这包括忠于自己和追求激情，即使这些可能与社会期望不符。对于提问者来说，瑜伽不仅是一项身体练习，也是一种精神修炼，关乎连接自我、倾听内心声音以及尊重自己的独特之处。[1]",
      "ground_context": [
        "我相信生命的意义是找到自己的真理并按照它生活。"
      ],
      "retrieved_contexts": [
        "大规模语言模型：从理论到实践 465\nllama-cli -m model.gguf -p \"I believe the meaning of life is\" -n 128 -no-cnv\n# I believe the meaning of life is to find your own truth and to live in accordance with\nit. For me, this means being true to myself and following my passions, even if they\ndon't align with societal expectations. I think that's what I love about yoga ⚶it's\nnot just a physical practice, but a spiritual one too. It's about connecting with\nyourself, listening to your inner voice, and honoring your own un...",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "x\n<latexit sha1_base64=\"IVqKn81Phb8FhcRP6SaEHpTS2es=\">AAAB+HicbVDLTgJBEOzFF+IL9ehlIzHxRHaNQY9ELx4hkYeBDZkdemHCzOxmZtYECV/gVe/ejFf/xqtf4gB7ULCSTipV3alOhQln2njel5NbW9/Y3MpvF3Z29/YPiodHTR2nimKDxjxW7ZBo5ExiwzDDsZ0oJCLk2ApHtzO/9YhKs1jem3GCgSADySJGibFS/aFXLHllbw53lfgZKUGGWq/43e3HNBUoDeVE647vJSaYEGUY5TgtdFONCaEjMsCOpZII1MFk/ujUPbNK341iZUcad67+vpgQofVYhHZTEDPUy95M/NcLxVKyia6DCZNJalDSRXCUctfE7qwFt88UUsPHlhCqmP3dpUOiCDW2q4ItxV+uYJU0L8p+pVypX5aqN1k9eTiBUzgHH66gCndQgwZQQHiGF3h1npw35935WKzmnOzmGP7A+fwB8QeTjg...",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "Y\n<latexit sha1_base64=\"17zqk6qKa6kdejpo5MShoZgkkWA=\">AAAB/XicbVC7SgNBFL0bXzG+opY2g0GwWnYlRBshaGMZwTwgWcLsZDYZMzO7zMwKYQn+gq32dmLrt9j6JU6SLTR64MLhnHs5lxMmnGnjeZ9OYWV1bX2juFna2t7Z3SvvH7R0nCpCmyTmseqEWFPOJG0aZjjtJIpiEXLaDsfXM7/9QJVmsbwzk4QGAg8lixjBxkqt5NJzq36/XPFcbw70l/g5qUCORr/81RvEJBVUGsKx1l3fS0yQYWUY4XRa6qWaJpiM8ZB2LZVYUB1k82+n6MQqAxTFyo40aK7+vMiw0HoiQrspsBnpZW8m/uuFYinZRBdBxmSSGirJIjhKOTIxmlWBBkxRYvjEEkwUs78jMsIKE2MLK9lS/OUK/pLWmevX3NpttVK/yuspwhEcwyn4cA51uIEGNIHAPTzBM7w4j86r8+a8L1YLTn5zCL/gfH..."
      ],
      "response_time": 1.8857197761535645,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.08438119133455314,
        "semantic_similarity": 0.7188799977302551
      }
    },
    {
      "sample_id": "0207",
      "question": "大语言模型在企业管理中有哪些应用？",
      "ground_truth": "大语言模型在企业管理和决策支持中表现出了很大的使用前景，能够从大量非结构化文本数据中快速提取关键信息，帮助企业科学制定决策。会议记录与摘要是大语言模型在企业管理中的另一重要应用场景。",
      "predicted_answer": "大语言模型在企业管理中有多种应用：\n\n1. 数据分析与报告生成：如微软的Power BI和阿里云Quick BI等工具利用大语言模型技术，可以帮助企业快速生成关键业务指标的分析结果和可视化报告[1]。Tableau GPT和国内的金蝶云、用友U8等工具也能根据输入的业务数据自动生成各类分析报告[1]。\n\n2. 合同审查与法律风险管理：Kira Systems和国内的“法大大”合同助手等工具运用大语言模型技术来识别合同中的潜在法律风险[2]。\n\n3. 会议记录与摘要：Otter.AI及腾讯会议、飞书会议等工具结合语音识别和自然语言处理技术，实现会议内容的自动转录和摘要生成[5]。\n\n4. 战略规划与管理优化：IBM Watson和华为云EI企业智能通过分析运营数据和行业趋势，为企业提供优化建议和战略规划支持[5]。\n\n这些应用不仅简化了数据分析流程，提高了工作效率，还能帮助企业更好地理解和应对市场变化，做出更加科学合理的决策。",
      "ground_context": [
        "大语言模型在企业管理和决策支持中表现出了很大的使用前景，能够从大量非结构化文本数据中快速提取关键信息，帮助企业科学制定决策。",
        "会议记录与摘要是大语言模型在企业管理中的另一重要应用场景。"
      ],
      "retrieved_contexts": [
        "大语言模型在企业管理和决策支持中表现出了很大的使用前景，能够从大量非结构化文本数\n据中快速提取关键信息， 帮助企业科学制定决策。 例如， 微软的Power BI 已结合大语言模型技术，\n允许用户通过自然语言输入查询，从而快速生成关键业务指标的分析结果。这使得非技术人员也\n能轻松完成复杂的数据分析任务。类似地，国内的阿里云 Quick BI 通过集成智能分析功能，能够\n挖掘出隐藏在复杂数据中的趋势、风险点和改进建议，并以图表或文本的形式输出，为企业提供\n实时的决策支持。这种技术的应用不仅简化了数据分析流程，还提升了分析的效率和精准度，帮\n助企业更快适应市场变化。\n在报告生成方面，大语言模型的应用极大地方便了企业日常运营中的信息处理需求。例如，\nTableau GPT 利用自然语言生成功能， 能够根据输入的业务数据自动生成可视化的分析报告， 包括\n销售趋势图、客户细分报告等，帮助管理者快速掌握业务状况并制定相应的策略。国内的金蝶云\n和用友 U8 等企业管理工具也开始引入大语言模型技术，支持财务报表、预算报告等的自动生成，\n甚至可以根据具体数据生成解释性文字，为用户提供清晰直观的业务洞察。这...",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "大规模语言模型：从理论到实践 455\n资源配置，并发现运营中的瓶颈。这些技术的应用让企业能够在激烈的市场竞争中快速调整方向，\n占据市场优势，同时也为管理层提供了数据驱动的决策依据，显著提升了管理效率和执行效果。\n12.1.9 法律与合规\n大语言模型在合同审查方面展现了极大的应用潜力，能够自动识别合同条款中的潜在法律风\n险， 显著提升审查效率。 例如，Kira Systems 是一款基于大语言模型的合同审查工具， 能够快速分\n析合同内容，标记关键条款，并指出可能存在的问题。这款工具已被众多律师事务所和企业采用，\n用于高效处理大量复杂的商业合同。在国内，类似的工具如“法大大”合同助手，通过大语言模\n型技术，支持对合同条款进行逐条审查，自动识别潜在的法律风险点，如不平等条款或隐藏的违\n约责任，从而帮助律师和企业快速发现问题并优化合同内容。\n在法律业务管理和协作领域，大语言模型通过优化工具帮助企业确保其政策和流程符合法律\n法规。 例如，HighQ 是 Thomson Reuters 推出的一款先进的法律业务管理和协作软件， 具备案件管\n理、合同生命周期管理、法务工作受理、文档自动化以及安全云端...",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "报告生成的效率，还能够减少人工操作中的错误几率，为企业管理者节省时间和精力。\n会议记录与摘要是大语言模型在企业管理中的另一重要应用场景。 例如，Otter.AI 结合语音识\n别和自然语言处理技术，能够实时记录会议内容并生成简洁的摘要，方便参会者快速回顾会议要\n点，或者让未参会人员轻松了解关键内容。在国内，腾讯会议和飞书会议等工具也集成了类似功\n能，支持会议内容的自动转录和要点提取，并且可以生成后续任务清单或行动计划。这种技术不\n仅降低了手动记录的时间成本，还保证了记录内容的完整性和准确性，同时提高了会议的整体效\n率和后续工作的执行力。\n此外，大语言模型在战略规划和管理优化方面也提供了强有力的支持。例如，IBM Watson 可\n以通过分析企业的运营数据和行业趋势， 生成优化建议并协助制定未来的策略规划。 在国内， 华为\n云 EI 企业智能提供了从运营监控到战略规划的全流程支持， 帮助企业识别潜在的市场机会、 优化"
      ],
      "response_time": 5.54011607170105,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.040839993401374404,
        "semantic_similarity": 0.8213360905647278
      }
    },
    {
      "sample_id": "0208",
      "question": "大语言模型与搜索的结合在什么领域展现了巨大潜力？",
      "ground_truth": "电子商务、知识管理、在线教育、医疗健康等领域",
      "predicted_answer": "大语言模型与搜索的结合在电子商务、知识管理、在线教育、医疗健康等领域展现了巨大潜力[1][2]。",
      "ground_context": [
        "这种结合在电子商务、知识管理、在线教育、医疗健康等领域展现了巨大潜力"
      ],
      "retrieved_contexts": [
        "不仅能获得即时的代码生成与优化支持， 还可以利用AI 自动化完成测试、 文档生成和代码重构等\n高难度任务，从而大幅提升软件开发的效率和质量。\n12.1.6 智能搜索与推荐\n大语言模型与搜索的结合是其最重要的应用之一，覆盖了非常广泛的场景。通过强大的语义\n理解和上下文分析能力，大语言模型可以帮助搜索引擎精准捕捉用户意图，提供更相关的搜索结\n果。这种结合在电子商务、知识管理、在线教育、医疗健康等领域展现了巨大潜力，不仅提升了\n搜索的智能化水平，还显著改善了用户体验，成为大模型应用的核心方向之一。\n目前， 几乎全部大语言模型公司推出的在线服务都引入了搜索增强功能， 以提升问答精准度。\n2023 年 10 月之暗面推出 Kimi 智能搜索产品，2024 年，OpenAI 推出了 SearchGPT， 结合大语言模\n型的语义理解与实时搜索能力，为用户提供更精确、即时的查询结果。这种结合在知识问答、技\n术支持和内容生成等场景中表现突出。类似地，微软的 Bing Chat 集成了 OpenAI 的接口，支持\n实时互联网搜索与智能问答，并已被嵌入到 Edge 浏览器和 Microsoft Office...",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "大语言模型还被广泛应用于改进搜索与推荐的多模态能力，即结合文本、图像、音频等多种\n数据类型提供更丰富的结果。例如，Y ouTube利用大语言模型结合视频内容的描述信息和用户观\n看行为，推荐相关视频。用户搜索“如何学习编程”时，模型不仅会推荐编程教学视频，还会根\n据用户的语言偏好、学习进度推荐对应的教程系列。电商平台淘宝、京东等智能搜索引擎同样集\n成了图像搜索功能，用户通过上传图片（如衣服样式）即可获得相似商品推荐。\n大语言模型在智能搜索与推荐中的应用将进一步扩展。 例如，ChatGPT 模型已经被集成到 No-\ntion AI 和 Zapier AI 等工具中，帮助用户快速搜索和推荐相关信息，使知识管理更加高效。此外，\n企业工具如 LinkedIn 的推荐系统借助大语言模型优化了职位推荐和人脉搜索功能，根据用户的职\n业背景和兴趣推荐相关的求职机会或潜在合作伙伴。\n12.1.7 教育与培训\n大语言模型在在线辅导领域有着广泛的应用，通过强大的语义理解和自然语言生成能力，为\n学生提供个性化的学习支持。例如，Khan Academy 推出的虚拟导师 Khanmigo 能够帮助学生解答\n各种学科...",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "入大语言模型后，跨语言检索的精准度和效率显著提升。大语言模型能够深入理解用户提问的语\n义， 即便面对模糊或复杂的问题， 也能准确解析， 并在多语言数据集中找到相关内容。 例如， 科研\n人员在研究人工智能领域时， 可通过中文输入问题， 模型不仅能理解核心要点， 还可以在英文、 法\n文或其他语言撰写的学术论文中精准定位相关信息。这种能力让科研人员能够全面获取全球研究\n成果，掌握前沿动态，进而推动科研项目的顺利开展。Open AI 2025 年 2 月推出的 Deep Research\n则是更一步，基于 o3 模型，专为复杂研究任务设计，能自动搜索、解读、整合海量在线信息，花\n费 5 到 30 分钟生成专业级研究报告。 它具备推理能力， 可自主调整研究方向， 研究结果附带完整\n文档、引用来源和逻辑摘要，适用于金融、科学等领域专业人士及有深度调研需求的消费者。\n在多语言客户服务方面，大语言模型为企业提供了更智能化的服务解决方案。例如，在线旅\n游平台 Booking.com 利用大语言模型驱动的智能客服系统，结合实时翻译技术，为全球用户提供\n个性化、多语言支持。当一位日本游客使用日语在平台上预订..."
      ],
      "response_time": 1.0015978813171387,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.33126155285220377,
        "semantic_similarity": 0.704954981803894
      }
    },
    {
      "sample_id": "0209",
      "question": "如何获取用户选择的文本？",
      "ground_truth": "const selectedText = window.getSelection().toString().trim();",
      "predicted_answer": "可以通过监听 `mouseup` 事件来获取用户选择的文本。当用户释放鼠标按钮时，检查是否有文本被选中。如果有选中的文本，则获取该文本并进行后续处理。[1][3]",
      "ground_context": [
        "const selectedText = window.getSelection().toString().trim();"
      ],
      "retrieved_contexts": [
        "458 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n图 12.2 FisherAI 网页划词翻译展示\n通过如下 JavaScript 脚本可以获取选中区域文本内容， 并开启翻译。 主要核心逻辑是监听鼠标\n点击事件，mouseup 事件监听： 当用户释放鼠标按钮时， 检查是否有文本被选中。 如果有选中， 显\n示按钮并定位到选中文本的位置。",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "大规模语言模型：从理论到实践 459\n//ࡱ\ndocument.addEventListener('mouseup', function (event) {\nconst selection = window.getSelection();\nconst selectedText = selection.toString().trim();\n//࿊ᇏਔ໓Ч\nif (selectedText) {\nconst rects = selection.getRangeAt(0).getClientRects();\nif (rects.length > 0) {\nconst rect = rects[0];\nbutton.style.top = `${rect.bottom + window.scrollY + 10}px`;\nbutton.style.left = `${rect.left + window.scrollX + 10}px`;\nbutton.style.display = 'block';\n}\n} else {\n//Գ\nbutton.style.display = 'none';\n...",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "460 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n//ࡱ\nbutton.addEventListener('click', function () {\nchrome.storage.sync.get([QUICK_TRANS], async function(config) {\ntranslationPopup.innerHTML = '';\nconst selection = window.getSelection();\nconst range = selection.getRangeAt(0);\nconst rects = range.getClientRects();\n....\n//ཁൕሑ෿\ntranslationPopup.style.top = `${topY}px`;\ntranslationPopup.style.left = `${middleX + window.scrollX}px`;\ntranslationPopup.style.display = 'block';\nbutton.style.display = 'none';\nconst selec..."
      ],
      "response_time": 1.2790331840515137,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0.0,
        "bleu": 0.0,
        "semantic_similarity": 0.5791531205177307
      }
    },
    {
      "sample_id": "0210",
      "question": "Otter.AI 的主要功能是什么？",
      "ground_truth": "能够实时记录会议内容并生成简洁的摘要，方便参会者快速回顾会议要点，或者让未参会人员轻松了解关键内容。",
      "predicted_answer": "Otter.AI 主要结合了语音识别和自然语言处理技术，能够实时记录会议内容并生成简洁的摘要，方便参会者快速回顾会议要点，或者让未参会人员轻松了解关键内容。[1]",
      "ground_context": [
        "例如，Otter.AI 结合语音识别和自然语言处理技术，能够实时记录会议内容并生成简洁的摘要，方便参会者快速回顾会议要点，或者让未参会人员轻松了解关键内容。"
      ],
      "retrieved_contexts": [
        "报告生成的效率，还能够减少人工操作中的错误几率，为企业管理者节省时间和精力。\n会议记录与摘要是大语言模型在企业管理中的另一重要应用场景。 例如，Otter.AI 结合语音识\n别和自然语言处理技术，能够实时记录会议内容并生成简洁的摘要，方便参会者快速回顾会议要\n点，或者让未参会人员轻松了解关键内容。在国内，腾讯会议和飞书会议等工具也集成了类似功\n能，支持会议内容的自动转录和要点提取，并且可以生成后续任务清单或行动计划。这种技术不\n仅降低了手动记录的时间成本，还保证了记录内容的完整性和准确性，同时提高了会议的整体效\n率和后续工作的执行力。\n此外，大语言模型在战略规划和管理优化方面也提供了强有力的支持。例如，IBM Watson 可\n以通过分析企业的运营数据和行业趋势， 生成优化建议并协助制定未来的策略规划。 在国内， 华为\n云 EI 企业智能提供了从运营监控到战略规划的全流程支持， 帮助企业识别潜在的市场机会、 优化",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "取信息、跨越语言障碍，并高效处理复杂的学习和工作任务。\nFisherAI 它支持多种大语言模型，包括 ChatGPT、Gemini、DeepSeek、Qwen、Mistral、Groq\n等主流模型，也可以通过 Ollama 调用本地模型，让用户能够根据需求选择最适合的工具。同时，\nFisherAI 允许用户自定义模型配置、API 密钥和代理地址，从而满足个性化和多样化的使用场景。\nFisherAI 还内置丰富的快捷工具。例如，它支持划词翻译、以及通过输入“/”触发快捷功能，包\n括翻译、摘要、等操作。如图12.1所示，通过插件可以对网页内容进行总结和翻译。\n¬ 可以通过 GitHub 搜索 FisherAI 查找项目，并获取源代码",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "得分高于 88% 的应试者，包括美国律师资格考试（ Uniform Bar Exam ） 、法学院入学考试（Law\nSchool Admission Test） 、学术能力评估（Scholastic Assessment Test，SA T）等。GPT-4o 是 OpenAI\n于 2024 年 5 月发布的多模态大模型，其中“o”代表“omni”即“全能” 。它能接受文本、音频和\n图像组合输入并生成文本、 音频和图像的任意组合输出， 可处理50 种语言， 在232 毫秒内对音频"
      ],
      "response_time": 1.3573675155639648,
      "retrieval_metrics": {
        "recall@1": 1.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 1.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.6231896303766875,
        "semantic_similarity": 0.7152904868125916
      }
    },
    {
      "sample_id": "0211",
      "question": "大语言模型如何在创作和信息处理领域同时发挥作用？",
      "ground_truth": "大语言模型在创作领域通过生成多种形式的文本内容，如故事片段、对话场景和歌词，帮助写作者和音乐人提升创作效率。而在信息处理领域，它们与浏览器智能插件结合，提供自动摘要和翻译功能，帮助用户高效获取信息和理解多语言内容。",
      "predicted_answer": "大语言模型在创作和信息处理领域均能发挥重要作用。在创作领域，它们能够自动生成高质量的文章[1]，如新闻报道、博客文章和产品描述等，这有助于提高内容创作者的工作效率。此外，大语言模型还能根据用户提供的提示或情节大纲生成完整的故事[1]，为创意写作提供支持。在信息处理方面，大语言模型可以从新闻报道和财务公告中自动提取关键信息，帮助企业构建实时更新的知识图谱[2]，从而更好地追踪市场动态和行业趋势。这些模型还可以用于其他任务，如生成代码片段以辅助编程工作[3]。因此，大语言模型不仅能够促进内容创作的效率和质量，还能够有效处理和利用大量的信息资源。",
      "ground_context": [
        "例如，Sudowrite 能够根据用户的提示词和需求生成多种形式的文本内容， 并提供包括润色、 摘要、大纲生成等各类能力。",
        "自动摘要可以帮助我们快速提取网页的核心内容， 避免浪费时间在冗长的信息中； 网页翻译能够打破语言障碍， 让我们轻松访问不同语言的内容资源。"
      ],
      "retrieved_contexts": [
        "模型在推动各行业效率提升、 创新发展中的核心作用， 并为未来技术与产业的深度融合提供启示。\n12.1.1 内容创作与生成\n大语言模型在内容创作与生成领域展现出了强大的能力，能够显著提高内容创作的效率与质\n量。在文章写作方面，大语言模型可以自动生成新闻报道、博客文章和产品描述等内容。例如，\nOpenAI 的 ChatGPT 已被多家媒体和企业应用于文章初稿的生成，通过输入简单的主题或关键词，\n即可快速生成结构清晰、语言流畅的文本。这种能力帮助内容创作者节省了大量时间，提高了内\n容发布的效率，尤其适用于需要高频更新的新闻媒体和电商平台。\n在故事创作方面，大语言模型能够根据用户提供的提示或情节大纲生成完整的故事情节，为\n创意写作提供了全新的方式。 许多作家和创意团队使用GPT-4 等模型， 生成故事大纲和角色设定，",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "然语言处理技术从新闻报道、财务公告中自动提取关键信息，为金融企业构建实时更新的知识图\n谱。通过这些系统，企业可以轻松追踪市场动态、行业趋势，并快速构建跨领域的知识图谱。\n未来，大语言模型在信息抽取与知识图谱领域的应用前景非常广阔。随着技术的进步，这些\n应用将更加智能化。企业可以通过这些工具快速构建多语言知识图谱，整合全球范围内的资源和\n信息。 此外， 像LinkedIn 公司的 Economic Graph 这样的知识图谱服务， 也可能进一步结合大语言\n模型的能力，帮助企业和个人更高效地管理职业网络和商业生态。\n12.1.5 代码生成与编程辅助\n大语言模型在辅助编程领域展现了显著的优势， 极大地提升了开发效率。 例如，GitHub Copilot\n是由 OpenAI 和 GitHub 联合推出的一款智能编程助手，能够在开发环境中根据上下文为开发者提",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "1. 绪论\n大语言模型是一种由包含数百亿个及以上参数的深度神经网络构建的语言模型，通常使用自\n监督学习方法通过大量无标注文本进行训练。2018 年以来，Google、OpenAI、Meta、百度、华为\n等公司和研究机构相继发布了 BERT[1]、GPT[2] 等多种模型， 这些模型在几乎所有自然语言处理任\n务中都表现出色。2019 年，大语言模型呈现爆发式的增长，特别是 2022 年 11 月 ChatGPT（Chat\nGenerative Pre-trained Transformer）的发布，引起了全世界的广泛关注。用户可以使用自然语言与\n系统交互，实现问答、分类、摘要、翻译、聊天等从理解到生成的各种任务。大语言模型展现出\n了强大的对世界知识的掌握和对语言的理解能力。\n本章主要介绍大语言模型的基本概念、发展历程和构建流程。\n1.1 大语言模型的基本概念\n使用语言是人类与其他动物最重要的区别之一， 而人类的多种智能也与此密切相关， 逻辑思维\n以语言的形式表达， 大量的知识也以文字的形式记录和传播。 如今， 互联网上已经拥有数万亿个网页\n的资源， 其中大部分信息都是用自然语言描述的。 ..."
      ],
      "response_time": 3.933837890625,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 0.0,
        "recall@5": 0.0,
        "mrr": 0.0
      },
      "answer_quality_metrics": {
        "rouge1": 0.0,
        "rouge2": 0.0,
        "rougeL": 0,
        "bleu": 0.04903067622910853,
        "semantic_similarity": 0.8588675260543823
      }
    },
    {
      "sample_id": "0212",
      "question": "Replika 如何利用大语言模型提供情感支持，同时 Ollama 在本地启动大语言模型的步骤是什么？",
      "ground_truth": "Replika 利用大语言模型学习用户的语言风格和情感反应，提供个性化的对话和情感支持，同时整合情感管理工具，帮助用户缓解压力和焦虑。Ollama 则可以通过命令 'ollama run llama3.2' 在本地启动 Llama 3.2，或使用 'ollama serve' 命令将大语言模型作为后端服务进行使用。",
      "predicted_answer": "Replika 利用大语言模型提供情感支持的方式包括学习用户的语言风格、兴趣爱好和情感反应，从而提供定制化对话体验；通过倾听和同理心回应，帮助缓解压力和焦虑；增强现实（AR）互动，让用户在现实环境中与虚拟形象进行交流；记忆功能则能记住用户的重要信息和喜好，增加互动的连贯性。此外，该应用整合了情感管理工具，为用户提供情绪识别和心理健康建议，以此在娱乐与情感陪伴方面表现出色，同时也为用户提供心理健康支持[4][5]。\n\n要使用 Ollama 在本地启动大语言模型，首先确保已安装 Ollama，然后只需输入相应的命令即可。例如，若要在本地启动 Llama 3.2 模型，可以执行以下命令：\n```\nollama run llama3.2\n``` \n如果想要将大语言模型作为后端服务运行，则可以使用 `ollama serve` 命令来启动服务。具体来说：\n```\n./ollama serve\n``` \n\n启动服务之后，就可以通过 REST API 来调用模型了[1][2]。",
      "ground_context": [
        "Replika 是由美国 Luka 公司开发的一款人工智能聊天机器人应用，致力于为用户提供个性化的对话和情感支持体验。",
        "Ollama 使用非常简单，在安装完成后，如果想在本地启动Llama 3.2，可以直接使用如下命令：ollama run llama3.2。"
      ],
      "retrieved_contexts": [
        "大规模语言模型：从理论到实践 467\n12.3.2 Ollama\nOllama 在基于 llama.cpp 开发是一款本地大语言模型运行工具， 支持macOS、Windows 和 Linux\n系统， 具备简洁的安装和使用流程。 用户无需复杂配置， 只需通过简单命令 （如 “ollama run [模型\n名]” ）即可快速启动和运行模型。Ollama 提供丰富的模型库，包括 Llama2、Mistral、DolphinPhi、\nCode Llama 等， 用户还可以通过modelfile 自定义和微调模型， 以满足特定任务需求。 此外，Ollama\n针对性能进行了优化， 即使在普通电脑上也能高效运行小型模型， 而在配备高性能GPU 的设备上\n则能充分发挥模型的推理能力。\nOllama 还支持多种交互方式，用户既可以通过命令行快速运行模型，也可以选择使用图形用\n户界面 （如Ollama WebUI 和 macOS 原生应用 Ollamac） 进行操作。 在数据隐私方面，Ollama 将模\n型完全本地化运行，数据保留在用户设备上，避免了云端运行可能导致的数据泄露风险，非常适\n合对隐私要求较高...",
        "合对隐私要求较高的用户和场景。\nOllama 使用非常简单， 在安装完成后， 如果想在本地启动Llama 3.2， 可以直接使用如下命令：\nollama run llama3.2\n如果想将大语言模型作为后端服务进行使用， 在不启动桌面应用的情况下， 可以ollama serve\n命令来启动。用如下命令：\n#׮Ollamaༀ\n./ollama serve\n# ᄎྛଆ྘\n./ollama run llama3.2\n服务启动后，Ollama 提供 REST API 对模型进行调用：",
        "ollama run llava \"What's in this image? /Users/jmorgan/Desktop/smile.png\"\n12.3.3 Open WebUI\nOpen WebUI 是一个可扩展、 功能丰富且用户友好的自托管大语言模型平台， 设计完全离线运\n行。 它支持多种大语言模型运行工具， 包括Ollama 以及所有兼容 OpenAI 的 API， 并内置用于RAG\n推理引擎，可以快速构建大语言模型部署解决方案。\n如何 Ollama 已经安装于本机， 可以使用如下命令， 非常方面的通过Docker 部署 Open WebUI：",
        "450 大语言模型：从理论到实践 -- 张奇、桂韬、郑锐、黄萱菁\n当情感陪伴和心理疏导的角色。例如，Replika 是由美国 Luka 公司开发的一款人工智能聊天机器\n人应用，致力于为用户提供个性化的对话和情感支持体验。其功能包括学习用户的语言风格、兴\n趣爱好和情感反应，提供定制化对话体验；通过倾听和同理心回应，帮助缓解压力和焦虑；增强\n现实（AR）互动，让用户在现实环境中与虚拟形象进行交流；记忆功能则能记住用户的重要信息\n和喜好，增加互动的连贯性。此外，Replika 整合了情感管理工具，为用户提供情绪识别和心理健\n康建议，在娱乐与情感陪伴方面表现出色，同时也为用户提供心理健康支持。这种情感支持类的\n对话机器人正在为心理健康服务提供一种低成本、高可达性的解决方案。\n此外，大语言模型驱动的对话系统在医疗、教育等专业领域也展现了巨大的潜力。例如，微\n软推出的“Azure AI Health Bot ”能够解答用户关于常见疾病的疑问，帮助他们初步判断病情并推\n荐适当的医疗资源。在教育领域，Duolingo 等语言学习应用通过大语言模型开发的对话功能，为\n用户提供更自然的互动体验，帮助他们有...",
        "大规模语言模型：从理论到实践 463\n如 Ollama，它基于 llama.cpp，具备简洁的安装和使用流程。此外，考虑到普通用户通常不会直接\n操作控制台界面，还需要开发支持 Web 界面和应用界面的解决方案。 Open Webui 就是一个旨在\n提供类似 ChatGPT 界面的工具，方便用户与模型交互。本地部署大语言模型的整体架构如图12.4\n所示。\nOpen WebUI\n大语言模型库\n各类型硬件\n大语言模型本地部署框架对话界面\n图 12.4 大语言模型本地部署系统结构图\n本节将首先介绍大语言模型本地部署的核心工具 llama.cpp，在此基础上介绍本地部署工具\nOllama，最后介绍大语言模型网页交互工具 Open WebUI。\n12.3.1 llama.cpp\nllama.cpp 是一个用纯 C/C++ 实现的大语言模型 （LLM） 推理项目， 旨在以最小的设置和高性\n能支持 LLaMA 及其他模型的本地运行。该项目的目标是让用户能够在各种硬件（包括本地设备\n和云端）上高效运行大型语言模型，同时优化对资源的使用。llama.cpp 支持多种硬件架构，包括\nApple Silico..."
      ],
      "response_time": 5.772210121154785,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 0.5
      },
      "answer_quality_metrics": {
        "rouge1": 0.6875000000000001,
        "rouge2": 0.5333333333333333,
        "rougeL": 0.5,
        "bleu": 0.12468983861723033,
        "semantic_similarity": 0.935067355632782
      }
    },
    {
      "sample_id": "0213",
      "question": "如何在使用Ollama启动Llama 3.2后，通过REST API实现大语言模型在内容创作中的应用？",
      "ground_truth": "在安装并启动Ollama的Llama 3.2后，可以通过REST API调用模型来实现内容创作。例如，可以使用ollama serve命令启动服务，然后利用生成的API接口，输入主题或关键词，快速生成高质量的文章初稿、新闻报道或创意故事情节。",
      "predicted_answer": "首先，你需要启动Ollama来运行Llama 3.2模型。这可以通过命令行完成，具体命令为：\n```\nollama run llama3.2\n```\n\n接下来，你可以使用Ollama提供的REST API来调用这个模型进行内容创作等应用。虽然文档中未详细说明具体的API接口，但一般情况下，这类API可能允许你发送HTTP请求给服务器，服务器收到请求后利用模型生成文本返回结果。例如，你可以发送POST请求至指定URL，并附带输入文本参数，然后接收返回的内容生成结果。建议查阅Ollama官方文档或GitHub仓库获取更详细的API使用指南。[3]",
      "ground_context": [
        "Ollama 使用非常简单， 在安装完成后， 如果想在本地启动Llama 3.2， 可以直接使用如下命令： ollama run llama3.2",
        "大语言模型在内容创作与生成领域展现出了强大的能力，能够显著提高内容创作的效率与质量。"
      ],
      "retrieved_contexts": [
        "大规模语言模型：从理论到实践 467\n12.3.2 Ollama\nOllama 在基于 llama.cpp 开发是一款本地大语言模型运行工具， 支持macOS、Windows 和 Linux\n系统， 具备简洁的安装和使用流程。 用户无需复杂配置， 只需通过简单命令 （如 “ollama run [模型\n名]” ）即可快速启动和运行模型。Ollama 提供丰富的模型库，包括 Llama2、Mistral、DolphinPhi、\nCode Llama 等， 用户还可以通过modelfile 自定义和微调模型， 以满足特定任务需求。 此外，Ollama\n针对性能进行了优化， 即使在普通电脑上也能高效运行小型模型， 而在配备高性能GPU 的设备上\n则能充分发挥模型的推理能力。\nOllama 还支持多种交互方式，用户既可以通过命令行快速运行模型，也可以选择使用图形用\n户界面 （如Ollama WebUI 和 macOS 原生应用 Ollamac） 进行操作。 在数据隐私方面，Ollama 将模\n型完全本地化运行，数据保留在用户设备上，避免了云端运行可能导致的数据泄露风险，非常适\n合对隐私要求较高...",
        "大语言模型从理论到实践（第二版）\n张奇桂韬郑锐⻩萱菁著\n预览版\n2025 年 3 月 5 日·",
        "合对隐私要求较高的用户和场景。\nOllama 使用非常简单， 在安装完成后， 如果想在本地启动Llama 3.2， 可以直接使用如下命令：\nollama run llama3.2\n如果想将大语言模型作为后端服务进行使用， 在不启动桌面应用的情况下， 可以ollama serve\n命令来启动。用如下命令：\n#׮Ollamaༀ\n./ollama serve\n# ᄎྛଆ྘\n./ollama run llama3.2\n服务启动后，Ollama 提供 REST API 对模型进行调用：",
        "索引 523\n知识推理, 415\n确定性策略, 192\n神经语言模型, 3\n离散动作空间, 192\n积分制得分, 444\n稀疏注意力, 49\n稀疏混合专家模型, 58\n稠密混合专家模型, 58\n符号推理, 416\n策略, 191\n算子内并行, 105\n结构化剪枝, 382\n统计语言模型, 2\n缩放法则, 3\n自动评估, 425\n自注意力, 19\n自监督学习, 3\n视觉语言模型, 233\n角色扮演, 287\n计算图, 100\n训练后量化, 378\n评估指标, 405\n评估者间一致性, 427\n词元分析, 71\n词元吞吐量, 374\n语境学习, 3\n语言模型, 1\n语音语言模型, 237\n请求吞吐量, 374\n软混合专家模型, 58\n输出词元间延迟, 373\n近端策略优化, 178\n连续动作空间, 192\n通用数据, 63\n重复循环, 68\n量化感知训练, 380\n随机性策略, 192\n集合通信, 123\n零冗余优化器, 118\n零样本思维链, 276\n预训练语言模型, 3\n领域数据, 63\n首词元延迟, 373\n高性能计算集群, 120\n麦克尼马尔检验, 429\n黑盒知识蒸馏, 383\n...",
        "大规模语言模型：从理论到实践 463\n如 Ollama，它基于 llama.cpp，具备简洁的安装和使用流程。此外，考虑到普通用户通常不会直接\n操作控制台界面，还需要开发支持 Web 界面和应用界面的解决方案。 Open Webui 就是一个旨在\n提供类似 ChatGPT 界面的工具，方便用户与模型交互。本地部署大语言模型的整体架构如图12.4\n所示。\nOpen WebUI\n大语言模型库\n各类型硬件\n大语言模型本地部署框架对话界面\n图 12.4 大语言模型本地部署系统结构图\n本节将首先介绍大语言模型本地部署的核心工具 llama.cpp，在此基础上介绍本地部署工具\nOllama，最后介绍大语言模型网页交互工具 Open WebUI。\n12.3.1 llama.cpp\nllama.cpp 是一个用纯 C/C++ 实现的大语言模型 （LLM） 推理项目， 旨在以最小的设置和高性\n能支持 LLaMA 及其他模型的本地运行。该项目的目标是让用户能够在各种硬件（包括本地设备\n和云端）上高效运行大型语言模型，同时优化对资源的使用。llama.cpp 支持多种硬件架构，包括\nApple Silico..."
      ],
      "response_time": 3.5104587078094482,
      "retrieval_metrics": {
        "recall@1": 0.0,
        "recall@3": 1.0,
        "recall@5": 1.0,
        "mrr": 0.3333333333333333
      },
      "answer_quality_metrics": {
        "rouge1": 0.5517241379310346,
        "rouge2": 0.2962962962962963,
        "rougeL": 0.5517241379310346,
        "bleu": 0.0469643096006372,
        "semantic_similarity": 0.8730623722076416
      }
    }
  ]
}