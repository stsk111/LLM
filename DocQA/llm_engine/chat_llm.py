"""
DocQA Pro - LLMå¼•æ“
vLLMå°è£…çš„ChatLLMç±»ï¼Œæ”¯æŒæµå¼è¾“å‡º
"""

from typing import Iterator, List, Dict, Any, Optional
import torch
from vllm import LLM, SamplingParams

from config import LLM_MODEL_PATH, LLM_TENSOR_PARALLEL_SIZE, SAMPLING_PARAMS


class ChatLLM:
    """åŸºäºvLLMçš„èŠå¤©LLMå°è£…"""
    
    def __init__(
        self, 
        model_path: str = str(LLM_MODEL_PATH),
        tensor_parallel_size: int = LLM_TENSOR_PARALLEL_SIZE,
        **kwargs
    ):
        """
        åˆå§‹åŒ–vLLMæ¨¡å‹
        
        Args:
            model_path: æ¨¡å‹è·¯å¾„
            tensor_parallel_size: å¼ é‡å¹¶è¡Œå¤§å°
            **kwargs: å…¶ä»–vLLMå‚æ•°
        """
        self.model_path = model_path
        self.tensor_parallel_size = tensor_parallel_size
        self.llm = None
        
        # åˆå¹¶é»˜è®¤å‚æ•°
        self.vllm_kwargs = {
            'tensor_parallel_size': tensor_parallel_size,
            'trust_remote_code': True,
            'dtype': 'float16' if torch.cuda.is_available() else 'float32',
            'gpu_memory_utilization': 0.90,  # ä½¿ç”¨90%çš„GPUå†…å­˜
            'max_model_len': 8192,  # é™åˆ¶æœ€å¤§åºåˆ—é•¿åº¦ï¼ŒèŠ‚çœKV cache
            **kwargs
        }
        
        self._load_model()
    
    def _load_model(self):
        """åŠ è½½vLLMæ¨¡å‹"""
        try:
            print(f"ğŸ”„ åŠ è½½LLMæ¨¡å‹: {self.model_path}")
            print(f"   å¼ é‡å¹¶è¡Œå¤§å°: {self.tensor_parallel_size}")
            
            self.llm = LLM(
                model=self.model_path,
                **self.vllm_kwargs
            )
            
            print("âœ… LLMæ¨¡å‹åŠ è½½æˆåŠŸ")
            
        except Exception as e:
            print(f"âŒ LLMæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise
    
    def _format_messages(
        self, 
        user_message: str, 
        system_prompt: str = "",
        chat_history: List[Dict[str, str]] = None
    ) -> str:
        """
        æ ¼å¼åŒ–æ¶ˆæ¯ä¸ºQwenæ ¼å¼
        
        Args:
            user_message: ç”¨æˆ·æ¶ˆæ¯
            system_prompt: ç³»ç»Ÿæç¤º
            chat_history: èŠå¤©å†å² [{"role": "user", "content": "..."}, ...]
            
        Returns:
            æ ¼å¼åŒ–çš„æç¤ºæ–‡æœ¬
        """
        messages = []
        
        # æ·»åŠ ç³»ç»Ÿæç¤º
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        
        # æ·»åŠ èŠå¤©å†å²
        if chat_history:
            messages.extend(chat_history)
        
        # æ·»åŠ å½“å‰ç”¨æˆ·æ¶ˆæ¯
        messages.append({"role": "user", "content": user_message})
        
        # ä½¿ç”¨Qwenæ ¼å¼åŒ–
        formatted_prompt = ""
        for message in messages:
            role = message["role"]
            content = message["content"]
            
            if role == "system":
                formatted_prompt += f"<|im_start|>system\n{content}<|im_end|>\n"
            elif role == "user":
                formatted_prompt += f"<|im_start|>user\n{content}<|im_end|>\n"
            elif role == "assistant":
                formatted_prompt += f"<|im_start|>assistant\n{content}<|im_end|>\n"
        
        # æ·»åŠ assistantå¼€å§‹æ ‡è®°
        formatted_prompt += "<|im_start|>assistant\n"
        
        return formatted_prompt
    
    def generate(
        self,
        prompt: str,
        system_prompt: str = "",
        chat_history: List[Dict[str, str]] = None,
        sampling_params: Dict[str, Any] = None,
        stream: bool = True
    ) -> Iterator[str]:
        """
        ç”Ÿæˆå›ç­”ï¼ˆæ”¯æŒæµå¼è¾“å‡ºï¼‰
        
        Args:
            prompt: ç”¨æˆ·è¾“å…¥
            system_prompt: ç³»ç»Ÿæç¤º
            chat_history: èŠå¤©å†å²
            sampling_params: é‡‡æ ·å‚æ•°
            stream: æ˜¯å¦æµå¼è¾“å‡º
            
        Yields:
            ç”Ÿæˆçš„æ–‡æœ¬ç‰‡æ®µ
        """
        if not self.llm:
            raise RuntimeError("LLMæ¨¡å‹æœªåŠ è½½")
        
        try:
            # æ ¼å¼åŒ–è¾“å…¥
            formatted_prompt = self._format_messages(
                prompt, system_prompt, chat_history
            )
            
            # è®¾ç½®é‡‡æ ·å‚æ•°
            if sampling_params is None:
                sampling_params = SAMPLING_PARAMS.copy()
            
            sampling_config = SamplingParams(**sampling_params)
            
            if stream:
                # æµå¼ç”Ÿæˆï¼ˆvLLMæš‚ä¸ç›´æ¥æ”¯æŒæµå¼ï¼Œæ¨¡æ‹Ÿå®ç°ï¼‰
                outputs = self.llm.generate(
                    [formatted_prompt], 
                    sampling_config
                )
                
                # è¿”å›å®Œæ•´ç»“æœï¼ˆvLLMé™åˆ¶ï¼‰
                if outputs and outputs[0].outputs:
                    generated_text = outputs[0].outputs[0].text
                    # æ¨¡æ‹Ÿæµå¼è¾“å‡º
                    for i in range(0, len(generated_text), 10):
                        yield generated_text[i:i+10]
                else:
                    yield ""
            else:
                # éæµå¼ç”Ÿæˆ
                outputs = self.llm.generate(
                    [formatted_prompt], 
                    sampling_config
                )
                
                if outputs and outputs[0].outputs:
                    yield outputs[0].outputs[0].text
                else:
                    yield ""
                    
        except Exception as e:
            print(f"âŒ LLMç”Ÿæˆå¤±è´¥: {e}")
            yield f"ç”Ÿæˆå¤±è´¥: {str(e)}"
    
    def generate_simple(
        self,
        prompt: str,
        system_prompt: str = "",
        **kwargs
    ) -> str:
        """
        ç®€å•ç”Ÿæˆï¼ˆéæµå¼ï¼‰
        
        Args:
            prompt: ç”¨æˆ·è¾“å…¥
            system_prompt: ç³»ç»Ÿæç¤º
            **kwargs: é‡‡æ ·å‚æ•°
            
        Returns:
            ç”Ÿæˆçš„å®Œæ•´æ–‡æœ¬
        """
        sampling_params = SAMPLING_PARAMS.copy()
        sampling_params.update(kwargs)
        
        result = ""
        for chunk in self.generate(
            prompt=prompt,
            system_prompt=system_prompt,
            sampling_params=sampling_params,
            stream=False
        ):
            result += chunk
        
        return result.strip()
    
    def batch_generate(
        self,
        prompts: List[str],
        system_prompts: List[str] = None,
        sampling_params: Dict[str, Any] = None
    ) -> List[str]:
        """
        æ‰¹é‡ç”Ÿæˆ
        
        Args:
            prompts: æç¤ºåˆ—è¡¨
            system_prompts: ç³»ç»Ÿæç¤ºåˆ—è¡¨
            sampling_params: é‡‡æ ·å‚æ•°
            
        Returns:
            ç”Ÿæˆç»“æœåˆ—è¡¨
        """
        if not self.llm:
            raise RuntimeError("LLMæ¨¡å‹æœªåŠ è½½")
        
        # æ ¼å¼åŒ–æ‰€æœ‰æç¤º
        formatted_prompts = []
        for i, prompt in enumerate(prompts):
            system_prompt = ""
            if system_prompts and i < len(system_prompts):
                system_prompt = system_prompts[i]
            
            formatted_prompt = self._format_messages(prompt, system_prompt)
            formatted_prompts.append(formatted_prompt)
        
        # è®¾ç½®é‡‡æ ·å‚æ•°
        if sampling_params is None:
            sampling_params = SAMPLING_PARAMS.copy()
        
        sampling_config = SamplingParams(**sampling_params)
        
        # æ‰¹é‡ç”Ÿæˆ
        try:
            outputs = self.llm.generate(formatted_prompts, sampling_config)
            
            results = []
            for output in outputs:
                if output.outputs:
                    results.append(output.outputs[0].text.strip())
                else:
                    results.append("")
            
            return results
            
        except Exception as e:
            print(f"âŒ æ‰¹é‡ç”Ÿæˆå¤±è´¥: {e}")
            return [""] * len(prompts)


def create_chat_llm(model_path: str = None, **kwargs) -> ChatLLM:
    """
    åˆ›å»ºChatLLMå®ä¾‹çš„ä¾¿æ·å‡½æ•°
    
    Args:
        model_path: æ¨¡å‹è·¯å¾„ï¼Œä¸ºNoneæ—¶ä½¿ç”¨é…ç½®ä¸­çš„è·¯å¾„
        **kwargs: å…¶ä»–å‚æ•°
        
    Returns:
        ChatLLMå®ä¾‹
    """
    if model_path is None:
        model_path = str(LLM_MODEL_PATH)
    
    return ChatLLM(model_path=model_path, **kwargs)