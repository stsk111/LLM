"""
DocQA Pro - Gradioç•Œé¢
å·¦å³åˆ†æ å¸ƒå±€ï¼Œæ”¯æŒPDFä¸Šä¼ å’Œæµå¼é—®ç­”
"""

import os
import gradio as gr
from pathlib import Path
from typing import List, Tuple, Optional, Dict, Any, Generator
import tempfile

# å¯¼å…¥æ ¸å¿ƒæ¨¡å—
from core.ingestion import PDFIngestionPipeline
from core.retrieval import create_retrieval_system, HybridRetriever
from core.reranker import create_reranker
from llm_engine.chat_llm import create_chat_llm
from core.qa_chain import create_qa_chain
from core.cache_manager import create_cache_manager

from config import (
    GRADIO_THEME, CHATBOT_HEIGHT, ENABLE_STREAMING,
    RETRIEVAL_TOP_K, RERANK_TOP_N, RERANK_SCORE_THRESHOLD, ENABLE_CACHE
)


class DocQAApp:
    """DocQAåº”ç”¨ä¸»ç±»"""
    
    def __init__(self):
        """åˆå§‹åŒ–åº”ç”¨"""
        self.pdf_pipeline = None
        self.qa_chain = None
        self.current_chunks = []
        self.chat_history = []
        self.processing = False
        
        # å…¨å±€å•ä¾‹æ¨¡å‹ï¼ˆé¿å…é‡å¤åŠ è½½ï¼‰
        self.embedding_engine = None
        self.reranker = None 
        self.chat_llm = None
        self._models_loaded = False
        
        # ç¼“å­˜ç®¡ç†å™¨
        self.cache_manager = create_cache_manager() if ENABLE_CACHE else None
        
        print("ğŸš€ DocQAåº”ç”¨åˆå§‹åŒ–...")
    
    def _update_progress(self, message: str, current: int = 0, total: int = 100):
        """è¿›åº¦æ›´æ–°å›è°ƒ"""
        progress = current / total if total > 0 else 0
        print(f"è¿›åº¦ {current}/{total}: {message}")
    
    def _format_sources_display(self, sources: List[Dict]) -> str:
        """
        æ ¼å¼åŒ–æ¥æºä¿¡æ¯ç”¨äºUIå±•ç¤º
        
        Args:
            sources: æ¥æºä¿¡æ¯åˆ—è¡¨
            
        Returns:
            æ ¼å¼åŒ–çš„æ¥æºæ–‡æœ¬
        """
        if not sources:
            return ""
        
        sources_text = "\n\n---\n**ğŸ“š å‚è€ƒæ¥æº:**\n\n"
        
        for source in sources:
            index = source['index']
            page = source['page']
            score = source['rerank_score']
            content = source.get('content', source.get('content_preview', ''))
            
            # é™åˆ¶æ¯ä¸ªç‰‡æ®µçš„æ˜¾ç¤ºé•¿åº¦ï¼ˆé¿å…è¿‡é•¿ï¼‰
            if len(content) > 300:
                content = content[:300] + "..."
            
            # æ ¼å¼åŒ–å•ä¸ªæ¥æº
            sources_text += f"**[{index}]** ç¬¬{page}é¡µ (ç›¸å…³æ€§: {score:.2f})\n"
            sources_text += f"> {content}\n\n"
        
        return sources_text
    
    def _initialize_models(self):
        """åˆå§‹åŒ–æ¨¡å‹ï¼ˆå…¨å±€å•ä¾‹ï¼Œé¿å…æ˜¾å­˜é‡å¤å ç”¨ï¼‰"""
        if self._models_loaded:
            print("â™»ï¸  å¤ç”¨å·²åŠ è½½çš„æ¨¡å‹")
            return
        
        try:
            print("ğŸ”„ é¦–æ¬¡åŠ è½½æ¨¡å‹...")
            
            # åŠ è½½Rerankeræ¨¡å‹ï¼ˆå•ä¾‹ï¼‰
            if self.reranker is None:
                self.reranker = create_reranker()
            
            # åŠ è½½LLMæ¨¡å‹ï¼ˆå•ä¾‹ï¼‰
            if self.chat_llm is None:
                self.chat_llm = create_chat_llm()
            
            self._models_loaded = True
            print("âœ… æ¨¡å‹åˆå§‹åŒ–å®Œæˆï¼ˆå…¨å±€å•ä¾‹ï¼‰")
            
        except Exception as e:
            error_msg = f"æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {str(e)}"
            print(f"âŒ {error_msg}")
            raise RuntimeError(error_msg)
    
    def process_pdf(self, file) -> Tuple[str, str, gr.update]:
        """
        å¤„ç†ä¸Šä¼ çš„PDFæ–‡ä»¶ï¼ˆæ”¯æŒç¼“å­˜åŠ é€Ÿï¼‰
        
        Args:
            file: ä¸Šä¼ çš„æ–‡ä»¶å¯¹è±¡
            
        Returns:
            (çŠ¶æ€æ¶ˆæ¯, æ–‡æ¡£ä¿¡æ¯, èŠå¤©åŒºåŸŸæ›´æ–°)
        """
        if self.processing:
            return "æ­£åœ¨å¤„ç†ä¸­ï¼Œè¯·ç¨å€™...", "", gr.update()
        
        if file is None:
            return "è¯·é€‰æ‹©PDFæ–‡ä»¶", "", gr.update()
        
        self.processing = True
        from_cache = False
        
        try:
            # è·å–æ–‡ä»¶è·¯å¾„
            file_path = file if isinstance(file, str) else file.name
            
            # åˆå§‹åŒ–æ¨¡å‹ï¼ˆå…¨å±€å•ä¾‹ï¼Œé¿å…é‡å¤åŠ è½½ï¼‰
            self._initialize_models()
            
            # å°è¯•ä»ç¼“å­˜åŠ è½½
            if self.cache_manager and self.cache_manager.cache_exists(file_path):
                print("ğŸ¯ æ£€æµ‹åˆ°ç¼“å­˜ï¼Œå°è¯•å¿«é€ŸåŠ è½½...")
                
                # ç¡®ä¿embedding_engineå·²åˆå§‹åŒ–
                if self.embedding_engine is None:
                    from core.retrieval import EmbeddingEngine
                    self.embedding_engine = EmbeddingEngine()
                
                cache_result = self.cache_manager.load_cache(
                    file_path,
                    self.embedding_engine.embeddings
                )
                
                if cache_result:
                    faiss_index, chunks, metadata = cache_result
                    self.current_chunks = chunks
                    
                    # é‡å»ºBM25ç´¢å¼•ï¼ˆBM25ç´¢å¼•å¾ˆè½»é‡ï¼Œé‡å»ºå¾ˆå¿«ï¼‰
                    from langchain_community.retrievers import BM25Retriever
                    from config import RETRIEVAL_TOP_K
                    sparse_retriever = BM25Retriever.from_documents(chunks)
                    sparse_retriever.k = RETRIEVAL_TOP_K
                    
                    # åˆ›å»ºæ··åˆæ£€ç´¢å™¨
                    hybrid_retriever = HybridRetriever(faiss_index, chunks)
                    
                    # åˆ›å»ºé—®ç­”é“¾ï¼ˆå¤ç”¨å…¨å±€æ¨¡å‹ï¼‰
                    self.qa_chain = create_qa_chain(self.chat_llm, hybrid_retriever, self.reranker)
                    
                    # æ¸…ç©ºèŠå¤©å†å²
                    self.chat_history = []
                    
                    from_cache = True
                    stats = metadata
                    
                    print("âœ… ä»ç¼“å­˜åŠ è½½æˆåŠŸï¼")
            
            # ç¼“å­˜ä¸å­˜åœ¨æˆ–åŠ è½½å¤±è´¥ï¼Œæ­£å¸¸å¤„ç†
            if not from_cache:
                print("ğŸ“„ ç¼“å­˜ä¸å­˜åœ¨ï¼Œå¼€å§‹å¤„ç†PDF...")
                
                # åˆå§‹åŒ–PDFå¤„ç†ç®¡é“
                if self.pdf_pipeline is None:
                    self.pdf_pipeline = PDFIngestionPipeline(
                        progress_callback=self._update_progress
                    )
                
                # å¤„ç†PDF
                result = self.pdf_pipeline.process_pdf(file_path)
                
                if not result["success"]:
                    return f"å¤„ç†å¤±è´¥: {result['error']}", "", gr.update()
                
                # ä¿å­˜æ–‡æ¡£ç‰‡æ®µ
                self.current_chunks = result["chunks"]
                
                # æ„å»ºæ£€ç´¢ç³»ç»Ÿ
                status_msg = "æ„å»ºæ£€ç´¢ç³»ç»Ÿ..."
                print(status_msg)
                
                # åˆ›å»ºæ£€ç´¢ç»„ä»¶ï¼ˆåªé‡å»ºç´¢å¼•ï¼Œå¤ç”¨æ¨¡å‹ï¼‰
                embedding_engine, index_builder, hybrid_retriever = create_retrieval_system(
                    self.current_chunks
                )
                
                # ä¿å­˜embedding_engineä¾›ç¼“å­˜ä½¿ç”¨
                if self.embedding_engine is None:
                    self.embedding_engine = embedding_engine
                
                # åˆ›å»ºé—®ç­”é“¾ï¼ˆå¤ç”¨å…¨å±€æ¨¡å‹ï¼‰
                self.qa_chain = create_qa_chain(self.chat_llm, hybrid_retriever, self.reranker)
                
                # æ¸…ç©ºèŠå¤©å†å²
                self.chat_history = []
                
                stats = result["stats"]
                
                # ä¿å­˜åˆ°ç¼“å­˜
                if self.cache_manager:
                    print("ğŸ’¾ ä¿å­˜åˆ°ç¼“å­˜...")
                    self.cache_manager.save_cache(
                        file_path,
                        index_builder.vector_store,
                        self.current_chunks,
                        stats
                    )
            
            # ç”Ÿæˆæ–‡æ¡£ä¿¡æ¯
            cache_indicator = "âš¡ **ä»ç¼“å­˜åŠ è½½**\n\n" if from_cache else ""
            doc_info = f"""
{cache_indicator}ğŸ“Š **æ–‡æ¡£å¤„ç†å®Œæˆ**

- **æ€»é¡µæ•°**: {stats['total_pages']}
- **æ–‡æœ¬å—æ•°**: {stats['total_chunks']}
- **å¹³å‡æ¯é¡µå—æ•°**: {stats['avg_chunks_per_page']:.1f}
- **å—å¤§å°**: {stats['chunk_size']} tokens
- **å—é‡å **: {stats['chunk_overlap']} tokens

âœ… **ç³»ç»Ÿå°±ç»ªï¼Œå¯ä»¥å¼€å§‹é—®ç­”ï¼**
            """.strip()
            
            status_prefix = "âš¡ ä»ç¼“å­˜åŠ è½½å®Œæˆï¼Œç³»ç»Ÿå°±ç»ª" if from_cache else "âœ… PDFå¤„ç†å®Œæˆï¼Œç³»ç»Ÿå°±ç»ª"
            return status_prefix, doc_info, gr.update(value=[])
            
        except FileNotFoundError:
            return "âŒ æ–‡ä»¶æœªæ‰¾åˆ°ï¼Œè¯·é‡æ–°ä¸Šä¼ PDFæ–‡ä»¶", "", gr.update()
        except ValueError as e:
            return f"âŒ æ–‡ä»¶æ ¼å¼é”™è¯¯: {str(e)}", "", gr.update()
        except RuntimeError as e:
            return f"âŒ ç³»ç»Ÿé”™è¯¯: {str(e)}", "", gr.update()
        except MemoryError:
            return "âŒ å†…å­˜ä¸è¶³ï¼Œè¯·å°è¯•ä¸Šä¼ æ›´å°çš„æ–‡ä»¶æˆ–é‡å¯åº”ç”¨", "", gr.update()
        except Exception as e:
            error_msg = f"âŒ å¤„ç†å¤±è´¥: {str(e)}"
            print(f"âŒ {error_msg}")
            import traceback
            traceback.print_exc()
            return "âŒ æ–‡ä»¶å¤„ç†å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æ ¼å¼å¹¶é‡è¯•", "", gr.update()
        
        finally:
            self.processing = False
    
    def chat_response(
        self, 
        message: str, 
        history: List,
        top_k: int,
        top_n: int,
        threshold: float
    ) -> Tuple[str, List]:
        """
        å¤„ç†èŠå¤©æ¶ˆæ¯ï¼ˆéæµå¼ï¼‰
        
        Args:
            message: ç”¨æˆ·æ¶ˆæ¯
            history: èŠå¤©å†å²ï¼ˆGradio 6.0æ ¼å¼ï¼‰
            top_k: æ£€ç´¢æ•°é‡
            top_n: é‡æ’æ•°é‡
            threshold: åˆ†æ•°é˜ˆå€¼
            
        Returns:
            ("", æ›´æ–°çš„å†å²)
        """
        if not self.qa_chain:
            error_msg = "è¯·å…ˆä¸Šä¼ å¹¶å¤„ç†PDFæ–‡ä»¶"
            history.append({"role": "user", "content": message})
            history.append({"role": "assistant", "content": error_msg})
            return "", history
        
        if not message.strip():
            return "", history
        
        try:
            # è½¬æ¢å†å²æ ¼å¼ - å…¼å®¹Gradio 6.0çš„å­—å…¸æ ¼å¼
            chat_history = []
            for i in range(0, len(history), 2):
                if i + 1 < len(history):
                    user_msg = history[i].get("content", "") if isinstance(history[i], dict) else history[i][0]
                    assistant_msg = history[i+1].get("content", "") if isinstance(history[i+1], dict) else history[i+1][1]
                    chat_history.append((user_msg, assistant_msg))
            
            # æ‰§è¡Œé—®ç­”
            result = self.qa_chain.ask(
                question=message,
                chat_history=chat_history,
                top_n=top_n,
                score_threshold=threshold,
                stream=False
            )
            
            # æ„å»ºå›ç­”
            answer = result.get("answer", "æ— æ³•ç”Ÿæˆå›ç­”")
            sources = result.get("sources", [])
            
            # æ·»åŠ æ¥æºä¿¡æ¯ï¼ˆä½¿ç”¨æ–°çš„æ ¼å¼åŒ–å‡½æ•°ï¼‰
            if sources:
                answer += self._format_sources_display(sources)
            
            # æ›´æ–°å†å² - Gradio 6.0æ ¼å¼
            history.append({"role": "user", "content": message})
            history.append({"role": "assistant", "content": answer})
            
        except MemoryError:
            history.append({"role": "user", "content": message})
            history.append({"role": "assistant", "content": "âŒ æ˜¾å­˜ä¸è¶³ï¼Œè¯·é‡å¯åº”ç”¨æˆ–å°è¯•æ›´ç®€å•çš„é—®é¢˜ã€‚"})
        except TimeoutError:
            history.append({"role": "user", "content": message})
            history.append({"role": "assistant", "content": "âŒ å“åº”è¶…æ—¶ï¼Œè¯·ç¨åé‡è¯•ã€‚"})
        except Exception as e:
            error_msg = f"å›ç­”ç”Ÿæˆå¤±è´¥: {str(e)}"
            print(f"âŒ {error_msg}")
            history.append({"role": "user", "content": message})
            if "model" in str(e).lower() or "cuda" in str(e).lower():
                history.append({"role": "assistant", "content": "âŒ æ¨¡å‹åŠ è½½å¼‚å¸¸ï¼Œè¯·é‡å¯åº”ç”¨ã€‚"})
            else:
                history.append({"role": "assistant", "content": "âŒ æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„é—®é¢˜æ—¶å‡ºç°äº†é”™è¯¯ï¼Œè¯·é‡è¯•ã€‚"})
        
        return "", history
    
    def chat_response_stream(
        self,
        message: str,
        history: List, 
        top_k: int,
        top_n: int,
        threshold: float
    ) -> Generator[Tuple[str, List], None, None]:
        """
        å¤„ç†èŠå¤©æ¶ˆæ¯ï¼ˆæµå¼ï¼‰
        
        Args:
            message: ç”¨æˆ·æ¶ˆæ¯
            history: èŠå¤©å†å²ï¼ˆGradio 6.0æ ¼å¼ï¼‰
            top_k: æ£€ç´¢æ•°é‡
            top_n: é‡æ’æ•°é‡  
            threshold: åˆ†æ•°é˜ˆå€¼
            
        Yields:
            ("", æ›´æ–°çš„å†å²)
        """
        if not self.qa_chain:
            error_msg = "è¯·å…ˆä¸Šä¼ å¹¶å¤„ç†PDFæ–‡ä»¶"
            history.append({"role": "user", "content": message})
            history.append({"role": "assistant", "content": error_msg})
            yield "", history
            return
        
        if not message.strip():
            yield "", history
            return
        
        try:
            # è½¬æ¢å†å²æ ¼å¼ - å…¼å®¹Gradio 6.0
            chat_history = []
            for i in range(0, len(history), 2):
                if i + 1 < len(history):
                    user_msg = history[i].get("content", "") if isinstance(history[i], dict) else history[i][0]
                    assistant_msg = history[i+1].get("content", "") if isinstance(history[i+1], dict) else history[i+1][1]
                    chat_history.append((user_msg, assistant_msg))
            
            # æ‰§è¡Œé—®ç­”ï¼ˆæµå¼ï¼‰
            result = self.qa_chain.ask(
                question=message,
                chat_history=chat_history,
                top_n=top_n,
                score_threshold=threshold,
                stream=True
            )
            
            # åˆå§‹åŒ–å›ç­” - Gradio 6.0æ ¼å¼
            history.append({"role": "user", "content": message})
            history.append({"role": "assistant", "content": ""})
            answer_stream = result.get("answer_stream")
            sources = result.get("sources", [])
            
            if answer_stream:
                # æµå¼æ›´æ–°å›ç­”
                current_answer = ""
                for chunk in answer_stream:
                    current_answer += chunk
                    history[-1]["content"] = current_answer
                    yield "", history
                
                # æ·»åŠ æ¥æºä¿¡æ¯ï¼ˆä½¿ç”¨æ–°çš„æ ¼å¼åŒ–å‡½æ•°ï¼‰
                if sources:
                    sources_text = self._format_sources_display(sources)
                    history[-1]["content"] = current_answer + sources_text
                    yield "", history
            else:
                # éæµå¼å›é€€
                answer = result.get("answer", "æ— æ³•ç”Ÿæˆå›ç­”")
                if sources:
                    answer += self._format_sources_display(sources)
                
                history[-1]["content"] = answer
                yield "", history
                
        except MemoryError:
            history.append({"role": "user", "content": message})
            history.append({"role": "assistant", "content": "âŒ æ˜¾å­˜ä¸è¶³ï¼Œè¯·é‡å¯åº”ç”¨æˆ–å°è¯•æ›´ç®€å•çš„é—®é¢˜ã€‚"})
            yield "", history
        except TimeoutError:
            history.append({"role": "user", "content": message})
            history.append({"role": "assistant", "content": "âŒ å“åº”è¶…æ—¶ï¼Œè¯·ç¨åé‡è¯•ã€‚"})
            yield "", history
        except Exception as e:
            error_msg = f"å›ç­”ç”Ÿæˆå¤±è´¥: {str(e)}"
            print(f"âŒ {error_msg}")
            history.append({"role": "user", "content": message})
            if "model" in str(e).lower() or "cuda" in str(e).lower():
                history.append({"role": "assistant", "content": "âŒ æ¨¡å‹åŠ è½½å¼‚å¸¸ï¼Œè¯·é‡å¯åº”ç”¨ã€‚"})
            else:
                history.append({"role": "assistant", "content": "âŒ æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„é—®é¢˜æ—¶å‡ºç°äº†é”™è¯¯ï¼Œè¯·é‡è¯•ã€‚"})
            yield "", history
    
    def clear_chat(self):
        """æ¸…ç©ºèŠå¤©å†å²"""
        self.chat_history = []
        return []
    
    def build_interface(self) -> gr.Blocks:
        """æ„å»ºGradioç•Œé¢"""
        with gr.Blocks(
            title="DocQA Pro - æ™ºèƒ½æ–‡æ¡£é—®ç­”åŠ©æ‰‹"
        ) as demo:
            
            gr.Markdown("# ğŸ¤– DocQA Pro - æ™ºèƒ½æ–‡æ¡£é—®ç­”åŠ©æ‰‹")
            gr.Markdown("åŸºäºRAGæŠ€æœ¯çš„æœ¬åœ°æ–‡æ¡£é—®ç­”ç³»ç»Ÿï¼Œæ”¯æŒPDFä¸Šä¼ å’Œæ™ºèƒ½é—®ç­”")
            
            with gr.Row():
                # å·¦ä¾§æ  - æ§åˆ¶é¢æ¿
                with gr.Column(scale=1):
                    gr.Markdown("## ğŸ“ æ–‡æ¡£ä¸Šä¼ ")
                    
                    # æ–‡ä»¶ä¸Šä¼ 
                    file_upload = gr.File(
                        label="é€‰æ‹©PDFæ–‡ä»¶",
                        file_types=[".pdf"],
                        type="filepath"
                    )
                    
                    process_btn = gr.Button("ğŸ“Š å¤„ç†æ–‡æ¡£", variant="primary")
                    
                    # å¤„ç†çŠ¶æ€
                    status_box = gr.Textbox(
                        label="å¤„ç†çŠ¶æ€",
                        value="ç­‰å¾…ä¸Šä¼ PDFæ–‡ä»¶...",
                        interactive=False,
                        lines=2
                    )
                    
                    # æ–‡æ¡£ä¿¡æ¯
                    doc_info = gr.Markdown(
                        value="",
                        label="æ–‡æ¡£ä¿¡æ¯"
                    )
                    
                    gr.Markdown("## âš™ï¸ å‚æ•°è®¾ç½®")
                    
                    # å‚æ•°æ§åˆ¶
                    top_k = gr.Slider(
                        minimum=1, maximum=20, value=RETRIEVAL_TOP_K,
                        step=1, label="æ£€ç´¢æ•°é‡ (Top K)"
                    )
                    
                    top_n = gr.Slider(
                        minimum=1, maximum=10, value=RERANK_TOP_N,
                        step=1, label="é‡æ’æ•°é‡ (Top N)"
                    )
                    
                    threshold = gr.Slider(
                        minimum=0.0, maximum=1.0, value=RERANK_SCORE_THRESHOLD,
                        step=0.1, label="ç›¸å…³æ€§é˜ˆå€¼"
                    )
                    
                    # æ¸…é™¤æŒ‰é’®
                    clear_btn = gr.Button("ğŸ—‘ï¸ æ¸…ç©ºå¯¹è¯", variant="secondary")
                
                # å³ä¾§æ  - å¯¹è¯åŒºåŸŸ  
                with gr.Column(scale=2):
                    gr.Markdown("## ğŸ’¬ æ™ºèƒ½é—®ç­”")
                    
                    # èŠå¤©ç•Œé¢
                    chatbot = gr.Chatbot(
                        height=CHATBOT_HEIGHT,
                        label="å¯¹è¯å†å²",
                        show_label=False
                    )
                    
                    # è¾“å…¥æ¡†
                    msg = gr.Textbox(
                        label="è¾“å…¥æ‚¨çš„é—®é¢˜",
                        placeholder="è¯·è¾“å…¥å…³äºæ–‡æ¡£çš„é—®é¢˜...",
                        lines=2
                    )
                    
                    with gr.Row():
                        submit_btn = gr.Button("å‘é€", variant="primary")
                        gr.Button("ç¤ºä¾‹é—®é¢˜", variant="secondary", visible=False)
            
            # äº‹ä»¶ç»‘å®š
            process_btn.click(
                fn=self.process_pdf,
                inputs=[file_upload],
                outputs=[status_box, doc_info, chatbot]
            )
            
            # é€‰æ‹©æµå¼æˆ–éæµå¼
            if ENABLE_STREAMING:
                msg.submit(
                    fn=self.chat_response_stream,
                    inputs=[msg, chatbot, top_k, top_n, threshold],
                    outputs=[msg, chatbot]
                )
                submit_btn.click(
                    fn=self.chat_response_stream,
                    inputs=[msg, chatbot, top_k, top_n, threshold],
                    outputs=[msg, chatbot]
                )
            else:
                msg.submit(
                    fn=self.chat_response,
                    inputs=[msg, chatbot, top_k, top_n, threshold],
                    outputs=[msg, chatbot]
                )
                submit_btn.click(
                    fn=self.chat_response,
                    inputs=[msg, chatbot, top_k, top_n, threshold],
                    outputs=[msg, chatbot]
                )
            
            clear_btn.click(
                fn=self.clear_chat,
                outputs=[chatbot]
            )
        
        return demo
    
    def launch(self, **kwargs):
        """å¯åŠ¨åº”ç”¨"""
        demo = self.build_interface()
        # å°†themeå‚æ•°ä¼ é€’ç»™launchæ–¹æ³•ï¼ˆGradio 6.0ï¼‰
        launch_kwargs = {
            'server_name': kwargs.get('server_name', '0.0.0.0'),
            'server_port': kwargs.get('server_port', 7860),
            'share': kwargs.get('share', False),
            'debug': kwargs.get('debug', False)
        }
        demo.launch(**launch_kwargs)


def main():
    """ä¸»å‡½æ•°"""
    app = DocQAApp()
    
    print("ğŸš€ å¯åŠ¨DocQA Proåº”ç”¨...")
    app.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=False,
        debug=False
    )


if __name__ == "__main__":
    main()